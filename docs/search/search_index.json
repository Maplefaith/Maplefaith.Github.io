{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Keep in low-key wind","text":"<p>Here you can catch a glimpse of notes of courses at ZJU from a student majoring in CSE.</p> <p>Also, I share some of my experience in Blog.</p> <p>Wish you a good reading time.</p> <ul> <li> <p> \u5fc3\u8a00\u5fc3\u8bed</p> <ul> <li>May Day Holiday</li> </ul> </li> </ul> <ul> <li> <p> \u7a7a\u8bf4\u65e0\u51ed</p> <ul> <li>Data Structure &amp; Algorithm</li> <li>Numerical Analysis</li> <li>Ordinary Differential Equation</li> </ul> </li> </ul>"},{"location":"friends/","title":"Welcome to Join my Friend Zone!","text":"Maythics Little Mouse SyncrnzdClk Medium Cat Little_Whale Great Whale LastingWind Giant in Stu'Union"},{"location":"Ctrl/","title":"Control","text":"<p>This is a part for control theory &amp; technology.</p>"},{"location":"Ctrl/#mathematical-base-ordinary-differential-equation","title":"Mathematical Base: Ordinary Differential Equation","text":"<p>To be continued...</p>"},{"location":"Ctrl/#modern-control-theory","title":"Modern Control Theory","text":"<p>completed.</p>"},{"location":"Ctrl/#process-control","title":"Process Control","text":"<p>completed.</p>"},{"location":"Ctrl/#sensing-detection","title":"Sensing &amp; Detection","text":"<p>Maybe will only present the midterm exam.</p>"},{"location":"Ctrl/Modern_Control_Theory/MCT/","title":"Modern Control Theory","text":"<p>I show the outline of the course for the coming exam.</p> {\"url\": \"../MCT_review.pdf\"}"},{"location":"Ctrl/Ordinary_Differential_Equation/","title":"Ordinary Differential Equation","text":"<p>Reference</p> <ul> <li>\u300a\u5e38\u5fae\u5206\u65b9\u7a0b\u300b \u67f3\u5f6c</li> <li>\u300a\u5e38\u5fae\u5206\u65b9\u7a0b\u300b \u65b9\u9053\u5143</li> </ul>"},{"location":"Ctrl/Ordinary_Differential_Equation/#preliminary","title":"\u9884\u5907\u77e5\u8bc6 | Preliminary","text":""},{"location":"Ctrl/Ordinary_Differential_Equation/#elementary-integration-method","title":"\u521d\u7b49\u79ef\u5206\u6cd5 | Elementary Integration Method","text":""},{"location":"Ctrl/Ordinary_Differential_Equation/#system-of-linear-differential-equations-lodes","title":"\u7ebf\u6027\u5fae\u5206\u65b9\u7a0b\u7ec4 | System of Linear Differential Equations (LODEs)","text":""},{"location":"Ctrl/Ordinary_Differential_Equation/#general-theory-of-ode","title":"\u5fae\u5206\u65b9\u7a0b\u7684\u4e00\u822c\u7406\u8bba | General Theory of ODE","text":""},{"location":"Ctrl/Ordinary_Differential_Equation/#existence-and-uniqueness-theorem","title":"\u5b58\u5728\u552f\u4e00\u6027\u5b9a\u7406 | Existence and Uniqueness Theorem","text":""},{"location":"Ctrl/Ordinary_Differential_Equation/#contraction-mapping-method","title":"\u538b\u7f29\u6620\u5c04\u6cd5 | Contraction Mapping Method","text":""},{"location":"Ctrl/Ordinary_Differential_Equation/#method-of-power-series","title":"\u5e42\u7ea7\u6570\u89e3\u6cd5 | Method of Power Series","text":""},{"location":"Ctrl/Ordinary_Differential_Equation/#continuous-dependence-of-solution-on-initial-value","title":"\u89e3\u5bf9\u521d\u503c\u7684\u8fde\u7eed\u4f9d\u8d56\u6027 | Continuous Dependence of Solution on Initial Value","text":""},{"location":"Ctrl/Ordinary_Differential_Equation/#stability-theory-of-ode","title":"\u5fae\u5206\u65b9\u7a0b\u7684\u7a33\u5b9a\u6027\u7406\u8bba | Stability Theory of ODE","text":""},{"location":"Ctrl/Ordinary_Differential_Equation/#qualitative-theory-of-ode","title":"\u5fae\u5206\u65b9\u7a0b\u7684\u5b9a\u6027\u7406\u8bba | Qualitative Theory of ODE","text":""},{"location":"Ctrl/Ordinary_Differential_Equation/#first-order-pratial-differential-equation","title":"\u4e00\u9636\u504f\u5fae\u5206\u65b9\u7a0b | First Order Pratial Differential Equation","text":""},{"location":"Ctrl/Ordinary_Differential_Equation/EIM/","title":"\u521d\u7b49\u79ef\u5206\u6cd5 | Elementary Integration Method","text":"<p>This chapter gives primary method of solving special differential functions, which plays a great role in future study.</p> <p>Outline</p> <ul> <li> <p>\u4e00\u9636\u7ebf\u6027\u5fae\u5206\u65b9\u7a0b, \u9f50\u6b21\u4e0e\u975e\u9f50\u6b21</p> </li> <li> <p>\u53d8\u91cf\u5206\u79bb\u65b9\u7a0b</p> </li> <li> <p>\u9f50\u6b21\u65b9\u7a0b, \u5316\u7b80\u9f50\u6b21\u7684\u65b9\u6cd5</p> </li> <li> <p>\u5168\u5fae\u5206\u65b9\u7a0b\u3001\u79ef\u5206\u56e0\u5b50\u3001\u5206\u7ec4\u6c42\u79ef\u5206\u56e0\u5b50</p> </li> <li> <p>Bernouli \u65b9\u7a0b\u3001Riccati \u65b9\u7a0b</p> </li> <li> <p>\u9690\u5f0f\u5fae\u5206\u65b9\u7a0b, \u53ef\u89e3\u51fax, \u53ef\u89e3\u51fay, \u53cc\u66f2\u51fd\u6570\u53c2\u6570\u6cd5</p> </li> </ul>"},{"location":"Ctrl/Ordinary_Differential_Equation/EIM/#exact-equation","title":"\u6070\u5f53\u65b9\u7a0b | Exact Equation","text":"<p>We focus on the symmetrical form</p> \\[ \\begin{equation} M(x,y)dx + N(x,y)dy = 0 \\label{eq-exact} \\end{equation} \\] <p>This can bring us great convenience for digging into one-order ODE because it can gives us both the relation \\(y=f(x)\\) or \\(x=g(y)\\).</p> <p>\u5168\u5fae\u5206\u65b9\u7a0b\u3001\u6070\u5f53\u65b9\u7a0b\u7684\u5b9a\u4e49 | Definition of Exact Equation</p> <p>If there exists a \\(\\mathit{\\varphi}(x, y) \\in C^{1}(D)\\) such that </p> \\[ d\\mathit{\\varphi}(x, y) = M(x,y)dx + N(x,y)dy \\] <p>then equation \\(\\ref{eq-exact}\\) is called Exact Equation.</p> <p>There are some questions to answer:</p> <ul> <li>How to judge an equation to be exact Equation?</li> <li>If so, how to find original function \\(\\varphi(x, y)\\)?</li> <li>If not, how to transform it into one exact Equation?</li> </ul> <p>In this pattern, we answer the first two equation and leave the third one after learning LFODE.</p> <p>\u65b9\u7a0b\u662f\u6070\u5f53\u7684\u5145\u8981\u6761\u4ef6 | Necessary and Sufficient Condition for exact Equation</p> <p>Assume \\(D\\) is a simply connected region, and \\(M(x, y)\\), \\(N(x, y) \\in C(D)\\) with \\(\\frac{\\partial M}{\\partial y}\\) and \\(\\frac{\\partial N}{\\partial x} \\in C^{1}(D)\\). Then equation \\(\\ref{eq-exact}\\) is exact Equation if and only if</p> \\[ \\frac{\\partial M}{\\partial y} = \\frac{\\partial N}{\\partial x} \\] <p>Prove it.</p> Hints <p>\\(\\Rightarrow\\) is easy, by using second-order mixed partial derivatives of \\(\\mathit{\\varphi}\\).</p> <p>\\(\\Leftarrow\\). Using Green Formula/Theorem. </p> \\[ \\begin{align*} &amp;\\frac{\\partial P}{\\partial y} = \\frac{\\partial Q}{\\partial x} \\\\ \\Leftrightarrow \\ &amp;\\int_{\\gamma}P(x, y)dx+Q(x,y)dy = 0 \\quad \\forall\\text{closed loop } \\gamma\\\\ \\Leftrightarrow \\ &amp;\\int_{\\gamma}P(x, y)dx+Q(x,y)dy = C \\quad \\forall\\text{curve } \\gamma \\text{ connecting } (x_0, y_0), (x, y) \\\\ \\Leftrightarrow \\ &amp;\\exists \\mathit{\\varphi}(x,y) \\in C^{1}(D) \\text{ s.t } d\\mathit{\\varphi}(x, y) = P(x, y)dx+Q(x,y)dy \\end{align*} \\]"},{"location":"Ctrl/Ordinary_Differential_Equation/EIM/#integral-factor","title":"\u79ef\u5206\u56e0\u5b50 | Integral Factor","text":"<p>This part we hope to find \\(\\mu(x, y)\\) so when we multiply it to both sides of equation \\(\\ref{eq-exact}\\) and we get</p> \\[ \\mu(x,y)M(x,y)dx + \\mu(x,y)N(x,y)dy = 0  \\] <p>such that there exists \\(\\mathit{\\varphi}(x, y)\\) which satisfies</p> \\[ d\\mathit{\\varphi}(x,y) = \\mu(x,y)M(x,y)dx + \\mu(x,y)N(x,y)dy  \\] <p>Naively, if \\(\\mathit{\\varphi}(x,y)\\in C^2\\), then </p> \\[ \\frac{\\partial (\\mu M)}{\\partial y} = \\frac{\\partial^2 \\mathit{\\varphi}}{\\partial x\\partial y}=\\frac{\\partial (\\mu N)}{\\partial x}  \\] <p>Theoretically speaking, we have to solve a PDE</p> \\[ \\begin{equation} M(x,y)\\frac{\\partial \\mu}{\\partial y} - N(x,y)\\frac{\\partial \\mu}{\\partial x} = \\left(\\frac{\\partial N}{\\partial x}-\\frac{\\partial M}{\\partial y}\\right)\\mu(x,y) \\label{eq-pde} \\end{equation} \\] <p>However, actually, it is very hard to solve the above PDE. So we focus on some special case like \\(\\mu(x,y)=\\mu(x)\\), \\(\\mu(y)\\), \\(\\mu(x+y)\\), \\(\\mu(xy)\\).</p> <p>Now we have the following theorem to judge whether we can get the above form of integral factors.</p> <p>\u65b9\u7a0b\u6709\u7279\u6b8a\u7c7b\u578b\u7684\u79ef\u5206\u56e0\u5b50\u7684\u5145\u8981\u6761\u4ef6 | Necessary and Sufficient Condition of special integral factor of ODE</p> <p>(i) Equation \\(\\ref{eq-pde}\\) has solution \\(\\mu(x)\\) depending only on \\(x\\), if and only if</p> \\[ \\frac{\\frac{\\partial M}{\\partial y}-\\frac{\\partial N}{\\partial x}}{M} \\overset{\\Delta}{=} G(x) \\] <p>only depends only on \\(x\\). Then </p> \\[ \\mu(x) = e^{\\int_{x_0}^{x}G(t)dt} \\] <p>(ii) Similarly, Equation \\(\\ref{eq-pde}\\) has solution \\(\\mu(y)\\) depending only on \\(y\\), if and only if</p> \\[ \\frac{\\frac{\\partial N}{\\partial x}-\\frac{\\partial M}{\\partial y}}{N} \\overset{\\Delta}{=} G(y) \\] <p>only depends only on \\(y\\). Then </p> \\[ \\mu(y) = e^{\\int_{y_0}^{y}G(t)dt} \\] <p>(iii) More generally, equation \\(\\ref{eq-pde}\\) has solution \\(\\mu(\\varphi(x,y))\\), if and only if</p> \\[ \\frac{\\frac{\\partial N}{\\partial x}-\\frac{\\partial M}{\\partial y}}{N\\frac{\\partial \\varphi}{\\partial x}-M\\frac{\\partial \\varphi}{\\partial y}} \\overset{\\Delta}{=} f(\\varphi(x,y)) \\]"},{"location":"Ctrl/Ordinary_Differential_Equation/EIM/#variable-separation-equation","title":"\u53d8\u91cf\u5206\u79bb\u65b9\u7a0b | Variable Separation Equation","text":"<p>This chapter we discuss how to solve equation when it is not Exact Equation. The basic idea is, through transformation, we can convert an equation into an exact Equation.</p> <p>\u53d8\u91cf\u5206\u79bb\u65b9\u7a0b\u7684\u5b9a\u4e49 | Definition of Variable Separation Equation</p> <p>If there exists \\(M_1(x), M_2(y), N_1(x), N_2(y) \\in C^1(D)\\) such that </p> \\[ M(x, y) =M_1(x)M_2(y), N(x, y) = N_1(x), N_2(y) \\] <p>then we call equation \\(\\ref{eq-exact}\\) Variable Separation Equation.</p> <p>For this type of equation, we can multiply both sides </p> \\[ \\begin{equation} \\frac{1}{M_2(y)N_1(x)} \\label{eq-sep-factor} \\end{equation} \\] <p>equation \\(\\ref{eq-exact}\\) becomes</p> \\[ \\frac{M_1(x)}{N_1(x)}dx + \\frac{M_2(y)}{N_2(y)}dy = 0 \\] <p>This is an exact equation, and \\(\\ref{eq-sep-factor}\\) is called an Integral Factor of the equation.</p> <p>we can get its integral</p> \\[ \\int_{x_0}^{x}\\frac{M_1(t)}{N_1(t)}dt + \\int_{y_0}^{y}\\frac{M_2(s)}{N_2(s)}ds = c \\] <p>which is easily seen a solution of the original equation.</p> <p>And don't forget that if there exists \\(a_i (i= 1,2,\\cdots, m)\\) such that \\(N_1(a_i) = 0\\), or exists \\(b_j (j=1,2,\\cdots, n)\\) such that \\(M_2(b_j) = 0\\), then of course \\(x = a_i, y = b_j\\) are also solutions of the original solution.</p> <ul> <li>\u9f50\u6b21\u65b9\u7a0b | Homogeneous Equation</li> </ul> <p>The following equation can also be transferred into Variable Separation Equation.</p> <p>\u9f50\u6b21\u65b9\u7a0b\u7684\u5b9a\u4e49 | Definition of Homogeneous Equation</p> <p>We call \\(f(x, y)\\) Homogeneous Function of degree \\(n\\) if</p> \\[ f(tx, ty) = t^n f(x, y) \\] <p>and call equation \\(\\ref{eq-exact}\\) Homogeneous equation if \\(M(x, y), N(x, y)\\) are Homogeneous Function.</p> <p>When we let \\(y = u x\\), then \\(dy = xdu + udx\\), substitute in the equation and get</p> \\[ \\begin{align*} M(x, ux)dx+N(x,ux)(xdu+udx)&amp;=0 \\\\ \\Leftrightarrow [M(x, ux)+N(x,ux)u]dx+N(x,ux)xdu&amp;=0  \\end{align*} \\] <p>extract \\(x\\) out by definition of Homogeneous Equation:</p> \\[ x^n[M(1, u)+N(1,u)u]dx+x^{n+1}N(1,u)du=0  \\] <p>If \\(x^{n+1}[M(1, u)+N(1,u)u]\\neq 0\\), then we divide both sides by this and get</p> \\[ \\frac{1}{x}dx + \\frac{N(1,u)}{M(1,u)+uN(1,u)}du=0 \\] <p>which is also Variable Separation Equation.</p>"},{"location":"Ctrl/Ordinary_Differential_Equation/EIM/#linear-first-order-differential-equation","title":"\u4e00\u9636\u7ebf\u6027\u5fae\u5206\u65b9\u7a0b | Linear First-Order Differential Equation","text":"<p>Now we focus on a really important expression of ODE: Linear First-Order Differential Equation(LFODE):</p> \\[ \\begin{equation} \\frac{dy}{dx} + p(x)y = q(x) \\label{eq-LFODE} \\end{equation} \\] <ul> <li>Homogeneous LFODE(H-LFODE)</li> </ul> <p>We let \\(q(x)\\equiv 0\\) in \\(\\ref{eq-LFODE}\\), we get</p> \\[ \\begin{equation} \\frac{dy}{dx} + p(x)y = 0 \\label{eq-H-LFODE} \\end{equation} \\] <p>rewrite it into symmetrical form:</p> \\[ p(x)ydx + dy = 0  \\] <p>which is Variable Separation Equation.</p> <p>So when \\(y\\neq 0\\), multiply both sides \\(1/y\\) and integrate</p> \\[ \\ln{|y|} + \\int_{x_0}^{x}p(t)dt = C \\] <p>get \\(y\\) out of form \\(x\\):</p> \\[ \\begin{align} y &amp;= \\pm e^{C}\\cdot e^{-\\int_{x_0}^{x}p(t)dt} \\nonumber \\\\ &amp;= C_1\\cdot e^{-\\int_{x_0}^{x}p(t)dt}  \\end{align} \\] <p>where \\(C_1=\\pm e^{C} \\neq 0\\), but we can include trivial solution \\(y \\equiv 0\\) by letting \\(C_1 = 0\\).</p> <ul> <li>Non-Homogeneous linear First-Order Differential Equation</li> </ul> <p>we have two ways to get the answer.</p> Version 1Version 2 <p>Making use of Integral Factors.</p> <p>To begin with, we convert equation \\(\\ref{eq-LFODE}\\) into symmetrical form:</p> \\[ \\begin{equation} (p(x)y-q(x))dx+dy=0 \\label{eq-SYM-LFODE} \\end{equation}  \\] <p>Multiply \\(e^{\\int_{x_0}^{x}p(t)dt}\\) to both sides of equation \\(\\ref{eq-SYM-LFODE}\\):</p> \\[ d\\left(\\ e^{\\int_{x_0}^{x}p(t)dt} y \\right) - e^{\\int_{x_0}^{x}p(t)dt} q(x)dx = 0 \\] <p>That is,</p> \\[ d\\left(\\ e^{\\int_{x_0}^{x}p(t)dt} y- \\int_{x_0}^{x}e^{\\int_{x_0}^{s}p(t)dt} q(x)ds \\right)  = 0 \\] <p>integrate and get</p> \\[ e^{\\int_{x_0}^{x}p(t)dt} y - \\int_{x_0}^{s}e^{\\int_{x_0}^{x}p(t)dt} q(s)ds + C = 0 \\] <p>extract \\(y\\) out and get:</p> \\[ y = e^{-\\int_{x_0}^{x}p(t)dt} \\left( C + \\int_{x_0}^{s}e^{\\int_{x_0}^{x}p(t)dt} q(s)ds \\right) \\] <p>Through Variation of Constants.</p> <p>We make a brave treatment: assume one special solution to equation \\(\\ref{eq-LFODE}\\) is </p> \\[ y = u\\cdot e^{-\\int_{x_0}^{x}p(t)dt} \\] <p>where \\(u\\) is a new variable.</p> <p>Subsititute in equation \\(\\ref{eq-LFODE}\\) and get</p> \\[ \\left(p(x) u e^{-\\int_{x_0}^{x}p(t)dt} -q(x) \\right) dx - u p(x) e^{-\\int_{x_0}^{x}p(t)dt} dx + e^{-\\int_{x_0}^{x}p(t)dt} du = 0 \\] <p>That is</p> \\[ -q(x) dx + e^{-\\int_{x_0}^{x}p(t)dt} du = 0 \\] <p>multiply \\(e^{\\int_{x_0}^{x}p(t)dt}\\) to both sides and  integrate </p> \\[ u =  \\int_{x_0}^{x}e^{\\int_{x_0}^{s}p(t)dt}q(s)ds \\] <p>So the special solution is</p> \\[ y =  e^{-\\int_{x_0}^{x}p(t)dt}\\cdot \\int_{x_0}^{x}e^{\\int_{x_0}^{s}p(t)dt}q(s)ds \\]"},{"location":"Ctrl/Ordinary_Differential_Equation/EIM/#first-order-implicit-differential-equation","title":"\u4e00\u9636\u9690\u5f0f\u5fae\u5206\u65b9\u7a0b | First-order Implicit Differential Equation","text":"<p>Now we focus on equation</p> \\[ \\begin{equation} F(x,y, y') = 0 \\label{eq-para} \\end{equation} \\] <p>where \\(y'\\) cannot be explicitly solved out.</p> <ul> <li>\u53c2\u6570\u6cd5 | parametric method</li> </ul> <p>If we let \\(p = y'\\), then equation</p> \\[ \\begin{equation} F(x,y,p) = 0 \\label{eq-para-p} \\end{equation} \\] <p>represents a curved surface in 3-D space.</p> <p>And we can juggle equation \\(\\ref{eq-para-p}\\) and \\(dy=pdx\\) to get a curve in the space.</p>"},{"location":"Ctrl/Ordinary_Differential_Equation/EIM/#singular-solution","title":"\u5947\u89e3 | Singular Solution","text":"<p>Definition of Singular Solution</p> <p>Assume \\(y=\\phi(x)\\) defined on region \\(J\\) is a special solution of ODE</p> \\[ F(x,y,y')=0 \\] <p>its integral curve is </p> \\[ \\varGamma=\\{(x,y): y=\\phi(x),x\\in J\\} \\] <p>If \\(\\forall M\\in \\varGamma\\), there exists \\(\\delta&gt;0\\),  such that a solution \\(\\psi(x)\\) different from \\(\\phi(x)\\) which is tangent to \\(\\phi(x)\\) at \\(M\\) in \\(O_\\delta(M)\\). Then we call \\(y=\\phi(x)\\) is a singular solution of the above ODE.</p> <p>In every point of \\(y-\\phi(x)\\), there exists another solution. That is, uniqueness is broken.</p> <p>Here we gives the necessary condition for the existence of singular solution.</p> <p>necessary condition for the existence of singular solution</p> <p>Assume \\(F(x,y,p)\\in C(G)\\), where \\(G\\subset \\mathbb{R}^3\\), and has continuous partial derivatives with respect to \\(y\\) and \\(p\\), i.e. \\(F_y'(x,y,p)\\) and \\(F_p'(x,y,p)\\). If \\(y=\\phi(x)(x\\in J)\\) is a singular solution of the ODE, and </p> \\[ (x,\\phi(x),\\phi'(x))\\in G,\\quad x\\in J \\] <p>then \\(y=\\phi(x)\\) satisfies</p> \\[ F(x,y,p)=0,\\quad F'_p(x,y,p)=0. \\] Proof <p>The first equation is easy because of singular solution is also a solution.</p> <p>We have to prove the second equation by contradiction.</p> <p>Then there exists \\((x_0,y_0,p_0)\\in G\\), such that we can use Implicite function theorem.</p> <p>Using the two equations are called p-test equation, we can get a formula of singular solution, but we still have to test whether it is the solution of ODE and whether it is singular solution. Here we give another theorem.</p> <p>Theorem for singular solution</p> <p>Assume \\(F(x,y,p)\\in C^2(G)\\), and \\(y=\\phi(x)\\) obtained from p-test equation is a solution of ODE. If </p> \\[ \\begin{align} &amp;F_y(x,\\phi(x),\\phi'(x))\\neq 0 \\label{F-y}\\\\ &amp;F_{pp}(x,\\phi(x),\\phi'(x))\\neq 0\\label{F-pp}\\\\ \\nonumber &amp;F_p(x,\\phi(x),\\phi'(x))=0 \\end{align} \\] <p>then \\(y=\\phi(x)\\) is the singular solution of ODE.</p> <p>Note: if equation \\(\\ref{F-y}\\) and \\(\\ref{F-pp}\\) does not satisfies, we still could not ensure whether \\(\\y=phi(x)\\) is a singular solution.</p>"},{"location":"Ctrl/Ordinary_Differential_Equation/FstPDE/","title":"First Order Pratial Differential Equation","text":"<p>We will prove that we can use First Integral Method of ODEs to solve Quasi-linear First order PDE (\u4e00\u9636\u62df\u7ebf\u6027\u504f\u5fae\u5206\u65b9\u7a0b).</p>"},{"location":"Ctrl/Ordinary_Differential_Equation/FstPDE/#first-integral-method","title":"First Integral Method","text":"<p>Definition of First Intergal</p> <p>Consider ODEs</p> \\[ \\begin{equation} \\frac{d y_j}{x}=f_{j}(x,y_1,\\cdots,y_n),\\quad j=1,2\\cdots, n\\label{ODEs} \\end{equation} \\] <p>where \\(f_j(x,y_1,\\cdots,y_n) (j=1,2,\\cdots,n)\\in C(D)\\), with \\(D\\subset \\mathbb{R}^{n+1}\\), and are continuously differentiable with respect to \\(y_1,y_2,\\cdots, y_n\\). </p> <p>Assume \\(V=V(x,y_1,\\cdots,y_n)\\in C^1(G)\\), where \\(G\\subset D\\). If \\(V\\) is not a constant function, but constant when following an arbitrary integral curve of the above ODEs \\(\\ref{ODEs}\\), i.e.</p> \\[ V(x,y_1(x),\\cdots,y_n(x))\\equiv c_0,\\quad x\\in J \\] <p>then we call \\(V(x,y_1,\\cdots,y_2)=c\\) is a First Integral of ODEs \\(\\ref{ODEs}\\).</p>"},{"location":"Ctrl/Ordinary_Differential_Equation/FstPDE/#property-of-first-integral","title":"Property of First Integral","text":"<p>Property of First Integral (From curve to Points)</p> <p>Assume non-constant function \\(\\varPhi(x,y_1,\\cdots,y_n)\\in C^1(G)\\), then </p> \\[ \\varPhi(x,y_1,\\cdots,y_n) = c \\] <p>is a first integral of ODEs \\(\\ref{ODEs}\\), iff</p> \\[ \\begin{equation} \\frac{\\partial \\varPhi}{\\partial x} + \\frac{\\partial \\varPhi}{\\partial y_1}f_1+\\cdots + \\frac{\\partial \\varPhi}{\\partial y_n}f_n =0, \\quad \\forall (x,y_1,\\cdots, y_n)\\in G\\label{Property} \\end{equation} \\] Proof <p>\"\\(\\Rightarrow\\)\". Assume \\(\\varPhi(x,y_1,\\cdots,y_n)\\) is a first integral of ODEs \\(\\ref{ODEs}\\), then \\(\\forall (x_0,y_{10},\\cdots,y_{n0})\\in G\\), which an integral curve passes is denoted as</p> \\[ y_1(x),y_2(x),\\quad y_n(x),\\forall x\\in J \\] <p>we have </p> \\[ \\varPhi(x,y_1(x),\\cdots,y_n(x))=c \\] <p>take partial derivatives from both sides and let \\(x=x_0\\), we have</p> \\[ \\frac{\\partial \\varPhi}{\\partial x} + \\frac{\\partial \\varPhi}{\\partial y_1}f_1+\\cdots + \\frac{\\partial \\varPhi}{\\partial y_n}f_n =0 \\] <p>which holds at point \\((x_0,y_{10},\\cdots,y_{n0})\\). But due to \\((x_0,y_{10},\\cdots,y_{n0})\\) is arbitrary, so the above equation holds forall points in \\(G\\).</p> <p>\"\\(\\Leftarrow\\)\". Assume equation \\(\\ref{Property}\\) holds, then it holds for arbitrary integral curve, i.e. denote \\(\\Gamma\\in G\\) </p> \\[ \\Gamma: y_1=\\phi_1(x),y_2=\\phi_2(x),cdots,y_n=\\phi_n(x),\\quad x\\in J \\] <p>and equation \\(\\ref{Property}\\) holds for \\(\\Gamma\\). That is, </p> \\[ \\frac{\\partial \\varPhi}{\\partial x} + \\frac{\\partial \\varPhi}{\\partial y_1}f_1+\\cdots + \\frac{\\partial \\varPhi}{\\partial y_n}f_n =0, \\quad \\forall (x,\\phi_1,\\cdots, \\phi_n)\\in G  \\] <p>which means</p> \\[ \\frac{d}{dx}\\varPhi(x,\\phi_1(x),\\cdots, \\phi_n(x))=0 \\] <p>So integrate we have</p> \\[ \\varPhi(x,\\phi_1(x),\\cdots,\\phi_n(x))=c_0, \\quad x\\in J. \\] <p>If we have a first integral for an ODEs, we can reduce the order for that ODEs. It is obvious if we can solve one variable \\(y_i\\) out and substitute it into ODEs \\(\\ref{ODEs}\\), then total number of variables are reduced to \\(n-1\\).</p> <p>Naively, if we have enough number of first integrals, we can solve total ODEs out. But we have to be careful, because some first integral are of the same type, or to be more specific, if a first integral could be expressed by another one, than we call them relevant.</p> <p>Independence of First Integrals</p> <p>We call \\(n\\) first Integrals in region \\(G\\)</p> \\[ V_j(x,y_1,\\cdots,y_n)=c_j,\\quad j=1,1\\cdots,n. \\] <p>are independent, if </p> \\[ \\text{det}\\frac{\\partial (V_1,V_2,\\cdots,V_n)}{\\partial(y_1,y_2,\\cdots,y_n)}\\neq 0,\\quad \\forall(x,y_1,\\cdots,y_n)\\in G. \\] <p>Then we have the following theorem.</p> <p>Solution by First Integral Method</p> <p>If</p> \\[ V_j(x,y_1,\\cdots,y_n)=c_j,\\quad j=1,2\\cdots,n \\] <p>are \\(n\\) independent first integrals in region \\(G\\) for ODEs \\(\\ref{ODEs}\\). Then the solution to ODEs \\(\\ref{ODEs}\\) is a function </p> \\[ y_j=\\phi_j(x,c_1,c_2,\\cdots,c_n),\\quad j=1,2,\\cdots,n \\] <p>specified by these \\(n\\) first integrals. And it expresses all the solution of ODEs \\(\\ref{ODEs}\\) on region \\(G\\).</p> Proof <p>(i) Cause \\(V_1,V_2,\\cdots,V_n\\) are independent, so </p> \\[ \\text{det}\\frac{\\partial (V_1,V_2,\\cdots,V_n)}{\\partial(y_1,y_2,\\cdots,y_n)}\\neq 0,\\quad \\forall(x,y_1,\\cdots,y_n)\\in G. \\] <p>By theorem of implicit function, from </p> \\[ V_j(x,y_1,\\cdots,y_n)=c_j,\\quad j=1,2\\cdots,n \\] <p>solve </p> \\[ y_j=\\phi_j(x,c_1,\\cdots,c_n),\\quad j=1,2\\cdots,n \\] <p>(ii) Then we have to prove \\(\\phi_j (j=1,2,\\cdots,n)\\) are solution to original ODEs \\(\\ref{ODEs}\\).</p> <p>Notice that</p> \\[ \\begin{equation} V_j(x,\\phi_1(x,c_1,\\cdots,c_n),\\cdots,\\phi_n(x,c_1,\\cdots,c_n))\\equiv c_j,\\quad j=1,2\\cdots,n\\label{eq3} \\end{equation} \\] <p>take derivative with respect to \\(x\\) on both sides, we have</p> \\[ \\begin{equation} \\frac{\\partial V_j}{\\partial x}+\\frac{\\partial V_j}{\\partial y_1}\\phi_1'+\\cdots+\\frac{\\partial V_j}{\\partial y_n}\\phi_n'=0,\\quad j=1,2\\cdots,n\\label{eq1} \\end{equation} \\] <p>by Property of First Integral, we also have</p> \\[ \\begin{equation} \\frac{\\partial V_j}{\\partial x} + \\frac{\\partial V_j}{\\partial y_1}f_1+\\cdots + \\frac{\\partial V_j}{\\partial y_n}f_n =0, \\quad \\forall (x,y_1,\\cdots, y_n)\\in G \\label{eq2} \\end{equation} \\] <p>So subtract equation \\(\\ref{eq1}\\) to equation \\(\\ref{eq2}\\) and get</p> \\[ \\frac{\\partial V_j}{\\partial y_1}(\\phi_1'-f_1)+\\cdots+\\frac{\\partial V_j}{\\partial y_n}(\\phi_n'-f_n)=0,\\quad j=1,2\\cdots,n \\] <p>If we define </p> \\[ A=\\frac{\\partial (V_1,V_2,\\cdots,V_n)}{\\partial(y_1,y_2,\\cdots,y_n)} \\] \\[ \\pmb{y}=[(\\phi_1'-f_1),(\\phi_2'-f_2),\\cdots,(\\phi_n'-f_n)]^T \\] <p>then we have</p> \\[ A\\pmb{y}=\\pmb{0} \\] <p>Because \\(\\text{det}A\\neq 0\\), so it follows </p> \\[ \\pmb{y}=\\pmb{0} \\] <p>which means</p> \\[ \\phi_j=f_j,\\quad j=1,2,\\cdots,n. \\] <p>The above equation is exactly original ODEs. So \\(\\phi_j (j=1,2,\\cdots,n)\\) are solution to ODEs \\(\\ref{ODEs}\\).</p> <p>(iii) Next, we have to prove one part of the last statement: \\(\\phi_j (j=1,2,\\cdots,n)\\) are general solution of ODEs \\(\\ref{ODEs}\\). Initially, these \\(n\\) solutions are independent, i.e.</p> \\[ \\text{det}\\frac{\\partial (\\phi_1,\\phi_2,\\cdots,\\phi_n)}{\\partial(c_1,c_2,\\cdots,c_n)}\\neq 0 \\] <p>Actually, from equation \\(\\ref{eq3}\\), we take partial derivatives with respect to \\(c_k\\), we have</p> \\[ \\frac{\\partial V_j}{\\partial y_1}\\frac{\\partial \\phi_1}{\\partial c_k}+\\cdots+\\frac{\\partial V_j}{\\partial y_n}\\frac{\\partial \\phi_n}{\\partial c_k}=\\delta_{jk}=\\begin{cases}0,\\quad j\\neq k\\\\ 1,\\quad j=k.\\end{cases},\\quad j,k=1,2,\\cdots,n \\] <p>which means one matrices times another equals to \\(E\\), i.e.</p> \\[ \\text{det}\\frac{\\partial (\\phi_1,\\phi_2,\\cdots,\\phi_n)}{\\partial(c_1,c_2,\\cdots,c_n)}=\\left[\\text{det}\\frac{\\partial (V_1,V_2,\\cdots,V_n)}{\\partial(y_1,y_2,\\cdots,y_n)}\\right]^{-1}\\neq 0 \\] <p>(iv) Lastly these solutions could express all other solutions. That is, \\(\\forall y_1=z_1(x),y_2=z_2(x),\\cdots,y_n=z_n(x)\\) are solution to ODEs \\(\\ref{ODEs}\\), could we express them with \\(\\phi_1,\\phi_2,\\cdots,\\phi_n\\)?</p> <p>If we denote \\(y_1^0=z_1(x_0),\\cdots,y_n^0=z_n(x_0)\\), and from this point we could determine</p> \\[ c_1^0=V_1(x_0,y_1^0,\\cdots,y_n^0), \\cdots,c_n^0=V_n(x_0,y_1^0,\\cdots,y_n^0) \\] <p>Solve </p> \\[ V_j(x,y_1,\\cdots,y_n)=c_j^0,\\quad j=1,2,\\cdots,n \\] <p>out with \\(n\\) answers</p> \\[ \\phi^0_j(x,c_1^0,\\cdots,c_n^0),\\quad j=1,2\\cdots,n \\] <p>which follows the whole above proof property, i.e. it is solution to ODEs \\(\\ref{ODEs}\\) and satisfies initial condition </p> \\[ y_j^0=\\phi^0_j(x_0,c_1^0,\\cdots,c_n^0),\\quad j=1,2\\cdots,n \\] <p>Due to Uniqueness of Solution, we have </p> \\[ z_j(x)\\equiv phi_j(x),\\quad j=1,2\\cdots,n \\] <p>which means \\(z_j\\) could be expressed by \\(\\phi_j\\).</p>"},{"location":"Ctrl/Ordinary_Differential_Equation/FstPDE/#existence-of-first-integral","title":"Existence of First Integral","text":"<p>Practically speaking, it is really hard to find a first integral of ODEs \\(\\ref{ODEs}\\). Here we prove that in a sufficiently wide condition, first integral exists (locally).</p> <p>Existence of First Integral</p> <p>Assume \\(P_0(x_0,y_1^0,y_2^0,\\cdots, y_n^0)\\in G\\), then there exists a neighborhood of \\(P_0\\), denoted as \\(G_1\\subset G\\), such that ODEs \\(\\ref{ODEs}\\) has \\(n\\) independent first integrals in \\(G_1\\).</p> Proof <p>ni</p>"},{"location":"Ctrl/Ordinary_Differential_Equation/FstPDE/#homogeneous-first-order-pde","title":"Homogeneous first order PDE","text":"<p>The form of homogeneoud first order PDE can be </p> \\[ \\sum_{j=1}^n A_j(x_1,x_2,\\cdots,x_n)\\frac{\\partial u}{\\partial x_j}=0 \\] <p>where \\(u=u(x_1,x_2,\\cdots,x_n)\\) is unknown function, coefficient function </p> \\[ A_j(x_1,x_2,\\cdots,x_n)\\in C^1,\\quad j=1,2,\\cdots,n,\\quad \\forall(x_1,x_2,\\cdots, n)\\in D \\] <p>and </p> \\[ \\sum_{j=1}^n|A_j(x_1,x_2,\\cdots,x_n)|&gt;0. \\]"},{"location":"Ctrl/Ordinary_Differential_Equation/FstPDE/#quasi-linear-first-order-pde","title":"Quasi-linear first order PDE","text":""},{"location":"Ctrl/Ordinary_Differential_Equation/LODEs/","title":"\u7ebf\u6027\u5fae\u5206\u65b9\u7a0b\u7ec4 | System of Linear Differential Equations (LODEs)","text":"<p>This chapter we focus on </p> \\[ \\begin{equation} \\frac{d \\symbfit{X}(t)}{dt} = \\symbfit{A}(t)\\symbfit{X}(t)+\\symbfit{B}(t) \\label{eq-LODEs} \\end{equation} \\] <p>with initial condition</p> \\[ \\begin{equation} \\symbfit{X}(t_0)=\\symbfit{X}_0 \\label{eq-initial-LODEs} \\end{equation} \\] <p>where \\(t_0\\in I=(a,b)\\), \\(\\symbfit{X}_0 = (x_1^0,x_n^0,\\cdots, x_n^0)^T\\) is a given constant vector.</p> <p>Outline</p> <ul> <li> <p>\u5e38\u7cfb\u6570\u7ebf\u6027\u65b9\u7a0b\u7ec4\u6c42\u89e3: \u5355\u7279\u5f81\u6839 (\u5b9e\u3001\u590d), \u91cd\u7279\u5f81\u6839 (\u5b9e\u3001\u590d), \u6c42\u901a\u89e3, \u7279\u89e3, \u5e26\u521d\u503c\u7684\u89e3</p> </li> <li> <p>\u9ad8\u9636\u5fae\u5206\u65b9\u7a0b: \u7279\u5f81\u6839\u6cd5</p> </li> <li> <p>\u6c42\u7279\u89e3: \u5f85\u5b9a\u7cfb\u6570\u6cd5, \u5e38\u6570\u53d8\u6613\u6cd5</p> </li> <li> <p>Euler \u65b9\u7a0b, \u4ee3\u5165 \\(y=x^r\\) \u5f97\u5230 \\(r\\) \u7684\u591a\u9879\u5f0f\uff0c\u9636\u6570\u524d\u9762\u5373\u4e3a\u7cfb\u6570\u3002</p> </li> <li> <p>\u5e42\u7ea7\u6570\u89e3\u6cd5</p> </li> </ul>"},{"location":"Ctrl/Ordinary_Differential_Equation/LODEs/#existence-and-uniqueness-of-lodes","title":"\u7ebf\u6027\u5fae\u5206\u65b9\u7a0b\u7ec4\u89e3\u7684\u5b58\u5728\u552f\u4e00\u6027 | Existence and Uniqueness of LODEs","text":"<p>This is quite similar to Picard Theorem in chapter Existence and Uniqueness Theorem, but it is still useful to give a special form of Picard Sequence for LODEs, which is also an approximation to solving it.</p> <p>\u7ebf\u6027\u5fae\u5206\u65b9\u7a0b\u7ec4\u89e3\u7684\u5b58\u5728\u552f\u4e00\u6027\u5b9a\u7406 | Theorem of Existence and Uniqueness of LODEs</p> <p>LODEs \\(\\ref{eq-LODEs}\\) with initial condition \\(\\ref{eq-initial-LODEs}\\) has only one solution on interval \\(I\\).</p> <p>Prove it.</p> Hints <p>We have to measure the distance in matrix. Now we need to give a definition of norm of vectors and matrixes(to see more details in Norm of vectors and matrixes) in Numerical Analysis.</p> \\[ \\Vert\\mathbfit{X} \\Vert = \\sum_{i=1}^{n}|x_i|, \\quad \\Vert\\mathbfit{A} \\Vert = \\sum_{i=1}^{n}\\sum_{j=1}^{n}|{a_{ij}}| \\] <p>It is easy to see that ...</p> <ul> <li>convert LODEs into its equivalent integral equations.</li> </ul> \\[ \\mathbfit{X}(t) = \\mathbfit{X}_0 + \\int_{t_0}^{t}\\left[ \\mathbfit{A}(s)\\mathbfit{X}(s)+\\mathbfit{B}(s)\\right] ds \\] <ul> <li>formulate Picard Sequence.</li> </ul> <p>Define:</p> \\[ \\begin{align} \\mathbfit{X}_0(t) &amp;= \\mathbfit{X}_0 \\nonumber\\\\ \\mathbfit{X}_1(t) &amp;= \\mathbfit{X}_0 + \\int_{t_0}^{t}\\left[ \\mathbfit{A}(s)\\mathbfit{X}_0(s)+\\mathbfit{B}(s)\\right] ds \\nonumber\\\\ \\mathbfit{X}_2(t) &amp;= \\mathbfit{X}_0 + \\int_{t_0}^{t}\\left[ \\mathbfit{A}(s)\\mathbfit{X}_1(s)+\\mathbfit{B}(s)\\right] ds \\nonumber\\\\ &amp;\\vdots \\nonumber\\\\ \\mathbfit{X}_n(t) &amp;= \\mathbfit{X}_0 + \\int_{t_0}^{t}\\left[ \\mathbfit{A}(s)\\mathbfit{X}_{n-1}(s)+\\mathbfit{B}(s)\\right] ds \\label{eq-LODEs-integral}\\\\ \\end{align} \\] <p>Consider similarly and we can say the above sequence is well-defined.</p> <ul> <li>Prove Picard Sequence convergent.</li> </ul> <p>denote</p> \\[ C = \\sup_{s\\in J}\\Vert\\mathbfit{A}(s)\\Vert, \\quad D = C\\Vert\\mathbfit{X}(s)\\Vert + \\sup_{s\\in J}\\Vert\\mathbfit{B}(s)\\Vert \\] <p>we can get </p> \\[ \\begin{align*} \\Vert \\mathbfit{X}_1(t) - \\mathbfit{X}_0(t) \\Vert &amp;\\leq D |t-t_0|\\\\ \\Vert\\mathbfit{X}_2(t) - \\mathbfit{X}_1(t) \\Vert &amp;\\leq \\int_{t_0}^{t} \\Vert \\mathbfit{A}(s) \\Vert \\Vert \\mathbfit{X}_{1}(s)- \\mathbfit{X}_{0}(s) \\Vert ds \\\\ &amp;\\leq \\int_{t_0}^{t} C D |s-t_0| ds = \\frac{D}{C} \\frac{(C|t-t_0|)^2}{2}\\\\ &amp;\\vdots\\\\ \\Vert\\mathbfit{X}_n(t) - \\mathbfit{X}_{n-1}(t) \\Vert &amp;\\leq \\frac{D}{C} \\frac{(C|t-t_0|)^{n}}{(n)!} \\end{align*} \\] <p>which shows the Picard Sequence converges.</p> <ul> <li>Prove the convergent function is solution of LODEs \\(\\ref{eq-LODEs}\\).</li> </ul> <p>If we denote \\(\\mathbfit{X}(t) = \\lim_{n\\rightarrow \\infty}\\mathbfit{X}_n(t)\\) and let \\(n\\rightarrow \\infty\\) on both sides of integral equation \\(\\ref{eq-LODEs-integral}\\), we get </p> \\[ \\mathbfit{X}(t) = \\mathbfit{X}_0 + \\int_{t_0}^{t}\\left[ \\mathbfit{A}(s)\\mathbfit{X}(s)+\\mathbfit{B}(s)\\right] ds \\] <p>which is a solution.</p> <ul> <li>prove uniqueness.</li> </ul> <p>Similar to proof in Picard Theorem.</p>"},{"location":"Ctrl/Ordinary_Differential_Equation/LODEs/#boundary-problem-of-second-order-lode","title":"\u4e8c\u9636\u65b9\u7a0b\u8fb9\u503c\u95ee\u9898 | Boundary Problem of Second-Order LODE","text":"<p>This pattern we focus on LODE</p> \\[ \\begin{equation} y''+p(x)'+q(x)y = f(x) \\label{eq: BP-SecondOrder} \\end{equation} \\] <p>with Boundary Condition \\(y(a)=\\alpha, y(b)=\\beta\\), where \\(p(x), q(x) \\in C^1[a, b]\\)</p> <ul> <li>H-LODE</li> </ul> <p>\u5171\u8f6d\u70b9 | Conjugate Point</p> <p>If homogeneous LODE(H-LODE)</p> \\[ \\begin{equation} y''+p(x)'+q(x)y = 0 \\label{eq: BP-SO-H} \\end{equation} \\] <p>with boundary condition \\(y(a)=0, y(b)=0\\), has non-zero solution, then \\(\\{a, b\\}\\) is called the Conjugate Point of the H-LODE.</p> <p>We usually take use of the following method to check if the boundary point will induce indefinite solutions or no solutions.</p> <p>\u5171\u8f6d\u70b9\u7684\u5145\u8981\u6761\u4ef6 | Necessary and Sufficient Condition for Conjugate Point</p> <p>\\(\\{a, b\\}\\) is the Conjugate Point of H-LODE, if and only if \\(\\forall y_1, y_2\\) of the solution of H-LODE, which are linear irrelevant, satisfies</p> \\[ \\left| \\begin{array}{cc} y_1(a)&amp; y_2(a)\\\\ y_1(b)&amp; y_2(b) \\end{array} \\right| =0 \\] Hints <p>substitute the boundary condition and we get two linear equation system for parameters \\(c_1, c_2\\). And the above is the determinant of the system.</p> <p>\u9f50\u6b21\u65b9\u7a0b\u5b58\u5728\u552f\u4e00\u89e3\u7684\u5145\u8981\u6761\u4ef6 | Necessary and Sufficient Condition for existing only one solution for H-LODE</p> <p>H-LODE \\(\\ref{eq: BP-SO-H}\\) with boundary point \\(y(a)=\\alpha, y(b)=\\beta\\) has only one solution, if and only if \\(\\{a, b\\}\\) is not the conjugate point of the H-LODE.</p> Hints <p>Focus on the determinant of the linear irelevantly solutions of \\(y_1, y_2\\).</p> <ul> <li>Non-H-LODE</li> </ul> <p>We partition the solution of LODE \\(\\ref{eq: BP-SecondOrder}\\) into three parts.</p> <p>\u7ebf\u6027\u975e\u9f50\u6b21\u65b9\u7a0b\u5b58\u5728\u552f\u4e00\u89e3\u7684\u5145\u5206\u6761\u4ef6 | Sufficient Condition for existing only one solution for Non-H-LODE</p> <p>If \\(\\{a, b\\}\\) is not the conjugate point of H-LODE \\(\\ref{eq: BP-SO-H}\\), then Non-H-LODE \\(\\ref{eq: BP-SecondOrder}\\) has only one solution.</p> <p>There are two ways to prove it.</p> Version 1Version 2 <p>Assume \\(y_1(x)\\) is a solution of H-LODE </p> \\[ \\begin{equation} y'' + p(x)y' +q(x)y = 0, \\quad y(a) = 0, \\quad y(b) = 1 \\label{eq: BP-1} \\end{equation} \\] <p>and \\(y_2(x)\\) is a solution of H-LODE</p> \\[ \\begin{equation} y'' + p(x)y' +q(x)y = 0, \\quad y(a) = 1, \\quad y(b) = 0 \\label{eq: BP-2} \\end{equation} \\] <p>and \\(y_3(x)\\) is a solution of Non-H-LODE</p> \\[ \\begin{equation} y'' + p(x)y' +q(x)y = f(x), \\quad y(a) = 0, \\quad y(b) = 0 \\label{eq: BP-3} \\end{equation} \\] <p>and the solution of LODE \\(\\ref{eq: BP-SecondOrder}\\) can be represented as</p> \\[ y = \\alpha y_1 + \\beta y_2 + y_3 \\] <p>where \\(y_1, y_2\\) are linearly irrelevant because of  existence and uniqueness theorem.</p> <p>We can get \\(y_3\\) through Variation of Constant. That is, let \\(y_3 = u_1(x) y_1(x)+u_2(x)y_2(x)\\), then </p> \\[ u_1 = \\int \\frac{-y_2 f}{W}dt, \\quad u_2 = \\int \\frac{y_1 f}{W}dt \\] <p>Substitute the boundary condition \\(\\ref{eq: BP-1}, \\ref{eq: BP-2}, \\ref{eq: BP-3}\\) we get </p> \\[ u_2(a)=0, \\quad u_1(b)=0 \\] <p>Based on this, we can transform the \\(u_1(x), u_2(x)\\) to definite integral whose upper limit of integral is variable</p> \\[ u_1 = \\int_{x}^{b} \\frac{y_2 f}{W}dt, \\quad u_2 = \\int_{x}^{a} \\frac{y_1 f}{W}dt \\] <p>So we can write particular solution </p> \\[ y_3 = y_1(x) \\int_{x}^{b} \\frac{y_2(t) f(t)}{W(t)}dt  + y_2(x)\\int_{x}^{a} \\frac{y_1(x) f(x)}{W(x)}dt \\] <p>If we define Green Function as</p> \\[ G(x,t)= \\begin{cases} \\displaystyle \\frac{y_2(x)y_1(t)}{W(t)}, \\quad a\\leq t\\leq x  \\\\ \\displaystyle \\frac{y_1(x)y_2(t)}{W(t)}, \\quad x\\leq t\\leq b  \\end{cases} \\] <p>then </p> \\[ y_3(x) = \\int_{a}^{b}G(x, t)f(t)dt \\] <p>According to the textbook.</p>"},{"location":"Ctrl/Ordinary_Differential_Equation/LODEs/#s-l-sturm-liouville-boundary-problem","title":"S-L \u8fb9\u503c\u95ee\u9898 | Sturm-Liouville Boundary Problem","text":"<p>Solve for PDE</p> \\[ \\begin{cases} \\displaystyle u_{tt} = a^2 u_{xx} , \\quad 0\\leq x\\leq L, t\\geq 0\\\\ \\displaystyle u|_{x=0} = u|_{t=0} = 0 \\end{cases} \\] <p>Assume we can seperate the variables \\(x, t\\). That is, let \\(u = X(x)T(t)\\) and substitute, we get </p> \\[ \\begin{align*} X(x)T''(t)&amp;=a^2X''(x)T(t)\\\\ \\Rightarrow \\frac{T''(t)}{a^2T(t)} = \\frac{X''(x)}{X(x)} &amp;\\overset{\\Delta}{=}constant =-\\lambda \\end{align*} \\] <p>Thus, we have to find \\(T(t), X(x)\\) such that </p> \\[ T''(t)+a^2\\lambda T(t)= 0 \\] \\[ \\begin{cases} X''(x) + \\lambda X(x)= 0 \\\\ X(0)=X(L)=0 \\end{cases} \\]"},{"location":"Ctrl/Ordinary_Differential_Equation/Lyapunov/","title":"Stability Theory of ODE","text":"<p>This chapter we focus on </p> \\[ \\begin{equation} \\dot{\\mathbfit{x}} = \\mathbfit{f}(t, \\mathbfit{x})\\label{eq1} \\end{equation} \\] <p>We consider continuous dependence of the solutions on initial condition on a larger interval, i.e. \\((\\beta,\\infty)\\), for we have show the dependency holds on finite intervals. However, this is not always as expected. So we have to introduce some basic ideas.</p>"},{"location":"Ctrl/Ordinary_Differential_Equation/Lyapunov/#lyapunov-lyapunov-stability","title":"Lyapunov \u7a33\u5b9a\u6027 | Lyapunov Stability","text":"<p>We assume that \\(\\pmb{x}=\\pmb{\\varphi}(t)\\) is a special solution of the ODE \\(\\ref{eq1}\\). </p> <p></p> <p>Lyapunov Stability</p> <p>(i) Lyapunov Stable</p> <p>If \\(\\forall \\varepsilon&gt;0\\), \\(\\forall t_0&gt;\\beta\\), \\(\\exists \\delta&gt;0\\), \\(\\forall \\pmb{x}_0\\), s.t. \\(\\|\\pmb{x}_0-\\pmb{\\varphi}(t_0)\\|&lt;\\delta\\), ODE \\(\\ref{eq1}\\) with initial value \\(\\pmb{x}(t_0)=\\pmb{x}_0\\) has a solution \\(\\pmb{x}(t;t_0,\\pmb{x}_0)\\) which exists on \\([t_0,+\\infty)\\) and satisfies</p> \\[ \\|\\pmb{x}(t;t_0,\\pmb{x}_0)-\\varphi(t)\\|&lt;\\varepsilon, \\quad \\forall t\\in [t_0,+\\infty) \\] <p>then we call the special solution \\(\\pmb{x}=\\pmb{\\varphi}(t)\\) is Lyapunov stable, or stable for short.</p> <p>(ii) Lyapunov Unstable</p> <p>If \\(\\exists \\varepsilon_0&gt;0\\), \\(\\exists t_0&gt;\\beta\\), \\(\\forall \\delta&gt;0\\), \\(\\exists \\pmb{x}_0\\), s.t. \\(\\|\\pmb{x}_0-\\pmb{\\varphi}(t_0)\\|&lt;\\delta\\), ODE \\(\\ref{eq1}\\) with initial value \\(\\pmb{x}(t_0)=\\pmb{x}_0\\) has a solution \\(\\pmb{x}(t;t_0,\\pmb{x}_0)\\) which exists on \\([t_0, \\alpha)\\)(\\(\\alpha&lt;+\\infty\\)) or there exists \\(t_1&gt;t_0\\), and it satisfies </p> \\[ \\|\\pmb{x}(t_1;t_0,\\pmb{x}_0)-\\varphi(t)\\|\\geq\\varepsilon_0 \\] <p>then the special solution is Lyapunov Unstable.</p> <p>(iii) Attractive</p> <p>If \\(\\exists t_0&gt;\\beta\\), \\(\\exists \\zeta&gt;0\\), \\(\\forall \\pmb{x}_0\\), s.t. \\(\\|\\pmb{x}_0-\\pmb{\\varphi}(t_0)\\|&lt;\\zeta\\), solution of ODE \\(\\ref{eq1}\\) with initial value \\(\\pmb{x}(t_0)=\\pmb{x}_0\\) satisfies</p> \\[ \\lim_{t\\rightarrow +\\infty}\\|\\pmb{x}(t;t_0,\\pmb{x}_0)-\\pmb{\\varphi}(t)\\|=0 \\] <p>then the special solution is Attractive.</p> <p>(iv) Asymptotic stable</p> <p>If a special solution is Lyapunov stable and attractive, then we call it Asymptotic stable.</p> <p>To simplify the analysis, we choose to make a translation. That is, if we let </p> \\[ \\pmb{y}=\\pmb{x}-\\pmb{\\varphi}(t), \\pmb{F}(t,\\pmb{y})=\\pmb{f}(t,\\pmb{y}+\\pmb{\\varphi}(t))-\\pmb{f}(t,\\pmb{\\varphi}(t)) \\] <p>then we only need to discuss </p> \\[ \\begin{align*} \\frac{d\\pmb{y}}{dt}&amp;=\\frac{d\\pmb{x}}{dt}-\\frac{d\\pmb{\\varphi}(t)}{dt}\\\\ &amp;=\\pmb{f}(t,\\pmb{x})-\\pmb{f}(t,\\pmb{\\varphi}(t))\\\\ &amp;=\\pmb{f}(t,\\pmb{y}+\\pmb{\\varphi}(t))-\\pmb{f}(t,\\pmb{\\varphi}(t))\\\\ &amp;=\\pmb{F}(t,\\pmb{y}) \\end{align*} \\] <p>with special solution \\(\\pmb{y}=0\\), i.e. \\(\\pmb{f}(t,\\pmb{0})=\\pmb{0}\\). We call solution \\(\\pmb{y}\\equiv 0\\) zero solution.</p>"},{"location":"Ctrl/Ordinary_Differential_Equation/Lyapunov/#linear-approximate-method","title":"\u7ebf\u6027\u8fd1\u4f3c\u5224\u522b\u6cd5 | Linear Approximate Method","text":"<p>If we represent ODE \\(\\ref{eq1}\\) as </p> \\[ \\begin{equation} \\frac{d\\pmb{x}}{dt}=\\pmb{A}(t)\\pmb{x}+\\pmb{R}(t,\\pmb{x})\\label{eq3} \\end{equation} \\] <p>where </p> \\[ \\pmb{A}(t)=\\frac{\\partial \\pmb{f}}{\\partial \\pmb{x}}\\Bigg|_{\\pmb{x}=\\pmb{0}} \\] <p>and \\(\\pmb{R}(t,\\pmb{x})\\) is the summation of higher order items, i.e.</p> \\[ \\lim_{\\|\\pmb{x}\\|\\rightarrow 0}\\frac{\\|\\pmb{R}(t,\\pmb{x})\\|}{\\|\\pmb{x}\\|}=0. \\]"},{"location":"Ctrl/Ordinary_Differential_Equation/Lyapunov/#lyapunov-stability-of-lodes","title":"Lyapunov Stability of LODEs","text":"<p>Firstly, we discuss the Lyapunov stability of LODEs</p> \\[ \\begin{equation} \\frac{d\\pmb{x}}{dt}=\\pmb{A}(t)\\pmb{x}. \\label{eq2} \\end{equation} \\] <p>Theorem of Lyapunov Stability of LODEs</p> <p>Assume \\(\\pmb{\\varPhi}(t)\\) is a basic solution matrix of LODEs \\(\\ref{eq2}\\), then its zero solution is </p> <p>(i) stable iff \\(\\forall t_0&gt;\\beta\\), \\(\\exists K(t_0)&gt;0\\), s.t.</p> \\[ \\|\\pmb{\\varPhi}(t)\\| \\leq K,\\quad, t\\geq t_0 \\] <p>(ii) asymptotic stable iff </p> \\[ \\lim_{t\\rightarrow +\\infty}\\|\\pmb{\\varPhi}(t)\\|=0 \\] <p>For time-varying system, it is hard to solve a basic solution matrix. But for time-invariant system, we can not only solve the basic solution matrix, but also deduce a better result.</p> <p>Theorem for Lyapunov Stability of time-invariant System</p> <p>Consider const-coeffient system</p> \\[ \\frac{d\\pmb{x}}{dt}=\\pmb{A}\\pmb{x} \\] <p>its zero solution is </p> <p>(i) asymptotic stable iff all the eigenbalues of \\(\\pmb{A}\\) are negative.</p> <p>(ii) stable iff all the eigenvalues of \\(\\pmb{A}\\) are not positive, and for eigenvalue whose real part is zero, its corresponding Jordan block is one roder (or can be diagonalized).</p> <p>(iii) unstable iff there exists one positive eigenvalue or for eigenvalue whose real part is zero, its corresponding Jordan block is one roder(or cannot be diagonalized).</p>"},{"location":"Ctrl/Ordinary_Differential_Equation/Lyapunov/#lyapunov-stability-of-non-linear-odes","title":"Lyapunov Stability of Non-linear ODEs","text":"<p>For non-linear system, we can have some results that is similar to the above. To simplify the problem, we let the linear part \\(\\pmb{A}(t)\\equiv \\pmb{A}\\) in ODE \\(\\ref{eq3}\\) be a constant matrix. Then, we can have the following Theorem</p> <p>Theorem for non-linear system using Linear Approximatie Method</p> <p>(i) If all the eigenvalues of \\(\\pmb{A}\\) are negative, then the zero solution of ODE \\(\\ref{eq3}\\) is asymptotic stable.</p> <p>(ii) If there exists an eigenvalue of \\(\\pmb{A}\\) which is positive, then the zero solution of ODE \\(\\ref{eq3}\\) is unstable.</p> HintsProof <p>Use Picard theorem and extension theorem to get a solution on \\([0,t*)\\), then show that solution can tend to \\(0\\) by using Gronwall inequation.</p>"},{"location":"Ctrl/Ordinary_Differential_Equation/Lyapunov/#second-method-of-lyapunov","title":"\u7b2c\u4e8c\u6cd5 | Second Method of Lyapunov","text":"<p>The first method does not carry on because it makes use of power series. But his second method did gain ground, which employs an energy function which characterizes the solution. So this method is also called Direct Method.</p> <p>Here, we only consider autonomous ODEs, that is,</p> \\[ \\begin{equation} \\frac{d\\pmb{x}}{dt}=\\pmb{f}(\\pmb{x})\\label{eq4} \\end{equation} \\] <p>with the right side of ODE not containing variable \\(t\\).</p> <p>The basic idea can be shown in the following diagram.</p> \\[ \\begin{align*} \\|\\pmb{x}_0\\|&amp;\\ll 1\\\\ &amp;\\Downarrow \\quad \\text{(by continuity of $V$)} \\\\ V(\\pmb{x}_0)&amp;\\ll 1\\\\  &amp;\\Downarrow \\quad \\text{(by $\\frac{d V}{dt}&lt; 0$)}\\\\ V[\\pmb{x}(t)]&amp;\\leq V[\\pmb{x}(0)]\\ll 1\\\\ &amp;\\Downarrow \\quad \\text{(by monotony)}\\\\ \\|\\pmb{x}(t)\\|&amp;\\leq\\|\\pmb{x}(0)\\|\\ll 1 \\end{align*} \\] <p>Definition of Definite Sign Function</p> <p>Assume \\(h&gt;0\\), function \\(V(\\pmb{x})\\in C^1(\\|\\pmb{x}\\|\\leq h)\\), if \\(V(\\pmb{0})=0\\) and </p> <p>(i) \\(V(\\pmb{x})&gt;0\\)(or \\(&lt;0\\)) on \\(0&lt;\\|\\pmb{x}\\|\\leq h\\), then we call \\(V(\\pmb{x})\\) is a definite positive(or negative) function.</p> <p>(ii) \\(V(\\pmb{x})\\geq0\\)(or \\(\\leq0\\)) on \\(0&lt;\\|\\pmb{x}\\|\\leq h\\), then we call \\(V(\\pmb{x})\\) is a constant positive(or negative) function.</p> <p>Denifite positive function has a property.</p> <p>Property of Definite Positive Function</p> <p>\\(\\forall \\varepsilon&gt;0\\), \\(\\exists \\delta&gt;0\\), \\(\\forall \\pmb{x}\\), s.t. \\(V(\\pmb{x})&lt;\\delta\\), then \\(\\|\\pmb{x}\\|&lt;\\varepsilon\\). </p> HintsProof <p>Pay attention to the positive definite function.</p> <p>\\(\\forall \\varepsilon&gt;0\\), let </p> \\[ \\delta=\\min_{\\varepsilon\\leq\\|\\pmb{x}\\|\\leq h}V(\\pmb{x}) \\] <p>then </p> \\[ \\|\\pmb{x}\\|\\geq \\delta \\Rightarrow V(\\pmb{x})\\geq \\min_{\\varepsilon\\leq\\|\\pmb{x}\\|\\leq h}V(\\pmb{x}) = \\delta. \\] <p>So</p> \\[ V(\\pmb{x})&lt;\\delta\\Rightarrow \\|\\pmb{x}\\|&lt;\\varepsilon \\] <p></p> <p>Theorem of Stability test</p> <p>In ODE \\(\\ref{eq4}\\), assume \\(h&gt;0\\). If there exists a definite positive function \\(V(\\pmb{x})\\) on \\(\\|\\pmb{x}\\|\\leq h\\), whose total derivative </p> \\[ \\frac{dV}{dt}=\\nabla^{\\pmb{x}}V(\\pmb{x})\\cdot \\frac{d\\pmb{x}}{dt}=\\sum_{i=1}^n\\frac{\\partial V}{\\partial x_i}\\frac{d x_i}{dt}=\\sum_{i=1}^n\\frac{\\partial V}{\\partial x_i}f_i \\] <p>(i) is a constant negative function, then the zero solution to ODE \\(\\ref{eq4}\\) is Lyapunov stable.</p> <p>(ii) is a definite negative function, then the zero solution to ODE \\(\\ref{eq4}\\) is Lyapunov asymptotic stable.</p> <p>(iii) is a definite positive function, then the zero solution to ODE \\(\\ref{eq4}\\) is Lyapunov unstable.</p> <p>For (i) in Theorem of Stability Test, if we find another condition, then the system can also be asymptotic stable, and this is exactly the following theorem.</p> <p>LaSalle's invariance principle</p> <p>If ODE \\(\\ref{eq4}\\) satisfies </p> <p>For (iii) in Theorem of Stability Test, we can have a weaker theorem.</p> <p>Theorem for Lyapunov Unstable</p> <p>For ODE \\(\\ref{eq4}\\), if there exists a open region \\(\\mathcal{N}\\subset B_h(\\pmb{0})\\), satisfies</p> <p>(i) \\(V(\\pmb{x})\\in C^1(\\mathcal{N})\\).</p> <p>(ii) \\(\\pmb{0}\\in \\partial \\mathcal{N}\\), \\(\\exists \\delta&gt;0\\), \\(\\forall \\pmb{x}\\in \\partial \\mathcal{N}\\cap B_\\delta(\\pmb{0})\\), such that</p> \\[ V(\\pmb{x})=0 \\] <p>(iii) \\(\\forall \\pmb{x}\\in \\mathcal{N}\\cap B_\\delta(\\pmb{0})\\), \\(V(\\pmb{x})\\) and \\(\\frac{dV}{dt}\\) are both definite positive function.</p> <p>For a specific problem, we can choose first quadrant of the plane </p> \\[ \\mathcal{N}=\\{(x,y): x&gt;0,y&gt;0\\} \\] <p>with a typical \\(V(\\pmb{x})=xy\\), then test if \\(\\frac{dV}{dt}\\) is definite positive on \\(\\mathcal{N}\\).</p> <p>Example1. Determine the Lyapunov stability of the following system.</p> \\[ \\begin{cases} x'=x^5+\\sin{x^5}+2y+6y^5\\\\ y'=-e^x+8x^3+e^{y^3} \\end{cases} \\] Answer <p>use Hamilton system approximation.</p> <p>Example2. Determine the Lyapunov stability of the following system.</p> \\[ \\begin{cases} x'=-3x^3y^4\\\\ y'=x^4y^3 \\end{cases} \\] Answer <p>The above one cannot use Hamilton system approximation for it only has one item. But we can divide one by two, and get</p> \\[ \\frac{dy}{dx}=-\\frac{x}{3y} \\] <p>which is a homogeneous equation. So the solution is \\(x^2+3y^2=C\\). We can choose Lyapunov function \\(V=0.5x^2+1.5y^3\\), then \\(dV/dt\\equiv 0\\), which is Lyapunov stable but not asymptotic stable.</p>"},{"location":"Ctrl/Ordinary_Differential_Equation/Preliminary/","title":"Preliminary","text":""},{"location":"Ctrl/Ordinary_Differential_Equation/Preliminary/#jordan-matrix","title":"Jordan Matrix","text":"<p>Jordan Normal Form Theorem</p> <p>\\(A\\in \\mathbb{C}^{n\\times n}\\), \\(P_A(\\lambda)=\\prod_{i=1}^s(\\lambda-\\lambda_i)^{n_i}\\), then it is similar to Jordan normal form \\(diag(J_1,J_2,\\cdots,J_s)\\), \\(J_i\\in \\mathbb{n_i\\times n_i}\\), where \\(n_i\\) is algebraic multiplicity of \\(A\\), </p> \\[ J_i=diag(J_{i_1}, J_{i_2},\\cdots, J_{i_{m_i}}), \\quad m_i \\text{ is the geometric multiplicity of } A \\] \\[ J_{ij}=J_{l_{ij}}(\\lambda_i)=\\left[\\begin{array}{cccc} \\lambda_i &amp; 1\\\\ &amp;\\lambda_i &amp;\\ddots\\\\ &amp;&amp;\\ddots&amp;1\\\\ &amp;&amp;&amp;\\lambda_i \\end{array}     \\right]\\in \\mathbb{C}^{l_{ij}\\times l_{ij}} \\] <p>And the Jordan normal form is unique if we do not consider the sequence of small blocks.</p> <p>Minimal polynomial</p> <p>Matrix \\(A\\) has a minimal polynomial </p> \\[ d_{A}(\\lambda)=d_{J}(\\lambda)=\\prod_{i=1}^s(\\lambda-\\lambda_i)^{k_i} \\] <p>where</p> \\[ k_i=\\max_{1\\leq j\\leq m_i}\\{l_{ij}\\} \\] <p>\\(m_i\\) determines how much blocks compose the whole Jordan normal form. For a Jordan normal form \\(J_i\\), with \\(n_i\\) algebraic multiplicity of \\(\\lambda_i\\), has \\(m_i\\) number of Jordan small blocks. We can easily see that the prime diagonal has \\(n_i-m_i\\) number of \\(1\\), which means rank\\((J-\\lambda_i I)=n_i-m_i\\). Then to confirm the composition of these small blocks, we have to write \\(n_i-m_i\\) as a summation of \\(m_i\\) number non-negative integers, each of which corresponds to a Jordan matrix.</p> <p>Not complicatedly speaking, we have to calculate the power of \\(J_i-\\lambda_i\\) to get the result.</p> <p>Denote \\(\\delta_p^i\\) as the number of \\(p\\)th Jordan small blocks corresponding to eigenvalue \\(\\lambda_i\\), where \\(p=1,2\\cdots, n_i\\), so</p> \\[ \\begin{align*} \\sum_{p=1}^{n_i}\\delta_p^i=m_i, \\quad \\text{total number of blocks}\\\\ \\sum_{p=1}^{n_i}p\\delta_p^i=n_i\\quad \\text{total rank of Jordan} \\end{align*} \\] <p>Introduce </p> \\[ r_k^i=r(A-\\lambda_i I)^k \\] <p>so</p> \\[ \\begin{align*} r_1^i&amp;=r(A-\\lambda_i I)\\\\ &amp;=r(J-\\lambda_i I)\\\\ &amp;=n-n_i+r(J_i-\\lambda_i I)\\\\ &amp;=n-n_i + \\sum_{p=1}^{n_i}(p-1)\\delta_p^i \\quad \\text{1 order block becomes 0 when substraction} \\end{align*} \\] <p>So after \\(k\\) times power</p> \\[ r_k^i=n-n_i+\\sum_{p=k}^{n_i}(p-k)\\delta_p^i \\quad \\text{Why?} \\] <p>and</p> \\[ \\begin{align*} r_{k-1}^i&amp;=n-n_i+\\sum_{p=k-1}^{n_i}(p-k+1)\\delta_p^i\\\\ &amp;=n-n_i+\\sum_{p=k}^{n_i}(p-k+1)\\delta_p^i \\end{align*} \\] <p>If we define</p> \\[ d_k^i:=r_{k-1}^i-r^i_k=\\sum_{p=k}^{n_i}\\delta_p^i \\] <p>and </p> \\[ d_{k+1}^i=\\sum_{p=k+1}^{n_i}\\delta_p^i \\] <p>Substract the aboce two equaiton, we get</p> \\[ \\delta_p^i=d_k^i-d_{k+1}^i \\] <p>So if a Jordan small block has \\(\\delta_p^i\\) number, then the rank decline from power \\(p-1\\) to power \\(p\\) of \\((A-\\lambda_i I)\\) would display.</p>"},{"location":"Ctrl/Ordinary_Differential_Equation/Qualitative_Theory/","title":"Qualitative Theory of ODE","text":""},{"location":"Ctrl/Ordinary_Differential_Equation/Qualitative_Theory/#singular-point-analysis","title":"Singular Point Analysis","text":""},{"location":"Ctrl/Ordinary_Differential_Equation/Qualitative_Theory/#linear-system-of-two-dimension","title":"Linear System of Two dimension","text":"<p>Consider </p> \\[ \\begin{equation} \\begin{cases} x'=ax+by\\\\ y'=cx+dy \\end{cases}, \\quad \\hat{\\pmb{x}}=\\pmb{A}x \\label{eq1} \\end{equation} \\] <p>with det\\((\\pmb{A})\\neq 0\\).</p> <p>General method for drawing sketch in a plane</p> <p>For system of two dimension in equation \\(\\ref{eq1}\\), follow steps below.</p> <p>(i) check \\(\\lambda_1\\), \\(\\lambda_2\\).</p> <p>To be more specific, directly check det\\((\\pmb{A})=\\lambda_1\\lambda_2\\) and tr\\((\\pmb{A})=\\lambda_1+\\lambda_2\\)</p> <p>See the pictrues below to pinpoint the position for type of Singular Point.</p> <p><p> </p></p> <p>If the position is above the parabola of the above plane, then go to part (iii) directly.</p> <p>(ii) determine the straight line.</p> <p>Solve for \\(k\\) of the straight line.</p> \\[ \\frac{y'}{x'}\\Bigg|_{y=kx}=k \\] <p>please do not forget \\(k=\\infty\\) would not be included from above.</p> <p>For deprecated node, we could only solve for one \\(k\\).</p> <p>(iii) determine the type of direction.</p> <p>Genaral method is to check for </p> \\[ xy'-yx'|_{(x_0,y_0)}=r^2\\theta'=\\begin{cases}&gt;0, \\quad \\text{anticlockwise}\\\\ &lt;0,\\quad \\text{clockwise}\\end{cases} \\] <p>But we can choose special point for direction.</p> Proof for part (ii)Proof for part (iii)Special cases for (iii) <p>On linear curve, we have \\(y(t)=kx(t)\\), so \\(y'(t)=kx'(t)\\), and </p> \\[ \\frac{y'}{x'}\\Bigg|_{y=kx}=k \\] <p>Solve for \\(k\\).</p> <p>we have polar expression</p> \\[ x=r\\cos \\theta, \\quad y=r\\sin \\theta \\] <p>then</p> \\[ \\begin{align} x'&amp;=r'\\cos \\theta -r\\sin \\theta\\cdot \\theta',\\quad \\label{xprime}\\\\y'&amp;=r'\\sin \\theta + r\\cos \\theta \\cdot\\theta'\\label{yprime} \\end{align} \\] <p>\\((\\ref{xprime})\\times \\sin\\theta\\) - \\((\\ref{yprime})\\times \\cos\\theta\\) and get:</p> \\[ x'\\sin\\theta-y'\\cos\\theta=-r\\cdot\\theta' \\] <p>multiply both sides by \\(r\\) and get</p> \\[ x'y-y'x=-r^2\\cdot \\theta' \\] <p>So </p> \\[ xy'-yx'=r^2\\cdot \\theta' \\] <p>both sides have the same sign.</p> <p>For \\(0&lt;\\lambda_1&lt;\\lambda_2\\) or \\(\\lambda_1&lt;\\lambda_2&lt;0\\), we can substitute \\((1,0)\\) into \\(y'\\) to see the direction.</p> <p>For \\(\\lambda_1&lt;0&lt;\\lambda_2\\), we can inspect a point \\((0,1)\\) to see the direction.</p> <p>For \\(\\lambda_1=\\lambda_2\\), first we check if there is indefinite solution of \\(k\\). If so, then ... we can inspect a point on linear curve.</p> <p>For \\(\\lambda=\\alpha\\pm i\\beta\\), we can also substitute \\((1,0)\\) into \\(y'\\) to see the direction.</p> <p>Example. Draw the craft.</p> \\[ \\begin{cases} x'=4x+\\sqrt{3}y\\\\ y'=2x+5y \\end{cases} \\] Answer <ul> <li> <p>check the type of \\(\\lambda_1\\), \\(\\lambda_2\\).</p> </li> <li> <p>determine the linear curve.</p> </li> <li> <p>determine the type of direction.</p> </li> </ul>"},{"location":"Ctrl/Ordinary_Differential_Equation/Qualitative_Theory/#non-linear-system-of-two-dimension","title":"Non-Linear System of Two dimension","text":"<p>This part we focus on </p> \\[ \\begin{equation} \\begin{cases} x'=P(x,y)=ax+by+\\varPhi(x,y)\\\\ y=Q(x,y)=cx+dy+\\varPsi(x,y) \\end{cases}\\label{eq2} \\end{equation} \\] <p>where \\(\\varPhi(x,y)\\), \\(\\varPsi(x,y)\\) satisfy</p> \\[ \\varPhi(x,y)=o(r),\\quad \\varPsi(x,y)=o(r), \\quad r=\\sqrt{x^2+y^2}\\rightarrow 0  \\] <p>with its linear appriximation system</p> \\[ \\begin{equation} \\begin{cases} x'=ax+by\\\\ y=cx+dy \\end{cases}\\label{eq3} \\end{equation} \\] <p>We can also use its linear spproximation system to test. To be more specific, we have the following theorem.</p> <p>Theorem for Non-Linear System of Two dimension</p> <p>(i) If singular point is \\(N_-\\)(\\(N_+\\)) of the linear approsimation system \\(\\ref{eq3}\\), then it is also the \\(N_-\\)(\\(N_+\\)) of the original non-linear system \\(\\ref{eq2}\\).</p> <p>(ii) If singular point is \\(F_-\\)(\\(F_+\\)) of the linear approsimation system \\(\\ref{eq3}\\), then it is also the \\(F_-\\)(\\(F_+\\)) of the original non-linear system \\(\\ref{eq2}\\).</p> <p>(iii) If singular point is \\(S\\) of the linear approsimation system \\(\\ref{eq3}\\), then it is also the \\(S\\) of the original non-linear system \\(\\ref{eq2}\\).</p> <p>(iv) If singular point is \\(S_-\\)(\\(S_+\\)) or \\(D_-\\)(\\(D_+\\)) of the linear approsimation system \\(\\ref{eq3}\\), and there exists \\(\\delta&gt;0\\), such that \\(\\varPhi(x,y)\\), \\(\\varPsi(x,y)\\) satisfy</p> \\[ \\varPhi(x,y)=o(r^{1+\\delta}), \\quad\\varPsi(x,y)=o(r^{1+\\delta}), \\quad r=\\sqrt{x^2+y^2}\\rightarrow 0 \\] <p>then it is also the \\(S_-\\)(\\(S_+\\)) or \\(D_-\\)(\\(D_+\\)) of the original non-linear system \\(\\ref{eq2}\\).</p>"},{"location":"Ctrl/Ordinary_Differential_Equation/General_Theory/","title":"General Theory of ODE","text":""},{"location":"Ctrl/Ordinary_Differential_Equation/General_Theory/#existence-and-uniqueness-theorem","title":"\u5b58\u5728\u552f\u4e00\u6027\u5b9a\u7406 | Existence and Uniqueness Theorem","text":""},{"location":"Ctrl/Ordinary_Differential_Equation/General_Theory/#proof-using-contraction-mapping","title":"\u538b\u7f29\u6620\u5c04\u6cd5 | Proof using Contraction Mapping","text":""},{"location":"Ctrl/Ordinary_Differential_Equation/General_Theory/#extension-of-solution","title":"\u89e3\u7684\u5ef6\u62d3 | Extension of Solution","text":""},{"location":"Ctrl/Ordinary_Differential_Equation/General_Theory/#comparison-theorem","title":"\u6bd4\u8f83\u5b9a\u7406 | Comparison Theorem","text":""},{"location":"Ctrl/Ordinary_Differential_Equation/General_Theory/#continuous-dependence-of-solution-on-initial-value","title":"\u89e3\u5bf9\u521d\u503c\u7684\u8fde\u7eed\u4f9d\u8d56\u6027 | Continuous Dependence of Solution on Initial Value","text":""},{"location":"Ctrl/Ordinary_Differential_Equation/General_Theory/#method-of-power-series","title":"\u5e42\u7ea7\u6570\u89e3\u6cd5 | Method of Power Series","text":""},{"location":"Ctrl/Ordinary_Differential_Equation/General_Theory/Comparison_Theorem/","title":"Comparison Theorem","text":"<p>Based on the theorem on Extension of solution, we can only deduce that solution of an ODE can extend to its boundary Once the right function \\(f(x,y)\\) is continous on an open set \\(G\\).</p> <p>But it does not tell us the existing interval of independent variable \\(x\\). In particular, for two dimension problem, \\(y'=f(x,y)\\in C(\\mathbb{R}^2)\\), it could either be </p> \\[ x\\rightarrow \\beta &lt;\\infty, y\\rightarrow \\infty \\] <p>or</p> \\[ x\\rightarrow \\infty, y\\rightarrow \\beta &lt;\\infty \\] <p>So to better know the existing interval of \\(x\\), we have to give a more detailed tool for analysis. </p> <p>Note: The theorem presented below are only appliable to two dimension problem of ODE, cause in high dimension problem, we cannot compare the magnitude of \\(\\pmb{y}\\) but under some metric.</p> <p>To unify the form, in this part we talk about ODE</p> \\[ \\begin{equation} \\frac{dy}{dx}=f(x,y),\\quad y(x_0)=y_0 \\label{eq1} \\end{equation} \\]"},{"location":"Ctrl/Ordinary_Differential_Equation/General_Theory/Comparison_Theorem/#first-comparison-theorem","title":"First Comparison Theorem","text":"<p>First Comparison Theorem</p> <p>Assume \\(f(x,y), F(x,y)\\in C(G)\\), where \\(G\\) is an open set, and </p> \\[ f(x,y) &lt;F(x,y),\\quad \\forall(x,y)\\in G \\] <p>And we assume \\(y=\\phi(x)\\) and \\(y=\\varPhi(x)\\) on \\((a,b)\\) are solution to ODE</p> \\[ \\begin{cases} y'=f(x,y)\\\\ y(x_0)=y_0 \\end{cases}, \\quad \\begin{cases} y'=F(x,y)\\\\ y(x_0)=y_0 \\end{cases} \\] <p>respectively, where \\((x_0,y_0)\\in G\\). Then </p> \\[ \\phi(x)&lt;\\varPhi(x), \\quad x_0&lt;x&lt;b \\] HintsProof <p>Formulate a function </p> \\[ \\psi(x)=\\varPhi(x)-\\phi(x) \\] <p>then prove by contradiction.</p> <p>Let</p> \\[ \\psi(x)=\\varPhi(x)-\\phi(x) \\] <p>then by initial condition</p> \\[ \\psi(x_0)=0,\\quad \\psi'(x_0)=F(x_0,y_0)-f(x_0,y_0)&gt;0 \\] <p>So we can see that there exists \\(\\delta&gt;0\\), such that</p> \\[ \\psi(x)=\\varPhi(x)-\\phi(x)&gt;0,\\quad x_0&lt;x\\leq x_0+\\delta \\] <p>If proposition </p> \\[ \\varPhi(x)&gt;\\phi(x),\\quad \\forall x\\in[x_0,b) \\] <p>does not hold, then \\(\\exists x_0+\\delta&lt;x_1&lt;b\\) such that</p> \\[ \\varPhi(x_1)&lt;\\phi(x_1) \\] <p>Let</p> \\[ \\alpha=\\min\\{x\\in (x_0+\\delta,b): \\psi(x)=0\\} \\] <p><p> </p></p> <p>so we can deduce that </p> \\[ \\psi(\\alpha)=0,\\quad \\psi(x)&gt;0,\\quad \\forall x\\in (x_0,\\alpha) \\] <p>easy to see that \\(\\psi(\\alpha)\\leq 0\\). But</p> \\[ \\psi'(\\alpha)= F(\\alpha,\\varPhi(\\alpha))-f(\\alpha,\\phi(\\alpha))&gt;0 \\] <p>which contradicts!</p>"},{"location":"Ctrl/Ordinary_Differential_Equation/General_Theory/Comparison_Theorem/#upper-and-lower-solution","title":"Upper and Lower Solution","text":"<p>To simplify the notation, we have to introduce upper and lower solution on the right side.</p> <p>Definition of upper and lower solution</p> <p>(i) If function \\(v(x)\\) on \\([x_0,b)\\) satisfies</p> \\[ \\frac{d v(x)}{dx}&lt;f(x,v(x)), \\quad v(x_0)\\leq y_0 \\] <p>then it is called the lower solution of ODE \\(\\ref{eq1}\\) on the right side.</p> <p>(ii) If function \\(w(x)\\) on \\([x_0,b)\\) satisfies</p> \\[ \\frac{d w(x)}{dx}&gt;f(x,v(x)), \\quad w(x_0)\\leq y_0 \\] <p>then it is called the upper solution of ODE \\(\\ref{eq1}\\) on the right side.</p> <p>From image point of view, we can say that the tangent of points on the upper solution on the right side must be bigger than the vector field on the same points. They are shown in the following graph.</p> <p><p> </p></p> <p>Theorem for upper and lower solution</p> <p>Assume \\(v(x)\\) and \\(w(x)\\) are defined in the above definition, then </p> \\[ v(x)&lt;\\phi(x)&lt;w(x),\\quad \\forall x\\in [x_0,b) \\] HintsProof <ul> <li>Define an assistant function</li> </ul> \\[ g(x)=\\frac{dw(x)}{dx}-f(x,w(x)),\\quad F(x,y)=f(x,y)+g(x) \\] <p>which is larger than zero on the line \\(y=w(x)\\), then can deduce the result just by First Comparison Theorem</p> <p>\\(w(x)\\) satisfies ODE</p> \\[ \\begin{align*} \\frac{dy}{dx}&amp;=F(x,y)\\\\ &amp;=f(x,y)+g(x)\\\\ &amp;=f(x,y)+\\frac{dw(x)}{dx}-f(x,w(x))\\\\ &amp;&gt;f(x,y) \\end{align*} \\] <p>So by First Comparison Theorem we have \\(w(x)&gt;\\phi(x)\\) on interval \\([x_0,b)\\).</p> <p>It is similar to prove \\(v(x)&lt;\\phi(x)\\).</p>"},{"location":"Ctrl/Ordinary_Differential_Equation/General_Theory/Comparison_Theorem/#maximum-and-minimum-solution","title":"Maximum and Minimum solution","text":"<p>Maximum and Minimum solution</p> <p>If there are two solution \\(\\varPsi(x)\\), \\(\\varPhi(x)\\) to the ODE \\(\\ref{eq1}\\), such that for all solution \\(y(x)\\) to the same ODE, </p> \\[ \\varPsi(x)\\leq y(x)\\leq \\varPhi(x),\\quad \\forall x\\in[x_0,b) \\] <p>then we call \\(\\varPsi(x)\\), \\(\\varPhi(x)\\) the minimum and maximum solution of ODE on interval \\([x_0,b)\\) respectively.</p> <p>Apparently, the minimum and maximum solution are unique. Here we prove their existence.</p> <p></p> <p>Theorem of existence for minimum and maximum solution</p> <p>Assume \\(f(x,y)\\in C(R)\\) where</p> \\[ R=\\{(x,y):x_0\\leq x\\leq x_0+a, |y-y_0|\\leq b\\} \\] <p>Then initial problem \\(\\ref{eq1}\\) has maximum and minimum solution on \\([x_0,x_0+h)\\), where</p> \\[ h&lt;\\alpha=\\min\\{a,b/M\\}, \\quad  M=\\max_{(x,y)\\in R}{|f(x,y)|} \\] HintsProof <p>Formulate assistant initial problems</p> \\[ \\frac{dy}{dx}=f(x,y)+\\varepsilon_n, \\quad y(x_0)=y_0 \\] <p>where \\(\\varepsilon_n&gt;0\\), \\((n=1,2,\\cdots)\\) which is monotonically decreasing to \\(0\\), e.g. \\(\\varepsilon_n=1/n\\). Then show its limit of solution sequence is the maxmimum solution.</p> <p>It is similar to use \\(-\\varepsilon_n\\) to prove the existence of minimum solution.</p> <p>Formulate assistant initial problems</p> \\[ \\begin{align} \\frac{dy}{dx}=f(x,y)+\\varepsilon_n, \\quad y(x_0)=y_0 \\label{eq2} \\end{align} \\] <p>where \\(\\varepsilon_n&gt;0\\), \\((n=1,2,\\cdots)\\) which is monotonically decreasing to \\(0\\), e.g. \\(\\varepsilon_n=1/n\\). </p> <p>By Peano Theorem, ODE \\(\\ref{eq2}\\) has solution on interval \\(|x-x_0|\\leq \\alpha_n\\), where</p> \\[ \\alpha_n=\\min\\{a,b/M_n\\},\\quad M_n=\\max_{(x,y)\\in D}\\{|f(x,y)+\\varepsilon_n|\\} \\] <p>Notice that</p> \\[ \\lim_{n\\rightarrow \\infty}\\alpha_n = \\alpha = \\min\\{a,b/M\\}  \\] <p>so we choose \\(h&lt;\\alpha\\) such that all ODE \\(\\ref{eq2}\\) can have a solution \\(y=\\phi_n(x)\\) on interval \\(|x-x_0|\\leq h\\).</p> <ul> <li>Show that \\(\\{\\phi_n(x)\\}\\) has a subsequence that converges to the solution of the original ODE \\(\\ref{eq1}\\).</li> </ul> <p>This is similar as we prove in Peano Theorem.</p> <ul> <li>Show that the limit function \\(y=\\varPhi(x)\\) is the maximum solution of ODE \\(\\ref{eq1}\\). </li> </ul> <p>By First Comparison Theorem we have \\(\\phi_n(x)&gt;y(x)\\), for all solution \\(y(x)\\) of the original ODE \\(\\ref{eq1}\\). Then replace \\(n\\) with \\(n_j\\) and let \\(n\\rightarrow \\infty\\) we have</p> \\[ \\varPhi(x)\\geq y(x),\\quad \\forall x\\in [x_0,h] \\] <p>which shows that \\(\\varPhi(x)\\) is the maximum solution of ODE \\(\\ref{eq1}\\).</p> <p>Corollary</p> <p>Initial problem \\(\\ref{eq1}\\) has unique solution iff its maximum solution equals to minimum solution for all independent variable \\(x\\) on a given interval.</p>"},{"location":"Ctrl/Ordinary_Differential_Equation/General_Theory/Comparison_Theorem/#second-comparison-theorem","title":"Second Comparison Theorem","text":"<p>In First Comparison Theorem, we need to find \\(F(x,y)\\) which is strictly larger than \\(f(x,y)\\). What if we happen to find \\(F(x,y)\\) also equals \\(f(x,y)\\)?</p> <p>Apparently, we need to strengthen our condition to get a corresponding answer. This gives us the Second Comparison Theorem as below.</p> <p>Second Comparison Theorem</p> <p>Assume \\(f(x,y), F(x,y)\\in C(G)\\), where \\(G\\) is an open set, and </p> \\[ f(x,y) \\leq F(x,y),\\quad \\forall(x,y)\\in G \\] <p>And we assume \\(y=\\phi(x)\\) and \\(y=\\varPhi(x)\\) on \\((a,b)\\) are solution to ODE</p> \\[ \\begin{cases} y'=f(x,y)\\\\ y(x_0)=y_0 \\end{cases}, \\quad \\begin{cases} y'=F(x,y)\\\\ y(x_0)=y_0 \\end{cases} \\] <p>respectively, where \\((x_0,y_0)\\in G\\). And \\(y=\\phi(x)\\) is the minimum solution on the right side. Then </p> \\[ \\phi(x)\\leq \\varPhi(x), \\quad x_0&lt;x&lt;b \\] HintsProof <p>Also by formulating assistant initial problems.</p> <p>We still consider assistant problems</p> \\[ \\frac{dy}{dx}=f(x,y)-\\varepsilon_n,\\quad y(x_0)=y_0 \\] <p>So by the proof of Theorem for Maximum and Minimum Solution, we can have function subsequence </p> \\[ \\phi_{n_j}\\rightrightarrows \\phi \\] <p>which satisfies</p> \\[ \\phi(x)\\leq \\varPhi(x) \\]"},{"location":"Ctrl/Ordinary_Differential_Equation/General_Theory/Comparison_Theorem/#result-for-estimation","title":"Result for estimation","text":"<p>Application for Comparison Theorem</p> <p>Assume \\(f(x,y)\\in C(G)\\) where</p> \\[ G=\\{(x,y): x_0&lt;x&lt;b,-\\infty&lt;y&lt;+\\infty\\} \\] <p>We have \\((x_0,y_0)\\in G\\), denote \\([x_0,\\alpha)\\) is the maximum existence interval on the right side for solution to ODE \\(\\ref{eq1}\\).</p> <p>(i) If ODE \\(\\ref{eq1}\\) has upper and lower solution \\(w(x)\\), \\(v(x)\\) with their public existing interval \\([x_0,\\beta)\\), then \\(\\beta\\leq \\alpha\\).</p> <p>(ii) If ODE \\(\\ref{eq1}\\) has upper solution \\(w(x)\\) (or lower solution \\(v(x)\\)) with its maximum existing interval \\([x_0,\\beta)\\), and satisfies</p> \\[ \\lim_{x\\rightarrow \\beta^-}w(x)=-\\infty (\\text{or } \\lim_{x\\rightarrow \\beta^-}v(x)=+\\infty ) \\] <p>then \\(\\alpha\\leq \\beta\\).</p>"},{"location":"Ctrl/Ordinary_Differential_Equation/General_Theory/Continuous_Dependence/","title":"Continuous Dependence of Solution on Initial Value","text":"<p>Here we focus on </p> \\[ \\begin{equation} \\dot{\\mathbfit{y}} = \\mathbfit{f}(x, \\mathbfit{y},\\pmb{\\lambda}),\\quad  \\mathbfit{y}(x_0) = \\mathbfit{y}_0 \\label{eq1} \\end{equation} \\] <p>where \\(\\mathbfit{x}\\in \\mathbb{R}^n\\) and \\(\\mathbfit{f}: \\mathbb{R}^{n+1} \\mapsto \\mathbb{R}^n \\in C(D)\\) is a vector function(or vector field).</p>"},{"location":"Ctrl/Ordinary_Differential_Equation/General_Theory/Continuous_Dependence/#continuous-dependency","title":"Continuous Dependency","text":"<p>Continuous Dependency means that the solution of ODE with initial values would not differ very much from real solution when the errors of parameters are small enough.</p> <p>Now we consider making a transformation. Let </p> \\[ t=x-x_0,\\quad \\pmb{u}=\\pmb{y}-\\pmb{y}_0 \\] <p>then</p> \\[ \\begin{align*} \\frac{d \\pmb{u}(t)}{dt} &amp;= \\frac{d\\pmb{y}}{d x} \\\\ &amp;=\\pmb{f}(x,\\pmb{y},\\pmb{\\lambda})\\\\ &amp;=\\pmb{f}(t+x_0,\\pmb{u}+\\pmb{y_0},\\pmb{\\lambda}) \\end{align*} \\] <p>and initial value becomes</p> \\[ \\pmb{u}(0)=\\pmb{0} \\] <p>So we only consider dependence on parameters \\(\\pmb{\\lambda}\\) of ODE</p> \\[ \\begin{equation} \\frac{d\\pmb{y}}{dx}=\\pmb{f}(x,\\pmb{y},\\pmb{\\lambda}),\\quad \\pmb{y}(0)=\\pmb{0}  \\label{eq2} \\end{equation} \\] <p>Theorem for ODE with parameters</p> <p>Consider a region</p> \\[ G=\\{(x,\\pmb{y},\\pmb{\\lambda}):|x|\\leq a,|\\pmb{y}|\\leq b, |\\pmb{\\lambda}|\\leq c\\} \\] <p>and \\(\\pmb{f}(x,\\pmb{y},\\pmb{\\lambda})\\in C(G)\\) and satisfies Lypschitz condition with respect to \\(\\pmb{y}\\), i.e. \\(\\forall (x,\\pmb{y}_1,\\pmb{\\lambda})\\), \\((x,\\pmb{y}_2,\\pmb{\\lambda})\\) we have</p> \\[ |\\pmb{f}(x,\\pmb{y}_1,\\pmb{\\lambda})-\\pmb{f}(x,\\pmb{y}_2,\\pmb{\\lambda})|\\leq L|\\pmb{y}_1-\\pmb{y}_2| \\] <p>where \\(L\\) is Lypschitz constant. If we let </p> \\[ M=\\max_{(x,\\pmb{y},\\pmb{\\lambda})\\in G}\\{\\pmb{f}(x,\\pmb{y},\\pmb{\\lambda})\\}, \\quad \\alpha = \\min\\{a,b/M\\} \\] <p>then the solution \\(\\pmb{y}=\\pmb{\\phi}(x,\\pmb{\\lambda})\\) of ODE \\(\\ref{eq2}\\) is continuous on region</p> \\[ D=\\{|x|\\leq \\alpha, |\\pmb{\\lambda}|\\leq c\\} \\] Proof <p>Similar to Picard Theorem</p> <p>Corollary</p> <p>Assume region </p> \\[ R=\\{|x-x_0|\\leq a, |\\pmb{y}-\\pmb{y}_0|\\leq b\\} \\] <p>If \\(\\pmb{f}(x,\\pmb{y})\\in C(R)\\) and satisfies Lypschitz condition with respect to \\(\\pmb{y}\\). Then initial problem</p> \\[ \\frac{d \\pmb{y}}{dx}=\\pmb{f}(x,\\pmb{y}),\\quad \\pmb{y}(x_0)=\\pmb{\\eta} \\] <p>has a solution \\(\\pmb{y}=\\pmb{\\phi}(x,\\pmb{\\eta})\\) is continuous on region</p> \\[ Q=\\left\\{(x,\\pmb{y}): |x-x_0|\\leq \\frac{\\alpha}{2}, |\\pmb{y}-\\pmb{y}_0|\\leq \\frac{b}{2}\\right\\} \\] <p>where </p> \\[ M=\\max_{(x,\\pmb{y},\\pmb{\\lambda})\\in G}\\{\\pmb{f}(x,\\pmb{y},\\pmb{\\lambda})\\}, \\quad \\alpha = \\min\\{a,b/M\\}. \\] <p>Actually, we do not need Lypschitz condition but the uniqueness of solution to ODE \\(\\ref{eq1}\\). Here comes the following theorem.</p> <p>Theorem for dependence on initial value</p> <p>Consider ODE</p> \\[ \\frac{d\\pmb{y}}{dx}=\\pmb{f}(x,\\pmb{y},\\pmb{\\lambda}) \\] <p>where \\(\\pmb{f}\\) is bounded and continuous on region \\(G\\subset \\mathbb{R}\\times \\mathbb{R}^n \\times \\mathbb{R}^m\\). Assume for all initial points \\((x_0,\\pmb{y}_0)\\), the solution \\(\\pmb{y}=\\pmb{\\phi}(x;x_0,\\pmb{y}_0,\\pmb{\\lambda})\\) to the ODE exists uniquely on interval \\(I_0\\), where \\(I_0\\subset \\mathbb{R}\\) is finite. Then \\(\\forall \\varepsilon&gt;0\\), \\(\\exists \\delta&gt;0\\), \\(\\forall (\\xi, \\pmb{\\eta}, \\pmb{\\lambda}')\\in G\\) s.t.</p> \\[ |(\\xi, \\pmb{\\eta}, \\pmb{\\lambda}')- (x_0, \\pmb{y}_0, \\pmb{\\lambda})|&lt;\\delta \\] <p>we have </p> \\[ |\\pmb{\\phi}(x;\\xi,\\pmb{\\eta},\\pmb{\\lambda}')-\\pmb{\\phi}(x;x_0,\\pmb{y}_0,\\pmb{\\lambda})|&lt;\\varepsilon,\\quad \\forall x\\in I_0 \\] HintsProof <p>By contradiction.</p> <p>We only prove for ODE \\(\\ref{eq1}\\).</p> <ul> <li>Transfer the problem into another form</li> </ul> <p>If we introduce \\(\\pmb{z}=\\pmb{\\lambda}\\), then </p> \\[ \\frac{d\\pmb{z}}{dx}=\\pmb{0} \\] <p>cause \\(\\pmb{\\lambda}\\) is a comstant. Then ODE \\(\\ref{eq1}\\) becomes</p> \\[ \\frac{d\\pmb{y}}{dx}=\\pmb{f}(x,\\pmb{y},\\pmb{z}), \\quad \\frac{d\\pmb{z}}{dx}=\\pmb{0} \\] <p>with initial value</p> \\[ \\pmb{y}(x_0)=\\pmb{y}_0, \\quad \\pmb{z}(x_0)=\\pmb{0} \\] <p>We transfer the dependence on parameters into the dependence on initial values. In the following part, we only consider dependence on initial value.</p> <ul> <li>By contradiction</li> </ul> <p>Assume the above theorem does not hold, then \\(\\exists \\varepsilon_0&gt;0\\), \\(\\forall \\delta_n=1/n\\), \\(\\exists(\\xi_n,\\pmb{\\eta}_n, \\pmb{\\lambda}'_n)\\in G\\) with </p> \\[ |(\\xi_n,\\pmb{\\eta}_n, \\pmb{\\lambda}'_n)-(x_0,\\pmb{y}_0,\\pmb{\\lambda})|&lt;\\delta_i \\] <p>which satisfies that, there exsits \\(x_n\\in I_0\\)</p> \\[ \\begin{equation} |\\pmb{\\phi}(x_n;\\xi_n,\\pmb{\\eta}_n,\\pmb{\\lambda}'_n)-\\pmb{\\phi}(x_n;x_0,\\pmb{y}_0,\\pmb{\\lambda})|\\geq\\varepsilon_0\\label{ieq1} \\end{equation}\\] <p>Because \\(I_0\\) is bouned closed interval, so \\(\\{x_n\\}\\) has a convergent subsquence \\(\\{x_{n_j}\\}\\) which converges to \\(\\overline{x}\\in I_0\\).</p> <p>Notice that </p> \\[ \\pmb{\\phi}(x;\\xi_n,\\pmb{\\eta}_n)=\\pmb{\\eta}_n+\\int_{{\\xi}_n}^{x}\\pmb{f}(s,\\pmb{\\phi}(s;\\xi_n,\\pmb{\\eta}_n))ds \\] <p>it is easy to show that sequence \\(\\{\\pmb{\\phi}(x;\\xi_n,\\pmb{\\eta}_n)\\}\\) is uniforly bounded and equicontinuous. Actually, if we assume \\(M=\\max_{(x,\\pmb{y},\\pmb{\\lambda})\\in G}|\\pmb{f}|\\) and denote \\(\\pmb{\\psi}_n(x)=\\pmb{\\phi}(x;\\xi_n,\\pmb{\\eta}_n)\\), then </p> \\[ |\\pmb{\\psi}_n(x)-\\pmb{\\eta}|\\leq M|I_0|,\\quad \\forall n \\] \\[ |\\pmb{\\psi}_n(x_1)-\\pmb{\\psi}_n(x_2)|\\leq M|x_1-x_2|,\\quad \\forall n \\] <p>Then by Ascoli-Arzel\u00e0 Theorem, we have a uniforly convergent subsequence \\(\\{\\pmb{\\phi}(x;\\xi_{n_j},\\pmb{\\eta}_{n_j})\\}\\) which converges to \\(\\pmb{\\psi}(x)\\) and they all satisfy</p> \\[ \\pmb{\\phi}(x;\\xi_{n_j},\\pmb{\\eta}_{n_j})=\\pmb{\\eta}_{n_j}+\\int_{\\xi_{n_j}}^{x}\\pmb{f}(s,\\pmb{\\phi}(s;\\xi_{n_j},\\pmb{\\eta}_{n_j}))ds \\] <p>let \\(j\\rightarrow \\infty\\) and we get </p> \\[ \\pmb{\\psi}(x)=\\pmb{y}_0+\\int_{x_0}^x\\pmb{f}(x_0,\\pmb{\\psi}(x))ds \\] <p>which means \\(\\pmb{\\psi}(x)\\equiv \\pmb{\\phi}(x;x_0,\\pmb{y}_0)\\). </p> <ul> <li>Use the subsequence of \\(\\{x_{n_j}\\}\\)</li> </ul> <p>from inequation \\(\\ref{ieq1}\\), we have</p> \\[ |\\pmb{\\psi}_{n_j}(x_{n_j})-\\pmb{\\psi}(x_{n_j})|\\geq \\varepsilon_0 \\] <p>let \\(j\\rightarrow \\infty\\), we deduce</p> \\[ |\\pmb{\\psi}(\\overline{x})-\\pmb{\\psi}(\\overline{x})|\\geq \\varepsilon_0 \\] <p>which contradicts the uniqueness!</p> <p>Here interval \\(I_0\\) is finite is necessary because in the proof we make use of the finite length of the interval. If a solution can satisfy the property on an infinite interval like \\((\\beta,\\infty)\\), then we call it Lyapunov stable.</p>"},{"location":"Ctrl/Ordinary_Differential_Equation/General_Theory/Continuous_Dependence/#continuous-differentiability","title":"Continuous Differentiability","text":"<p>Similar to the above part, if \\(\\pmb{f}(x,\\pmb{y},\\pmb{\\lambda})\\in C^1(G)\\) in ODE \\(\\ref{eq1}\\), then we can say the solution is continuous differential on a region. To be more specific, we have the following theorem.</p> <p>Theorem for Continuous Differentiability</p> <p>Assume \\(\\pmb{f}(x,\\pmb{y},\\pmb{\\lambda})\\in C(G)\\) where</p> \\[ G=\\{(x,\\pmb{y},\\pmb{\\lambda}):|x|\\leq a,|\\pmb{y}|\\leq b, |\\pmb{\\lambda}|\\leq c\\} \\] <p>and has continuous partial derivatives with respect to \\(\\pmb{y}\\) and \\(\\pmb{\\lambda}\\). Then the solution \\(\\pmb{y}=\\pmb{\\phi}(x,\\pmb{\\lambda})\\) to ODE \\(\\ref{eq1}\\) is continuous differential on region \\(D\\), where </p> \\[ D=\\{|x|\\leq \\alpha, |\\pmb{\\lambda}|\\leq c\\} \\] Proof <p>Show that the partial derivatives can be the limit of divided difference.</p> <p>Corollary 1</p> <p>Assume \\(\\pmb{f}(x,\\pmb{y})\\in C(R)\\) where</p> \\[ R=\\{(x,y): |x-x_0|\\leq a, |\\pmb{y}-\\pmb{y}_0|\\leq b\\} \\] <p>and has partial derivative with respect to \\(\\pmb{y}\\), i.e. \\(\\pmb{f}'_{\\pmb{y}}(x,\\pmb{y})\\). Then for all \\(\\pmb{\\eta}\\), s.t. \\(|\\pmb{\\eta}-\\pmb{y}_0|&lt;\\frac{b}{2}\\), initial problem </p> \\[ \\frac{d\\pmb{y}}{dx}=\\pmb{f}(x,\\pmb{y}), \\quad \\pmb{y}(x_0)=\\pmb{\\eta} \\] <p>has solution \\(\\pmb{y}=\\pmb{\\phi}(x,\\pmb{\\eta})\\) is conntinuous on region </p> \\[ D=\\left\\{(x,y): |x-x_0|\\leq \\frac{h}{2}, |\\pmb{\\eta}-\\pmb{y}_0|\\leq \\frac{b}{2}\\right\\} \\] <p>Corollary 2</p> <p>Assume \\(\\pmb{f}(x,\\pmb{y})\\in C(G)\\) were</p> \\[ G=\\{(x,y): |x-x_0|\\leq a, |\\pmb{y}-\\pmb{y}_0|\\leq b,|\\pmb{\\lambda}|\\leq c\\} \\] <p>and has partial derivative with respect to \\(\\pmb{y}\\) and \\(\\pmb{\\lambda}\\). Then initial problem</p> \\[ \\begin{align} \\frac{d\\pmb{y}}{dx}=\\pmb{f}(x,\\pmb{y},\\pmb{\\lambda}), \\quad \\pmb{y}(x_0)=\\pmb{y}_0\\label{eq3} \\end{align} \\] <p>has unique solution \\(\\pmb{y}=\\pmb{\\phi}(x;x_0;\\pmb{y}_0,\\pmb{\\lambda})\\) which is differentiable for \\((x_0,\\pmb{y}_0,\\pmb{\\lambda})\\) on region</p> \\[ R=\\{|x-x_0|\\leq, |\\pmb{\\lambda}|\\leq c\\} \\]"},{"location":"Ctrl/Ordinary_Differential_Equation/General_Theory/Continuous_Dependence/#solve-for-derivatives","title":"Solve for Derivatives","text":"<p>Assume \\(\\pmb{y}=\\pmb{\\phi}(x;x_0,\\pmb{y}_0,\\pmb{\\lambda})\\) is the solution to ODE \\(\\ref{eq3}\\), then it satisfies the integral equation</p> \\[ \\begin{equation} \\pmb{\\phi}(x;x_0,\\pmb{y}_0,\\pmb{\\lambda})=\\pmb{y}_0+\\int_{x_0}^x \\pmb{f}(s,\\pmb{\\phi}(x;x_0,\\pmb{y}_0,\\pmb{\\lambda}),\\pmb{\\lambda})ds \\label{eq4} \\end{equation} \\] <p>Take partial derivative on both sides with respect to \\(x_0\\), \\(\\pmb{y}_0\\), \\(\\pmb{\\lambda}\\), we get</p> \\[ \\begin{align*} \\frac{\\partial \\pmb{\\phi}}{\\partial x_0}&amp;= -\\pmb{f}(x_0,\\pmb{y}_0,\\pmb{\\lambda}) + \\int_{x_0}^x \\frac{\\partial }{\\partial \\pmb{y}}\\pmb{f}(s,\\pmb{\\phi}(x;x_0,\\pmb{y}_0,\\pmb{\\lambda}),\\pmb{\\lambda}) \\frac{\\partial \\pmb{\\phi}}{\\partial x_0}ds\\\\ \\frac{\\partial \\pmb{\\phi}}{\\partial \\pmb{y}_0}&amp;=\\pmb{E}_n+ \\int_{x_0}^x \\frac{\\partial }{\\partial \\pmb{y}}\\pmb{f}(s,\\pmb{\\phi}(x;x_0,\\pmb{y}_0,\\pmb{\\lambda}),\\pmb{\\lambda}) \\frac{\\partial \\pmb{\\phi}}{\\partial \\pmb{y}_0}ds\\\\ \\frac{\\partial \\pmb{\\phi}}{\\partial \\pmb{\\lambda}}&amp;=\\int_{x_0}^x \\left(\\frac{\\partial }{\\partial \\pmb{y}}\\pmb{f}(s,\\pmb{\\phi}(x;x_0,\\pmb{y}_0,\\pmb{\\lambda}),\\pmb{\\lambda}) \\frac{\\partial \\pmb{\\phi}}{\\partial \\pmb{\\lambda}}+\\frac{\\partial }{\\partial \\pmb{\\lambda}}\\pmb{f}(s,\\pmb{\\phi}(x;x_0,\\pmb{y}_0,\\pmb{\\lambda}),\\pmb{\\lambda}) \\right)ds \\end{align*} \\] <p>Or to be more concise:</p> \\[ \\begin{align*} \\frac{\\partial \\pmb{\\phi}}{\\partial x_0}&amp;= -\\pmb{f}(x_0,\\pmb{y}_0,\\pmb{\\lambda}) + \\int_{x_0}^x \\frac{\\partial \\pmb{f}}{\\partial \\pmb{y}}\\frac{\\partial \\pmb{\\phi}}{\\partial x_0}ds\\\\ \\frac{\\partial \\pmb{\\phi}}{\\partial \\pmb{y}_0}&amp;=\\pmb{E}_n+ \\int_{x_0}^x \\frac{\\partial \\pmb{f}}{\\partial \\pmb{y}} \\frac{\\partial \\pmb{\\phi}}{\\partial \\pmb{y}_0}ds\\\\ \\frac{\\partial \\pmb{\\phi}}{\\partial \\pmb{\\lambda}}&amp;=\\int_{x_0}^x \\left(\\frac{\\partial \\pmb{f}}{\\partial \\pmb{y}} \\frac{\\partial \\pmb{\\phi}}{\\partial \\pmb{\\lambda}}+\\frac{\\partial }{\\partial \\pmb{\\lambda}}\\pmb{f} \\right)ds \\end{align*} \\] <p>ODE for Derivatives</p> <p>If we let</p> \\[ \\begin{align*} \\pmb{u}(x;x_0,\\pmb{y}_0,\\pmb{\\lambda}) &amp;= \\frac{\\partial \\pmb{\\phi}}{\\partial x_0}\\\\ \\pmb{V}(x;x_0,\\pmb{y}_0,\\pmb{\\lambda}) &amp;= \\frac{\\partial \\pmb{\\phi}}{\\partial \\pmb{y}_0}\\\\ \\pmb{W}(x;x_0,\\pmb{y}_0,\\pmb{\\lambda}) &amp;= \\frac{\\partial \\pmb{\\phi}}{\\partial \\pmb{\\lambda}_0}\\\\ \\pmb{A}(x;x_0,\\pmb{y}_0,\\pmb{\\lambda}) &amp;= \\frac{\\partial \\pmb{f}}{\\partial \\pmb{y}}(x,\\pmb{\\phi},\\pmb{\\lambda})\\\\ \\pmb{B}(x;x_0,\\pmb{y}_0,\\pmb{\\lambda}) &amp;= \\frac{\\partial \\pmb{f}}{\\partial \\pmb{\\lambda}}(x,\\pmb{\\phi},\\pmb{\\lambda})\\\\ \\end{align*} \\] <p>then \\(\\pmb{u}\\), \\(\\pmb{V}\\) and \\(\\pmb{W}\\) satisfy ODE with initial value respectively</p> \\[ \\begin{align*} \\frac{d\\pmb{u}}{dx}&amp;=\\pmb{A}(x;x_0,\\pmb{y}_0,\\pmb{\\lambda})\\pmb{u},\\quad \\pmb{u}(x_0)=-\\pmb{f}(x_0,\\pmb{y}_0,\\pmb{\\lambda})\\\\ \\frac{d\\pmb{V}}{dx}&amp;=\\pmb{A}(x;x_0,\\pmb{y}_0,\\pmb{\\lambda})\\pmb{V},\\quad\\pmb{V}(x_0)=\\pmb{E}_0\\\\ \\frac{d\\pmb{W}}{dx}&amp;=\\pmb{A}(x;x_0,\\pmb{y}_0,\\pmb{\\lambda})\\pmb{W}+\\pmb{B}(x;x_0,\\pmb{y}_0,\\pmb{\\lambda}),\\quad \\pmb{W}(x_0)=0 \\end{align*} \\] <p>If \\(\\pmb{y}\\) and \\(\\pmb{\\lambda}\\) is one dimension, then the above ODE becomes much simpler. And we can use method from Elementary Integration Method.</p> <p>Q1. Assume \\(\\phi(x;x_0,y_0,\\mu)\\) is the solution to ODE</p> \\[ y'=y+\\mu(x^2+y^2),\\quad y|_{x=x_0}=y_0 \\] <p>Solve \\(\\frac{\\partial y}{\\partial \\mu}\\) when \\(x_0=0\\), \\(y_0=1\\), \\(\\mu=0\\).</p> Answer <p>Write the integral form</p> \\[ \\phi(x;x_0,y_0,\\mu)=y_0+\\int_{x_0}^x(\\phi(s;x_0,y_0,\\mu)+\\mu(s+\\phi(s;x_0,y_0,\\mu)^2))ds  \\] <p>we only show the dependence of variables:</p> \\[ \\phi(\\mu)=y_0+\\int_{x_0}^x(\\phi(\\mu)+\\mu(s+\\phi(\\mu)^2))ds  \\] <p>take derivative of \\(\\mu\\), we get</p> \\[ \\frac{\\partial \\phi}{\\partial \\mu}=\\int_{x_0}^x\\left(\\frac{\\partial\\phi}{\\partial \\mu}+s+\\phi(\\mu)^2+2\\phi\\mu\\frac{\\partial \\phi}{\\partial \\mu}\\right)ds \\] <p>So function \\(\\frac{\\partial\\phi}{\\partial \\mu}\\) satisfies ODE</p> \\[ \\begin{equation} u'=x+\\phi^2+(1-2\\mu\\phi)u, \\quad u(0)=0 \\label{mu} \\end{equation}\\] <p>For initial value, we can solve the solution to ODE </p> \\[ \\phi=e^x \\] <p>substitute initial value and solution into equation \\(\\ref{mu}\\) and we get</p> \\[ u'=x+e^{2x}+u,\\quad u(0)=0 \\] <p>so we get </p> \\[ \\begin{align*} u &amp;= e^x\\left[\\int_{0}^x(s+e^{2s})e^{-s}ds\\right]\\\\ &amp;=e^{2x}-x-1 \\end{align*} \\]"},{"location":"Ctrl/Ordinary_Differential_Equation/General_Theory/Contraction_Mapping_Method/","title":"Contraction Mapping Method","text":"<p>Reference</p> <p>Ordinary Differential Equation, Vladimir Igorevich Arnold</p> <p>\u300a\u5e38\u5fae\u5206\u65b9\u7a0b\u300b \u041b.\u0421.\u5e9e\u7279\u91cc\u4e9a\u91d1</p>"},{"location":"Ctrl/Ordinary_Differential_Equation/General_Theory/Contraction_Mapping_Method/#proof-using-contraction-mapping","title":"\u538b\u7f29\u6620\u5c04\u6cd5 | Proof using Contraction Mapping","text":"<p>This part we want to prove Picard Theorem from another perspective. And this method is also universal on multi-dimensional Cauchy Problem. So to simplify the notation, we change the problem equivalently to </p> \\[ \\begin{equation} \\dot{\\mathbfit{x}} = \\mathbfit{f}(t, \\mathbfit{x}),\\quad  \\mathbfit{x}(t_0) = \\mathbfit{x}_0 \\label{eq-vector-cauchy} \\end{equation} \\] <p>where \\(\\mathbfit{x}\\in \\mathbb{R}^n\\) and \\(\\mathbfit{f}: \\mathbb{R}^{n+1} \\mapsto \\mathbb{R}^n \\in C(D)\\) is a vector function(or vector field). To simplify the problem, we assume \\(\\pmb{f} \\in C^r(D)\\), where \\(r\\geq 1\\).</p> <p>Assume we give a Euvlid Structure in region \\(D\\in \\mathbb{R}^{n+1}\\). For all \\((t_0,\\pmb{x}_0)\\in D\\), we consider a cylinder(\u67f1\u4f53) with sufficiently small parameters \\(a\\) and \\(b\\)</p> \\[ \\Gamma= \\{(t, \\pmb{x}): |t-t_0|\\leq a, \\|\\pmb{x}-\\pmb{x}_0\\|\\leq b\\} \\] <p>which is still a subset of \\(D\\).</p> <p>Then the Picard Theorem becomes:</p> <p>Picard Theorem</p> <p>Assume \\(\\pmb{f}\\) of problem \\(\\ref{eq-vector-cauchy}\\) is continous and differentiable(or Lipschitz condition) on region \\(\\Gamma\\), then for all given \\(\\pmb{x}\\) which is sufficiently close to \\(\\pmb{x}_0\\), there exsits a neighberhood of \\(t_0\\), such that there exists a unique solution \\(\\pmb{\\varphi}(t)\\).</p> Hints <ul> <li>Using the fixed point theorem (existence and uniqueness).</li> </ul>"},{"location":"Ctrl/Ordinary_Differential_Equation/General_Theory/Contraction_Mapping_Method/#group-basis","title":"\u7fa4\u8bba\u57fa\u7840 | Group Basis","text":"<p>Firstly, let us introduce some basic ideas about groups.</p> <p>\u7fa4\u7684\u5b9a\u4e49 | Definition of Group</p> <p>Firstly, we have to define Law of Composition.</p> <p>Assume \\(S\\) is a set. A Law of Composition is a map </p> \\[ S\\times S \\mapsto S. \\] <p>\\(S\\times S\\) deontes the product set, whose elements are pairs \\(a\\), \\(b\\) of elements of \\(S\\).</p> <p>A group is a set \\(G\\) together with a law of composition(Here we use sign \\(\\cdot\\) (multiplicative notation), and sign \\(+\\) (additive notation) also can be applied) that has the following properties:</p> <ul> <li>The law of composition is associative.</li> </ul> \\[ (ab)c=a(bc), \\quad \\forall a,b,c\\in G \\] <ul> <li>\\(G\\) contains an identity element \\(1\\) (\\(0\\) in sign \\(+\\)) such that </li> </ul> \\[ 1a=a \\text{ and } a1=a,\\quad \\forall a\\in G \\] <ul> <li>Every element \\(a\\) of \\(G\\) has an inverse, i.e. an element \\(b\\) such that </li> </ul> \\[ ab=1 \\text{ and } ba=1 \\] <p>which is denoted by \\(a^{-1}\\) (\\(-a\\) in additive notation).</p> <p>\u7fa4\u7684\u6027\u8d28 | Properties of Group</p> <ul> <li>Cancellation Law</li> </ul> <p>Let \\(a\\), \\(b\\), \\(c\\) be elements of a group \\(G\\) whose law of composition is written multiplicatively. If </p> \\[ ab=ac \\text{ or } ba = ca \\Rightarrow b=c. \\] <p>If </p> \\[ ab=a \\text{ or } ba =a \\Rightarrow b=1. \\] Proof <p>Multiply both sides of the above equation on the left by \\(a^{-1}\\).</p> <p>Example.</p> <p>\\(n\\times n\\) general liear group is the group of all invertible \\(n\\times n\\) matrices, denoted by</p> \\[ GL_{n} =\\{n\\times n \\text{ invertible matrices } A\\}. \\] <p>where the law of composition is matrix multiplication.</p> <p>\u540c\u6001 | Homomorphism</p> <p>Let \\(G\\) and \\(G'\\) be groups written with multiplicative notation. A Homomorphism \\(\\varphi:G\\mapsto G'\\) is a map from \\(G\\) to \\(G'\\) such that </p> \\[ \\varphi(ab) = \\varphi(a)\\varphi(b), \\quad \\forall a,b\\in G \\] <p>If we let \\(\\varphi\\) to be a bijection, then we call it Isomorphism(\u540c\u6784).</p> <p>Example.</p> <p>the determinent function det: \\(GL_n(\\mathbb{R})\\mapsto \\mathbb{R}^\\times\\)</p> <p>\u540c\u6001\u7684\u6027\u8d28 | Properties of Homomorphism</p> <p>Let \\(\\varphi: G\\mapsto G'\\) be a group homomorphism.</p> <p>(i) If \\(a_1,a_2,\\cdots, a_k\\) are elements of \\(G\\), then </p> \\[ \\varphi(a_1 a_2\\cdots a_k)=\\varphi(a_1)\\varphi(a_2)\\cdots\\varphi(a_n) \\] <p>(ii) \\(\\varphi\\) maps the identity to identity, i.e. </p> \\[ \\varphi(1_G)=1_{G'} \\] <p>(iii) \\(\\varphi\\) maps inverses to inverses, i.e.</p> \\[ \\varphi(a^{-1}) = \\varphi(a)^{-1} \\] Proof <p>(i) by induction(Strong induction).</p> <p>(ii) using \\(1\\cdot 1=1\\) and \\(\\varphi(1)\\varphi(1)=\\varphi(1\\cdot 1)=\\varphi(1)\\), cancel both sides to obtain and get \\(\\varphi(1)=1_{G'}\\).</p> <p>(iii) similarly, \\(a^{-1}a=1\\) and \\(\\varphi(a^{-1})\\varphi(a)=\\varphi(a^{-1}a)=\\varphi(1_G)=1_{G'}\\), and we are done.</p>"},{"location":"Ctrl/Ordinary_Differential_Equation/General_Theory/Contraction_Mapping_Method/#phase-space","title":"\u76f8\u7a7a\u95f4 | Phase Space","text":"<p>Then we have to introduce some comception about phase.</p> <p>\u5355\u53c2\u6570\u53d8\u6362\u7fa4 | One-Parameter Group of Transformation</p> <p>\\(M\\) is a set. A family of mapping \\(\\{g^t\\}_{t\\in \\mathbb{R}}\\) which maps set \\(M\\) into itself is called One-Parameter Group of Transformation of set \\(M\\), if </p> \\[ g^{t+s} = g^tg^s, \\quad \\forall t,s\\in \\mathbb{R} \\] <p>and \\(g^0\\) is identity mapping(\u6052\u7b49\u6620\u5c04).</p> <p>\u76f8\u6d41\u3001\u76f8\u7a7a\u95f4\u3001\u8fd0\u52a8\u3001\u76f8\u66f2\u7ebf\u7684\u5b9a\u4e49 | Definition of Phase Flow</p> <p>A couple composed of set \\(M\\) and its one-parameter group of transformation \\(\\{g^t\\}\\), denoted as \\((M, \\{g^t\\})\\), is called Phase Flow. And here \\(M\\) is called the phase space of the phase flow. The element of \\(M\\) is called Phase Point.</p> <p>Consider a mapping </p> \\[ \\begin{equation} \\varphi: \\mathbb{R}\\mapsto M, \\quad \\varphi(t) = g^tx,\\quad x\\in M \\label{map-motion} \\end{equation} \\] <p>which maps a real straight line into phase space. Then it is called a motion of phase point \\(x\\) under the action of phase flow. </p> <p>The image of \\(\\mathbb{R}\\) under mapping \\(\\varphi\\) is called the phase curve of phase flow \\((M,\\{g^t\\})\\).</p> <ul> <li>Fixed Point</li> </ul> <p>If the phase curve of a phase point \\(x\\in M\\) is itself, i.e.</p> \\[ g^tx=x,\\quad \\forall t\\in \\mathbb{R} \\] <p>then \\(x\\) is called the fixed point of the phase flow \\((M, \\{g^t\\})\\).</p> <p>In fact one-parameter group of transformation is exchangeable(\\(g^tg^s=g^{t+s}=g^{s+t}=g^sg^t\\)). And it is also a bijection. This is easy to prove. </p> <p>Firstly we prove it is surjection. \\(\\forall x\\in M\\), \\(\\exists g^{-t}x \\in M\\), such that \\(g^t(g^{-t}x)=x\\). Then we prove it is a injective mapping. \\(g^tx=g^ty\\), then \\(x=g^0x=g^{-t}g^tx=g^{-t}g^ty=g^0y=y\\).</p> <p>With the above property we can easily see the following theorem.</p> <p>\u76f8\u7a7a\u95f4\u4e2d\u7684\u70b9\u4ec5\u6709\u4e00\u6761\u76f8\u66f2\u7ebf</p> <p>For all \\(x\\in M\\), there only exists one phase curve.</p> Proof <p>Because \\(g^t\\) is a bijection.</p> <p>Now we introduce two important conceptions.</p> <p>\u6620\u5c04\u7684\u56fe\u5f62 | Graph of a mapping</p> <p>The graph of a mapping \\(f : A\\mapsto B\\) is a subset of the direct product(\u76f4\u79ef) \\(A\\times B\\):</p> \\[ \\{(a,f(a))| a\\in A\\} \\] <p>\u6269\u5f20\u76f8\u7a7a\u95f4\u3001\u79ef\u5206\u66f2\u7ebf | Expanded Phase Space, Integral Curve</p> <p>The Expanded Phase Space of phase flow \\((M, \\{g^t\\})\\) is the direct product \\(\\mathbb{R} \\times M\\).</p> <p>The Integral Curve of phase flow \\((M, \\{g^t\\})\\) is the graph of the motion \\(\\ref{map-motion}\\).</p> <p>Now we have to make use of DIfferential in Euclid Space.</p> <p>\u53ef\u5fae\u51fd\u6570\u3001\u53ef\u5fae\u6620\u5c04\u3001\u5fae\u5206\u540c\u80da | Differentiable Function, Differentiable Mapping, Diffeomorphism</p> <p>Assume \\(U \\subset\\mathbb{R}^n, V \\subset\\mathbb{R}^m\\). Then</p> <p>A Differentiable Function is a function \\(f: U\\mapsto \\mathbb{R}\\) which is \\(r\\) times differentiable.</p> <p>A Differentiable Mapping is a mapping \\(f:U\\mapsto V\\) defined by </p> \\[ y_i = f_i(x_1,x_2,\\cdots,x_n), \\quad i=1,2\\cdots, m \\] <p>where \\(f_i: U\\mapsto \\mathbb{R}\\) is a Differentiable function. If \\(y_i: V\\mapsto \\mathbb{R}\\) is a coordinate of \\(\\pmb{y}\\in \\mathbb{R}^m\\), then \\(y_i\\circ f: U\\mapsto \\mathbb{R}\\) is a Differentiable function in \\(U\\).</p> <p>A Diffeomorphism is a bijection \\(f:U\\mapsto V\\), such that \\(f\\) and \\(f^{-1}\\) are both Differentiable mappings.</p> <p>\u4e0e\u5750\u6807\u8f74\u6709\u5173\u7684\u76f8\u901f\u5ea6\u3001\u5411\u91cf\u573a | Phase Speed, Vector Field</p> <p>The Phase Speed \\(\\pmb{v}(x)\\) of phase flow \\(g^t\\) at point \\(x\\in M\\) is </p> \\[ \\pmb{v}(x) = \\frac{d}{dt}\\Bigg|_{t=0}g^tx \\] <p>And at time \\(\\tau\\), we have phase speed </p> \\[ \\pmb{v}(g^\\tau x) = \\frac{d}{dt}\\Bigg|_{t=\\tau}g^tx. \\] <p>Now we let \\(M\\) to be a region in Euclid Space \\(\\mathbb{R}^n\\) with coordinates \\(x_1,x_2,\\cdots,x_n\\). And if \\(x_i:M\\mapsto \\mathbb{R}\\) is the coordinate of \\(\\pmb{x}\\in M\\), then the vector \\(\\pmb{v}(x)\\) is defined by \\(v_i: M\\mapsto \\mathbb{R}, i=1,2\\cdots, n\\):</p> \\[ v_i(\\pmb{x}) = \\frac{d}{dt}\\Bigg|_{t=0}x_i(g^t\\pmb{x}) \\] <p>So we define a Vector Field \\(\\pmb{v}\\) on \\(M\\) when neglecting \\(t\\).</p> <p>\u4e0e\u5750\u6807\u8f74\u65e0\u5173\u7684\u76f8\u901f\u5ea6\u3001\u5411\u91cf\u573a | Phase Speed, Vector Field</p> <p>Firstly, we have to define that a coordinate \\(\\{y_i\\}_{i=1}^{n}:U\\mapsto \\mathbb{R}\\) is admissive, if mapping</p> \\[ y:U\\mapsto \\mathbb{R}^n,\\quad y(\\pmb{x}) = y_1(\\pmb{x})\\pmb{e}_1 + y_2(\\pmb{x})\\pmb{e}_2 + \\cdots +y_n(\\pmb{x})\\pmb{e}_n \\] <p>is a diffeomorphism.</p> <p>So we have a proposition:</p> <p>Two curves \\(\\varphi_1\\), \\(\\varphi_2\\) that pass \\(\\pmb{x}\\in U\\) are tangent with each other if and only if two curves \\(y\\circ \\varphi_1\\), \\(y\\circ\\varphi_2\\) that pass \\(y(\\pmb{x})\\in V\\) are tangent with each other.</p> <p><p> </p></p> <p>So the velocity vector of curve \\(\\varphi:I\\mapsto U\\) that pass \\(\\pmb{x}\\in U\\) is </p> \\[ \\pmb{v}=\\dot{\\varphi}(0), \\quad \\pmb{v}=\\frac{d\\varphi}{dt}\\Bigg|_{t=0}. \\] <p>\u5207\u5411\u91cf\u3001\u5207\u7a7a\u95f4 | Tangent Vector, Tangent Space</p> <p>Assume \\(U\\in \\mathbb{R}^n\\) with coordinates \\(x_1,x_2,\\cdots, n\\) and map \\(\\varphi:I\\mapsto U\\) maps an interval on \\(\\mathbb{R}\\) to \\(U\\) such that \\(\\varphi(0)=\\pmb{x}\\in U\\). and also its velocity is determined by </p> \\[ v_i = \\frac{d}{dt}\\Bigg|_{t=0}(x_i\\circ \\varphi), i=1,2,\\cdots, n. \\] <p>Two curves \\(\\varphi_1, \\varphi_2:I\\mapsto U\\) pass the same point \\(\\pmb{x}\\) are tangent with each other if \\(t\\rightarrow 0\\), \\(\\rho(\\varphi_1, \\varphi_2)\\rightarrow 0\\).</p> <p><p> </p></p> <p>A set composed by all tangent vectors of the curves passing \\(\\pmb{x}\\) is a linear space with dimension \\(n\\), which is called Tangent Space, denoted by \\(TU_x\\).</p> <p>Now we want to give a space without coordinates.</p> <p>\u6620\u5c04\u7684\u5bfc\u6570 | Derivative of Mapping</p> <p>Assume that \\(f:U\\mapsto V\\) is a differentiable mapping from \\(\\pmb{x}=(x_1,x_2,\\cdots,x_n)\\) to \\(\\pmb{y}=(y_1,y_2,\\cdots,y_m)\\). Let \\(\\pmb{y}=f(\\pmb{x})\\in V\\).</p> <p>The Derivative of mapping \\(f\\) at \\(\\pmb{x}\\) is a mapping from tangent space at \\(\\pmb{x}\\in U\\) to tangent space at \\(\\pmb{y}\\in V\\)</p> \\[ f_*|_{\\pmb{x}}: TU_{\\pmb{x}} \\mapsto TV|_{f(\\pmb{x})} \\] <p>This mapping maps velocity vector of curve \\(\\varphi\\) at \\(\\pmb{x}\\) into velocity vector of \\(f\\circ \\varphi\\) at \\(f(\\pmb{x})\\), i.e.</p> \\[ f_*|_{\\pmb{x}}\\left(\\frac{d\\varphi}{dt}\\Bigg|_{t=0} \\right) = \\frac{d}{dt}\\Bigg|_{t=0}(f\\circ \\varphi). \\] <p>which defines a linear mapping from \\(TU_{\\pmb{x}}\\) to \\(TV_{f(\\pmb{x})}\\).</p>"},{"location":"Ctrl/Ordinary_Differential_Equation/General_Theory/Contraction_Mapping_Method/#contraction-mapping-in-metric-space","title":"\u5ea6\u91cf\u7a7a\u95f4\u4e0b\u7684\u538b\u7f29\u6620\u5c04 | Contraction Mapping in Metric Space","text":"<p>Then, let us recall the definition of metric space(\u5ea6\u91cf\u7a7a\u95f4).</p> <p>\u5ea6\u91cf\u7a7a\u95f4\u7684\u5b9a\u4e49 | Definition of Metric Space</p> <p>Assume \\(M\\) is a set. If there is a function \\(\\rho: M\\times M\\mapsto \\mathbb{R}\\) defined on it, satisfies that, \\(\\forall x,y,z \\in M\\)</p> <ul> <li>Positive</li> </ul> \\[ \\rho(x,y)&gt;0,\\quad x\\neq y \\] <ul> <li>Definite</li> </ul> \\[ \\rho(x, y)=0 \\Leftrightarrow x=y \\] <ul> <li>Symmetry</li> </ul> \\[ \\rho(x,y) = \\rho(y,x) \\] <ul> <li>Triangle inequality</li> </ul> \\[ \\rho(x,z) \\leq \\rho(x,y) +\\rho(y,z) \\] <p>then \\(M\\) is called metric space with metrc \\(\\rho\\).</p> <p>Now we can rewrite the Lipschitz Condition with terms of metric space.</p> <p>Lipschitz \u6761\u4ef6 | Lipschitz Condition</p> <p>Assume \\(A: M_1 \\mapsto M_2\\) is a mapping from a metric space \\(M_1\\) (with metric \\(\\rho_1\\)) to another matric space \\(M_2\\) (with metric \\(\\rho_1\\)). If there exists a constant \\(L&gt;0\\), s.t.</p> \\[ \\rho_2(Ax,Ay)\\leq L\\rho_1(x,y),\\quad \\forall x,y \\in M_1 \\] <p>then we say mapping \\(A\\) satisfies Lipschitz Condition.</p> <p>In Proof 1 we will show how to use Lipschitz condition to shrink the region of \\((t, \\pmb{x})\\)(Expanded Phase Space) such that the contraction mapping maps the metric space into itself.</p> <p>Usually, a mapping that maps a space \\(M\\) into itself would be of great use for us, so here comes the following definition.</p> <p>\u538b\u7f29\u6620\u5c04\u7684\u5b9a\u4e49 | Definition of Contraction mapping</p> <p>Assume \\(A: M \\mapsto M\\) is a mapping from complete metric space with metric \\(\\rho\\) to itself. If there exists a constant \\(k\\), \\(0&lt;k&lt;1\\), s.t. </p> \\[ \\rho(Ax,Ay)\\leq k\\rho(x, y),\\quad \\forall x, y\\in M. \\] <p>then \\(A\\) is called a Contraction mapping.</p> <p>If \\(Ax =x \\in M\\), we call \\(x\\) is a fixed point of mapping \\(A\\). Then the following theorem is quite useful in our future proof. </p> <p>Banach\u4e0d\u52a8\u70b9\u5b9a\u7406 | Banach Fixed-Point Theorem</p> <p>(This also known as contraction mapping theorem.)</p> <p>Assume \\(A: M\\mapsto M\\) is a contraction mapping, then \\(A\\) has a unique fixed-point.</p> HintsProof <p>We prove by showing that \\(A^nx\\) is a Cauchy Sequence and its limit \\(X\\) falls in \\(M\\). Then use the property of limits to show it is fixed-point of \\(A\\). Then prove the uniqueness by using property of metric \\(\\rho\\).</p> <p>Let \\(d = \\rho(x,Ax)\\), then </p> \\[ \\rho(A^nx,A^{n+1}x)\\leq k^n\\rho(x,Ax) = k^nd \\] <p>Sequence \\(A^nx\\), \\(n=1,2,\\cdots\\) is convergent. So its limit exists and let \\(X \\overset{\\Delta}{=} \\lim\\limits_{n\\rightarrow \\infty}A^nx\\).</p> <p>So </p> \\[ AX = A \\lim\\limits_{n\\rightarrow \\infty}A^nx = \\lim\\limits_{n\\rightarrow \\infty}A^{n+1}x = X \\] <p>which means \\(X\\) is a fixed-point of \\(A\\). Then we prove the uniqueness of fixed point. Assume there are two fixed points of \\(A\\), denoted as \\(X\\), \\(Y\\). See that</p> \\[ \\rho(X, Y)  =\\rho(AX, AY) \\leq k\\rho(X,Y) \\] <p>Because \\(0&lt;k&lt;1\\), so \\(\\rho(X,Y)=0\\) \\(\\Rightarrow\\) \\(X=Y\\).</p> <p>Readers can compare this theorem to Fixed-Point Theorem in Numerical Analysis. </p> <p>Note that we have an abstract set \\(M\\) with abstract metric \\(\\rho\\). In the next two parts, we will let \\(M\\) to be a set of mappings and \\(\\rho\\) to be defined based on natural norm of vectors. </p>"},{"location":"Ctrl/Ordinary_Differential_Equation/General_Theory/Contraction_Mapping_Method/#Proof-1","title":"\u8bc1\u660e1 | Proof 1","text":"<p>Now we consider Picard mapping \\(A\\) is a mapping from a mapping to another mapping</p> \\[ A: (\\pmb{\\varphi}: t\\mapsto \\pmb{x}) \\mapsto (A\\pmb{\\varphi}: t\\mapsto \\pmb{x}) \\] <p>defined by</p> \\[ (A\\pmb{\\varphi})(t) = \\pmb{x}_0 + \\int_{t_0}^{t}\\pmb{f}(\\tau, \\pmb{\\varphi}(\\tau))d\\tau \\] <p>Geometrically speaking, tangent line of each point on \\(A\\pmb{\\varphi}\\), i.e. \\((A\\pmb{\\varphi})'(t)\\), equals to the vector field determined by \\(\\pmb{\\varphi}(t)\\), i.e. \\(f(t, \\pmb{\\varphi}(t))\\). This can help us find a smaller region for \\(A\\pmb{\\varphi}\\) to be well-defined.</p> <p>And note that \\(\\pmb{\\varphi}\\) is a solution of Cauchy Problem \\(\\ref{eq-vector-cauchy}\\) if and only if \\(\\pmb{\\varphi} = A \\pmb{\\varphi}\\). So we have to prove \\(A\\) is a contraction mapping in a complete metric space.</p> <ul> <li>\u5b9a\u4e49\u57df\\((t, \\pmb{x})\\) | Some Constants \\(a\\), \\(b\\), \\(M\\)</li> </ul> <p>Let </p> \\[ M=\\max_{\\pmb{x}\\in \\Gamma}\\|\\pmb{f}\\|, \\quad L= \\max_{\\pmb{x}\\in \\Gamma}\\|\\pmb{f}_*\\| \\text{(or by Lipschitz condition)} \\] <p>which can be obtained because \\(\\Gamma\\) is a compact set. Firstly, we consider a Cylinder</p> \\[ \\Gamma = \\{(t,\\pmb{x}): |t-t_0|\\leq a, \\|\\pmb{x}-\\pmb{x}_0\\|\\leq b\\} \\] <p><p> </p></p> <p>Now consider a Cone(\u9525\u4f53) with opening \\(M\\) and sufficient small height \\(a\\) at point \\((t_0,\\pmb{x}_0)\\)</p> \\[ K_0 = \\{(t,\\pmb{x}): |t-t_0|\\leq a, \\|\\pmb{x}-\\pmb{x}_0\\|\\leq M|t-t_0|\\} \\] <p>Which means our mapping \\(\\pmb{\\varphi}\\) after contraction mapping \\(A\\) is still well-defined. See details in the following proof.</p> Proof \\[ \\begin{align*} \\|A\\pmb{\\varphi}(t) - \\pmb{x}_0\\| &amp;=\\left\\|\\int_{t_0}^t\\pmb{f}(\\tau,\\pmb{\\varphi}(\\tau))d\\tau\\right\\| \\\\ &amp;\\leq \\left|\\int_{t_0}^t\\|\\pmb{f}(\\tau,\\pmb{\\varphi}(\\tau))\\|d\\tau\\right|\\\\ &amp;\\leq M |t-t_0| \\end{align*} \\] <p>So as long as \\(a&lt;b/M\\), \\(\\|A\\pmb{\\varphi}(t)-\\pmb{x}_0\\|\\leq b\\).</p> <ul> <li>Prove \\(\\pmb{\\varphi}\\) compose a complete metric space.</li> </ul> <p>We define a Space \\(M\\) composed of mappings \\(\\pmb{\\varphi}: \\mathbb{R}\\mapsto \\mathbb{R}^n\\) defined by \\(\\pmb{\\varphi}(t)=\\pmb{x}\\).</p> <p>Define a metric \\(\\rho\\) to be</p> \\[ \\rho(\\pmb{\\varphi}_1, \\pmb{\\varphi}_2) = \\|\\pmb{\\varphi}_1-\\pmb{\\varphi}_2\\| = \\max_{|t-t_0|\\leq a}|\\pmb{\\varphi}_1(t)-\\pmb{\\varphi}_2(t)| \\] <p>Then \\(\\{M, \\rho\\}\\) is a complete metric space.</p> <ul> <li>Prove \\(A\\) is a contraction mapping.</li> </ul> Proof <p>That is, \\(\\forall \\pmb{\\varphi}_1, \\pmb{\\varphi}_2 \\in M\\)</p> \\[ \\begin{align*} \\|A\\pmb{\\varphi}_1- A\\pmb{\\varphi}_2\\| &amp;= \\left\\|\\int_{t_0}^t[\\pmb{f}(\\tau, \\pmb{\\varphi}_1(\\tau))-\\pmb{f}(\\tau, \\pmb{\\varphi}_2(\\tau))]d\\tau\\right\\|\\\\ &amp;\\leq \\left|\\int_{t_0}^t\\|\\pmb{f}(\\tau, \\pmb{\\varphi}_1(\\tau))-\\pmb{f}(\\tau, \\pmb{\\varphi}_2(\\tau))\\|d\\tau \\right|\\\\ &amp;\\leq L \\int_{t_0}^t \\|\\pmb{\\varphi}_1(\\tau) - \\pmb{\\varphi}_2(\\tau)\\|d\\tau \\\\ &amp;\\leq L \\|\\pmb{\\varphi}_1 - \\pmb{\\varphi}_2\\| \\int_{t_0}^t d\\tau = L |t-t_0| \\|\\pmb{\\varphi}_1 - \\pmb{\\varphi}_2\\|  \\end{align*} \\] <p>So as long as \\(a&lt;1/L\\), \\(A\\) is a contraction mapping.</p> <p>By Banach Fixed Point Theorem, there exists only one soluion of ODE.</p>"},{"location":"Ctrl/Ordinary_Differential_Equation/General_Theory/Contraction_Mapping_Method/#2-proof-2","title":"\u8bc1\u660e2 | Proof 2","text":"<p>In this part, we consider a different mapping.</p> <ul> <li>\u5b9a\u4e49\u57df\\((t, \\pmb{x})\\)</li> </ul> <p>In order to let all the possible cone to be in the cylinder, we have to shrink the region to make the region is well defined. That is, there exsits sufficiently small parameters \\(b'&lt;b\\) such that \\(\\|\\pmb{x}-\\pmb{x}_0\\|&lt;b'\\). So now we get a smaller region (a smaller cylinder)</p> \\[ \\Gamma' = \\{(t, \\pmb{x}): |t-t_0|&lt;a', \\|\\pmb{x}-\\pmb{x}_0\\|&lt;b'\\} \\] <p><p> </p></p> <p>We also have to prove that \\(A\\) maps \\(M\\) into itself. That is, \\(\\|\\varphi(t)-\\pmb{x}_0\\|\\leq b'\\). Here we have to make \\(a'\\) to satisfy some condition.</p> <p>Consider all the possible continuous mapping \\(\\pmb{h}\\) which maps the above cylinder into \\(\\mathbb{R}^n\\), which is a solution of original ODE problem. Assume \\(M\\) is a set composed of these mappings with additional condition</p> \\[ \\|\\pmb{h}(t, \\pmb{x})\\| \\leq C|t-t_0| \\] <p>Specially, we let \\(\\pmb{h}(t_0,\\pmb{x})=0\\). We introduce a metric </p> \\[ \\rho(\\pmb{h}_1,\\pmb{h}_2)=\\|\\pmb{h}_1-\\pmb{h}_2\\|=\\max_{(t,\\pmb{x})\\in \\Gamma' }\\|\\pmb{h}_1(t,\\pmb{x})-\\pmb{h}_2(t,\\pmb{x})\\| \\] <p> </p> <p>Note: Space \\(M\\) is dependent on \\(a'\\), \\(b'\\) and \\(C\\).</p> <p>Now we really have to consider a mapping </p> \\[ A: M\\mapsto M \\] <p>defined by</p> \\[ (A\\pmb{h})(t, \\pmb{x}) = \\int_{t_0}^t\\pmb{f}(\\tau, \\pmb{x}+\\pmb{h}(\\tau, \\pmb{x}))d\\tau \\] <p>\\(A\\) \u662f\u538b\u7f29\u6620\u5c04 | \\(A\\) is a contraction mapping</p> <p>\\(A\\) is a contraction mapping from \\(M\\) to \\(M\\), if \\(a'\\) is sufficiently small.</p> <p>Prove it. The following proof are set to find how small \\(a'\\) shoulc be.</p> HintsProof <p>Firstly, prove \\(A\\) maps \\(M\\) to itself. Then Prove if is a constraction mapping.</p> <ul> <li>\\(A\\) maps \\(M\\) to itself.</li> </ul> \\[ \\begin{align*} \\|(A\\pmb{h}(t, \\pmb{x}))\\| &amp;\\leq \\|\\int_{t_0}^t\\pmb{f}(\\tau, \\pmb{x}+\\pmb{h}(\\tau,\\pmb{x}))d\\tau\\| \\\\ &amp;\\leq |\\int_{t_0}^tCd\\tau| \\leq C|t-t_0| \\end{align*} \\] <p>so \\(AM\\subset M\\).</p> <ul> <li>\\(A\\) is a constraction mapping.</li> </ul> <p>Estimate \\(A\\pmb{h}_1- A\\pmb{h}_2\\):</p> \\[ (A\\pmb{h}_1-A\\pmb{h}_2)(t,\\pmb{x}) = \\int_{t_0}^t[\\pmb{f}(\\tau, \\pmb{x}+\\pmb{h}_1(\\tau,\\pmb{x}))-\\pmb{f}(\\tau, \\pmb{x}+\\pmb{h}_2(\\tau,\\pmb{x}))]d\\tau \\] <p>Denote \\(\\pmb{f}_i(\\tau) = \\pmb{f}(\\tau, \\pmb{x}+\\pmb{h}_i(\\tau,\\pmb{x}))\\), \\(i=1,2\\), so</p> \\[ \\begin{align*} \\|\\pmb{f}_1(\\tau)- \\pmb{f}_2(\\tau)\\| &amp;\\leq L\\|\\pmb{h}_1(\\tau)-\\pmb{h}_2(\\tau)\\| \\quad\\text{(using Lipschitz condition)}\\\\  &amp;\\leq L\\|\\pmb{h}_1-\\pmb{h}_2\\| = L\\rho(\\pmb{h}_1, \\pmb{h}_2) \\quad \\text{(by definition of metric)} \\end{align*} \\] <p>And </p> \\[ \\|(A\\pmb{h}_1-A\\pmb{h}_2)(t,\\pmb{x})\\| \\leq L a'\\rho(\\pmb{h}_1, \\pmb{h}_2) \\] <p>So if \\(La'&lt;1\\), then \\(A\\) is a contraction mapping.</p>"},{"location":"Ctrl/Ordinary_Differential_Equation/General_Theory/Contraction_Mapping_Method/#lipschitz-appendix-from-continuity-differentiability-to-lipschitz-condition","title":"\u9644\u5f55\uff1a\u8fde\u7eed\u53ef\u5fae\u5230Lipschitz\u8fde\u7eed | Appendix: from Continuity &amp; Differentiability to Lipschitz Condition","text":"<p>we will choose natural metric </p> \\[ \\rho(\\pmb{x}, \\pmb{y}) = \\|\\pmb{x}-\\pmb{y}\\| = \\sqrt{(\\pmb{x}-\\pmb{y},\\pmb{x}-\\pmb{y})} \\] <p>so space \\(\\mathbb{R}^n\\) with the above metric is a complete metric space.</p> <p>So in this case, we have a parallel theorem for smoothness and Lipschitz condition in \\(\\mathbb{R}\\).</p> <p>\u8fde\u7eed\u53ef\u5fae\u6620\u5c04\u6ee1\u8db3Lipschitz\u6761\u4ef6 | Continuously Differentiable mapping satisfies Lipschitz condition</p> <p>Assume \\(V\\subset U \\subset\\mathbb{R}^m\\) is a convex and contract set, and continuously differentiable mapping \\(\\pmb{f}\\) which maps \\(U\\) to \\(\\mathbb{R}^n\\) satisfies Lipschitz condition, and the constant </p> \\[ L = \\sup_{\\pmb{x}\\in V} \\|\\pmb{f}_{*\\pmb{x}}\\|  \\] <p>where \\(\\pmb{f}_*|_{\\pmb{x}}=\\pmb{f}_{*\\pmb{x}}:T\\mathbb{R}^m_{\\pmb{x}}\\mapsto T\\mathbb{R}^n_{\\pmb{x}}\\)</p> <p><p> </p></p> <p>Prove it.</p> Hints <p>Let \\(\\pmb{z}(t)=\\pmb{x}+t(\\pmb{y}-\\pmb{x})\\), \\(0\\leq t\\leq 1\\). Then</p> \\[ \\begin{align*} \\pmb{f}(\\pmb{y})-\\pmb{f}(\\pmb{x})&amp;=\\int_{0}^1\\frac{d}{dt}\\pmb{f}(\\pmb{z}(\\tau))d\\tau\\\\ &amp;=\\int_0^1\\pmb{f}_{*\\pmb{z}(\\tau)}[\\pmb{\\dot{z}}(\\tau)]d\\tau \\\\ &amp;=\\int_0^1 \\pmb{f}_{*\\pmb{z}(\\tau)} (\\pmb{y}-\\pmb{x})d\\tau \\end{align*}\\\\ \\] <p>where \\(\\pmb{y}-\\pmb{x}\\) is a constant, so</p> \\[ \\begin{align*} \\left\\| \\pmb{f}(\\pmb{y})-\\pmb{f}(\\pmb{x}) \\right\\| &amp;=\\left\\|\\int_0^1 \\pmb{f}_{*\\pmb{z}(\\tau)} (\\pmb{y}-\\pmb{x})(\\tau)d\\tau\\right\\|\\\\ &amp;\\leq \\int_0^1 \\|\\pmb{f}_{*\\pmb{x}}\\| \\|\\pmb{y}-\\pmb{x}\\|d\\tau\\\\ &amp;\\leq L\\|\\pmb{y}-\\pmb{x}\\| \\end{align*} \\] <p>and we are done.</p>"},{"location":"Ctrl/Ordinary_Differential_Equation/General_Theory/Existence_Uniqueness_Theorem/","title":"Existence & Uniqueness Theorem","text":""},{"location":"Ctrl/Ordinary_Differential_Equation/General_Theory/Existence_Uniqueness_Theorem/#Existence-and-Uniqueness-Theorem","title":"\u5b58\u5728\u552f\u4e00\u6027\u5b9a\u7406 | Existence and Uniqueness Theorem","text":"<p>This chapter we focus on ODE with initial value problem</p> \\[ \\begin{equation} \\frac{dy}{dx} = f(x, y), \\quad y(x_0) = y_0. \\label{eq-cauchy} \\end{equation} \\] <p>The above problem is also called Cauchy Problem, for Cauchy firstly prove that the solution to the problem \\(\\ref{eq-cauchy}\\) exists uniquely when \\(f(x, y)\\) has continuous partial derivative to \\(y\\), that is, \\(\\frac{\\partial f}{\\partial y} \\in C(G)\\).</p>"},{"location":"Ctrl/Ordinary_Differential_Equation/General_Theory/Existence_Uniqueness_Theorem/#preliminary-knowledge","title":"\u9884\u5907\u77e5\u8bc6 | Preliminary knowledge","text":"<p>The following inequation is really useful in estimating solution of ODE by offering the upper bound.</p> <p>Gronwall \u4e0d\u7b49\u5f0f | Gronwall Inequation</p> <p>Assume \\(\\alpha(x), u(x) \\in C[a, b]\\) are non-negative functions and \\(C, K\\) are non-negative constants. If </p> \\[ u(x) \\leq C + \\int_{a}^{x} \\left[ \\alpha(s) u(s)+K\\right] ds \\] <p>then </p> \\[ \\begin{align*} u(x) &amp;\\leq \\left[ C + \\int_{a}^{x}Ke^{-\\int_{a}^{s}\\alpha(t)dt}ds\\right]e^{\\int_{a}^{x}\\alpha(s)ds}\\\\ &amp;\\leq \\left[ C + K(x-a)\\right]e^{\\int_{a}^{x}\\alpha(s)ds} \\end{align*} \\] <p>Prove it.</p> Hints <p>Convert it into Ordinary Differential Inequation. Then solve it like what you do in solving ODE.</p> <p>\u4e00\u81f4\u6709\u754c\u7684\u5b9a\u4e49 | Definition of Uniform Bound</p> <p>Assume \\(\\Lambda\\) is an infinite set, set \\(I\\)(or interval \\([x, b]\\)) \\(\\subset \\mathbb{R}\\). A family of function \\(\\{f_\\lambda\\}_{\\lambda\\in\\Lambda}\\) defined on \\(I\\) is Uniformly Bounded, if \\(\\exists M&gt;0\\), s.t.</p> \\[ |f_\\lambda(x)|\\leq M, \\quad \\forall \\lambda\\in \\Lambda, \\forall x \\in I \\] <p>The above definition means that functions in \\(\\{f_\\lambda\\}_{\\lambda\\in\\Lambda}\\) are all bounded by some constant \\(M\\).</p> <p>\u7b49\u5ea6\u8fde\u7eed\u7684\u5b9a\u4e49 | Definition of Equicontinuous</p> <p>Assume \\(\\Lambda\\) is an infinite set, set \\(I\\)(or interval \\([x, b]\\)) \\(\\subset \\mathbb{R}\\). A family of function \\(\\{f_\\lambda\\}_{\\lambda\\in\\Lambda}\\) defined on \\(I\\) is Equicontinuous, if \\(\\forall \\varepsilon&gt;0, \\exists \\delta&gt;0\\), s.t. \\(\\forall x_1, x_2\\in I\\) and \\(|x_1-x_2|&lt;\\delta\\)</p> \\[ |f_\\lambda(x_1)-f_\\lambda(x_2)|&lt; \\varepsilon \\] <p>The above definition means that functions in \\(\\{f_\\lambda\\}_{\\lambda\\in\\Lambda}\\) are continuous equally. Usually \\(\\delta&gt;0\\) depends on \\(f_\\lambda\\), but here we mean the degree of continuity of these functions is similar.</p> <p>Example. Function sequence \\(\\{f_n(x)\\}\\) satisfying</p> \\[ f_n(x) = (-1)^{n} + x^n \\] <p>is uniformly bounded and equicontinuous on region \\(\\{x: |x|\\leq 1/2\\}\\), is uniformly bounded but not equicontinuous on region \\(\\{x: |x|\\leq 1\\}\\), and is neither uniformly bounded nor equicontinuous on region \\(\\{x: |x|\\leq 2\\}\\).</p> <p>The condition of the following theorem can be narrowed down to denumerable sets \\(\\Lambda\\) and interval \\(R \\subset \\mathbb{R}\\).</p> <p></p> <p>\u5f15\u74061: \u4e00\u81f4\u6709\u754c\u7684\u51fd\u6570\u5217\u6709\u6536\u655b\u51fd\u6570\u5b50\u5217(\u70b9\u6001) | Lemma 1: Uniformly Bounded Function Sequence has convergent Function Subsequence</p> <p>Assume set \\(\\Lambda\\) is denumerable. If a family of function \\(\\{f_\\lambda\\}_{\\lambda\\in\\Lambda}\\) defined on \\(I \\subset \\mathbb{R}\\) is uniformly bounded, then \\(\\forall A = \\{x_m\\}_{m=1}^{\\infty} \\subset I\\), \\(\\exists \\{f_{\\lambda_k}\\}_{k=1}^\\infty\\), a function subsequence of \\(\\{f_\\lambda\\}_{\\lambda\\in\\Lambda}\\), such that </p> \\[ \\{f_{\\lambda_k}(x)\\}, \\forall x \\in A \\] <p>is convergent.</p> <p>Prove it.</p> HintsProof <p>Using diagonal methods. </p> <p>Note that \\(\\{f_\\lambda(x_1)\\}_{\\lambda \\in \\Lambda}\\) is obviously bounded. So by Weierstrass Balzano Theorem, we get a convergent subsequence </p> \\[ f_{11}(x_1), f_{12}(x_1), \\cdots, f_{1n}(x_1) \\cdots \\] <p>which converges to \\(y_1\\).</p> <p>Now consider substitute \\(x_1\\) by \\(x_2\\), which becomes</p> \\[ f_{11}(x_2), f_{12}(x_2), \\cdots, f_{1n}(x_2) \\cdots \\] <p>According to the condition \"Uniformly Bounded\", the above sequence is also bounded, so we can find another convergent subsequence from it</p> \\[ f_{21}(x_2), f_{22}(x_2), \\cdots, f_{2n}(x_2) \\cdots \\] <p>which converges to \\(y_2\\).</p> <p>So we can say function subsequence of \\(\\{f_\\lambda(x_1)\\}_{\\lambda \\in \\Lambda}\\)</p> \\[ f_{21}(x), f_{22}(x), \\cdots, f_{2n}(x) \\cdots \\] <p>is convergent on \\(x\\in \\{x_1,x_2\\}\\). That is, \\(\\lim\\limits_{n\\rightarrow \\infty}f_{2n}(x_1) = y_1\\), \\(\\lim\\limits_{n\\rightarrow \\infty}f_{2n}(x_2) = y_2\\).</p> <p>Continue the above procedure, we can get another function subsequnce</p> \\[ f_{31}(x), f_{32}(x), \\cdots, f_{3n}(x) \\cdots \\] <p>which is convergent on \\(x \\in \\{x_1,x_2,x_3\\}\\). That is, \\(\\lim\\limits_{n\\rightarrow \\infty}f_{3n}(x_1) = y_1\\), \\(\\lim\\limits_{n\\rightarrow \\infty}f_{3n}(x_2) = y_2\\), \\(\\lim\\limits_{n\\rightarrow \\infty}f_{3n}(x_3) = y_3\\).</p> <p>Continue, it is not hard to imagine we get a function subsequence \\(\\{f_{nn}(x)\\}\\) which converges to \\(y(x)\\) on \\(x \\in \\{x_1,x_2,\\cdots, x_n\\}\\), with \\(y(x)\\) defined as </p> \\[ y(x_m) = y_m,\\quad  m=1,2,\\cdots \\] <p>So to express the subsequence more specificly, we can define \\(\\tilde{f}_n(x) = f_{nn}(x)\\), and get a subsequence \\(\\{\\tilde{f}_n(x)\\}_{n=1}^{\\infty}\\) such that </p> \\[ \\lim_{n\\rightarrow \\infty}\\tilde{f}_n(x) = y(x), \\forall x\\in I. \\] <p>And we are done.</p> <p>The above theorem offers that we can find a function sequence convergent on denumerable set \\(A\\), which is pointwise convergence.</p> <p>Now if we let the above function subsequence \\(\\{f_{\\lambda_k}(x)\\}\\) to be Equicontinuous, then it can be uniformly convergent on the whole interval \\(I\\). This is exactly the following theorem(in which \\(I\\) is a general region, not just an interval). Note that the above denumerable set can be said as \"dense set\".</p> <p></p> <p>\u5f15\u74062: \u70b9\u6001\u6536\u655b\u3001\u7b49\u5ea6\u8fde\u7eed\u7684\u51fd\u6570\u5217\u5728\u7d27\u96c6\u4e0a\u4e00\u81f4\u6536\u655b | Lemma 2: Equicontinuous Function Sequence is Uniformly Convergent given Pointwise Convergence on Denumerable Sets</p> <p>Assume \\(\\{f_n\\}_{n=1}^\\infty\\) defined on compact set \\(I \\subset \\mathbb{R}\\) is Equicontinuous, and there exists a dense subset \\(R \\subset I\\), such that \\(\\{f_n\\}_{n=1}^\\infty\\) is convergent on \\(R\\), then \\(\\{f_n\\}_{n=1}^\\infty\\) is uniformly convergent on \\(I\\). And denote</p> \\[ f_n(x) \\rightrightarrows f(x), \\quad n\\rightarrow \\infty \\] <p>where the convergent function \\(f(x)\\) is continuous on \\(I\\). </p> <p>Prove it.</p> HintsProofIf \\(I=[a,b]\\) <p>Using Equicontinuity and compact characteristic of set \\(I\\).</p> <p>We hope to prove that \\(\\forall \\varepsilon, \\exists N&gt;0\\), s.t. \\(\\forall n&gt;m&gt;N\\), \\(\\forall x \\in I\\), we have </p> \\[ |f_n(x)-f_m(x)| &lt; \\varepsilon \\] <ul> <li>use the Equicontinuity. </li> </ul> <p>That is, \\(\\forall \\varepsilon\\), \\(\\exists \\delta&gt;0\\), \\(\\forall x_1,x_2\\in I\\) and \\(|x_1-x_2|&lt;\\delta\\), \\(\\forall n\\), we have</p> \\[ |f(x_1)-f(x_2)| &lt; \\frac{\\varepsilon}{9} \\] <ul> <li>use the characteristic of compact set.</li> </ul> <p>Note that there exists open coverings of finite number for compact set \\(I\\). That is, we have \\(x_j\\in I\\), \\(j=1,2,\\cdots, k_0\\), and \\(\\{O_{\\delta'}(x_j)\\}_{j=1}^{k_0}\\) s.t. </p> \\[ I \\subset \\bigcup_{j=1}^{k_0}O_{\\delta'}(x_j) \\] <p>To let it help us prove, we can let \\(\\delta'&lt;\\delta\\) and we can still have a valid corresponding \\(k_0\\) which is finite.</p> <p>Because \\(R\\) is a dense set on \\(I\\), so \\(\\forall O(x_j)\\), \\(\\exists y_j\\in R\\) such that \\(y_j \\in O(x_j)\\).</p> <ul> <li>use the hypothesis of pointwise convergence.</li> </ul> <p>Because \\(\\{f_n\\}_{n=1}^{\\infty}\\) is convergent on dense set \\(R\\), we can have \\(\\forall \\varepsilon&gt;0\\)(choose the same one as the above one), \\(\\exists N&gt;0\\), \\(\\forall n&gt;m&gt;N\\), \\(\\forall y_j \\in R\\), we have</p> \\[ |f_n(y_j)-f_m(y_j)|&lt;\\frac{\\varepsilon}{9} \\] <p>That is, as long as \\(n&gt;m&gt;N\\), we have</p> \\[ \\begin{align} |f_n(x_j)-f_m(x_j)| &amp;\\leq |f_n(x_j)-f_n(y_j)| + |f_n(y_j)-f_m(y_j)| + |f_m(y_j)-f_m(x_j)| \\nonumber\\\\ &amp;&lt; |f_n(x_j)-f_n(y_j)| + \\frac{\\varepsilon}{9} + |f_m(y_j)-f_m(x_j)|\\quad \\text{by pointwise-convergence}\\nonumber\\\\ &amp;&lt;\\frac{\\varepsilon}{3} \\quad \\text{the 1st and 3rd hold for equicontinuity} \\label{bound-open-covering} \\end{align} \\] <p>Note that \\(\\forall x \\in I\\), \\(\\exists j\\in {1,2,\\cdots, k_0}\\), such that \\(x\\in O_{\\delta'}(x_j)\\), so</p> \\[ \\begin{align*} |f_n(x)-f_m(x)| &amp;\\leq |f_n(x)-f_n(x_j)| + |f_n(x_j)-f_m(x_j)| + |f_m(x_j)-f_m(x)|\\\\ &amp;&lt; |f_n(x)-f_n(x_j)| + \\frac{\\varepsilon}{3} + |f_m(x_j)-f_m(x)|\\quad \\text{by the above condition }\\ref{bound-open-covering} \\\\ &amp;&lt;\\varepsilon \\quad \\text{the 1st and 3rd hold for equicontinuity} \\end{align*} \\] <p>In this case, we do not have to find a open covering of \\([a,b]\\) but just partition the closed interval \\([a,b]\\) into \\(k_0\\) distinct closed intervals whose length are less then \\(\\delta\\). Denote these intervals as \\(I_j\\), \\(j=1,2\\cdots, k_0\\).</p> <p>Then \\(\\forall x \\in [a, b]\\), \\(\\exists j\\in {1,2,\\cdots, k_0}\\) such that \\(x\\in I_j\\). Because \\(R\\) is dense on \\([a,b]\\), then we are able to find an element \\(x_j \\in I_j\\), which means </p> \\[ |x-x_j|&lt; \\text{length of }I_q &lt;\\delta \\] <p>So </p> \\[ \\begin{align*} |f_n(x)-f_m(x)| &amp;\\leq |f_n(x)-f_n(x_j)| + |f_n(x_j)-f_m(x_j)| + |f_m(x_j)-f_m(x)|\\\\ &amp;&lt; |f_n(x)-f_n(x_j)| + \\frac{\\varepsilon}{9} + |f_m(x_j)-f_m(x)|\\quad \\text{by pointwise-convergence} \\\\ &amp;&lt;\\frac{\\varepsilon}{3}&lt;\\varepsilon \\quad \\text{the 1st and 3rd hold for equicontinuity} \\end{align*} \\] <p>By utilizing the above two theorems, we can prove the following important theorem. </p> <p>Ascoli-Arzel\u00e0 \u5b9a\u7406 | Ascoli-Arzel\u00e0 Theorem</p> <p>Assume \\(\\Lambda\\) is denumerable. If a family of sequence \\(\\{f_\\lambda\\}_{\\lambda\\in\\Lambda}\\) is uniformly bounded and equicontinuous on closed interval \\([a, b]\\), then there exists a function subsequence \\(\\{f_{\\lambda_k}\\}_{k=1}^{\\infty}\\) of \\(\\{f_\\lambda\\}_{\\lambda\\in\\Lambda}\\) which is uniformly convergent on \\([a, b]\\).</p> <p>Prove it.</p> HintsProof <p>Making use of the above two lemmas.</p> <p>Firstly, we have to find a dense set \\(R\\). Naively, we can choose rational numbers on \\([a,b]\\). That is,</p> \\[ R \\overset{\\Delta}{=}\\mathbb{Q}\\cap [a,b]. \\] <p>Then by lemma 1, using its uniformly bounded,  we can find a function subsequence of \\(\\{f_\\lambda(x)\\}_{\\lambda\\in\\Lambda}\\), denoted as \\(\\{f_{n}\\}_{n=1}^{\\infty}\\), which is convergent on \\(R\\). Finally, by lemma 2, using its equicontinous, the subsequence is uniformly convergent on \\([a,b]\\).</p> <p>And by property of uniformly convergent function sequence, we can see that the convergent funtion is continuous on \\(I\\).</p>"},{"location":"Ctrl/Ordinary_Differential_Equation/General_Theory/Existence_Uniqueness_Theorem/#Picard-Theorem","title":"Picard\u5b58\u5728\u552f\u4e00\u6027\u5b9a\u7406 | Picard Theorem of Existence and Uniqueness","text":"<p>Picard uses the following condition to prove his theorem.</p> <p>Lipschitz\u6761\u4ef6\u7684\u5b9a\u4e49 | Definition of Lipschitz Condition</p> <p>Function \\(f(x, y)\\) defined at region \\(G\\), satisfies Lipschitz condition with respect to \\(y\\), if \\(\\exists L\\) s.t. \\(\\forall (x, y_1), (x, y_2) \\in G\\)</p> \\[ |f(x, y_1)-f(x,y_2)|\\leq L|y_1-y_2| \\] <p>Also, Picard focuses on a typical rectangular region</p> \\[ \\begin{equation} R = \\{(x,y): |x-x_0| \\leq a, |y-y_0|\\leq b\\} \\label{region} \\end{equation} \\] <p>to give his iterative method.</p> <p>Picard\u5b9a\u7406 | Picard Theorem</p> <p>Assume \\(f(x, y) \\in C(G)\\) satisfies Lipschitz condition with respect to \\(y\\), then Cauchy Problem has unique solution on interval \\([x-\\alpha, x+\\alpha]\\), where</p> \\[ \\alpha = \\min \\left\\{a, \\frac{b}{M} \\right\\}, \\quad M = \\max_{(x, y) \\in R}\\left\\{\\left|f(x, y)\\right|\\right\\} \\] <p>Prove it.</p> HintsClassical Proof <p>There typically 4 steps. </p> <p>Firstly, Convert the differential problem into an equivalent integral problem. Then, formulate the so-called Picard sequence and prove it convergent. Furthermore, we have to prove that Picard sequence converges to the solution of integral equation. Finally, we prove the uniqueness by Gronwall Inequation.</p> <ul> <li>Convert the differential problem into an integral problem. That is, solving equation \\(\\ref{eq-cauchy}\\) equals to solving integral equation</li> </ul> \\[ \\begin{align} y(x) &amp;= y(0) + \\int_{x_0}^{x} f(s, y(s))ds \\nonumber\\\\ &amp;=y_0+ \\int_{x_0}^{x} f(s, y(s))ds \\label{eq-integral} \\end{align} \\] <p>readers can prove its equivalence(by proving solution of one side is also solution of the other).</p> <ul> <li>Formulate Picard sequence.</li> </ul> <p>Define:</p> \\[ \\begin{align*} y_0(x) &amp;= y_0\\\\ y_1(x) &amp;= y_0 + \\int_{x_0}^{x} f(s, y_0(s))ds\\\\ y_2(x) &amp;= y_0 + \\int_{x_0}^{x} f(s, y_1(s))ds\\\\ &amp;\\vdots\\\\ y_n(x) &amp;= y_0 + \\int_{x_0}^{x} f(s, y_{n-1}(s))ds\\\\ \\end{align*} \\] <p>We can say the above function sequence \\(\\{y_n(x)\\}_{n=1}^\\infty\\) is well-defined because of the following condition it satisfies:</p> \\[ |y_n(x)-y(x)| \\leq b \\quad \\&amp; \\quad y_n(x)\\in C(R) \\] <p>The above conition enables \\(f(x, y_{n-1}(x))\\) still falls on \\(R\\) and can be integrated(readers can prove that above two condition by induction).</p> <ul> <li>Prove Picard Sequence convergent.</li> </ul> <p>This is the most magic part. The following deduction may be the inspiration:</p> \\[ \\begin{align*} |y_1(x)-y_0(x)| &amp;= \\left| \\int_{x_0}^{x} f(s, y_0)ds \\right| \\leq M\\left| x - x_0\\right| \\\\ |y_2(x)-y_1(x)| &amp;= \\left| \\int_{x_0}^{x}\\left[ f(s, y_1(s)) - f(s, y_0(s))\\right]ds \\right| \\\\  &amp;\\leq \\int_{x_0}^{x} \\left| f(s, y_1(s)) - f(s, y_0(s)) \\right| ds  \\\\  &amp;\\leq L \\int_{x_0}^{x} \\left|y_1(s) - y_0(s)\\right| ds\\quad \\text{(Using Lipschitz Condition)}\\\\ &amp;\\leq LM \\int_{x_0}^{x}\\left|s - x_0\\right|ds =\\frac{LM}{2}|x-x_0|^2 \\quad \\text{(Using the first item)} \\end{align*} \\] <p>So we can allege that</p> \\[ |y_n(x)-y_{n-1}(x)| \\leq \\frac{ML^{n-1}}{n!}|x-x_0|^n \\] <p>and prove it by induction(to be proved by readers).</p> <p>With the above condition, we can use Weierstrass test to prove Picard sequence converges. To be specific, we can see that the Picard Sequence is controlled by a Series of constant terms, which satisfies Cauchy Convergence Theorem. That is, \\(\\forall \\varepsilon&gt;0, \\exists N&gt;0, \\forall n&gt;m&gt;N\\), s.t.</p> \\[ \\begin{align*} |y_n(x)-y_{m}(x)| &amp;\\leq \\sum_{k=m+1}^{n}\\frac{ML^{k-1}}{k!}|x-x_0|^k \\\\ &amp;= \\frac{M}{L}\\sum_{k=m+1}^{n}\\frac{L^{k}}{k!}|x-x_0|^k \\\\ &amp;\\leq \\frac{M}{L}\\sum_{k=m+1}^{n}\\frac{(L\\alpha)^{k}}{k!} \\end{align*} \\] <p>And we notice that Series of constant terms</p> \\[ \\sum_{n=1}^\\infty \\frac{(L\\alpha)^{n}}{n!} \\] <p>converges(to be specific, converges to \\(e^{L\\alpha}\\)), so Picard Sequence also converges.</p> <ul> <li>Prove Picard Sequence converges to solution of equation \\(\\ref{eq-cauchy}\\).</li> </ul> <p>This part is quite easy, to be done by readers.</p> <ul> <li>Prove uniqueness.</li> </ul> <p>Follow the traditional logic: contradiction.</p> <p>Assume there are \\(\\varphi_1(x), \\varphi_2(x)\\) two distinct solutions to equation \\(\\ref{eq-cauchy}\\), then subtract one from the other:</p> \\[ \\begin{align} |\\varphi_1(x) - \\varphi_2(x)| &amp;=\\left| \\int_{x_0}^{x} \\left[ f(s, \\varphi_1(s)) - f(s, \\varphi_2(s))\\right]ds \\right| \\nonumber\\\\ &amp;\\leq L \\int_{x_0}^{x} \\left|\\varphi_1(s) - \\varphi_2(s)\\right|ds  \\quad \\text{(Using Lipschitz Condition)} \\label{eq-same} \\end{align} \\] <p>And by using Gronwall inequation, we can get </p> \\[ |\\varphi_1(x) - \\varphi_2(x)| \\leq 0 \\] <p>So \\(\\varphi_1(x) = \\varphi_2(x)\\), that is, there exists only one solution for equation \\(\\ref{eq-cauchy}\\).</p> <p>In another way, if we don't want to use Gronwall Inequation, we can still say that the two solutions are the same. Assume that \\(\\varphi_1(x), \\varphi_2(x)\\) have a common region \\(J = [x_0-d,x_0+d]\\), where \\(0&lt;d\\leq \\alpha\\). Then \\(|\\varphi_1(x)-\\varphi_2(x)|\\) is continuous and bounded on region \\(J\\) and denote</p> \\[ K = \\max_{x\\in J}\\{|\\varphi_1(x)-\\varphi_2(x)|\\} \\] <p>So the right side of inequation \\(\\ref{eq-same}\\) can be bounded:</p> \\[ \\begin{align} |\\varphi_1(x) - \\varphi_2(x)| &amp;\\leq L \\int_{x_0}^{x} \\left|\\varphi_1(s) - \\varphi_2(s)\\right|ds  \\nonumber\\\\ &amp;\\leq LK |x-x_0| \\label{eq-same1} \\end{align} \\] <p>Substitute inequation \\(\\ref{eq-same1}\\) again into the right side of inequation \\(\\ref{eq-same}\\) and get</p> \\[ \\begin{align*} |\\varphi_1(x) - \\varphi_2(x)| \\leq KL^2 \\frac{|x-x_0|^2}{2} \\end{align*} \\] <p>repeat the above method iteratively and we can prove the following by induction:</p> \\[ |\\varphi_1(x) - \\varphi_2(x)| \\leq K \\frac{(L|x-x_0|)^n}{n!} \\] <p>Let \\(n \\rightarrow \\infty\\), we get \\(|\\varphi_1(x) - \\varphi_2(x)| \\rightarrow 0\\).</p>"},{"location":"Ctrl/Ordinary_Differential_Equation/General_Theory/Existence_Uniqueness_Theorem/#osgood-osgood-condition","title":"Osgood \u6761\u4ef6 | Osgood Condition","text":"<p>We consider a condition which is slightly weaker than Lipschitz condition but still can guarrantee the convergence of Picard Sequence and its uniqueness.</p> <p>Osgood \u6761\u4ef6 | Osgood Condition</p> <p>Assume \\(D\\) is a region on \\(\\mathbb{R}^2\\), and function \\(f(x,y)\\in C(D)\\). If \\(\\forall (x,y_1), (x,y_2)\\in D\\), s.t.</p> \\[ |f(x,y_1)-f(x,y_2)|\\leq F(|y_1-y_2|) \\] <p>where \\(F(r)&gt;0 \\in C(\\mathbb{R})\\) satisfies </p> \\[ \\int_{0}^{\\varepsilon}\\frac{1}{F(r)}dr = +\\infty, \\quad \\forall \\varepsilon &gt;0 \\] <p>then we say that \\(f(x,y)\\) satisfies Osgood condition with respect to \\(y\\).</p> <p>Obviously, if \\(f(x,y)\\) satisfies Lipschitz condition, it satisfies Osgood condition. In fact, we can choose \\(F(r) = Lr\\).</p> <p>Osgood \u5b9a\u7406 | Osgood Theorem</p> <p>If  \\(f(x,y) \\in C(D)\\) satisfies Osgood condition with respect to \\(y\\), then \\(\\forall (x_0,y_0)\\in D\\), the solution to Cauchy problem \\(\\ref{eq-cauchy}\\) exists uniquely.</p> <p>Prove it.</p> HintsProof <p>The existence is guarranteed by Peano Theorem in the following part. You only need to prove the uniqueness, which is proved by contradiction, which is similar to prove the uniqueness of \\(f(x, y)\\) that decrease monotonically with respect to \\(y\\).</p> <p>Assume we have two distinct solution \\(y_1(x)\\), \\(y_2(x)\\), then there exists \\(x_2\\) such that \\(y_1(x_2)\\neq y_2(x_2)\\). Let \\(y_1(x) &gt; y_2(x)\\). </p> <p>Then by feature of guarantee code(\u4fdd\u53f7\u6027), there must exist a region such that \\(y_1(x) &gt; y_2(x)\\), so let \\(x_1 = \\max\\{x\\in [x_0,x_2] : y_1(x) = y_2(x)\\}\\).</p> <p><p> </p></p> <p>So we have </p> \\[ y_1(x) &gt; y_2(x), \\quad \\forall x\\in (x_1,x_2] \\] <p>define \\(r(x) = y_1(x) - y_2(x) \\in (0, m]\\), where m is determined by \\(m = \\max\\limits_{x\\in (x_1,x_2]}\\{y_1(x) - y_2(x)\\}\\)</p> <p>So by condition of the proposition, we have</p> \\[ |r'(x)| = |y_1'(x) - y_2'(x)| = |f(x,y_1(x))- f(x,y_2(x))| \\leq F(|y_1(x) - y_2(x)|) \\] <p>because \\(y_1(x) &gt; y_2(x)\\), so we take \"\\(\\vert \\cdot \\vert\\)\" out and get</p> \\[ r'(x) = y_1'(x) - y_2'(x) = f(x,y_1(x))- f(x,y_2(x)) \\leq F(y_1(x) - y_2(x)) \\] <p>divide both sides \\(F(y_1(x) - y_2(x))\\) and integrate on \\((x_1, x_2]\\), which is an improper integral on the left</p> \\[ \\int_{x_1}^{x_2}\\frac{r'(x)}{F(r(x))}dx\\leq \\int_{x_1}^{x_2}dx = x_2-x_1  \\] <p>so the left item can be \\(+\\infty\\) by condition while the right item is less than \\(+\\infty\\), i.e.</p> \\[ +\\infty=\\int_{0}^{r(x_2)}\\frac{1}{F(r)}dr=\\int_{r(x_1)}^{r(x_2)}\\frac{1}{F(r)}dr\\leq x_2-x_1 &lt;+\\infty \\] <p>which contradicts!</p>"},{"location":"Ctrl/Ordinary_Differential_Equation/General_Theory/Existence_Uniqueness_Theorem/#Peano-Theorem","title":"Peano\u5b9a\u7406 | Peano Theorem","text":"<p>When \\(f(x,y)\\) does not satisfy Lipschitz Condition with respect to \\(y\\), we cannot guarantee the existence and uniqueness of the solution to Cauchy Problem \\(\\ref{eq-cauchy}\\). However, when \\(f(x,y)\\) is continuous, Peano proved that Cauchy Problem \\(\\ref{eq-cauchy}\\) has solution.</p> <p>Peano\u5b9a\u7406 | Peano Theorem</p> <p>Assume \\(f(x, y)\\) is continous in \\(R(\\ref{region})\\), then Cauchy Problem \\(\\ref{eq-cauchy}\\) has at least one solution in interval \\([x_0-\\alpha, x_0+\\alpha]\\), where</p> \\[ \\alpha = \\min \\left\\{a, \\frac{b}{M} \\right\\}, \\quad M = \\max_{(x, y) \\in R}\\left\\{\\left|f(x, y)\\right|\\right\\} \\] <p>Prove it.</p> HintsProof <p>There are several methosd to prove Peano Theorem, like Euler's Arc method, Tonelli(\u6258\u5185\u5229) Sequence method, fixed-point method in functional analysis, while Euler's Arc method is thought to be the dawm to calculating ODE numerically, so we will prove it this way. </p> <p>The idea is to construct approximation solution which converges to the real solution. In fact, Picard Sequence is also an approximation solution.</p> <ul> <li>Cauchy Problem \\(\\ref{eq-cauchy}\\) can be converted equivalently to integral equation \\(\\ref{eq-integral}\\).</li> </ul> <p>We only discuss the existence of solution of right side(\\([x_0, x_0+\\alpha]\\)). We can make similar treatment to the left side.</p> <ul> <li>Formulate Euler Polygons/Polygonal Arc(\u6b27\u62c9\u6298\u7ebf).</li> </ul> <p>The idea is simple: go ahead step by step as small as possible, employing the slope of this sampling points.</p> <p>Firstly, we partition the region \\([x_0,x_0+\\alpha]\\) into \\(n\\) parts(usually of equal length), that is</p> \\[ x_0 &lt; x_1 &lt; x_2 &lt; \\cdots &lt; x_n = x_0 + \\alpha \\] <p>and then define curves</p> \\[ \\begin{align*} l_1: y &amp;= y_0 + f(x_0, y_0)(x-x_0), \\quad x_0\\leq x\\leq x_1 \\\\ l_2: y &amp;= y_1 + f(x_1, y_1)(x- x_1), \\quad x_1 \\leq x\\leq x_2\\\\ &amp;\\vdots \\\\ l_n: y &amp;= y_{n-1} + f(x_{n-1}, y_{n-1})(x - x_{n-1}), \\quad x_{n-1} \\leq x \\leq x_n \\end{align*} \\] <p><p> </p> we can formulate a polygonal arc on \\([x_0, x_0+\\alpha]\\):</p> \\[ E_n = \\bigcup_{s=1}^{n}l_s \\] <p>and its function expresstion is </p> \\[ \\begin{equation} y_n(x) = y_0 + \\sum_{i=0}^{j-1}f(x_i, y_i)(x_{i+1} - x_{i}) + f(x_j, y_j)(x - x_j) \\label{eq-euler} \\end{equation} \\] <p>where \\(\\forall x \\in (x_0, x_0 +\\alpha], \\exists j\\) s.t. </p> \\[ x_j &lt; x\\leq x_{j+1} \\] <p>Here closed interval \\([x_0, x_0+\\alpha]\\) is of great importance, for it can guarantee the condition for Ascoli-Arzel\u00e0 Theorem to be true.</p> <ul> <li>\u4e00\u81f4\u6709\u754c | Uniform Bound</li> </ul> <p>By definition, it is easy to prove that </p> \\[ |y_n(x)-y_0| \\leq b, \\quad \\forall n, \\forall x \\in [x_0,x_0+\\alpha] \\] <ul> <li>\u7b49\u5ea6\u8fde\u7eed | Equicontinuous</li> </ul> <p>By definition, it is easy to see that \\(\\forall \\varepsilon&gt;0\\), \\(\\exists \\delta = \\varepsilon/M\\), s.t. \\(\\forall x_1, x_2 \\in [x_0,x_0+\\alpha]\\) and \\(|x_1-x_2|&lt;\\delta\\), we have</p> \\[ |y_n(x_1)-y_n(x_2)|\\leq \\max_{(x,y)\\in R}|f(x,y)| |x_1-x_2| &lt; M \\cdot \\frac{\\varepsilon}{M} = \\varepsilon \\] <ul> <li>Using Ascoli-Arzel\u00e0 Theorem</li> </ul> <p>So the function sequence \\(\\{y_n(x)\\}\\) has a uniformly convergent subsequence \\(\\{y_{n_j}(x)\\}\\).</p> <ul> <li>Prove Euler's Arc converges to the solution of ODE.</li> </ul> <p>This means that we have to consider the error of \\(y_{n_j}(x)\\) and \\(y(x)\\). Firstly, we rewrite the formula of Euler's Arc \\(\\ref{eq-euler}\\).</p> <p>Note that</p> \\[ \\begin{align*} f(x_i,y_i)(x_{i+1}-x_i)&amp;=\\int_{x_i}^{x_{i+1}}f(x_i,y_i)dx\\\\ &amp;=\\int_{x_i}^{x_{i+1}}f(x,y_n(x)) + f(x_i,y_i) - f(x,y_n(x))dx\\\\ \\end{align*} \\] <p>Define</p> \\[ d_n(i) \\overset{\\Delta}{=} \\int_{x_i}^{x_{i+1}}[f(x_i,y_i) - f(x,y_n(x))]dx \\] <p>then </p> \\[ \\begin{equation} f(x_i,y_i)(x_{i+1}-x_i) = \\int_{x_i}^{x_{i+1}}f(x,y_n(x))dx + d_n(i) \\label{eq-front} \\end{equation} \\] <p>Similarly, we have \\(x_j&lt;x\\leq x_{j+1}\\)</p> \\[ \\begin{align*} f(x_j,y_j)(x-x_j)&amp;=\\int_{x_j}^{x_{j+1}}f(x_j,y_j)dx\\\\ &amp;=\\int_{x_j}^{x_{j+1}}f(x,y_n(x)) + f(x_j,y_j) - f(x,y_n(x))dx\\\\ \\end{align*} \\] <p>Define</p> \\[ d^*_n(x) \\overset{\\Delta}{=} \\int_{x_j}^{x}[f(x_j,y_j) - f(x,y_n(x))]dx  \\] <p>then </p> \\[ \\begin{equation} f(x_j,y_j)(x-x_j) = \\int_{x_j}^{x_{j+1}}f(x,y_n(x))dx +d_n^*(x) \\label{eq-back} \\end{equation} \\] <p>So with the above two transformation \\(\\ref{eq-front}\\), \\(\\ref{eq-back}\\), the formula of Euler's Arc \\(\\ref{eq-euler}\\)(summation of linear expresstions) becomes an integral-like expression:</p> \\[ \\begin{align*} y_n(x) &amp;= y_0 + \\sum_{i=0}^{j-1}f(x_k, y_k)(x_{i+1} - x_{i}) + f(x_j, y_j)(x - x_j)\\\\ &amp; = y_0 + \\int_{x_0}^{x}f(x,y_n(x))dx + \\delta_n(x) \\end{align*} \\] <p>where </p> \\[ \\begin{equation} \\delta_n(x) = \\sum_{i=0}^{j-1}d_n(i) + d^*_n(x) \\label{eq-residue} \\end{equation} \\] <p>we know that for each \\(y_n\\), we can have variables \\(x, y\\) bounded by</p> \\[ |x-x_j|\\leq \\frac{\\alpha}{n}, \\quad |y-y_j|\\leq M \\frac{\\alpha}{n} \\] <p>So \\(\\forall \\varepsilon &gt;0\\), \\(\\exists N(\\varepsilon) = \\frac{\\alpha}{\\varepsilon}\\), s.t. \\(n&gt;N\\), </p> \\[ |x-x_j|\\leq \\frac{\\alpha}{N} = \\varepsilon, \\quad |y-y_j|\\leq M \\frac{\\alpha}{N} = M\\varepsilon \\] <p>which means the region of each integral interval can be as small as possible so long as \\(n\\rightarrow \\infty\\).</p> <p>So in this case, for each item in residue \\(\\ref{eq-residue}\\), we can bound it into small value corresponding to \\(\\varepsilon\\) by making use of the continuity of \\(f(x,y)\\), which means \\(\\forall\\varepsilon&gt;0\\), \\(\\exists \\delta&gt;0\\), s.t. \\(\\forall (x,y) \\in O[(x_j,y_j),\\delta]\\), </p> \\[ |f(x,y)-f(x_j,y_j)| &lt;\\varepsilon \\] <p>The above one is easy to accomplish because we can let \\(n\\) be large enough, so all \\(x \\in [x_j,x_{j+1}]\\) with its \\(y_n(x)\\) can fall in \\(O[(x_j,y_j),\\delta]\\).</p> <p>So the whole residue \\(\\ref{eq-residue}\\) can be bounded:</p> \\[ \\delta_n(x) \\leq \\varepsilon \\frac{\\alpha}{n} + j \\varepsilon \\frac{\\alpha}{n} &lt; \\alpha \\varepsilon \\] <p>So we can say \\(y_{n_j}(x)\\) defined as</p> \\[ y_{n_j}(x) = y_0 + \\int_{x_0}^{x}f(s, y_{n_j}(s))ds + \\delta_{n_j}(x), \\quad x \\in [x_0,x_0+\\alpha] \\] <p>where \\(\\delta_{n_j}(x)\\) satisfies</p> \\[ \\delta_{n_j}(x)\\rightrightarrows 0 \\] <p>Which also means, if we denote </p> \\[ \\varphi(x) \\overset{\\Delta}{=} \\lim_{n_j\\rightarrow \\infty}y_{n_j}(x) \\] <p>and let \\(n_j \\rightarrow \\infty\\), we get</p> \\[ \\varphi(x) = y_0 + \\int_{x_0}^{x}f(s, \\varphi(s))ds \\] <p>which means function subsequence \\(\\{y_{n_j}(x)\\}\\) converges to the solution of integral form \\(\\ref{eq-integral}\\) of ODE \\(\\ref{eq-cauchy}\\).</p>"},{"location":"Ctrl/Ordinary_Differential_Equation/General_Theory/Existence_Uniqueness_Theorem/#comments-on-previous","title":"Comments on Previous","text":"<p>Here we list the difference between Picard Theorem and Peano Theorem.</p> Theorem Picard Theorem Peano Theorem Condition \\(f\\in C(R)\\), Lipschitz Condition with respect to \\(y\\) only \\(f\\in C(R)\\) Sequence the whole Picard Sequence \\(\\{y_n(x)\\}\\) converges subsequence of Euler's Arc \\(\\{\\varphi_n(x)\\}\\) converges Solution exists uniquely exists only, might have a lot of solutions <ul> <li>Note 1: Uniqueness can help bound Euler's Arc. See the following theorem.</li> </ul> <p>\u6709\u552f\u4e00\u89e3\u7684\u67ef\u897f\u95ee\u9898\uff0c\u5176\u6b27\u62c9\u6298\u7ebf\u5168\u5e8f\u5217\u6536\u655b | Euler's Arc converges given uniqueness</p> <p>If Cauchy problem \\(\\ref{eq-cauchy}\\) has unique solution, then the whole sequence of Euler's Arc \\(\\{\\varphi_n(x)\\}\\) converges.</p> <p>Prove it.</p> HintsProof <p>Use contradiction.</p> <p>Assume that the conclusion does not hold, then by definition of negative proposition, we have</p> \\[ \\exists \\varepsilon_0&gt;0, \\exists \\overline{x} \\in I, \\forall N_0&gt;0, \\exists n_0,m_0&gt;N_0, |\\varphi_{n_0}(\\overline{x})-\\varphi_{m_0}(\\overline{x})|\\geq\\varepsilon_0 \\] <p>which is the negative proposition of the Cauchy Convergence form</p> \\[ \\forall \\varepsilon&gt;o, \\forall x\\in I, \\exists N&gt;0, \\forall n&gt;m&gt;N, |\\varphi_{n}(x)-\\varphi_{m}(x)|&lt;\\varepsilon. \\] <p>In order to help us prove, we can take two sequence from the above negative proposition. That is, let \\(N_0=j\\), \\(j=1,2,\\cdots\\), we can take the corresponding \\(n_j\\), \\(m_j\\) out to compose a subsequence of Euler's arc \\(\\{\\varphi_n(x)\\}\\), i.e.</p> \\[ \\begin{equation} \\exists \\varepsilon_0&gt;0, \\exists \\overline{x} \\in I, \\forall j=1,2,\\cdots, \\exists n_j,m_j&gt;j, |\\varphi_{n_j}(\\overline{x})-\\varphi_{m_j}(\\overline{x})|\\geq\\varepsilon_0 \\label{con-non-convergent} \\end{equation} \\] <p>See that \\(\\{\\varphi_{n_j}(x)\\}\\) is still uniformly bounded and equicontinous on \\(I\\), so there exists its subsequence \\(\\{\\tilde{\\varphi}_{n_j}(x)\\}\\) such that</p> \\[ \\{\\tilde{\\varphi}_{n_j}(x)\\} \\rightrightarrows \\varphi_1(x), \\quad \\forall x\\in I \\] <p>and also \\(\\varphi_1(x)\\) is still the solution of Cauchy problem \\(\\ref{eq-cauchy}\\). Similarly, we can have  </p> \\[ \\{\\tilde{\\varphi}_{m_j}(x)\\} \\rightrightarrows \\varphi_2(x), \\quad \\forall x\\in I \\] <p>where \\(\\{\\tilde{\\varphi}_{m_j}(x)\\}\\) is a subsequence of \\(\\{\\varphi_{m_j}(x)\\}\\). According to the condition of the theorem, we have </p> \\[ \\varphi_1(x) = \\varphi_2(x), \\quad \\forall x \\in I \\] <p>while in \\(\\ref{con-non-convergent}\\) we also take the subsequence of \\(\\{\\varphi_{n_j}(x)\\}\\) and \\(\\{\\varphi_{m_j}(x)\\}\\)</p> \\[ |\\tilde{\\varphi}_{n_j}(\\overline{x})-\\tilde{\\varphi}_{m_j}(\\overline{x})|\\geq\\varepsilon_0 \\] <p>which means \\(\\varphi_1(x)\\) and \\(\\varphi_2(x)\\) must have distance on some \\(\\overline{x}\\), which contradicts with \\(\\varphi_1(x) = \\varphi_2(x)\\) on all \\(x\\in I\\)!</p>"},{"location":"Ctrl/Ordinary_Differential_Equation/General_Theory/Extension_of_Solution/","title":"Extension of Solution","text":""},{"location":"Ctrl/Ordinary_Differential_Equation/General_Theory/Extension_of_Solution/#extension-of-solution","title":"\u89e3\u7684\u5ef6\u62d3 | Extension of Solution","text":"<p>For Cauchy problem</p> \\[ \\begin{equation} \\frac{dy}{dx} = f(x, y), \\quad y(x_0) = y_0. \\label{eq-cauchy} \\end{equation} \\] <p>it is clear that we are satisfied with the result of intervals in the previous chapter, especially when we have to shrink the interval of solution using Contraction Mapping Method.</p> <p>So a naive idea is, can we use the Peano/Picard theorem repeatedly to extend the interval of parameter \\(t\\)? If so, in what cases can we extend it and to where can we extend?</p> <p>Here comes the following theorem.</p> <p>\u89e3\u7684\u5ef6\u62d3\u5b9a\u7406 | Theorem of extension of solution</p> <p>Assume \\(f(x,y)\\in C(G)\\), where \\(G\\) is an open set(region). Then the solution of Cauchy problem \\(\\ref{eq-cauchy}\\) can extend to its boundary. </p> HintsProof <p>The proof is equivalent to prove that, for each closed set \\(G_1\\subset G\\) and \\((x_0,y_0)\\in G_1\\), the solution \\(\\Gamma\\) can extend to \\(G\\) \\ \\(G_1\\).</p> <p>We only consider positive extension, i.e. \\(x\\geq x_0\\).</p> <p>Consider closed region \\(G_1\\subset G\\) that has point \\((x_0,y_0)\\) with the solution denoted by \\(\\varphi(x)\\) that satisfies initial condition \\(y(x_0)=y_0\\). </p> <ul> <li>Use the property of Open Set.</li> </ul> <p>Because \\(G\\) is an open set, the distance between \\(G\\) and \\(G_1\\) can not be zero, that is, \\(\\exists \\delta_0&gt;0\\)., s.t.</p> \\[ \\{(x,y): |x-a|&lt;\\delta_0, |y-b|&lt;\\delta_0, (a,b)\\in \\partial G_1\\} \\subset G \\] <p>Here \\(\\delta_0\\) is a constaint condition that guarantees the extended interval of solution will not exceed the boundary of \\(G_1\\).</p> <ul> <li>Using \\(\\delta_0\\) to generate extended intervals with length \\(\\delta'\\)</li> </ul> <p>For any closed region \\(G_1\\subset G\\), Denote </p> \\[ M=\\max_{(x,y)\\in G_1}|f(x_y)|+1 &lt; \\infty, \\quad R_{\\delta_0}(x',y')=\\{(x,y): |x-x'|\\leq \\delta_0, |y-y'|\\leq \\delta_0\\} \\] <p>where \\((x',y')\\in G_1\\).</p> <p>For Cauchy problem with initial condition \\(y(x_0)=y_0\\) in region \\(R_{\\delta_0} (x_0,y_0)\\), according to Peano Theorem, we can have a solution \\(\\varphi_0(x)\\) on interval \\([x_0-\\delta',x_0+\\delta']\\), where \\(\\delta'=\\min\\{\\delta_0, \\delta_0/M\\}\\). If there exists point on the curve \\((x,\\varphi_0(x)) \\in G\\)\\\\(G_1\\), then we prove it.</p> <p><p> </p></p> <p>If not, then the furthest point on the right \\((x_0+\\delta', \\varphi_0(x_0+\\delta'))\\) must be in \\(G_1\\). So consider cauchy problem with initial condition \\(y(x_0+\\delta') = \\varphi_0(x_0+\\delta')\\) in region \\(R_{\\delta_0} (x_0+\\delta',\\varphi_0(x_0+\\delta'))\\), according to Peano Theorem, we can get another solution \\(\\varphi_1(x)\\) on interval \\(x_0,x_0+2\\delta'\\).</p> <p>Repeat the above procedure, we can say the interval can be extended to \\([x_0, x_0+n\\delta']\\). If we denote the distance between \\((x_0,y_0)\\) and \\(\\partial G_1\\) as \\(D_1\\), distance between \\((x_0,y_0)\\) and \\(\\partial G\\) as \\(D\\), then choose \\(n\\) such that \\(n\\delta'&gt;D_1\\) and in the same time \\(n\\delta'&lt;D\\).</p> <p>And we are done.    </p>"},{"location":"Ctrl/Ordinary_Differential_Equation/General_Theory/Power_Series/","title":"Method of Power Series","text":"<p>Lots of ODE cannot be sovled using elementary integration method, so we have to give up solution of finite form and try to find solution of infinite form, like series.</p> <p>To have a more global understanding, we focus on</p> \\[ \\begin{equation} \\frac{d \\pmb{y}}{dx}=\\pmb{f}(x,\\pmb{y}), \\quad \\pmb{y}(x_0)=\\pmb{y}_0 \\label{eq-cauchy} \\end{equation} \\] <p>where \\(\\pmb{f}(x,\\pmb{y})\\) is analytic on region \\(R\\subset \\mathbb{R}\\times \\mathbb{R}^n\\), i.e.</p> \\[ \\pmb{f}(x,\\pmb{y})=\\sum_{i,j_1,j_2,\\cdots,j_n=0}^\\infty \\pmb{a}_{i{j_1}{j_2}\\cdots{j_n}}(x-x_0)^i(y_1-y_{10})^{j_1}\\cdots(y_n-y_{n0})^{j_n} \\] <p>where \\(\\pmb{a}_{i{j_1}{j_2}\\cdots{j_n}}\\in \\mathbb{R}^n\\).</p> <p>To simplify the notation, we denote \\(\\pmb{j}=(j_1,j_2,\\cdots,j_n)\\), and </p> \\[ (\\pmb{y}-\\pmb{y}_0)^{\\pmb{j}}=(y_1-y_{10})^{j_1}\\cdots(y_n-y_{n0})^{j_n} \\] <p>denote </p> \\[ \\sum_{i=0,\\pmb{j}=0}^\\infty=\\sum_{i,j_1,j_2,\\cdots,j_n=0}^\\infty \\] <p>Since \\(\\pmb{f}(x,\\pmb{y})\\) is analytic with respect to \\(\\pmb{y}\\), so by Picard Theorem, there exists only one solution. Now the question is, to prove that the solution is analytic, i.e. \\(\\exists \\delta&gt;0\\), s.t. \\(x\\in O_\\delta(x_0)\\)</p> \\[ \\pmb{y}(x)=\\sum_{k=0}^{\\infty}\\pmb{c}_k(x-x_0)^k \\] <p>where \\(\\pmb{c}_k=(c_{n1},c_{n2},\\cdots,c_{nk}) \\in \\mathbb{R}^n\\).</p>"},{"location":"Ctrl/Ordinary_Differential_Equation/General_Theory/Power_Series/#excellent-series","title":"Excellent Series","text":"<p>To prove that the solution is analytic, we have to use a method of proof, that is, using Excellent Series.</p> <p>Definition of Excellent Series</p> <p>Assume there are two power series</p> \\[ \\begin{equation} \\sum_{i=0.\\pmb{j}=0}^\\infty a_{i\\pmb{j}}(x-x_0)^i(\\pmb{y}-\\pmb{y}_0)^{\\pmb{j}} \\label{series1} \\end{equation} \\] <p>and</p> \\[ \\begin{equation} \\sum_{i=0.\\pmb{j}=0}^\\infty A_{i\\pmb{j}}(x-x_0)^i(\\pmb{y}-\\pmb{y}_0)^{\\pmb{j}}. \\label{series2} \\end{equation} \\] <p>If \\(A_{i\\pmb{j}}&gt;0\\) and they satisfies</p> \\[ |a_{i\\pmb{j}}|&lt;A_{i\\pmb{j}}, \\quad \\forall i,\\pmb{j} \\] <p>Then we call series \\(\\ref{series2}\\) is an excellent series of series \\(\\ref{series1}\\). If series \\(\\ref{series2}\\) converges on closed region</p> \\[ \\{(x,y): |x-x_0|\\leq \\alpha,|\\pmb{y}-\\pmb{y}_0|\\leq\\beta\\} \\] <p>then we call its summing function \\(\\pmb{f}(x,\\pmb{y})\\) is an Excellent function of series \\(\\ref{series1}\\).(note that in this case, series \\(\\ref{series1}\\) also converges)</p> <p></p> <p>\u5f15\u74061: \u89e3\u6790\u51fd\u6570\u6709\u5b9a\u4e49\u57df\u6536\u7f29\u7684\u4f18\u51fd\u6570 | Lemma 1: A analytic function has an excellent function within smaller region</p> <p>If \\(f(x,\\pmb{y})\\) is analytic on region</p> \\[ R: \\{(x,y): |x-x_0|&lt;\\alpha, |\\pmb{y}-\\pmb{y}_0|&lt;\\beta\\} \\] <p>then \\(\\exists M&gt;0\\), s.t.</p> \\[ F(x,\\pmb{y})=\\frac{M}{\\left(1-\\frac{x-x_0}{a}\\right)\\left(1-\\frac{y_1-y_{10}}{b}\\right)\\cdots\\left(1-\\frac{y_n-y_{n0}}{b}\\right)} \\] <p>is an Excellent function of \\(f(x,\\pmb{y})\\) on a smaller region </p> \\[ R_0:\\{(x,y): |x-x_0|&lt;a,|\\pmb{y}-\\pmb{y}_0|&lt;b\\}. \\] HintsProof <p>Use Abel Second Theorem. Note that we can not guarrantee the convergence on boundaries, so we choose open intervals.</p> <p>We can represent \\(f(x,\\pmb{y})\\) in terms of power series</p> \\[ f(x,\\pmb{y})=\\sum_{i=0.\\pmb{j}=0}^\\infty a_{i\\pmb{j}}(x-x_0)^i(\\pmb{y}-\\pmb{y}_0)^{\\pmb{j}} \\] <p>in region \\(R\\). Then by Abel Second Theorem, \\(\\exists a\\in(0,\\alpha), b\\in(0,\\beta)\\) s.t.</p> \\[ \\sum_{i=0.\\pmb{j}=0}^\\infty a_{i\\pmb{j}}a^i b^{j_1+j_2+\\cdots+j_n} \\] <p>is convergent, so each item of the above series can be bounded by a number \\(M&gt;0\\):</p> \\[ |a_{i\\pmb{j}}|a^i b^{j_1+j_2+\\cdots+j_n}\\leq M \\Rightarrow |a_{i\\pmb{j}}|\\leq \\frac{M}{a^i b^{j_1+j_2+\\cdots+j_n}} \\] <p>Now, the following thing is a little tricky. Define </p> \\[ A_{i\\pmb{j}}=\\frac{M}{a^i b^{j_1+j_2+\\cdots+j_n}} \\] <p>Consider a power of series </p> \\[ \\begin{equation} \\sum_{i=0.\\pmb{j}=0}^\\infty A_{i\\pmb{j}}(x-x_0)^i(\\pmb{y}-\\pmb{y}_0)^{\\pmb{j}} \\label{series-exce} \\end{equation} \\] <p>which is convergent because it can add up to:</p> \\[ F(x,\\pmb{y})=\\frac{M}{\\left(1-\\frac{x-x_0}{a}\\right)\\left(1-\\frac{y_1-y_{10}}{b}\\right)\\cdots\\left(1-\\frac{y_n-y_{n0}}{b}\\right)} \\] <p>with its range of definition \\(R_0\\). By definition, it is an excellent function of \\(f(x,\\pmb{y})\\).</p> <p>With the definition of Excellent function, we have to use it to formulate an excellent series of the original series. This is the following theorem.</p> <p></p> <p>\u5f15\u74062: \u7528\u4e0a\u8ff0\u4f18\u51fd\u6570\u5efa\u7acb\u7684\u5fae\u5206\u65b9\u7a0b\u6709\u89e3\u6790\u89e3 | Lemma2: ODE combined with The above Excellent Function has a solution that can be represented by Power Series</p> <p>Cauchy problem </p> \\[ \\frac{d \\pmb{y}}{dx}=\\pmb{F}(x,\\pmb{y}), \\quad \\pmb{y}(x_0)=\\pmb{y}_0 \\label{eq-cauchy-prime} \\] <p>has a analytic solution \\(\\pmb{y}=\\pmb{y}(x)\\) on region \\(O_\\rho(x_0)\\), where \\(F_i(x,\\pmb{y}) = F(x,\\pmb{y})\\) that is same all over \\(i\\) is given from the above lemma and </p> \\[ \\rho=a\\{1-e^{b/[(n+1)aM]}\\}. \\] HintsProof <p>use elementary integration method.</p> <p>We let \\(u=y_i\\), \\(i=1,2,\\cdots,n\\) and we only need to solve the equation </p> \\[ \\frac{d u}{dx}=F(x,u), \\quad u(x_0)=u_0 \\] <p>where </p> \\[ F(x,u) = \\frac{M}{\\left(1-\\frac{x-x_0}{a}\\right)\\left(1-\\frac{u-u_{0}}{b}\\right)^n} \\] <p>(Here readers can see that \\(u-u_0=y_i-y_{i0}\\).)</p> <p>The above ODE is a variable separation equation. So change the form and integrate on \\([x_0,x]\\)</p> \\[ \\frac{-b}{n+1}\\left(1-\\frac{u-u_0}{b}\\right)^{n+1} +\\frac{b}{n+1}= -aM\\ln\\left(1-\\frac{x-x_0}{a}\\right) \\] <p>get \\(u\\) out:</p> \\[ u = u_0 + b- b\\left[\\frac{aM(n+1)}{b} \\ln\\left(1-\\frac{x-x_0}{a}\\right) + 1\\right]^{\\frac{1}{n+1}} \\] <p>That is,</p> \\[ y_i(x) = y_{i0} + b- b\\left[\\frac{aM(n+1)}{b} \\ln\\left(1-\\frac{x-x_0}{a}\\right) + 1\\right]^{\\frac{1}{n+1}}, \\quad \\forall i=1,2\\cdots,n \\] <p>We want to use this form to get a power series. See that \\(\\ln{\\left(1-\\frac{x-x_0}{a}\\right)}\\) can be represented by power series of \\((x-x_0)\\) once \\(|x-x_0|&lt;a\\). And also \\((1+s)^{\\frac{1}{n+1}}\\) can be represented by power series of \\(s\\) when \\(|s|&lt;1\\). So by combine the above two, we know \\(y_i(x)\\) can be represented by \\((x-x_0)\\) once \\(|x-x_0|&lt;a\\). To be more specific, We have to let the radius of converence to satisfy</p> \\[ \\begin{cases} \\displaystyle 1-\\frac{\\rho}{a} \\geq 0 \\\\ \\displaystyle \\frac{aM(n+1)}{b}\\ln\\left(1-\\frac{\\rho}{a}\\right)\\leq 1  \\end{cases} \\Rightarrow \\begin{cases} \\rho\\leq a \\\\ \\rho\\leq a\\{1-e^{b/[(n+1)aM]}\\} \\end{cases} \\] <p>choose</p> \\[ \\rho=a\\{1-e^{b/[(n+1)aM]}\\} \\] <p>So solution of the above ODE</p> \\[ \\pmb{y}(x)=(y_1(x),y_2(x),\\cdots, y_n(x)) \\] <p>can be represented by power series of \\((x-x_0)\\) when \\(|x-x_0|&lt;\\rho\\).</p>"},{"location":"Ctrl/Ordinary_Differential_Equation/General_Theory/Power_Series/#proof","title":"\u8bc1\u660e | Proof","text":"<p>Cauchy \u5b9a\u7406 | Cauchy Theorem</p> <p>Assume \\(\\pmb{f}(x,\\pmb{y})=[f_1(x,\\pmb{y}), f_2(x,\\pmb{y}),\\cdots, f_n(x,\\pmb{y})]\\) is an analytic function on region \\(R\\). So problem \\(\\ref{eq-cauchy}\\) has a unique analytic solution \\(\\pmb{y} = \\pmb{y}(x)\\) on \\(O_\\rho(x)\\), where \\(\\rho\\) is given in Lemma 2.</p> HintsProof <ul> <li> <p>Represent solution with power series. Show that it is unique.</p> </li> <li> <p>Use an excellent series to prove the above power series convergent.</p> </li> </ul> <ul> <li>Represent \\(f_k(x,\\pmb{y})\\) with power series</li> </ul> \\[ \\begin{equation} f_k(x,\\pmb{y})=\\sum_{i=0,\\pmb{j}=0}^\\infty a_{i\\pmb{j}}^k (x-x_0)^i(\\pmb{y}-\\pmb{y}_0)^{\\pmb{j}} \\label{eq-f} \\end{equation} \\] <p>And represent solution with power series</p> \\[ \\begin{equation} y_k(x) = y_{k0} + \\sum_{i=1}^\\infty c_i^k(x-x_0)^i,\\quad k=1,2,\\cdots,n \\label{eq-y} \\end{equation} \\] <p>substitute \\(\\ref{eq-f}\\) and \\(\\ref{eq-y}\\) into ODE</p> \\[ \\frac{d y_k}{dx} = f_k(x,\\pmb{y}),\\quad k=1,2\\cdots,n \\] <p>and get</p> \\[ \\begin{align*} \\sum_{i=0}^\\infty (i +1) c_{i+1}^k(x-x_0)^i = \\sum_{i,j_1,j_2,\\cdots,j_n=0}^\\infty \\Bigg\\{ &amp; a^k_{i{j_1}{j_2}\\cdots{j_n}}(x-x_0)^i \\times \\\\  &amp;\\left[\\sum_{i=1}^\\infty c_i^1(x-x_0)^i\\right]^{j_1} \\times \\\\ &amp;\\left[\\sum_{i=1}^\\infty c_i^2(x-x_0)^i \\right]^{j_2} \\times \\\\ &amp;\\cdots \\\\ &amp; \\left[\\sum_{i=1}^\\infty c_i^n(x-x_0)^i \\right]^{j_n}\\Bigg\\},\\quad k=1,2\\cdots,n. \\end{align*} \\] <p>Denote \\(X = x-x_0\\), and we have</p> \\[ \\sum_{i=0}^\\infty (i +1) c_{i+1}^kX^i = \\sum_{i,j_1,j_2,\\cdots,j_n=0}^\\infty a^k_{i{j_1}{j_2}\\cdots{j_n}}X^i \\left(\\sum_{i=1}^\\infty c_i^1X^i\\right)^{j_1} \\left(\\sum_{i=1}^\\infty c_i^2X^i\\right)^{j_2} \\cdots \\left(\\sum_{i=1}^\\infty c_i^nX^i\\right)^{j_n} \\] <p>and get \\(c_i^k\\) out in terms of \\(a^k_{i\\pmb{j}}\\)</p> \\[ \\begin{align*} c_1^k &amp;= a^k_{00\\cdots 0}\\\\ c_2^k &amp;= \\frac{1}{2!} (a^k_{10\\cdots 0}+a^k_{010\\cdots 0}c^1_1 + a^k_{0010\\cdots 0}c^2_1+\\cdots a^k_{00\\cdots01}c^n_1) \\\\ &amp;= \\frac{1}{2!} (a^k_{10\\cdots 0} + a^k_{010\\cdots 0}a^1_{00\\cdots 0} + a^k_{0010\\cdots 0}a^2_{00\\cdots 0}+\\cdots a^k_{00\\cdots01}a^n_{00\\cdots 0}) \\end{align*} \\] <p>Generally, we have </p> \\[ c^k_m=P_m^k(a^l_{00\\cdots 0}, a^l_{01\\cdots 0},\\cdots,a^l_{i{j_1}\\cdots{j_n}}) \\] <p>where \\(i+j_1+j_2+\\cdots+j_n\\leq m-1\\), \\(1\\leq l\\leq n\\). Thus, \\(P_m^k\\) is a polynomial represented by \\(a^l_{00\\cdots 0}\\), \\(a^l_{01\\cdots 0}\\), \\(\\cdots\\), \\(a^l_{i{j_1}\\cdots{j_n}}\\) with positve operator \"+\". Theoretically, we can represent the solution by definite power series.</p> <p>We leave the proof of this part as an additional work in Appendix at the end of the doc.</p> <ul> <li>Prove the above series converges.</li> </ul> <p>Here we formulate another ODE and use Excellent function to bound the above power series.</p> <p>Since \\(f_k(x,\\pmb{y})\\) is analytic on region \\(R\\), by Lemma 1, there exists an excellent function of \\(f_k(x,\\pmb{y})\\) on a smaller region \\(R_0\\):</p> \\[ F_k(x,\\pmb{y}) = \\frac{M}{\\left(1-\\frac{x-x_0}{a}\\right)\\left(1-\\frac{y_1-y_{10}}{b}\\right)\\cdots\\left(1-\\frac{y_n-y_{n0}}{b}\\right)}, \\quad k=1,2\\cdots n \\] <p>If we represent both of them in terms of power series</p> \\[ \\begin{align*} f_k(x,\\pmb{y})&amp;=\\sum_{i=0.\\pmb{j}=0}^\\infty a_{i\\pmb{j}}(x-x_0)^i(\\pmb{y}-\\pmb{y}_0)^{\\pmb{j}} \\\\ F_k(x,\\pmb{y})&amp;=\\sum_{i=0.\\pmb{j}=0}^\\infty A_{i\\pmb{j}}(x-x_0)^i(\\pmb{y}-\\pmb{y}_0)^{\\pmb{j}} \\end{align*} \\] <p>then we have a relation \\(|a_{i\\pmb{j}}|&lt;A_{i\\pmb{j}}\\), which matters in the following proof.</p> <p>Now consider an ODE</p> \\[ \\frac{d y_k}{dx} = F_k(x,\\pmb{y}),\\quad y_k(x_0)=y_k,\\quad  k=1,2,\\cdots,n,\\quad   \\] <p>by Lemma 2, the above ODE has an analytic solution \\(\\pmb{y}=\\pmb{y}(x)\\), represented by power series</p> \\[ y_k(x) = y_{k0} + \\sum_{i=1}^\\infty C_i^k(x-x_0)^i,\\quad k=1,2,\\cdots,n \\] <p>Similarly, we have </p> \\[ \\begin{align*} C^k_m&amp;=P_m^k(A^l_{00\\cdots 0}, A^l_{01\\cdots 0},\\cdots,A^l_{i{j_1}\\cdots{j_n}})\\\\ &amp;=P_m^k(|A^l_{00\\cdots 0}|, |A^l_{01\\cdots 0}|,\\cdots,|A^l_{i{j_1}\\cdots{j_n}}|)\\\\ &amp;\\geq P_m^k(|a^l_{00\\cdots 0}|, |a^l_{01\\cdots 0}|,\\cdots,|a^l_{i{j_1}\\cdots{j_n}}|)\\\\ &amp;\\geq |c_m^k| \\end{align*} \\] <p>So power series \\(\\sum\\limits_{i=1}^\\infty C_i^k(x-x_0)^i\\) is en excellent series of \\(\\sum\\limits_{i=1}^\\infty c_i^k(x-x_0)^i\\). Since the former is convergent by Lemma 2, so the latter also converges.</p>"},{"location":"Ctrl/Ordinary_Differential_Equation/General_Theory/Power_Series/#Appendix","title":"\u9644\u5f55 | Appendix: Relation between \\(c\\) and \\(a\\)","text":"<p> Theorem. Prove  \\[ c^k_m=P_m^k(a^l_{00\\cdots 0}, a^l_{01\\cdots 0},\\cdots,a^l_{i{j_1}\\cdots{j_n}}) \\] <p>where \\(i+j_1+j_2+\\cdots+j_n\\leq m-1\\), \\(1\\leq l\\leq n\\). Thus, \\(P_m^k\\) is a polynomial represented by \\(a^l_{00\\cdots 0}\\), \\(a^l_{01\\cdots 0}\\), \\(\\cdots\\), \\(a^l_{i{j_1}\\cdots{j_n}}\\) with positve operator \"+\".  </p> HintsProof <p>Use induction.</p>"},{"location":"Ctrl/ProcessControl/","title":"Process Control","text":""},{"location":"Ctrl/ProcessControl/#introduction","title":"Introduction","text":""},{"location":"Ctrl/ProcessControl/#pid-controller","title":"PID Controller","text":""},{"location":"Ctrl/ProcessControl/#control-strategy","title":"Control Strategy","text":""},{"location":"Ctrl/ProcessControl/#nonlinear-conpensation","title":"Nonlinear Conpensation","text":""},{"location":"Ctrl/ProcessControl/Intro/","title":"Introduction","text":""},{"location":"Ctrl/ProcessControl/Intro/#basic-ideas","title":"Basic Ideas","text":"<p>Besic Concepts</p> <ul> <li> <p>\u88ab\u63a7\u53d8\u91cf(Controlled Variable, CV)</p> </li> <li> <p>\u8bbe\u5b9a\u503c(Setpoint, SP)</p> </li> <li> <p>\u64cd\u7eb5\u53d8\u91cf(Manipulated Variable, MV). Directed controlled by actuator and influence CV.</p> </li> <li> <p>\u6270\u52a8\u53d8\u91cf(Disturbance Variables, DV). Variables that also influence CV and is not influenced by MV.</p> </li> </ul>"},{"location":"Ctrl/ProcessControl/Intro/#dynamic-characteristics-of-process","title":"Dynamic Characteristics of Process","text":""},{"location":"Ctrl/ProcessControl/Intro/#typical-process","title":"Typical Process","text":"<ul> <li>\u81ea\u8861\u8fc7\u7a0b(self-balanced process)</li> </ul> <p>When input changes, process can converge to another equilibrium. This process includes \u7eaf\u6ede\u540e\u8fc7\u7a0b, \u5355\u5bb9\u8fc7\u7a0b and \u591a\u5bb9\u8fc7\u7a0b.</p> <p>\u7eaf\u6ede\u540e\u8fc7\u7a0b\u3001\u5355\u5bb9\u8fc7\u7a0b and \u591a\u5bb9\u8fc7\u7a0b</p> <ul> <li>\u7eaf\u6ede\u540e\u8fc7\u7a0b</li> </ul> \\[ G(s)=e^{-\\tau s} \\] <ul> <li>\u5355\u5bb9\u8fc7\u7a0b</li> </ul> \\[ G(s)=\\frac{K}{1+T s}e^{-\\tau s} \\] <ul> <li>\u591a\u5bb9\u8fc7\u7a0b</li> </ul> <p>we use one order or two order model to approximate high order object</p> \\[ G(s)=\\frac{K}{(1+T_1 s)(1+T_2 s)}e^{-\\tau s} \\] <ul> <li>\u975e\u81ea\u8861\u8fc7\u7a0b(Non-self-balanced process)</li> </ul> <p>\u79ef\u5206\u8fc7\u7a0b, \u5f00\u73af\u4e0d\u7a33\u5b9a</p> <ul> <li>\u79ef\u5206\u8fc7\u7a0b</li> </ul> <p>we change valve into \u8ba1\u91cf\u6cf5, then </p> \\[ A\\frac{dH}{dt}= q_i-q_o \\] <p>making Laplace transformation and we get</p> \\[ \\frac{H}{Q_i}=\\frac{1}{As} \\] <ul> <li>\u5f00\u73af\u4e0d\u7a33\u5b9a</li> </ul> \\[ G(s)=\\frac{|K|}{|T|s-1} \\]"},{"location":"Ctrl/ProcessControl/Intro/#mechanisim-modelling","title":"Mechanisim Modelling","text":"<p>There are typical steps.</p> <ul> <li> <p>List Differential equations by physical mechanisim.</p> </li> <li> <p>find its static point.</p> </li> <li> <p>find dynamic relationship at this static point. Linearize the model if necessary.</p> </li> </ul> <p>Q1. Model dynamic process of liquid tank.</p> Answer <p>we know that </p> \\[ A\\frac{dh}{dt}=Q_i-Q_o \\] <p>with \\(Q_O=k\\sqrt{h}\\), where \\(A\\) is the cross area of the tank, \\(k\\) is the coefficient of the pipe. So </p> \\[ A\\frac{dh}{dt}=Q_i-k\\sqrt{h} \\] <p>Now we have to linearize it.</p> <p>at static point \\(t=0\\), we have \\(dh/dt=0\\), so \\(Q_{i0}=Q_{o0}\\). Denote</p> \\[ \\begin{align*} \\Delta h&amp;=h-h_0 \\\\ \\Delta Q_i&amp;=Q_i-Q_{i0}\\\\ \\Delta Q_o&amp;=Q_o-Q_{o0} \\end{align*} \\] <p>So</p> \\[ \\begin{align} A\\frac{d\\Delta h}{dt}=\\Delta Q_i-\\Delta Q_o \\label{eqh} \\end{align} \\] <ul> <li>Use Taylor extansion to get the linear part.</li> </ul> \\[ \\begin{align*} \\Delta Q_o&amp;=k\\sqrt{h} - Q_{o0}\\\\ &amp;\\approx \\frac{d Q_o}{dt}\\Bigg|_{h=h_0} \\Delta h \\\\ &amp;= \\frac{k}{2\\sqrt{h_0}}\\Delta h \\end{align*} \\] <p>So back in euqation \\(\\ref{eqh}\\), we have</p> \\[ A\\frac{d\\Delta h}{dt}=\\Delta Q_i- \\frac{k}{2\\sqrt{h_0}}\\Delta h \\] <p>Make Laplace Transiformation and we get</p> \\[ AsH(s)=Q_i(s)-\\frac{k}{2\\sqrt{h_0}}H(s) \\] <p>If we denote \\(R=\\frac{2\\sqrt{h_0}}{k}\\), we get</p> \\[ G(s)=\\frac{H(s)}{Q_i(s)}=\\frac{R}{RAs+1} \\]"},{"location":"Ctrl/ProcessControl/Intro/#detection-part","title":"Detection Part","text":"<p>This Detection part can usually be described as an one order model</p> \\[ G_m(s) = \\frac{K_m}{T_m s+1}e^{-\\tau_m s} \\]"},{"location":"Ctrl/ProcessControl/Intro/#valve-part","title":"Valve Part","text":"<ul> <li>\u6c14\u52a8\u9600\u7684\u6c14\u5f00\u3001\u6c14\u5173\u9009\u62e9</li> </ul> <p>\u6c14\u5f00\u3001\u6c14\u5173\u9600</p> <ul> <li> <p>\u6c14\u5f00\u9600: \u6709\u6c14\u5219\u5f00\uff0c\u65e0\u6c14\u5219\u5173</p> </li> <li> <p>\u6c14\u5173\u9600: \u65e0\u6c14\u5219\u5f00\uff0c\u6709\u6c14\u5219\u5173</p> </li> </ul> <p>Criteria: Without signal, should the valve be closed or open.</p> <ul> <li>Characteristics</li> </ul> <p>A valve is composed by two parts in series.</p> \\[ G(s)=G_{v1}(s)\\times G_{v2}(s) \\] <p>where \\(G_{v1}(s)\\) denotes the relationship between the input signal and the displacement of valve core, which is usually one order model and \\(G_{v2}(s)\\) denotes the relationship between the displacement and the output flow, which is usually non-linear and can be used to compensate some non-linear part in the control system.</p> <p>Here we talk about some details about the latter relationship. Firstly, we denote the flow characteristic \\(f\\) to be </p> \\[ \\frac{Q}{Q_{max}}=f\\left(\\frac{l}{L}\\right) \\] <p>where \\(\\frac{Q}{Q_{max}}\\) denotes the relative flow and \\(\\frac{l}{L}\\) denotes the relative displacement of the valve core. And we define \\(R=\\frac{Q_{max}}{Q_{min}}=30\\). The following graph is acquired when the pressure difference between the front and the back of the vavle stays the same.</p> <p>Typical Valves</p> <ul> <li>(i) Linear valve</li> </ul> \\[ \\frac{Q}{Q_{max}}=K\\frac{l}{L}+C \\] <p>with \\(K=\\frac{R-1}{R}\\) and \\(C=\\frac{1}{R}\\).</p> <ul> <li>(ii) Equipercent valve</li> </ul> \\[ \\frac{Q}{Q_{max}}=R^{\\left( \\frac{l}{L}-1 \\right)} \\] <p>which is deduced by ODE</p> \\[ \\frac{d(Q/Q_{max})}{d(l/L)}=K\\frac{Q}{Q_{max}} \\] <ul> <li>(iii) Quick-Opening valve</li> </ul> \\[ \\frac{Q}{Q_{max}}=\\frac{1}{R}\\left[1+(R^2-1)\\frac{l}{L}\\right]^{1/2} \\] <p>which is deduced by </p> \\[ \\frac{d(Q/Q_{max})}{d(l/L)}=K\\left(\\frac{Q}{Q_{max}}\\right)^{-1} \\] <ul> <li>(iv) ParaBola valve</li> </ul> \\[ \\frac{Q}{Q_{max}}=\\frac{1}{R}\\left[1+(\\sqrt{R}-1)\\frac{l}{L}\\right]^{2} \\] <p>which is deduced by </p> \\[ \\frac{d(Q/Q_{max})}{d(l/L)}=K\\left(\\frac{Q}{Q_{max}}\\right)^{1/2} \\] <p><p> </p></p> <p>When we install a valve, the pressure would not be constant, cause the resistence of the pipe. Usually the curve would bend left up, which is determined by Pressure-drop ratio </p> \\[ s=\\frac{\\Delta P|_{l=L}}{P} \\] <p>if \\(s=1\\), then the curve does not change.</p> <p>Define the gain of valve to be</p> \\[ K_v=\\frac{\\Delta Q/Q_{max}}{\\Delta l/L} \\] <p>which is the tangent of the above graph.</p>"},{"location":"Ctrl/ProcessControl/Intro/#general-object-and-its-modelling","title":"General Object and its modelling","text":"<ul> <li>Test Method</li> </ul> <p>For one order object with pure delay, we can use a phase step as an input and use the response to get its corresponding model.</p> <p>Method for One order object with pure time delay</p> <p>We use Two-Point Method usually.</p> <p>(i) Gain</p> \\[ K=\\frac{O_f-O_i}{I_f-I_i} \\] <p>with units.</p> <p>(ii) time constant \\(T\\) and time daley constant \\(\\tau\\)</p> <p>choose \\(y_{283}(t_1)\\) and \\(y_{632}(t_2)\\) and then </p> \\[ T=1.5\\times (t_2-t_1),\\quad \\tau=t_2-t_0-T    \\] <p>where \\(t_0\\) denotes the start time of input step function.</p>"},{"location":"Ctrl/ProcessControl/Intro/#pid-parameters-determination","title":"PID Parameters Determination","text":"<p>After getting a one order model of the system, i.e. \\(K\\), \\(T\\), \\(\\tau\\), then we can use them to get PID controller parameters.</p>"},{"location":"Ctrl/ProcessControl/Intro/#_1","title":"\u54cd\u5e94\u66f2\u7ebf\u6cd5","text":"<ul> <li>ZN method: only applies to \\(0&lt;\\tau&lt;T\\)</li> </ul> \u63a7\u5236\u5668\u7c7b\u578b K_C T_i T_d P \\(T/(\\tau K)\\) \\(\\infty\\) \\(0\\) PI \\(0.9T/(\\tau K)\\) \\(3.33\\tau\\) \\(0\\) PID \\(1.2T/(\\tau K)\\) \\(2.0\\tau\\) \\(0.5\\tau\\) <ul> <li>\\(\\lambda\\) method: all application</li> </ul> \u63a7\u5236\u5668\u7c7b\u578b K_C T_i T_d P \\(T/(\\tau K)\\) \\(\\infty\\) \\(0\\) PI \\(T/(2\\tau K)\\) \\(T\\) \\(0\\) PID \\(T/(1.2\\tau K)\\) \\(T\\) \\(0.5\\tau\\) <p>Both method are same for \\(T_d\\) and P controller.</p>"},{"location":"Ctrl/ProcessControl/Intro/#others","title":"Others","text":"<ul> <li> <p>\u7ecf\u9a8c\u6cd5</p> </li> <li> <p>\u4e34\u754c\u6bd4\u4f8b\u5ea6\u6cd5</p> </li> </ul> <p>\u5728\u95ed\u73af\u4e0b\uff0c\u5148\u4f7f\u7528\u7eaf\u6bd4\u4f8b\u63a7\u5236\uff0c\u4ece\u5c0f\u5230\u5927\u8c03\u8282\\(K_C\\)\uff0c\u5bf9\u4e8e\u7ed9\u5b9a\\(K_C\\).\u4f7f\u7528\u5c0f\u5e45\u5ea6\u9636\u8dc3\u8f93\u5165\uff0c\u4f7f\u5f97\u7b49\u5e45\u632f\u8361\u4e0b\uff0c\u8bb0\u5f55\u5468\u671f\\(P_u\\)\u548c\u6b64\u65f6\u7684\u6bd4\u4f8b\u589e\u76ca\\(K_{Cmax}\\)\uff0c\u6c42\u51fa\u5bf9\u5e94\u53c2\u6570\u3002</p> <p>\u6ce8: \u8be5\u65b9\u6cd5\u4ee5\u5f97\u52304\uff1a1\u8870\u51cf\u6bd4\uff0c\u5408\u9002\u8d85\u8c03\u4e3a\u76ee\u6807\u3002</p> <ul> <li>\u8870\u51cf\u66f2\u7ebf\u6cd5: \u9488\u5bf9\u7eaf\u6bd4\u4f8b\u95ed\u73af\u4e0b\u5f97\u4e0d\u5230\u7b49\u5e45\u632f\u8361\u7684\u60c5\u51b5</li> </ul> <p>\u6ce8\uff1a\u548c\u7ecf\u9a8c\u6cd5\u7ed3\u5408\u4f7f\u7528</p>"},{"location":"Ctrl/ProcessControl/Nonlinear_Compensation/","title":"Nonlinear Compensation","text":""},{"location":"Ctrl/ProcessControl/Nonlinear_Compensation/#_1","title":"\u5bf9\u8c61\u975e\u7ebf\u6027","text":""},{"location":"Ctrl/ProcessControl/Nonlinear_Compensation/#_2","title":"\u7eaf\u6ede\u540e","text":""},{"location":"Ctrl/ProcessControl/PID/","title":"PID Controller","text":""},{"location":"Ctrl/ProcessControl/PID/#basic-concept","title":"Basic concept","text":"<p>\u65f6\u57df\u6307\u6807</p> <ul> <li>\u8870\u51cf\u6bd4\\(n=\\frac{B}{B'}\\)</li> </ul> <p>\\(B\\), \\(B'\\) \u662f\u76f8\u90bb\u4e24\u4e2a\u6ce2\u5cf0\u503c\u3002\u8981\u51cf\u53bb\u7a33\u6001\u503c\\(C\\).</p> <ul> <li>\u8d85\u8c03\u91cf\u4e0e\u6700\u5927\u52a8\u6001\u504f\u5dee</li> </ul> <p>\u8d85\u8c03\u91cf\uff1a</p> \\[ \\sigma=\\frac{B}{C} \\] <p>\u6700\u5927\u52a8\u6001\u504f\u5dee\u7528\u5728 \u7a33\u6001\u503c \\(C\\) \u6bd4\u8f83\u5c0f\u65f6\u5019</p> \\[ A=|B+C| \\] <ul> <li> <p>\u4f59\u5dee: \u8bbe\u5b9a\u503c\u548c\u7a33\u6001\u503c\u7684\u5dee \\(e(\\infty)=r-c(\\infty)=r-C\\)</p> </li> <li> <p>\u8c03\u8282\u65f6\u95f4 \\(t_s\\) : \u8868\u793a\u8fdb\u5165\u7a33\u6001\u503c\u9644\u8fd1 \\(5\\%\\)/\\(2\\%\\) (\u76f8\u5bf9\u4e8e\u65e7\u7a33\u6001\u503c)\u7684\u65f6\u95f4\uff0c\u5373 \\(y_\\infty \\pm 0.05|y_\\infty-y_0|\\)</p> </li> <li> <p>\u632f\u8361\u9891\u7387: \\(\\beta=\\frac{2\\pi}{T}\\) \u662f\u8c03\u8282\u65f6\u95f4\u7684\u5012\u6570\u3002</p> </li> <li> <p>\u5cf0\u503c\u65f6\u95f4: \u7b2c\u4e00\u6b21\u5230\u8fbe\u6700\u5927\u503c/\u6700\u5c0f\u503c\u7684\u65f6\u95f4 \\(t_p\\)</p> </li> <li> <p>\u4e0a\u5347\u65f6\u95f4\uff1a \u7b2c\u4e00\u6b21\u8fbe\u5230\u7a33\u6001\u503c\u7684\u65f6\u95f4 \\(t_r\\).</p> </li> </ul> <p>\u504f\u5dee\u79ef\u5206(IE)\u6027\u80fd\u6307\u6807</p> <ul> <li> <p>IE: \\(\\int_0^\\infty e dt\\) \u5bf9\u4e8e\u7b49\u5e45\u632f\u8361\u65e0\u6548\u3002</p> </li> <li> <p>IAE: \\(\\int_0^\\infty |e| dt\\)\uff0c\u5bf9\u4e8e\u5177\u6709\u8f83\u5feb\u7684\u8fc7\u6e21\u8fc7\u7a0b\u3001\u4e0d\u5927\u8d85\u8c03\u9002\u7528</p> </li> <li> <p>ISE: \\(\\int_0^\\infty e^2 dt\\), \u6291\u5236\u5927\u504f\u5dee\uff0c\u6570\u5b66\u597d\u7528</p> </li> <li> <p>ITAE: \\(\\int_0^\\infty t |e| dt\\), \u524d\u671f\u6291\u5236\u5c0f\uff0c\u540e\u671f\u5927\uff0c\u4ece\u800c\u524d\u671f\u504f\u5dee\u5927</p> </li> </ul>"},{"location":"Ctrl/ProcessControl/PID/#p-control","title":"P control","text":"\\[ u(t)=K_C(y_{sp}(t)-y_m(t))+u_0 \\] <p>where \\(u_0\\) \u662f\u7a33\u6001\u5de5\u4f5c\u70b9\u3002\u4f20\u9012\u51fd\u6570\u4e3a</p> \\[ G_C(s)=\\frac{U(s)}{E(s)}=K_C \\] \\(K_C\\) \u4f59\u5dee \u8870\u51cf\u6bd4 \u7a33\u5b9a\u6027 \u53d8\u5927 \u53d8\u5c0f \u53d8\u5c0f \u4ece\u8d1f\u5b9e\u6839\u5230\u865a\u6839\u5230\u590d\u6839\uff0c\u53d8\u5dee <p>\u6ce8: \u59cb\u7ec8\u5b58\u5728\u4f59\u5dee\uff0c\u589e\u76ca\u8d8a\u5927\uff0c\u4f59\u5dee\u8d8a\u5c0f\u3002</p>"},{"location":"Ctrl/ProcessControl/PID/#pi-control","title":"PI control","text":"<p>\u79ef\u5206\u4f5c\u7528\uff1a</p> \\[ u(t)=u_0+\\int_0^t e(s)ds \\] <p>\u6bd4\u4f8b\u79ef\u5206\u4f5c\u7528\uff1a</p> \\[ u(t)=u_0+K_c\\left(e(t)+\\frac{1}{T_i}\\int_0^te(s)ds\\right) \\] <p>\u4f20\u9012\u51fd\u6570\u4e3a</p> \\[ G_C(s)=K_C(1+\\frac{1}{T_i s}) \\] <ul> <li>\u53ef\u6d88\u9664\u4f59\u5dee\uff0c\u76f8\u540c\u60c5\u51b5\u4e0b\uff0c\u6709\u4e86\u79ef\u5206\uff0c\u6bd4\u4f8b\u8981\u4e0b\u964d\uff0c\u8fd9\u5bfc\u81f4\u63a7\u5236\u7cbe\u5ea6\u4e0b\u964d</li> </ul> \\(K_C\\) \\(T_I\\) \u4f59\u5dee \u8870\u51cf\u6bd4 \u7a33\u5b9a\u6027 \u4e0d\u53d8 \u53d8\u5c0f 0 \u53d8\u5dee <ul> <li> <p>\u5e94\u7528\u573a\u5408\uff1a \u7cfb\u7edf\u52a8\u6001\u7279\u6027\u6bd4\u8f83\u5feb\uff08\u4e0d\u80fd\u7528\u5fae\u5206\uff09\uff0c\u5927\u60ef\u6027\u7cfb\u7edf\uff0c\u79ef\u5206\u4f5c\u7528\u4e0d\u80fd\u592a\u5f3a</p> </li> <li> <p>\u6ce8\u610f\u79ef\u5206\u9971\u548c\u95ee\u9898\u3002</p> </li> </ul>"},{"location":"Ctrl/ProcessControl/PID/#pid-control","title":"PID control","text":"\\[ u(t)=u_0+K_c\\left(e(t) +\\frac{1}{T_i}\\int_0^te(s)ds+ T_d\\frac{d}{dt} e(t) \\right) \\] <p>\u4f20\u9012\u51fd\u6570\u4e3a</p> \\[ G_C(s)=K_C\\left(1+\\frac{1}{T_i s}+T_ds\\right) \\] <ul> <li> <p>\u7cfb\u7edf\u65f6\u95f4\u5e38\u6570\u51cf\u5c0f\uff0c</p> </li> <li> <p>\u9ad8\u9891\u4fe1\u53f7\u4e0d\u5b9c\u4f7f\u7528\u5fae\u5206\uff0c\u4f1a\u9020\u6210\u632f\u8361\uff0c\u5efa\u8bae\u5f15\u5165\u5fae\u5206\u524d\u5148\u4f7f\u7528\u4e00\u8282\u6ee4\u6ce2\u6216\u5e73\u5747\u6ee4\u6ce2\u3002</p> </li> <li> <p>\u5fae\u5206\u5bf9\u4e8e\u7eaf\u6ede\u540e\u6ca1\u6709\u4f5c\u7528</p> </li> </ul>"},{"location":"Ctrl/ProcessControl/PID/#pid","title":"PID \u5e94\u7528\u573a\u5408","text":"\u63a7\u5236\u5668 P\u63a7\u5236 PI\u63a7\u5236 PID\u63a7\u5236 \u5e94\u7528\u573a\u5408 \u5de5\u827a\u8981\u6c42\u4e0d\u9ad8\uff0c\u5982\u50a8\u7f50\u6db2\u4f4d\uff08\u4fdd\u6301\u5728\u4e0a\u4e0b\u754c\u5185\u90fd\u53ef\uff0c\u4e0d\u8981\u6c42\u4f59\u5dee\u4e3a0\uff09 \u6ede\u540e\u5c0f\uff0c\u52a8\u6001\u7279\u6027\u597d\uff0c\u5982\u538b\u529b\u3001\u6d41\u91cf\u63a7\u5236 \u8d1f\u8f7d\u53d8\u5316\u5927\uff0c\u65f6\u95f4\u5e38\u6570\u5927\u7684\u7cfb\u7edf\uff0c\u5982\u6e29\u5ea6\u4e0e\u6210\u5206\u63a7\u5236"},{"location":"Ctrl/ProcessControl/PID/#pid_1","title":"\u6570\u5b57PID\u63a7\u5236\u5668","text":""},{"location":"Ctrl/ProcessControl/PID/#_1","title":"\u6570\u5b57\u6ee4\u6ce2\u5668","text":"<ul> <li>\u4e2d\u4f4d\u503c\u6ee4\u6ce2: \u8fde\u7eed\u91c7\u6837\u4e09\u6b21\uff0c\u4ece\u4e2d\u9009\u62e9\u5927\u5c0f\u5c45\u4e2d\u7684\u503c\u4f5c\u4e3a\u6709\u6548\u6d4b\u91cf\u4fe1\u53f7\u3002</li> </ul> <p>\u6ce8: \u5bf9\u4e8e\u6d41\u91cf\u68c0\u6d4b\u4e0d\u9002\u7528\uff0c\u56e0\u4e3a\u6d41\u91cf\u53d8\u5316\u592a\u5feb\u3002</p> <ul> <li>\u9012\u63a8\u5e73\u5747\u6ee4\u6ce2: \u9009\u62e9\u7b2ck\u6b21\u91c7\u6837\u4e4b\u524d\u7684N\u6b21\u91c7\u6837\u7684\u5e73\u5747</li> </ul> \\[ \\overline{y}(k)=\\frac{1}{N}\\sum_{i=0}^{N-1}y(k-i) \\] <p>\u6ce8: \\(N\\) \u8d8a\u5927\uff0c\u6ee4\u6ce2\u6548\u679c\u597d\uff0c\u4f46\u4f1a\u6709\u6ede\u540e</p> <p>\u6ce8: \u4e00\u822c\u6d41\u91cf \\(N=12\\), \u538b\u529b \\(N=4\\), \u6e29\u5ea6\u4e0d\u4f7f\u7528</p> <ul> <li> <p>\u52a0\u6743\u5e73\u5747\u6ee4\u6ce2: \u52a0\u6743\u7684\u9012\u63a8\u5e73\u5747\u6ee4\u6ce2\u3002</p> </li> <li> <p>\u4e00\u9636\u6ede\u540e\u6570\u5b57\u6ee4\u6ce2: \u53ea\u6709\u4e24\u9879\u7684\u52a0\u6743\u5e73\u5747\u6ee4\u6ce2\u3002</p> </li> </ul>"},{"location":"Ctrl/ProcessControl/PID/#pid_2","title":"\u6570\u5b57PID\u63a7\u5236\u7b97\u6cd5","text":"<ul> <li>\u4f4d\u7f6e\u5f0f\u548c\u589e\u91cf\u5f0f</li> </ul> \u63a7\u5236\u7b97\u6cd5 \u4f4d\u7f6e\u5f0f \u589e\u91cf\u5f0f \u8bef\u5dee \u5bb9\u6613\u7d2f\u8ba1\u8bef\u5dee \u4e0d\u5bb9\u6613\u4ea7\u751f\u7d2f\u8ba1\u8bef\u5dee \u79ef\u5206\u9971\u548c \u5bb9\u6613\u9650\u5e45\uff0c\u9020\u6210\u79ef\u5206\u9971\u548c \u4e0d\u5bb9\u6613 <ul> <li> <p>\u4e0d\u5b8c\u5168\u5fae\u5206\uff1a\u5c06\u4e00\u9636\u4f4e\u901a\u6ee4\u6ce2\u5668\\(\\frac{1}{1+T_is}\\)\u52a0\u5230\u5fae\u5206\u73af\u8282\uff0c\u4f7f\u5f97\u5fae\u5206\u73af\u8282\u5206\u591a\u6b21\u8f93\u51fa\uff0c\u6bcf\u6b21\u8f93\u51fa\u5e45\u503c\u8f83\u5c0f\u3002\u76ee\u6807\u662f\u9632\u6b62\u9636\u8dc3\u5f88\u5927\u65f6\u5fae\u5206\u8f93\u51fa\u5f88\u5927\u3002</p> </li> <li> <p>\u5fae\u5206\u5148\u884c\uff1a\u5c06\u5fae\u5206\u73af\u8282\u53ea\u52a0\u5728\u6d4b\u91cf\u53d8\u9001\u73af\u8282\uff0c\u4e0d\u52a0\u5728\u8bbe\u5b9a\u503c\u540e\uff0c\u53ef\u4ee5\u9632\u6b62\u8bbe\u5b9a\u503c\u7a81\u53d8\u7684\u5f71\u54cd\u3002\u5bf9\u4e8e\u6e29\u5ea6\u63a7\u5236\u5668\u6bd4\u8f83\u5e38\u7528\u3002</p> </li> </ul>"},{"location":"Ctrl/ProcessControl/PID/#_2","title":"\u6d41\u91cf\u63a7\u5236\u56de\u8def","text":"<ul> <li> <p>\u7279\u70b9: \u54cd\u5e94\u5feb, \u6d4b\u91cf\u566a\u58f0\u5927</p> </li> <li> <p>PI\u63a7\u5236\uff0c\u4e14\u6bd4\u4f8b\u5c0f\uff0c\u79ef\u5206\u5927</p> </li> </ul>"},{"location":"Ctrl/ProcessControl/Control_Strategy/","title":"Control Strategy","text":""},{"location":"Ctrl/ProcessControl/Control_Strategy/#cascade-control","title":"Cascade Control","text":""},{"location":"Ctrl/ProcessControl/Control_Strategy/#control-with-intension","title":"Control with Intension","text":""},{"location":"Ctrl/ProcessControl/Control_Strategy/#multi-variables-control","title":"Multi-Variables Control","text":""},{"location":"Ctrl/ProcessControl/Control_Strategy/#feedforward-control","title":"Feedforward Control","text":""},{"location":"Ctrl/ProcessControl/Control_Strategy/Cascade_Control/","title":"Cascade Control","text":"<ul> <li>\u65b9\u5757\u56fe</li> </ul> <p>\u5e38\u7528\u672f\u8bed</p> <ul> <li> <p>\u526f\u56de\u8def: \u968f\u52a8\u63a7\u5236</p> </li> <li> <p>\u4e3b\u56de\u8def: \u5b9a\u5236\u63a7\u5236</p> </li> </ul> <p>\u76f8\u6bd4\u4e8e\u4e00\u822c\u63a7\u5236\u56de\u8def\uff0c\u5f15\u5165\u4e86\u4e2d\u95f4 (\u526f) \u53d8\u91cf\uff0c\u4f46\u662f\u4ecd\u7136\u662f\u4e00\u4e2a\u5b9a\u503c\u63a7\u5236\u3002</p>"},{"location":"Ctrl/ProcessControl/Control_Strategy/Cascade_Control/#_1","title":"\u4e32\u7ea7\u63a7\u5236\u7684\u4f18\u70b9","text":"<p>\u4e32\u7ea7\u63a7\u5236\u7cfb\u7edf\u7684\u4f18\u70b9</p> <ul> <li>\u526f\u56de\u8def\u5feb\u901f\u8c03\u8282\uff0c\u6709\u6548\u514b\u670d\u4e8c\u6b21\u6270\u52a8\u3002</li> </ul> \\[ F'_{D2}(s)=\\frac{F_{D2}(s)}{1+G_{c2}G_vG_{p2}G_{m2} } \\] <p>\u4e00\u822c\u6765\u8bf4\uff0c\\(G_{c2}G_vG_{p2}G_{m2}\\gg 1\\), \u6545\u80fd\u964d\u4f4e\u5e72\u6270.</p> <ul> <li>\u6539\u5584\u5bf9\u8c61\u7684\u52a8\u6001\u7279\u6027</li> </ul> <p>\u7ecf\u8fc7\u63a7\u5236\uff0c\u526f\u5bf9\u8c61\u7684\u4f20\u9012\u51fd\u6570\u4e3a</p> \\[ \\begin{equation} G'_{p2}(s)=\\frac{G_{c2} G_v G_{p2} }{1+G_{c2}G_vG_{p2}G_{m2} }\\label{Prime-Object} \\end{equation} \\] <p>\u5982\u679c\u539f\u526f\u5bf9\u8c61\\(G_{p2}(s)=\\frac{K_{p2}}{1+T_{p2}s}\\), \u5219</p> \\[ \\begin{align*} G'_{p2}(s)&amp;=\\frac{G_{c2} G_v K_{p2} }{(1+T_{p2}s)+(G_{c2}G_vK_{p2}G_{m2})}\\\\ &amp;=\\frac{G_{c2} G_v K_{p2} }{1+G_{c2}G_vK_{p2}G_{m2}+T_{p2}s}\\\\ &amp;=\\frac{G_{c2} G_v \\frac{K_{p2}}{1+G_{c2}G_vK_{p2}G_{m2}} }{1+\\frac{T_{p2}}{1+G_{c2}G_vK_{p2}G_{m2}}s}\\\\ \\end{align*} \\] <p>which means </p> \\[ \\begin{align*} K_{p2}'&amp;=\\frac{G_{c2} G_v}{1+G_{c2}G_vK_{p2}G_{m2}}K_{p2}\\\\ T_{p2}'&amp;=\\frac{1}{1+G_{c2}G_vK_{p2}G_{m2}}T_{p2} \\end{align*} \\] <p>Usually \\(1+G_{c2}G_vK_{p2}G_{m2}\\gg 1\\), then \\(T_{p2}'&lt;T_{p2}\\). \u65f6\u95f4\u5e38\u6570\u51cf\u5c0f\uff0c\u52a0\u5feb\u4e86\u526f\u73af\u7684\u54cd\u5e94\u901f\u5ea6\uff0c\u63d0\u9ad8\u4e86\u7cfb\u7edf\u7684\u5de5\u4f5c\u9891\u7387\u3002</p> <ul> <li>\u9c81\u68d2\u6027\u589e\u5f3a, \u80fd\u514b\u670d\u526f\u5bf9\u8c61\u589e\u76ca\u3001\u8c03\u8282\u9600\u7279\u6027\u53d8\u5316\u5bf9\u63a7\u5236\u6027\u80fd\u7684\u5f71\u54cd</li> </ul> <p>\u5728\u5f0f \\(\\ref{Prime-Object}\\) \u4e2d\uff0c\u82e5 \\(G_{c2}G_vK_{p2}G_{m2}\\gg 1\\), \u4e8e\u662f\u6709</p> \\[ G'_{p2}\\approx\\frac{G_{c2}G_vK_{p2}}{G_{c2}G_vK_{p2}G_{m2}}=\\frac{1}{G_{m2}} \\] <p>\u4e5f\u5c31\u662f\u8bf4\uff0c\u57fa\u672c\u4e0a\u526f\u5bf9\u8c61\u548c\u8c03\u8282\u9600 \\(G_v\\) , \u526f\u5bf9\u8c61\u589e\u76ca \\(G_{p2}\\) \u65e0\u5173\u3002\u4f46\u662f\u6211\u4eec\u7684\u63a7\u5236\u7cfb\u7edf\u5bf9\u4e8e \\(G_{m2}\\) \u6ca1\u6709\u9c81\u68d2\u6027\u3002</p>"},{"location":"Ctrl/ProcessControl/Control_Strategy/Cascade_Control/#_2","title":"\u526f\u53c2\u6570\u7684\u9009\u62e9","text":"<ul> <li> <p>\u4e3b\u526f\u5bf9\u8c61\u65f6\u95f4\u5e38\u6570\u8981\u9519\u5f00\u3002\u5373 \\(T_{02}&lt; T_{01}\\). \u5373\u526f\u56de\u8def\u65f6\u95f4\u5e38\u6570\u8981\u5c0f\uff0c\u4e00\u822c\u9009\u62e9\u6d41\u91cf\u3002\u4f46\u9664\u4e3b\u63a7\u5236\u5bf9\u8c61\u7684\u65f6\u95f4\u5e38\u6570\u4e0d\u80fd\u53d8\u4e4b\u5916\uff0c\u526f\u56de\u8def\u5e94\u8be5\u5305\u542b\u4e00\u4e9b\u4e3b\u56de\u8def\u4e4b\u5916\u7684\u5bf9\u8c61\uff0c\u4ee5\u52a0\u5feb\u8c03\u8282\u3002\u5f53\u7136\uff0c\u526f\u73af\u5305\u542b\u8d8a\u591a\u5bf9\u8c61\uff0c\u8c03\u8282\u4f1a\u8d8a\u6162\u3002</p> </li> <li> <p>\u5305\u542b\u5e72\u6270\u7684\u53c2\u6570\u8d8a\u591a\u8d8a\u597d\uff0c\u5c06\u975e\u7ebf\u6027\u5bf9\u8c61\u5f52\u4e8e\u526f\u56de\u8def\u3002</p> </li> <li> <p>\u526f\u53c2\u6570\u611f\u53d7\u7684\u5e72\u6270\u8981\u8d8a\u5feb\u8d8a\u597d</p> </li> </ul>"},{"location":"Ctrl/ProcessControl/Control_Strategy/Cascade_Control/#_3","title":"\u526f\u63a7\u5236\u5668\u9009\u62e9","text":"<ul> <li> <p>\u5141\u8bb8\u4f59\u5dee\u3002\u4e00\u822c\u7528P (\u6e29\u5ea6) , PI (\u6d41\u91cf, \u538b\u529b)\u3002</p> </li> <li> <p>PI\u63a7\u5236\u5668\u8981\u5f3a\u6bd4\u4f8b+\u5f31\u79ef\u5206\u3002</p> </li> </ul> <p>\u6ce8: \u6db2\u4f4d\u4e32\u7ea7\u63a7\u5236\u4e3b\u63a7\u5236\u5668\u4e0d\u9009\u62e9PID, \u9009\u62e9PI.</p>"},{"location":"Ctrl/ProcessControl/Control_Strategy/Cascade_Control/#_4","title":"\u4e3b\u526f\u63a7\u5236\u5668\u7684\u6297\u79ef\u5206\u9971\u548c","text":"<ul> <li> <p>\u526f\u63a7\u5236\u5668\u548c\u5e38\u89c4\u63a7\u5236\u5668\u4e00\u81f4\u3002</p> </li> <li> <p>\u4e3b\u63a7\u5236\u5668\u4e0d\u91c7\u7528\u526f\u63a7\u5236\u5668\u7684\u8f93\u5165\\(r_2=u_1\\), \u800c\u662f\u91c7\u7528\u526f\u5bf9\u8c61\u6d4b\u91cf\u503c \\(y_{m2}\\) \u4f5c\u4e3a\u79ef\u5206\u53cd\u9988\u503c\u3002\u6709\u4e24\u70b9\u89e3\u91ca\u3002</p> </li> </ul> Explanation <ul> <li> <p>\u5f53\u526f\u56de\u8def\u5931\u63a7\u65f6\uff0c\\(y_{m2}\\) \u548c \\(r_2\\) \u76f8\u4e92\u72ec\u7acb\uff0c\u4f7f\u5f97\u539f\u672c\u91c7\u7528 \\(r_2\\) \u6765\u79ef\u5206\u53cd\u9988\u7684\u56de\u8def\u56e0\u91c7\u7528 \\(y_{m2}\\) \u800c\u65ad\u5f00\u79ef\u5206\u4f5c\u7528\u3002</p> </li> <li> <p>\u53e6\u5916\uff0c\u5927\u6270\u52a8\u53bb\u9664\u540e\uff0c\u4e3b\u63a7\u5236\u5668\u504f\u5dee\u56de\u5230\u96f6\uff0c\u526f\u63a7\u5236\u5668\u504f\u5dee\u4e5f\u56de\u5230\u96f6\u3002</p> </li> </ul>"},{"location":"Ctrl/ProcessControl/Control_Strategy/Control_Intension/","title":"Control with Intension","text":""},{"location":"Ctrl/ProcessControl/Control_Strategy/Control_Intension/#uniform-control","title":"\u5747\u5300\u63a7\u5236 | Uniform Control","text":"<ul> <li> <p>\u539f\u56e0: \u6d41\u91cf\u8c03\u8282\u8f83\u4e3a\u8fc5\u901f\u548c\u5267\u70c8\u3002</p> </li> <li> <p>\u76ee\u6807: \u517c\u987e\u6d41\u91cf\u5e73\u7a33\u3001\u6db2\u4f4d\u5728\u5141\u8bb8\u533a\u95f4\u6ce2\u52a8\u3002</p> </li> <li> <p>\u5b9e\u73b0\u624b\u6bb5: \u8c03\u8282PID\u63a7\u5236\u5668\u7684\u53c2\u6570</p> </li> </ul>"},{"location":"Ctrl/ProcessControl/Control_Strategy/Control_Intension/#_1","title":"\u5747\u5300\u63a7\u5236\u7684\u5b9e\u73b0","text":"<ul> <li>\u4e00\u822c\u6765\u8bf4\uff0c\u7eaf \"\u6db2\u4f4d-\u6d41\u91cf\" \u63a7\u5236\u91c7\u7528\u5355\u56de\u8def\u6216\u8005\u4e32\u7ea7\u63a7\u5236\u56de\u8def\u3002\u5747\u5300\u63a7\u5236\u5728\u7ed3\u6784\u4e0a\u662f\u548c\u4e00\u822c\u63a7\u5236\u4e00\u81f4\u3002</li> </ul> <p>\u63a7\u5236\u5668\u53c2\u6570\u6574\u5b9a</p> <p>\u5747\u5300\u63a7\u5236\u7684\u63a7\u5236\u5668\u63a7\u5236\u4f5c\u7528\u5f31\u3002\u53cd\u6620\u5728\u63a7\u5236\u5668\u53c2\u6570\u4e0a\uff0c\u8981\u6c42</p> <ul> <li> <p>\u5bbd\u7684\u6bd4\u4f8b\u5ea6\uff0c\u6bd4\u4f8b\u589e\u76ca\u5c3d\u53ef\u80fd\u5c0f\u3002</p> </li> <li> <p>\u5927\u7684\u79ef\u5206\u65f6\u95f4\u3002</p> </li> <li> <p>\u4e0d\u4f7f\u7528\u5fae\u5206</p> </li> </ul>"},{"location":"Ctrl/ProcessControl/Control_Strategy/Control_Intension/#_2","title":"\u53cc\u51b2\u91cf\u5747\u5300\u63a7\u5236","text":"<p>\u4e24\u4e2a\u6d4b\u91cf\u91cf\uff0c\u5e76\u4f5c\u76f8\u51cf\u8fd0\u7b97\u540e\u4f5c\u4e3a\u6d4b\u91cf\u53d8\u9001\u3002\u63a7\u5236\u7684\u662f\u4e24\u8005\u7684\u5dee\u6052\u5b9a\u3002</p>"},{"location":"Ctrl/ProcessControl/Control_Strategy/Control_Intension/#ratio-control","title":"\u6bd4\u503c\u63a7\u5236 | Ratio Control","text":""},{"location":"Ctrl/ProcessControl/Control_Strategy/Control_Intension/#_3","title":"\u5b9a\u6bd4\u503c","text":"<p>\u5206\u7c7b</p> <p>\u6839\u636e\u95ed\u73af\u6570\u76ee\u6765\u5206\u7c7b\u3002</p> <ul> <li> <p>\u5f00\u73af\u6bd4\u503c\u63a7\u5236</p> </li> <li> <p>\u5355\u95ed\u73af\u6bd4\u503c\u63a7\u5236</p> </li> <li> <p>\u53cc\u95ed\u73af\u6bd4\u503c\u63a7\u5236</p> </li> </ul>"},{"location":"Ctrl/ProcessControl/Control_Strategy/Control_Intension/#_4","title":"\u53d8\u6bd4\u503c\u63a7\u5236","text":"<p>\u4ece\u63a7\u5236\u8d28\u91cf\u6765\u8bf4\uff0c\u5b9a\u6bd4\u503c\u63a7\u5236\u662f\u7406\u8bba\u4e0a\u7684\uff0c\u5f00\u73af\u63a7\u5236\uff0c\u6ca1\u6709\u8003\u8651\u5176\u4ed6\u5e72\u6270 (\u5982\u6e29\u5ea6\u3001\u538b\u529b\u3001\u6210\u5206\u4ee5\u53ca\u53cd\u5e94\u5668\u7684\u50ac\u5316\u5242\u8001\u5316\u95ee\u9898)</p> <p>\u6309\u7167\u67d0\u4e00\u5de5\u827a\u6307\u6807\u6765\u4fee\u6b63\u6bd4\u503c\u7cfb\u6570\u3002</p> <p>\u6ce8: \u4e5f\u53eb\u4e32\u7ea7\u6bd4\u503c\u63a7\u5236 (\u9759\u6001\u524d\u9988-\u4e32\u7ea7\u63a7\u5236\u7cfb\u7edf)\u3002</p> Explanation <ul> <li> <p>\u4e3b\u6d41\u91cf (\u53ef\u89c6\u4f5c\u5e72\u6270\u901a\u9053) \u5bf9\u526f\u6d41\u91cf\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u9759\u6001\u524d\u9988\u6d88\u9664\u3002</p> </li> <li> <p>\u526f\u6d41\u91cf\u81ea\u8eab\u7684\u5e72\u6270\uff0c\u901a\u8fc7\u526f\u56de\u8def\u52a0\u4ee5\u63a7\u5236\u3002</p> </li> <li> <p>\u6e29\u5ea6\u7684\u5f71\u54cd\uff0c\u6539\u53d8\u526f\u56de\u8def\u63a7\u5236\u5668\u7684\u8bbe\u5b9a\u503c\u3002</p> </li> </ul> <p>\u53ea\u662f\u8fd9\u91cc\u7684\u8bbe\u5b9a\u503c\u6070\u597d\u662f\u6bd4\u503c\u3002\u6545\u53c8\u53eb\u6bd4\u503c\u63a7\u5236\u3002</p>"},{"location":"Ctrl/ProcessControl/Control_Strategy/Control_Intension/#_5","title":"\u4e3b\u526f\u6d41\u91cf\u9009\u62e9","text":"<ul> <li>\u4e3b\u6d41\u91cf: \u8d77\u4e3b\u5bfc\u4f5c\u7528\u3001\u53ef\u6d4b\u4e0d\u53ef\u63a7 (\u5e72\u6270)\u3001\u6602\u8d35\u7684\u7269\u6599</li> </ul>"},{"location":"Ctrl/ProcessControl/Control_Strategy/Control_Intension/#_6","title":"\u6bd4\u503c\u7cfb\u6570\u7684\u786e\u5b9a","text":"<ul> <li>\u533a\u5206: \u6d41\u91cf\u6bd4\u503c \\(k\\) \u548c\u4eea\u8868\u6bd4\u503c \\(K\\).</li> </ul> <p>\u6298\u7b97\u65b9\u6cd5</p> <p>\u4e00\u822c\u6d41\u91cf\u7684\u6d4b\u91cf\u53d8\u9001\u4e3a\u5dee\u538b\u53d8\u9001\u5668\u3002\u5982\u679c</p> <p>(i) \u7ecf\u8fc7\u5f00\u65b9\u5668\uff0c\u5219\u6d41\u91cf\u6d4b\u91cf\u53d8\u9001\u503c(\u7535\u4fe1\u53f7, \\(I\\))\u548c\u771f\u5b9e\u6d41\u91cf \\(Q\\) \u4e4b\u95f4\u4e3a\u7ebf\u6027\u5173\u7cfb\u3002\u8bb0\u53d8\u9001\u503c\u91cf\u7a0b \\([I_a, I_b]\\), \u6d41\u91cf\u91cf\u7a0b \\([Q_a,Q_b]\\), \u4e00\u822c\u8ba4\u4e3a\\(Q_a=0\\), \u5219</p> \\[ I=\\frac{Q-Q_a}{(Q_b-Q_a)}(I_b-I_a)+I_a=\\frac{Q}{Q_b}(I_b-I_a)+I_a \\] <p>\u4e3b\u526f\u6d41\u91cf\u5206\u522b\u7528\u4e0b\u6807 \\(1\\), \\(2\\) \u8868\u793a\uff0c\u5219 \u4e24\u4e2a\u5dee\u538b\u53d8\u9001\u5668\u7684\u91cf\u7a0b\u6700\u5927\u503c\u5206\u522b\u4e3a \\(Q_{b1}\\), \\(Q_{b2}\\), \u7531 \\(k\\overset{\\Delta}{=}\\frac{Q_2}{Q_1}\\) (\u4e00\u822c\u4e3a\u526f/\u4e3b\uff0c\u7531\u4e3b\u6d41\u91cf\u4e58\u4ee5\u6bd4\u503c\u7cfb\u6570),</p> \\[ \\begin{align*} K&amp;\\overset{\\Delta}{=}\\frac{I_2-I_a}{I_1-I_a}\\\\ &amp;=\\frac{Q_2}{Q_1}\\frac{Q_{b1}}{Q_{b2}}\\\\ &amp;=k\\frac{Q_{b1}}{Q_{b2}} \\end{align*} \\] <p>(ii) \u4e0d\u7ecf\u8fc7\u5f00\u65b9\u5668\uff0c\u5219\u6d41\u91cf\u6d4b\u91cf\u53d8\u9001\u503c(\u7535\u4fe1\u53f7, \\(I\\))\u548c\u771f\u5b9e\u6d41\u91cf \\(Q\\) \u4e4b\u95f4\u4e3a\u5e73\u65b9\u5173\u7cfb\uff0c\u5373</p> \\[ I=\\frac{Q^2}{Q_b^2}(I_b-I_a)+I_a \\] <p>\u6b64\u65f6\u6d41\u91cf\u6bd4\u503c\u4ecd\u7136\u662f\u4e00\u6b21\u5f0f\u4e4b\u6bd4 \\(k\\overset{\\Delta}{=}\\frac{Q_2}{Q_1}\\), \u4ece\u800c</p> \\[ \\begin{align*} K&amp;\\overset{\\Delta}{=}\\frac{I_2-I_a}{I_1-I_a}\\\\ &amp;=\\frac{Q^2_2}{Q^2_1}\\frac{Q^2_{b1}}{Q^2_{b2}}\\\\ &amp;=k^2\\frac{Q^2_{b1}}{Q^2_{b2}} \\end{align*} \\] <p>\u6b64\u65f6\u7684\u653e\u5927\u500d\u6570\\(K_m=\\frac{dI}{dQ}=2(I_b-I_a)\\frac{Q}{Q_b^2}\\), \u975e\u6052\u5b9a\u503c\u3002\u800c\u52a0\u5f00\u65b9\u5668\uff0c\u653e\u5927\u7cfb\u6570\u4e3a\u6052\u5b9a\u503c\uff0c\u4e0d\u968f\u8d1f\u8f7d\u53d8\u5316\u3002</p>"},{"location":"Ctrl/ProcessControl/Control_Strategy/Control_Intension/#_7","title":"\u52a8\u6001\u8ddf\u8e2a","text":"<p>\u5982\u679c\u5e0c\u671b\u52a8\u6001\u8ddf\u8e2a\uff0c\u4f7f\u5f97\u526f\u6d41\u91cf\u80fd\u591f\u968f\u65f6\u8ddf\u968f\u4e3b\u6d41\u91cf\u7684\u53d8\u5316\u800c\u53d8\u5316\uff0c\u5219\u53ef\u4ee5\u5c1d\u8bd5\u5728\u5f00\u73af\u7ed3\u6784 (\u4e00\u822c\u4e0d\u518d\u5df2\u7ecf\u95ed\u73af\u7684\u7ed3\u6784\u91cc\u518d\u52a0\u8865\u507f\u5668, \u5f53\u7136\u53cc\u95ed\u73af\u80fd\u7528) \u4e2d\u52a0\u5165\u4e00\u4e2a\u8865\u507f\u5668 \\(G_z\\)\u3002\u5176\u4e8e\u4e3b\u6d41\u91cf\u53d8\u9001\u4fe1\u53f7\u8fdb\u5165\u4e58\u6cd5\u5668\u4e4b\u524d\u8fdb\u884c\u3002</p> <p>\u5229\u7528 \\(Q_1\\) \u5bf9 \\(Q_2\\) \u7684\u5f71\u54cd</p> \\[ \\frac{Q_2}{Q_1}=\\frac{G_{m1}G_z G_{p2}}{1+G_{m2}G_{p2}}=k \\] <p>\u4ece\u800c\u6c42\u5f97 \\(G_z\\).</p>"},{"location":"Ctrl/ProcessControl/Control_Strategy/Control_Intension/#_8","title":"\u6bd4\u503c\u5b9e\u73b0\u65b9\u6848","text":"<ul> <li> <p>\u4e58\u6cd5: \u5e38\u7528</p> </li> <li> <p>\u9664\u6cd5: \u4f1a\u5177\u6709\u5f88\u5f3a\u7684\u975e\u7ebf\u6027\u3002\u5bf9\u4e8e\u5f00\u73af\u9664\u6cd5\u5b9e\u73b0\u7684\u6bd4\u503c\u63a7\u5236\uff0c\u5176\u56de\u8def (\u526f\u6d41\u91cf) \u589e\u76ca\u5728\u53d8\u5316\u3002\u5047\u8bbe\u4f7f\u7528\u4e86\u5f00\u65b9\u5668\uff0c\u5219 \\(K=\\frac{Q_2}{Q_1}\\frac{Q_{b1}}{Q_{b2}}\\)</p> </li> </ul> \\[ \\frac{dK}{dQ_2}=\\frac{1}{Q_1}\\frac{Q_{b1}}{Q_{b2}}=\\frac{k}{Q_2}\\frac{Q_{b1}}{Q_{b2}} \\] <p>\u9759\u6001\u589e\u76ca\u4f1a\u53d8\u5316\u3002</p>"},{"location":"Ctrl/ProcessControl/Control_Strategy/Control_Intension/#_9","title":"\u53cc\u4ea4\u53c9\u6bd4\u503c\u63a7\u5236: \u903b\u8f91\u63d0\u964d\u95ee\u9898","text":""},{"location":"Ctrl/ProcessControl/Control_Strategy/Feedforward_Control/","title":"Feedforward Control (FFC)","text":"<p>\u6838\u5fc3: \u5bf9\u5e72\u6270\u4fe1\u53f7\u505a\u4e00\u5b9a\u5904\u7406\uff0c\u9001\u5230\u8c03\u8282\u9600\u3002\u4e4b\u540e\u7684\u53d8\u79cd\u524d\u9988\u5219\u662f\u5e7f\u4e49\u4e0a\u7684\u3002\u53ea\u8981\u6ca1\u6709\u5bf9\u88ab\u63a7\u53d8\u91cf\u8fdb\u884c\u76f4\u63a5\u68c0\u6d4b\uff0c\u5c31\u662f\u5f00\u73af\u63a7\u5236\u3002</p> <p>\u4e00\u822c\u5730\uff0c\u6709</p> \\[ G_{ff}=-\\frac{G_{pd}}{G_{v}G_{pc}G_{md}} \\] <p>\u6ce8: \u9009\u62e9\u7684\u5e72\u6270\u8981\u548c\u8c03\u8282\u9600\u76f8\u72ec\u7acb\u3002</p>"},{"location":"Ctrl/ProcessControl/Control_Strategy/Feedforward_Control/#_1","title":"\u7406\u8bba\u57fa\u7840: \u4e0d\u53d8\u6027\u539f\u7406","text":"<p>\u4e0d\u53d8\u6027\u539f\u7406</p> <ul> <li>\u7edd\u5bf9\u4e0d\u53d8\u6027 (\u52a8\u6001\u4e0d\u53d8\u6027)</li> </ul> <p>\u4f7f\u7528\u7ebf\u6027\u524d\u9988\u6765\u5efa\u6a21\uff0c\u6709</p> \\[ G_{ff}=-\\frac{G_{pd}}{G_{v}G_{pc}G_{md}} \\] <p>\u4e14 \\(G_{pd}(s)=\\frac{K_{pd}}{1+T_{pd}s}e^{-\\tau_{pd}s}\\), \\(G_{pc}(s)=\\frac{K_{pc}}{1+T_{pc}s}e^{-\\tau_{pc}s}\\), \\(G_{v}G_{md}=K_{m}\\) \u5219</p> \\[ G_{ff}=-K_{ff}\\frac{T_{pc}s+1}{T_{pd}s+1}e^{-\\tau_{ff} s} \\] <p>\u5176\u4e2d \\(K_{ff}=\\frac{K_{pd}}{K_{pc}k_{m}}\\), \\(\\tau_{ff}=\\max \\{0,\\tau_{pd}-\\tau_{pc}\\}\\).</p> <ul> <li>\u7a33\u6001\u4e0d\u53d8\u6027: \u9759\u6001\u524d\u9988\u7cfb\u7edf</li> </ul> \\[ K_{ff}(0)=-\\frac{K_{pd}}{K_{v}K_{pc}K_{md}} \\] <p>\u5bb9\u6613\u7528\u6bd4\u503c\u5668\u3001\u52a0\u6cd5\u5668\u5b9e\u73b0</p> <ul> <li> <p>\u8bef\u5dee\u4e0d\u53d8\u6027: w\u6270\u52a8\u5b58\u5728\u65f6\uff0c\u8f93\u51fa\u548c\u9884\u5b9a\u503c\u6709\u5fae\u5c0f\u7684\u504f\u5dee</p> </li> <li> <p>\u9009\u62e9\u4e0d\u53d8\u6027</p> </li> </ul>"},{"location":"Ctrl/ProcessControl/Control_Strategy/Feedforward_Control/#_2","title":"\u4e0e\u53cd\u9988\u63a7\u5236\u7684\u5bf9\u6bd4","text":"\u63a7\u5236\u65b9\u5f0f \u524d\u9988\u63a7\u5236 \u53cd\u9988\u63a7\u5236 \u68c0\u6d4b\u4fe1\u53f7 \u5e72\u6270\u91cf \u88ab\u63a7\u53d8\u91cf \u63a7\u5236\u4f5c\u7528\u53d1\u751f\u65f6\u95f4 \u5e72\u6270\u4f5c\u7528\u4e8e\u88ab\u63a7\u5bf9\u8c61\u524d \u5e72\u6270\u4f5c\u7528\u4e8e\u88ab\u63a7\u5bf9\u8c61\u4e4b\u540e \u5f00/\u95ed\u73af \u5f00\u6000 \u95ed\u73af\uff0c\u4f1a\u6709\u7a33\u5b9a\u6027\u95ee\u9898 \u611f\u53d7\u7684\u6270\u52a8\u4e2a\u6570 \u53ef\u6d4b\u91cf\u7684\u5e72\u6270\uff0c\u6709\u9650\u4e2a \u5305\u542b\u5f71\u54cd\u88ab\u63a7\u53d8\u91cf\u7684\u6240\u6709\u5e72\u6270 \u5bf9\u65f6\u53d8\u3001\u975e\u7ebf\u6027\u5bf9\u8c61\u7684\u9002\u5e94\u6027 \u4e0d\u591f\u5f3a\uff0c\u6a21\u578b\u5f80\u5f80\u65f6\u4e0d\u53d8\u3001\u7ebf\u6027\uff0c\u5bf9\u4e8e\u6b64\u7c7b\u4fe1\u53f7\u4e0d\u591f\u9c81\u68d2 \u5177\u6709\u9c81\u68d2\u6027"},{"location":"Ctrl/ProcessControl/Control_Strategy/Feedforward_Control/#-","title":"\u524d\u9988-\u53cd\u9988\u63a7\u5236","text":"<p>\u7531\u4e8e\u6709\u53cd\u9988\u4f5c\u7528\uff0c\u6211\u4eec\u4f7f\u7528\u7ebf\u6027\u53e0\u52a0\u7684\u65b9\u6cd5\u6c42\u5f97\u4f20\u9012\u51fd\u6570. \u4f46\u6700\u540e\u7ed3\u8bba\u662f\u4e00\u81f4\u7684\u3002</p> <p> </p>"},{"location":"Ctrl/ProcessControl/Control_Strategy/Feedforward_Control/#-_1","title":"\u524d\u9988-\u4e32\u7ea7\u63a7\u5236","text":"<p>\u4e3e\u4e00\u4e2a\u6362\u70ed\u5668\u7684\u524d\u9988\u4e32\u7ea7\u63a7\u5236</p> <p> </p>"},{"location":"Ctrl/ProcessControl/Control_Strategy/Multi_Control/","title":"Multi-Variables Control","text":""},{"location":"Ctrl/ProcessControl/Control_Strategy/Multi_Control/#selection-control","title":"\u9009\u62e9\u63a7\u5236 | Selection Control","text":"<p>\u53c8\u53eb\u7ea6\u675f\u63a7\u5236\u3001\u8d85\u9a70\u63a7\u5236 (\u5bf9\u6781\u9650\u7684\u63a7\u5236)</p> <p>\u76ee\u6807: \u4e3a\u5b9e\u73b0\u8f6f\u4fdd\u62a4\u800c\u8bbe\u8ba1</p> <p>\u6838\u5fc3\u70b9: \u4e00\u4e2a\u64cd\u4f5c\u53d8\u91cfMV, \u4e24\u4e2a\u53ca\u4ee5\u4e0a\u7684\u88ab\u63a7\u53d8\u91cfCV. \u4e00\u4e2a\u662f\u5e38\u89c4CV, \u8981\u7cbe\u786e\u63a7\u5236, \u5269\u4f59\u7684\u662f\u8f85\u52a9CV, \u8981\u63a7\u5236\u5728\u4e00\u5b9a\u8303\u56f4\u5185\u4ee5\u786e\u4fdd\u5b89\u5168\u3002</p>"},{"location":"Ctrl/ProcessControl/Control_Strategy/Multi_Control/#_1","title":"\u9ad8\u4f4e\u9009\u62e9\u5668","text":"<p>\u6839\u636e\u8d85\u9650\u65f6\uff0c\u9600\u95e8\u8f93\u5165\u503c\u7684\u53d8\u5316\u6765\u786e\u5b9a\u3002</p>"},{"location":"Ctrl/ProcessControl/Control_Strategy/Multi_Control/#_2","title":"\u9632\u79ef\u5206\u9971\u548c","text":"<p>\u603b\u6709\u4e00\u53f0\u5904\u4e8e\u5f00\u73af\u72b6\u6001\u3002</p> <p>\u6211\u4eec\u9009\u62e9 \u9650\u5e45\u6cd5\u3001\u5916\u53cd\u9988\u6cd5\u3001\u79ef\u5206\u5207\u9664\u6cd5 \u4e2d\u7684 \u5916\u53cd\u9988\u6cd5 (RFB)\u3002</p>"},{"location":"Ctrl/ProcessControl/Control_Strategy/Multi_Control/#_3","title":"\u5e94\u7528","text":"<p>(i) \u88ab\u63a7\u53d8\u91cf\u7684\u6d4b\u91cf\u503c\u7684\u9009\u62e9</p> <ul> <li> <p>\u56fa\u5b9a\u5e8a\u53cd\u5e94\u5668\u7684\u70ed\u70b9\u6e29\u5ea6\uff0c\u6709\u591a\u4e2a\u6d4b\u91cf\u70b9\uff0c\u9009\u62e9\u9ad8\u8005</p> </li> <li> <p>\u4e24\u53f0\u6210\u5206\u5206\u6790\u4eea\u8868\uff0c\u9009\u62e9\u4e24\u8005\u6d4b\u91cf\u8005\u7684\u9ad8\u8005\u4f5c\u4e3a\u53cd\u9988</p> </li> </ul> <p>(ii) \u53d8\u7ed3\u6784\u63a7\u5236</p>"},{"location":"Ctrl/ProcessControl/Control_Strategy/Multi_Control/#split-range-control","title":"\u5206\u7a0b\u63a7\u5236 | Split-range Control","text":"<p>\u4e00\u4e2a\u63a7\u5236\u5668\u8f93\u51fa\u4fe1\u53f7\uff0c\u63a7\u5236\u591a\u4e2a\u9600\u95e8\uff0c\u4e0d\u540c\u7684\u4fe1\u53f7\u533a\u95f4\uff0c\u5bf9\u6620\u4e0d\u540c\u7684\u9600\u95e8\u88ab\u63a7\u5236\u3002\u4e0d\u4f1a\u540c\u65f6\u63a7\u5236\u591a\u4e2a\u9600\u95e8</p> <p>\u6838\u5fc3\u70b9: \u4e00\u4e2a\u88ab\u63a7\u53d8\u91cfCV, \u4e24\u4e2a\u53ca\u4ee5\u4e0a\u64cd\u4f5c\u53d8\u91cfMV\u3002</p>"},{"location":"Ctrl/ProcessControl/Control_Strategy/Multi_Control/#_4","title":"\u53cc\u63a7\u5236\u9600\u7684\u5206\u7a0b\u7ec4\u5408","text":"<ul> <li> <p>\u8fde\u7eed\u5206\u7a0b</p> </li> <li> <p>\u95f4\u9699\u5206\u7a0b: \u6709\u6b7b\u533a\uff0c\u63a7\u5236\u5668\u6ca1\u6709\u63a7\u5236\u4efb\u4f55\u4e00\u4e2a\u9600\u95e8</p> </li> <li> <p>\u91cd\u53e0\u5206\u7a0b: \u6709\u91cd\u53e0\u533a\uff0c\u63a7\u5236\u5668\u540c\u65f6\u63a7\u5236\u4e24\u4e2a\u9600\u95e8\uff1b100%\u91cd\u53e0\u533a\uff0c\u53eb\u505a\u4e24\u4e2a\u9600\u7684\u5e76\u8054\u3002</p> </li> </ul>"},{"location":"Ctrl/ProcessControl/Control_Strategy/Multi_Control/#_5","title":"\u5b9e\u73b0","text":"<p>\u4f7f\u7528\u6bd4\u503c\u5668+\u8d1f\u8377\u5206\u914d\u5668</p>"},{"location":"Ctrl/ProcessControl/Control_Strategy/Multi_Control/#_6","title":"\u76ee\u7684","text":"<ul> <li>\u6269\u5927\u63a7\u5236\u9600\u7684\u53ef\u8c03\u8303\u56f4</li> </ul> <p>\u6d89\u53ca\u5230\u4e00\u4e2a\u589e\u76ca\u7a81\u53d8\u95ee\u9898\u3002\u9700\u8981\u6362\u7b97\u3002\u6839\u636e\u56fe\u50cf\u7684\u8d77\u70b9\u3001\u7ec8\u70b9\uff0c\u6839\u636e\u5206\u7a0b\u7eb5\u5750\u6807 \\(Q_{Amax}\\) \u6c42\u53d6\u76f4\u7ebf\u5bf9\u5e94\u70b9\u7684\u6a2a\u5750\u6807\u3002</p> <ul> <li>\u6ee1\u8db3\u64cd\u4f5c\u5de5\u827a\u7684\u7279\u6b8a\u8981\u6c42 (\u6c14\u5f00\u3001\u6c14\u5173\u7ec4\u5408)</li> </ul> <p>\u95f4\u6b47\u653e\u70ed\u53cd\u5e94\u5668: \u8003\u8651\u521d\u6001\u4fe1\u53f7\u3001\u7ec8\u6001\u4fe1\u53f7\u3002\u4e24\u8005\u589e\u76ca\u4e0d\u540c\uff0c\u9020\u6210\u975e\u7ebf\u6027\uff0c\u5982\u679c\u51b7\u5374\u90e8\u5206\u589e\u76ca\u662f\u52a0\u70ed\u90e8\u5206\u7684 \\(1/3\\)\uff0c\u5219\u53ef\u4ee5\u589e\u52a0\u51b7\u5374\u90e8\u5206\u7684\u659c\u7387\u3002</p> <p>\u50a8\u6c2e\u5206\u7a0b\u63a7\u5236\u3002\u4e3a\u907f\u514d\u4e24\u9600\u95e8\u9891\u7e41\u5f00\u95ed\uff0c\u5728\u4e2d\u95f4\u8bbe\u7f6e\u4e00\u4e2a\u95f4\u6b47\u533a (\u6b7b\u533a)\u3002</p>"},{"location":"Ctrl/ProcessControl/Control_Strategy/Multi_Control/#valve-control","title":"\u9600\u4f4d\u63a7\u5236 | Valve Control","text":"<p>\u4e5f\u662f\u591a\u4e2a\u64cd\u7eb5\u53d8\u91cf\uff0c\u4e00\u4e2a\u88ab\u63a7\u53d8\u91cf\u3002</p> <p>\u4e3b\u8981\u662f\u8981\u8003\u8651\u4e24\u4e2a\u9600\u95e8\u5730\u4f4d\u7684\u4e0d\u5747\u7b49\u3002\u65e2\u8003\u8651\u7ecf\u6d4e\u6027\u3001\u5408\u7406\u6027 (\u4e00\u4e2a\u64cd\u7eb5\u53d8\u91cf)\uff0c\u53c8\u8003\u8651\u5feb\u901f\u6027\u548c\u6709\u6548\u6027 (\u53e6\u4e00\u4e2a\u64cd\u7eb5\u53d8\u91cf)\u3002</p> <ul> <li> <p>\u4e3b\u56de\u8def\u9009\u62e9: \u9009\u62e9\u63a7\u5236\u54c1\u8d28\u597d\u7684\uff0c\u52a8\u6001\u6548\u679c\u597d\u7684\u9600\u95e8\u4f5c\u4e3a\u4e3b\u56de\u8def\u63a7\u5236\u5668\u7684\u8f93\u51fa\u63a7\u5236\u9600\u3002\u8bbe\u5b9a\u503c\u662f\u6e29\u5ea6\u6307\u6807\u3002</p> </li> <li> <p>\u9600\u4f4d\u63a7\u5236\u5668: \u9009\u62e9\u5408\u7406\u3001\u7ecf\u6d4e\u7684\u9600\u4f5c\u4e3a\u9600\u4f4d\u63a7\u5236\u5668\u7684\u63a7\u5236\u9600\u3002\u8ba9\u4e3b\u56de\u8def\u7684\u9600\u95e8\u7684\u5f00\u5ea6\u6210\u4e3a\u9600\u4f4d\u63a7\u5236\u5668\u7684\u88ab\u63a7\u53d8\u91cf (\u8981\u4f5c\u4e3a\u6d4b\u91cf\u53d8\u9001\u8f93\u5165), \u8bbe\u5b9a\u503c\u662f\u4e3b\u56de\u8def\u9600\u95e8\u7684\u8bbe\u5b9a\u503c\uff0c\u4e00\u822c\u6bd4\u8f83\u5c0f\uff0c\u4ee5\u5b9e\u73b0\u7ecf\u6d4e\u6027\u3002\u901a\u8fc7\u9600\u4f4d\u63a7\u5236\u5668\u7684\u53c2\u6570\u6574\u5b9a\uff0c\u7528\u5bbd\u6bd4\u4f8b\u5ea6\u548c\u5927\u7684\u79ef\u5206\u65f6\u95f4\uff0c\u5b9e\u73b0\u7f13\u6162\u7684\u63a7\u5236\u3002</p> </li> </ul>"},{"location":"Ctrl/ProcessControl/Control_Strategy/Multi_Control/#review-of-the-above-control-strategy","title":"Review of the above control strategy","text":"\u63a7\u5236\u65b9\u5f0f \u9009\u62e9\u63a7\u5236 \u5206\u7a0b\u63a7\u5236 \u9600\u4f4d\u63a7\u5236 \u88ab\u63a7\u53d8\u91cfCV (\u6d4b\u91cf\u53d8\u9001\u7684\u53d8\u91cf) \\(\\geq 2\\) \\(1\\) \\(1\\) \u64cd\u7eb5\u53d8\u91cfMV (\u9600\u95e8\u4e2a\u6570) \\(1\\) \\(\\geq 2\\) \u5730\u4f4d\u7b49\u4ef7 \\(\\geq 2\\) \u5730\u4f4d\u4e0d\u7b49\u4ef7"},{"location":"Ctrl/Sensing%26Detection/","title":"Sensing &amp; Detection","text":""},{"location":"Ctrl/Sensing%26Detection/#outline","title":"Outline","text":""},{"location":"Ctrl/Sensing%26Detection/#calculation","title":"Calculation","text":"<ul> <li> <p>\u5e94\u53d8\u7247\uff0c\u6e29\u5ea6\u8865\u507f</p> </li> <li> <p>\u70ed\u7535\u5076\u8865\u507f\u5bfc\u7ebf\u3001\u7535\u6865\u8ba1\u7b97</p> </li> <li> <p>\u68c0\u6d4b\u4eea\u8868\uff08\u538b\u529b\u68c0\u6d4b\u4eea\u8868\uff09\u7684\u51c6\u786e\u5ea6\u7b49\u7ea7\u3001\u91cf\u7a0b\u9009\u62e9</p> </li> <li> <p>\u7269\u4f4d\u6d4b\u91cf\uff0c\u5dee\u538b\u5f0f\u6db2\u4f4d\u8ba1\u7684\u91cf\u7a0b\u8fc1\u79fb</p> </li> <li> <p>\u8282\u6d41\u5f0f\u6d41\u91cf\u8ba1\uff0c\u5e26\u5f00\u65b9\u5668\u548c\u4e0d\u5e26\u5f00\u65b9\u5668</p> </li> </ul>"},{"location":"Ctrl/Sensing%26Detection/Midterm/","title":"Midterm Exam","text":""},{"location":"Ctrl/Sensing%26Detection/Midterm/#_1","title":"\u7b80\u7b54\u9898","text":""},{"location":"Ctrl/Sensing%26Detection/Midterm/#_2","title":"\u201c\u5dee\u52a8\u201d\u548c\u201c\u53c2\u6bd4\u201d\u8bbe\u8ba1\u65b9\u6cd5","text":"\u7b80\u8981\u6bd4\u8f83\u4e00\u4e0b\u201c\u5dee\u52a8\u5f0f\u201d\u548c\u201c\u53c2\u6bd4\u5f0f\u201d\u4e24\u79cd\u68c0\u6d4b\u4eea\u8868\u8bbe\u8ba1\u65b9\u6cd5\u7684\u5f02\u540c\u70b9\u3002  <p>\u5f02\uff1a</p> <ul> <li>\u5dee\u52a8\u5f0f\u662f\u91c7\u7528\u4e24\u4e2a\u8f6c\u6362\u5143\u4ef6\u540c\u65f6\u611f\u53d7\u654f\u611f\u5143\u4ef6\u7684\u8f93\u51fa\u91cf\uff0c\u5e76\u628a\u5b83\u8f6c\u6362\u6210\u4e24\u4e2a\u6027\u8d28\u76f8\u540c\uff0c\u4f46\u6cbf\u76f8\u53cd\u65b9\u5411\u53d8\u5316\u7684\u7269\u7406\u91cf\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u4f7f\u5f97\u6709\u6548\u8f93\u51fa\u4fe1\u53f7\u63d0\u9ad8\u4e00\u500d\uff0c\u4fe1\u566a\u6bd4\u5f97\u5230\u6539\u5584\uff0c\u975e\u7ebf\u6027\u8bef\u5dee\u51cf\u5c0f\uff1b\u6613\u4e8e\u5b9e\u73b0\u521d\u59cb\u72b6\u6001\uff08\u201c\u96f6\u201d\u8f93\u5165\uff09\u7684\u96f6\u8f93\u51fa\uff0c\u80fd\u6d88\u9664\u90e8\u5206\u73af\u5883\u56e0\u7d20\u7684\u5f71\u54cd\u3002</li> <li>\u53c2\u6bd4\u5f0f\u662f\u91c7\u7528\u4e24\u4e2a\u6027\u80fd\u5b8c\u5168\u76f8\u540c\u7684\u68c0\u6d4b\u5143\u4ef6\uff0c\u4ed6\u4eec\u540c\u65f6\u611f\u53d7\u73af\u5883\u6761\u4ef6\u91cf\uff0c\u4f46\u53ea\u6709\u4e00\u4e2a\u611f\u53d7\u88ab\u6d4b\u91cf\u3002\u5176\u4f5c\u7528\u662f\u5c06\u540c\u65f6\u4f5c\u7528\u5728\u4e24\u4e2a\u68c0\u6d4b\u5143\u4ef6\u4e0a\u7684\u73af\u5883\u6761\u4ef6\u91cf\u7684\u5e72\u6270\u4fe1\u606f\u9664\u53bb\uff0c\u5bf9\u88ab\u6d4b\u91cf\u4fe1\u606f\u8fdb\u884c\u653e\u5927\u3002\u53c2\u6bd4\u53ef\u4ee5\u8f83\u597d\u5730\u6d88\u9664\u5e72\u6270\u6765\u6e90\u660e\u786e\u5730\u73af\u5883\u6761\u4ef6\u91cf\u7684\u5f71\u54cd\u3002</li> </ul> <p>\u540c\uff1a</p> <ul> <li>\u90fd\u80fd\u4e00\u5b9a\u7a0b\u5ea6\u514b\u670d\u73af\u5883\u5e72\u6270\u3002</li> </ul>"},{"location":"Ctrl/Sensing%26Detection/Midterm/#_3","title":"\u538b\u963b\u5f0f\u3001\u538b\u7535\u5f0f\u3001\u538b\u78c1\u5f0f","text":"\u4ece\u5e94\u7528\u89d2\u5ea6\u8ba8\u8bba\u5e76\u5206\u6790\u538b\u963b\u5f0f\u3001\u538b\u7535\u5f0f\u548c\u538b\u78c1\u5f0f\u68c0\u6d4b\u5143\u4ef6\u5404\u6709\u4ec0\u4e48\u7279\u70b9\uff1f  <ul> <li>\u538b\u963b\u5f0f</li> </ul> <p>\u6d4b\u91cf\u8303\u56f4\u5bbd\u3001\u51c6\u786e\u5ea6\u9ad8\uff0c\u54cd\u5e94\u901f\u5ea6\u5feb\uff0c\u9002\u5408\u9759\u6001\u548c\u52a8\u6001\u6d4b\u91cf\uff0c\u4f7f\u7528\u5bff\u547d\u957f\uff0c\u6027\u80fd\u7a33\u5b9a\uff0c\u4ef7\u683c\u4fbf\u5b9c\uff0c\u53ef\u4ee5\u5728\u9ad8\u5f3a\u5ea6\u6076\u52a3\u73af\u5883\u4e0b\u5de5\u4f5c\u3002</p> <p>\u4f46\u6709\u8f93\u51fa\u4fe1\u53f7\u5fae\u5f31\uff0c\u6297\u5e72\u6270\u80fd\u529b\u5dee\uff0c\u6613\u53d7\u6e29\u5ea6\u7b49\u73af\u5883\u56e0\u7d20\u7684\u5f71\u54cd\uff0c\u5927\u5e94\u53d8\u72b6\u6001\u4e0b\u6709\u8f83\u5927\u7684\u975e\u7ebf\u6027\u3002</p> <p>\u5e94\u7528\uff1a\u88ab\u7c98\u8d34\u5728\u5404\u79cd\u5f39\u6027\u5143\u4ef6\u4e0a\uff0c\u4ee5\u611f\u53d7\u538b\u529b\u53d8\u5316\u3002</p> <ul> <li>\u538b\u7535\u5f0f</li> </ul> <p>\u5177\u6709\u9891\u5e26\u5bbd\u3001\u7075\u654f\u5ea6\u9ad8\u3001\u7ed3\u6784\u7b80\u5355\u3001\u5de5\u4f5c\u53ef\u9760\u3001\u91cd\u91cf\u8f7b\u7b49\u4f18\u70b9\u3002</p> <p>\u4f46\u53ea\u9002\u5408\u52a8\u6001\u6d4b\u91cf\u3002</p> <p>\u5e94\u7528\uff1a\u53ef\u4ee5\u5b9e\u73b0\u529b\u3001\u538b\u529b\u3001\u52a0\u901f\u5ea6\u548c\u626d\u77e9\u7b49\u7269\u7406\u91cf\u7684\u6d4b\u91cf\u3002</p> <ul> <li>\u538b\u78c1\u5f0f</li> </ul> <p>\u8f93\u51fa\u529f\u7387\u5927\uff0c\u6297\u5e72\u6270\u80fd\u529b\u53ca\u8fc7\u8f7d\u80fd\u529b\u5f3a\uff0c\u4fbf\u4e8e\u5236\u9020\uff0c\u7ecf\u6d4e\u5b9e\u7528\uff0c\u5e76\u80fd\u5728\u6076\u52a3\u7684\u6761\u4ef6\u4e0b\u957f\u671f\u4f7f\u7528\u3002</p> <p>\u4f46\u6d4b\u91cf\u7cbe\u5ea6\u4e0d\u9ad8\uff0c\u53cd\u5e94\u901f\u5ea6\u8f83\u6162\u3002</p> <p>\u5e94\u7528\uff1a\u4e3b\u8981\u5e94\u7528\u4e8e\u6d4b\u529b\u3001\u79f0\u91cd\u3001\u6e29\u5ea6\u6d4b\u91cf\u53ca\u76c8\u5229\u65e0\u635f\u68c0\u6d4b\u7b49\u65b9\u9762\u3002</p>"},{"location":"Ctrl/Sensing%26Detection/Midterm/#_4","title":"\u51cf\u5c11\u968f\u673a\u8bef\u5dee\u3001\u975e\u7ebf\u6027\u8865\u507f","text":"\u68c0\u6d4b\u4eea\u8868\u5e38\u7528\u7684\u51cf\u5c11\u968f\u673a\u8bef\u5dee\u548c\u8fdb\u884c\u975e\u7ebf\u6027\u8865\u507f\u7684\u65b9\u6cd5\u4e3b\u8981\u6709\u54ea\u4e9b\uff1f  <ul> <li>\u51cf\u5c11\u968f\u673a\u8bef\u5dee\u7684\u65b9\u6cd5\u3002</li> </ul> <p>\u63d0\u9ad8\u68c0\u6d4b\u7cfb\u7edf\u51c6\u786e\u5ea6\uff0c\u5bf9\u6d4b\u91cf\u7ed3\u679c\u8fdb\u884c\u7edf\u8ba1\u5904\u7406\uff0c\u6291\u5236\u566a\u58f0\u5e72\u6270\u3002</p> <ul> <li>\u8fdb\u884c\u975e\u7ebf\u6027\u8865\u507f\u3002</li> </ul> <p>\u76f4\u63a5\u4e32\u8054\u6cd5\uff0c\u975e\u7ebf\u6027\u8d1f\u53cd\u9988\u6cd5\uff0c\u8f6f\u4ef6\u7ebf\u6027\u5316\u6cd5</p>"},{"location":"Ctrl/Sensing%26Detection/Midterm/#_5","title":"\u5f00\u73af\u3001\u95ed\u73af\u7ed3\u6784\u4eea\u8868","text":"\u6bd4\u8f83\u5206\u6790\u4e00\u4e0b\u5f00\u73af\u7ed3\u6784\u548c\u95ed\u73af\u7ed3\u6784\u4eea\u8868\u5404\u81ea\u7684\u7279\u70b9\u3002(\u53cb\u60c5\u63d0\u9192:\u7b2c\u4e00\u7ae0\u548c\u7b2c\u4e09\u7ae0\u7684\u76f8\u5173\u5185\u5bb9\u8981\u4e00\u8d77\u8003\u8651\uff0c\u5e76\u4ece\u81ea\u52a8\u63a7\u5236\u539f\u7406\u7684\u89d2\u5ea6\u6765\u8fdb\u884c\u5206\u6790\u3002)  <ul> <li>\u5f00\u73af\u7ed3\u6784\u4eea\u8868</li> </ul> <p>\u7531\u82e5\u5e72\u4e2a\u73af\u8282\u4e32\u8054\u7ec4\u6210\uff0c\u4eea\u8868\u7684\u4fe1\u606f\u548c\u53d8\u6362\u53ea\u6cbf\u4e00\u4e2a\u65b9\u5411\u4f20\u9012\u3002\u5176\u603b\u7684\u4f20\u9012\u51fd\u6570\u4e3a\u5404\u73af\u8282\u4f20\u9012\u51fd\u6570\u4e4b\u79ef\uff0c\u6574\u53f0\u4eea\u8868\u7684\u76f8\u5bf9\u8bef\u5dee\u4e3a\u5404\u4e2a\u73af\u8282\u7684\u76f8\u5bf9\u8bef\u5dee\u4e4b\u548c\u3002</p> <p>\u603b\u4f53\u6765\u8bf4\u7ed3\u6784\u7b80\u5355\u3002\u5f53\u7ec4\u6210\u4eea\u8868\u7684\u73af\u8282\u8f83\u591a\u65f6\uff0c\u51c6\u786e\u5ea6\u8f83\u4f4e\u3002</p> <ul> <li>\u95ed\u73af\u7ed3\u6784\u4eea\u8868</li> </ul> <p>\u7531\u6b63\u5411\u901a\u9053\u548c\u53cd\u9988\u901a\u9053\u7ec4\u6210\u3002\u5bf9\u4e8e\u4e00\u9636\u73af\u8282\\(G = \\frac{k}{1+Ts}\\)\uff0c\u82e5\u6709\u53cd\u9988\u589e\u76ca\\(\\beta\\)\uff0c\u5176\u653e\u5927\u500d\u6570\u548c\u65f6\u95f4\u5e38\u6570\u90fd\u662f\u5f00\u73af\u7ed3\u6784\u4eea\u8868\u7684\\(1/(1+k\\beta)\\)\u3002\u82e5\\(k\\beta \\rightarrow \\infty\\)\uff0c\u5219\\(G' = K_0/\\beta\\)\uff0c\u5373\u4eea\u8868\u7279\u6027\u4e3b\u8981\u53d6\u51b3\u4e8e\u53cd\u9988\u901a\u9053\u7279\u6027\u3002</p> <p>\u603b\u4f53\u7ed3\u6784\u4f1a\u590d\u6742\u4e00\u4e9b\uff0c\u7a33\u5b9a\u6027\u4f1a\u8f83\u5dee\u3002\u4f46\u662f\u53cd\u5e94\u901f\u5ea6\u5feb\uff0c\u7ebf\u6027\u597d\uff0c\u51c6\u786e\u5ea6\u9ad8\u3002</p>"},{"location":"Ctrl/Sensing%26Detection/Midterm/#_6","title":"\u6807\u51c6\u5dee\u4e0e\u7cfb\u7edf\u3001\u7c97\u5927\u8bef\u5dee","text":"\u8bf7\u5217\u51fa\u6d4b\u91cf\u4fe1\u53f7\u5747\u503c\u548c\u6807\u51c6\u5dee\u7684\u5b9a\u4e49\u516c\u5f0f\uff0c\u5e76\u7b80\u8981\u8bba\u8ff0\u4e00\u4e0b\u8be5\u4e24\u4e2a\u91cd\u8981\u7edf\u8ba1\u91cf\u5728\u7cfb\u7edf\u8bef\u5dee\u548c\u7c97\u5927\u8bef\u5dee\u5224\u522b\u4e2d\u7684\u4f5c\u7528\u3002  <ul> <li>\u5747\u503c</li> </ul> \\[ \\overline{x}=\\sum\\limits_{i=1}^{n}x_i \\] <ul> <li>\u6807\u51c6\u5dee</li> </ul> <p>\u5355\u6b21\u6d4b\u91cf\u503c\u7684\u6807\u51c6\u5dee(\u8d1d\u585e\u5c14Bessel\u516c\u5f0f)</p> \\[ \\sigma_B=s(x) = \\sqrt{\\frac{\\sum\\limits_{i=1}^{n}(x_i - \\overline{x})^2}{n-1}} \\] <p>\u7b97\u672f\u5e73\u5747\u503c\\(\\overline{x}\\)\u7684\u6807\u51c6\u5dee</p> \\[ s(\\overline{x}) = \\frac{s(x)}{\\sqrt{n}} \\] <p>\u53ef\u4ee5\u7528\u6807\u51c6\u5dee\u5224\u636e\uff0c\u770b\u4e24\u4e2a\u4f30\u8ba1\u6807\u51c6\u5dee\u662f\u5426\u6e10\u8fdb\u76f8\u7b49\u3002\u82e5\u5b58\u5728\u7cfb\u7edf\u8bef\u5dee\uff0c\u5219\u4e24\u8005\u76f8\u5dee\u5f88\u5927\u3002</p> <p>\\(\\sigma\\)\u6cd5\u53ef\u4ee5\u5254\u9664\u7c97\u5927\u8bef\u5dee\u3002\u5f53\u6d4b\u91cf\u503c\\(x_i\\)\u6ee1\u8db3\u62c9\u4f0a\u8fbe\u6cd5\u6216\u683c\u62c9\u5e03\u65af\u6cd5\u7684\u6761\u4ef6\u65f6\uff0c\u53ef\u4ee5\u8ba4\u5b9a\u4e3a\u7c97\u5927\u8bef\u5dee\uff08\u5982\\(3\\sigma\\)\uff09\u3002</p>"},{"location":"Ctrl/Sensing%26Detection/Midterm/#_7","title":"\u6700\u5927\u7edd\u5bf9\u8bef\u5dee\u3001\u6807\u51c6\u5dee\u7684\u4f30\u8ba1","text":"\u73b0\u6709\u4e00\u53f0\u6e29\u5ea6\u4f20\u611f\u5668\uff0c\u91cf\u7a0b\u4e3a0~100\u00b0C\uff0c\u5176\u51c6\u786e\u5ea6(\u7cbe\u5ea6)\u7b49\u7ea7\u4e3a1.0\u3002\u8bf7\u95ee\u8be5\u4f20\u611f\u5668\u5728\u91cf\u7a0b\u8303\u56f4\u5185\u53ef\u80fd\u51fa\u73b0\u7684\u6700\u5927\u7edd\u5bf9\u8bef\u5dee\u7684\u5408\u7406\u4f30\u8ba1\u503c\u548c\u53ef\u80fd\u51fa\u73b0\u7684\u6700\u5927\u6807\u51c6\u5dee\u7684\u5408\u7406\u4f30\u8ba1\u503c\u5206\u522b\u4e3a\u591a\u5c11?\u4e3a\u4ec0\u4e48?(\u53cb\u60c5\u63d0\u9192:\u4ed4\u7ec6\u770b\u4e00\u4e0b\u6559\u6750P13\u9875)  <ul> <li>\u6700\u5927\u7edd\u5bf9\u8bef\u5dee</li> </ul> \\[ e = 100 \\times 1\\% = 1 \u00b0C \\] <ul> <li>\u6700\u5927\u6807\u51c6\u5dee</li> </ul> \\[ 3\\sigma = 1 \u00b0C\\Rightarrow \\sigma = 1/3\u00b0C \\]"},{"location":"Ctrl/Sensing%26Detection/Midterm/#_8","title":"\u8ba1\u7b97\u9898","text":"<p>\u4e3a\u4e86\u4fdd\u8bc1\u6d4b\u91cf\u51c6\u786e\u5ea6\uff0c\u5728\u538b\u529b\u68c0\u6d4b\u8868\u9009\u578b\u65f6\uff0c\u4e00\u822c\u8981\u6c42\u6700\u5927\u5de5\u4f5c\u538b\u529b\u4e0d\u5e94\u8d85\u8fc7\u4eea\u8868\u6ee1\u91cf\u7a0b\u7684\\(3/4\\)\uff0c\u6700\u5c0f\u5de5\u4f5c\u538b\u529b\u4e0d\u5e94\u4f4e\u4e8e\u6ee1\u91cf\u7a0b\u7684\\(1/3\\)\u3002\u76ee\u524d\u6211\u56fd\u51fa\u5382\u7684\u538b\u529b(\u5305\u62ec\u5dee\u538b)\u68c0\u6d4b\u4eea\u8868\u6709\u7edf\u4e00\u7684\u91cf\u7a0b\u7cfb\u5217\uff0c\u5b83\u4eec\u662f\\(1\\)\u3001\\(1.6\\)\u3001\\(2.5\\)\u3001\\(4.0\\)\u3001\\(6.0kPa\\) \u4ee5\u53ca\u5b83\u4eec\u7684\\(10^n\\)\u500d\u6570(\\(n\\)\u4e3a\u6574\u6570)\u3002</p> <p>\u67d0\u538b\u529b\u5bb9\u5668\u6b63\u5e38\u5de5\u4f5c\u65f6\u538b\u529b\u8303\u56f4\u4e3a\\(1.0~1.5MPa\\),\u8981\u6c42\u6d4b\u91cf\u8bef\u5dee\u4e0d\u5927\u4e8e\u88ab\u6d4b\u538b\u529b\u76845%,\u8bd5\u786e\u5b9a\u8be5\u8868\u7684\u91cf\u7a0b\u548c\u51c6\u786e\u5ea6\u7b49\u7ea7\u3002</p> <ul> <li>\u89e3\uff1a</li> </ul> <p>\u8bbe\u91cf\u7a0b\u4e3a\\(A (MPa)\\)\uff0c\u5219\u6ee1\u8db3</p> \\[ \\begin{cases} \\frac{1}{3}A \\leq 1.0 \\\\ 1.5 \\leq \\frac{3}{4}A \\end{cases} \\] <p>\u89e3\u5f97\\(2.0\\leq A \\leq 3.0\\)\uff0c\u6545\u53d6\\(A=2.5 MPa\\)</p> <p>\u7531\u9898\u610f\uff0c\u4eea\u8868\u57fa\u672c\u8bef\u5dee\\(e \\leq 5\\% P\\)\uff0c\u6545\u5fc5\u987b\u8981</p> \\[ e \\leq 5\\% \\min{P} = 0.05\\times 1.0 = 0.05MPa \\] <p>\u6240\u4ee5\u4eea\u8868\u6ee1\u91cf\u7a0b\u57fa\u672c\u8bef\u5dee</p> \\[ e' \\leq \\frac{e}{A} = 2\\% \\] <p>\u6545\u9009\u62e9\u51c6\u786e\u5ea6\\(1.5\\)\u3002</p>"},{"location":"blog/","title":"Blog","text":"<p>In this part, I hope you can stand my childish thoughts and points.</p>"},{"location":"blog/Travel/","title":"Preface","text":"<p>Write down the life before it passes by...</p>"},{"location":"blog/Travel/5th_05_24/","title":"\u5c0f\u957f\u5047\u51fa\u6e38","text":"<p>\u5047\u671f\u76ee\u6807\uff1a\u4ece\u5404\u7c7b\u4e8b\u9879\u4e2d\u89e3\u653e\u51fa\u6765\uff0c\u601d\u8003\u603b\u7ed3\u524d\u534a\u4e2a\u5b66\u671f\u7684\u751f\u5b58\u72b6\u6001\u3002</p> <p>\u884c\u52a8\u539f\u5219\uff1a\u6162\u4e0b\u6765\uff0c\u6162\u4e0b\u6765\u3002</p>"},{"location":"blog/Travel/5th_05_24/#430-52","title":"4.30-5.2","text":"<p>\u9996\u5148\u611f\u8c22\u591c\u5bb5\u7ec4\u4f19\u4f34\u7684\u5b89\u6392\uff0c\u7279\u522b\u611f\u8c22\u5c0f\ud83d\udc1f\u7684\u7edf\u7b79\uff0c\u5c0f\ud83c\udf43\u7684\u666f\u5fb7\u9547\u8ba1\u5212\uff0c\u8ba9\u6211\u4eec\u7684\u6574\u4e2a\u884c\u7a0b\u90fd\u975e\u5e38\u987a\u5229\u3002</p> <p>\u4e00\u5927\u65e9\u5c31\u5f00\u59cb\u7fd8\u8bfe\u51fa\u884c\uff0c\u6211\u4eec\u5148\u5750\u9ad8\u94c1\u5230\u8fbe\u5a7a\u6e90\uff0c\u542f\u52a8\u81ea\u9a7e\u6a21\u5f0f\uff0c\u4e09\u5929\u53bb\u4e86\u7bc1\u5cad\u3001\u4e09\u6e05\u5c71\u548c\u666f\u5fb7\u9547\u3002</p>"},{"location":"blog/Travel/5th_05_24/#430","title":"4.30","text":"<p>\u7b2c\u4e00\u5929\u7684\u7bc1\u5cad\u7740\u5b9e\u8ba9\u4eba\u773c\u524d\u4e00\u4eae\uff0c\u5f88\u591a\u7167\u7247\uff0c\u65e0\u8bba\u662f\u98ce\u666f\u8fd8\u662f\u4eba\u7269\uff0c\u90fd\u5f88\u4e0a\u955c\u3002\u4ece\u73bb\u7483\u6808\u9053\uff08\u5792\u5fc3\u6865\uff09\u5230\u508d\u5c71\u5c0f\u9547\uff08\u82b1\u6eaa\u6c34\u8857\uff09\uff0c\u4ece\u675c\u9e43\u56ed\u5230\u98d8\u96ea\u6c11\u56fd\u9986\uff0c\u7edd\u7f8e\u7684\u5c71\u95f4\u98ce\u5149\u3001\u7eaf\u51c0\u7684\u5c71\u95f4\u7a7a\u6c14\uff0c\u8ba9\u4e00\u4e2a\u957f\u671f\u751f\u6d3b\u5728\u95ed\u585e\u6821\u56ed\u91cc\u7684\u4eba\u611f\u5230\u65e2\u964c\u751f\u53c8\u4eb2\u5207\u3002</p>"},{"location":"blog/Travel/5th_05_24/#51","title":"5.1","text":"<p>\u65e9\u6668\u9192\u6765\uff0c\u521a\u53d1\u751f\u7684\u5e7f\u4e1c\u6885\u5927\u9ad8\u901f\u584c\u9677\u8ba9\u6211\u5403\u4e86\u4e00\u5927\u60ca\u3002\u540e\u9762\u4ece\u4e09\u6e05\u5c71\u56de\u6765\u5f00\u9ad8\u901f\u7684\u65f6\u5019\uff0c\u5176\u5b9e\u6709\u4e00\u70b9\u5bd2\u98a4\u3002</p> <p>\u4e09\u6e05\u5c71\u6211\u66fe\u7ecf\u53bb\u8fc7\u4e00\u6b21\uff0c\u8bb0\u5fc6\u4e2d\u5370\u8c61\u975e\u5e38\u4e0d\u9519\uff0c\u4e91\u96fe\u7f2d\u7ed5\uff0c\u4eba\u4eec\u508d\u5c71\u800c\u884c\uff0c\u5982\u4e34\u4ed9\u5883\u3002\u7136\u800c\uff0c\u8fd9\u6b21\u975e\u5e38\u7684\u4e0d\u884c\uff0c\u4e91\u96fe\u8fc7\u4e8e\u6d53\u91cd\uff0c\u5927\u90e8\u5206\u7684\u5c71\u95f4\u7f8e\u666f\u90fd\u65e0\u6cd5\u6e05\u65b0\u7528\u773c\u770b\u89c1\uff08\u6216\u8bb8\u53ef\u4ee5\u8bd5\u8bd5\u5176\u4ed6\u6ce2\u6bb5\u7684\u5149hh\uff09\u3002</p> <p>\u5728\u80fd\u89c1\u5ea6\u4e0d\u523010m\u7684\u5c71\u95f4\uff0c\u62cd\u7167\u51e0\u4e4e\u662f\u4e00\u4ef6\u5403\u529b\u4e0d\u8ba8\u597d\u7684\u4e8b\u60c5\u3002\u597d\u5728\u6211\u4eec\u6700\u540e\u4e5f\u627e\u5230\u4e86\u8bb8\u591a\u7684\u62cd\u7167\u70b9\uff0c\u8d4b\u4e88\u6d53\u96fe\u91cc\u7684\u5c71\u548c\u6211\u4eec\u4e00\u70b9\u7279\u522b\u7684\u610f\u4e49\u3002</p> <p>\u53e6\u5916\uff0c\u4e94\u4e00\u4eba\u6570\u8fc7\u591a\uff0c\u4e0b\u5c71\u6392\u961f\u7b49\u5019\u8d85\u8fc7\u4e00\u4e2a\u534a\u5c0f\u65f6\uff0c\u4e5f\u662f\u4e00\u4e2a\u51cf\u5206\u9879\u3002</p>"},{"location":"blog/Travel/5th_05_24/#52","title":"5.2","text":"<p>\u6211\u4eec\u53bb\u5230\u4e86\u666f\u5fb7\u9547\uff0c\u4f53\u4f1a\u4e86\u4e00\u4e0b\u4e2d\u56fd\u74f7\u90fd\u7684\u97f5\u5473\u3002\u6211\u672c\u4eba\u5bf9\u4e8e\u74f7\u5668\u5e76\u65e0\u592a\u5927\u611f\u53d7\uff0c\u74f7\u5668\u53ca\u5176\u4ecb\u7ecd\u4e5f\u53ea\u662f\u8d70\u9a6c\u89c2\u82b1\u5306\u5306\u800c\u8fc7\u3002\u4f46\u662f\uff0c\u7ec6\u7ec6\u8d70\u6765\uff0c\u53d1\u73b0\u5176\u5236\u4f5c\u6d41\u7a0b\u7684\u7cbe\u7ec6\u3001\u7e41\u6742\uff0c\u5f97\u5230\u7684\u74f7\u5668\u8d28\u91cf\u4e0a\u4e58\u3001\u4ef7\u503c\u9ad8\u6602\uff0c\u5185\u542b\u5927\u5bb6\u5bf9\u5320\u5fc3\u7684\u8ffd\u6c42\u3002</p> <p>\u665a\u4e0a\u56de\u6765\uff0c\u53c2\u4e0e\u4e86\u5927\u8868\u54e5\u7684\u5a5a\u793c\u3002\u4e0d\u77e5\u4e3a\u4f55\uff0c\u6211\u4e00\u8fb9\u4e3a\u4ed6\u4eec\u611f\u5230\u5f88\u5f00\u5fc3\uff0c\u4e00\u8fb9\u611f\u53d7\u5230\u65f6\u5149\u98de\u901d\uff0c\u6bcf\u4e2a\u4eba\u90fd\u5728\u5f80\u524d\u8d70\u3002\u6211\u5bf9\u672a\u6765\u6709\u671f\u5f85\uff0c\u4e5f\u6709\u7126\u8651\u3002</p>"},{"location":"blog/Travel/5th_05_24/#53-55","title":"5.3-5.5","text":"<p>\u56de\u5230\u6e29\u5dde\uff0c\u9664\u4e86\u4e45\u8fdd\u7684\u6d77\u9c9c\u4e0d\u65ad\u6ee1\u8db3\u6211\u81ea\u5df1\u4e4b\u5916\uff0c\u6211\u8fd8\u5c06\u81ea\u5df1\u7684\u751f\u6d3b\u6162\u4e86\u4e0b\u6765\u3002\u6211\u548c\u5bb6\u91cc\u4eba\u90fd\u76f8\u5904\u4e86\u4e00\u4e0b\uff0c\u6211\u660e\u767d\u4e4b\u540e\u53ef\u80fd\u6ca1\u6709\u5f88\u591a\u7684\u65f6\u95f4\u966a\u4f34\u5bb6\u4eba\u3002</p> <p>\u6211\u5237\u4e86\u4e00\u4e0b\u8001\u53cb\u8bb0\uff0c\u5176\u5b9e\u8fd9\u4e2a\u662f\u53ef\u4ee5\u4e0a\u763e\u7684\uff01\u770b\u5267\u4e0a\u763e\u7684\u673a\u5236\uff0c\u6e90\u81ea\u4e8e\u5267\u60c5\u5bf9\u4eba\u7684\u5174\u8da3\u7684\u4e0d\u65ad\u6311\u9017\uff0c\u4eba\u770b\u5267\u65f6\u80fd\u591f\u6709\u7684\u677e\u5f1b\u611f\u548c\u5fd8\u6211\u611f\u3002</p> <p>\u5b9e\u9645\u4e0a\uff0c\u8fd9\u4e2a\u4e1c\u897f\u662f\u53ef\u4ee5\u8c03\u63a7\u7684\uff0c\u56e0\u4e3a\u5f88\u591a\u79ef\u6781\u7684\u4e8b\u60c5\uff0c\u90fd\u5728\u52aa\u529b\u4e2d\uff0c\u53d8\u5f97\u4e0a\u763e\u3002</p> <p>\u9ebb\u5c06\u5f53\u7136\u4e5f\u662f\u4e00\u6b21\u805a\u4f1a\u7684\u597d\u65f6\u673a\u3002\u5927\u5bb6\u90fd\u80fd\u591f\u804a\u804a\u5404\u81ea\u7684\u751f\u6d3b\uff0c\u4e0d\u540c\u7684\u4eba\u4e0d\u540c\u7684\u5730\u65b9\uff0c\u6709\u7740\u76f8\u4f3c\u7684\u60c5\u611f\u3002</p> <p>\u6700\u91cd\u8981\u7684\u662f\uff0c\u6211\u80fd\u591f\u4e3a\u6211\u7684\u5bb6\u5ead\u628a\u628a\u8109\uff0c\u770b\u770b\u6709\u6ca1\u6709\u4ec0\u4e48\u6bdb\u75c5\uff0c\u63a5\u4e0b\u6765\u79bb\u5bb6\u540e\uff0c\u6211\u662f\u4e0d\u662f\u8981\u505a\u4e00\u4e9b\u8c03\u6574\u3002\u6700\u91cd\u8981\u7684\u662f\u6211\u548c\u7238\u5988\u7684\u8ddd\u79bb\u53d8\u5f97\u66f4\u52a0\u9065\u8fdc\uff0c\u6211\u53d1\u73b0\u4e4b\u524d\u65e5\u5e38\u7684\u4e2d\u9910\u5e76\u4e0d\u80fd\u5f88\u597d\u5730\u89e3\u51b3\u95ee\u9898\u3002\u6211\u9700\u8981\u591a\u53d1\u4e00\u4e9b\u7167\u7247\uff0c\u5173\u4e8e\u6211\u7684\u751f\u6d3b\uff0c\u6211\u7684\u5468\u672b\uff0c\u8ba9\u4ed6\u4eec\u591a\u770b\u770b\u6211\u5230\u5e95\u662f\u600e\u4e48\u505a\u7684\u3002</p> <p>\u5047\u671f\u7ed3\u5c3e\u6211\u7adf\u7136\u80fd\u591f\u8bfb\u5230\u4e00\u53e5\u597d\u6587\uff1a \u4e09\u8054\u751f\u6d3b\u5468\u520a<pre><code>\u4e0d\u8981\u8bd5\u56fe\u5bfb\u627e\u4e00\u79cd\u201c\u4e0d\u7126\u8651\u201d\u7684\u72b6\u6001\uff0c\u56e0\u4e3a\u7126\u8651\u4e0e\u4eba\u7684\u9009\u62e9\u548c\u751f\u5b58\u76f8\u4f34\u968f\uff0c\u4e0d\u5982\u5b66\u4f1a\u4e00\u4e9b\u5c0f\u7684\u6280\u5de7\uff0c\u8ba9\u81ea\u5df1\u80fd\u548c\u7126\u8651\u5171\u5904\u3002\n</code></pre></p> <p>\u5b89\u6170\u4e86\u66fe\u7ecf\u52aa\u529b\u7684\u81ea\u5df1\uff0c\u6e05\u6670\u4e86\u5bf9\u672a\u6765\u7684\u52aa\u529b\u3002</p> <p>\u4ee5\u524d\u63a2\u7d22\u7684\u5404\u79cd\u6709\u76ca\u7684\u65b9\u5f0f\uff0c\u90fd\u80fd\u591f\u548c\u7126\u8651\u4e32\u8054\u8d77\u6765\uff0c\u9a7e\u9a6d\u7126\u8651\uff0c\u4e3a\u6211\u6240\u7528\u3002</p> <p>\u611f\u89c9\u81ea\u5df1\u7684\u5f88\u591a\u53d1\u8a00\u90fd\u6709\u529f\u5229\u5b9e\u7528\u4e3b\u4e49\u7684\u8272\u5f69\uff0c\u65e5\u6e10\u957f\u5927\u7684\u81ea\u5df1\u5f80\u5f80\u90fd\u662f\u5bf9\u7406\u60f3\u5931\u53bb\u8010\u5fc3\u548c\u4fe1\u5fc3\uff0c\u8d70\u4e00\u6b65\u770b\u4e00\u6b65\uff0c\u800c\u603b\u662f\u8981\u56de\u5934\u770b\u770b\uff0c\u81ea\u5df1\u4e3a\u4ec0\u4e48\u51fa\u53d1\u3002</p> <p>\u6211\u89c9\u5f97\u5bf9\u6211\u6765\u8bf4\uff0c\u6211\u9700\u8981\u4e0d\u65ad\u6253\u5f00\u81ea\u5df1\u3002\u6211\u80fd\u591f\u653e\u4e0b\u5f88\u591a\u4ee5\u524d\u6240\u8ba4\u4e3a\u7684\u7981\u5fcc\uff0c\u53bb\u505a\u5f88\u591a\u5b9e\u9645\u4e0a\u90fd\u80fd\u591f\u505a\u7684\u4e8b\u60c5\u3002</p> <p>\u5176\u5b9e\u5f88\u591a\u4e1c\u897f\u90fd\u662f\u4f1a\u4e0a\u763e\u7684\u3002</p> <p>\u5b66\u4f1a\u628a\u4e0d\u65ad\u7684\u4e0a\u763e\u53d8\u6210\u81ea\u5df1\u6700\u559c\u6b22\u7684\u6a21\u6837\u3002</p>"},{"location":"blog/Travel/Mount_Lu/","title":"Travel 2 Mount Lu!","text":""},{"location":"blog/Travel/Mount_Lu/#schedule","title":"Schedule","text":""},{"location":"blog/Travel/Mount_Lu/Schedule/","title":"Schedule","text":""},{"location":"blog/Travel/Mount_Lu/Schedule/#time","title":"Time","text":"<p>Jan 12<sup>th</sup>, 2025 to Jan 17<sup>th</sup>, 2025, 5 days in total</p>"},{"location":"blog/Travel/Mount_Lu/Schedule/#destination","title":"Destination","text":"<p>Mount Lu (\u5e90\u5c71) &amp; City of Jiujiang (\u4e5d\u6c5f)</p>"},{"location":"blog/Travel/Mount_Lu/Schedule/#accommodation","title":"Accommodation","text":"Time(night) Places Details Jan 12 Luqi Hotel (\u5e90\u6816\u9152\u5e97) check in around 21:30, It is a good time to make plans for the following days, buy some necessary goods and materials, which might be cheap at the foot of mountain Jan 13 - 15 Lushan Hotel (\u5e90\u5c71\u9152\u5e97) check in on the morning of Jan 13, and check out on the morning of Jan 16, three days in total Jan 16 Yuanqi Hotel (\u6e90\u542f\u9152\u5e97) check in on the evening of Jan 16 and check out on Jan 17, then go back"},{"location":"blog/Travel/Mount_Lu/Schedule/#time-table","title":"Time Table","text":"Time 1 Time 2 Places Details 1.12 Evening Assemble at Lushan Railway Station Zzh 18:49, Lj 20:47, Lwq &amp; Txc 21:03 1.12 Evening Have a good night, maybe we can wander around 1.13 Morning Take the Cable way up check in on Gulin Avenue (\u726f\u5cad\u8857), try the middle route 1.13 Afternoon 1.13 Evening 1.14 Morning Try the east route 1.14 Afternoon 1.14 Evening 1.15 Morning 1.15 Afternoon Try the west route 1.15 Evening 1.16 Morning check out and go down the mountain by tour car 1.16 Afternoon 1.16 Evening check in at Jiujiang Walk around Yuliangnan street (\u5ebe\u4eae\u7537\u8def\u5386\u53f2\u6587\u5316\u8857) 1.17 Morning check out 1.17 Afternoon Assemble at Jiujiang Railway Station return"},{"location":"blog/Travel/Mount_Lu/Schedule/#warm-reminder","title":"Warm Reminder","text":"<ul> <li>\u4e09\u53e0\u6cc9\u5c3d\u91cf\u4e0a\u5348\u53bb\uff0c\u4e0b\u53481-3\u70b9\u6216\u4eba\u6d41\u91cf\u592a\u5927</li> </ul>"},{"location":"courses/","title":"Content","text":"<p>The following contains some notes of courses at ZJU.</p>"},{"location":"courses/#artificial-intelligence-machine-learning","title":"Artificial Intelligence &amp; Machine Learning","text":"<p>To be continued...</p>"},{"location":"courses/#c-plus-plus","title":"C Plus Plus","text":"<p>To be continued...</p>"},{"location":"courses/#data-structure-analysis","title":"Data Structure &amp; Analysis","text":"<p>To be continued...</p>"},{"location":"courses/#introduction-to-visualization","title":"Introduction to Visualization","text":"<p>Completed.</p>"},{"location":"courses/#material-point-method","title":"Material Point Method","text":"<p>Completed.</p>"},{"location":"courses/#numerical-analysis","title":"Numerical Analysis","text":"<p>To be continued...</p>"},{"location":"courses/AIML/","title":"Artificial Intelligence &amp; Machine Learning","text":""},{"location":"courses/AIML/#artificial-intelligence","title":"Artificial Intelligence","text":""},{"location":"courses/AIML/#introduction","title":"Introduction","text":""},{"location":"courses/AIML/#agent","title":"Agent","text":""},{"location":"courses/AIML/#problem-solving","title":"Problem Solving","text":""},{"location":"courses/AIML/#knowledge-logic","title":"Knowledge &amp; Logic","text":""},{"location":"courses/AIML/#machine-learning","title":"Machine Learning","text":""},{"location":"courses/AIML/#introduction_1","title":"Introduction","text":""},{"location":"courses/AIML/#concept-learning","title":"Concept Learning","text":""},{"location":"courses/AIML/AI/","title":"Artificial Intelligence","text":""},{"location":"courses/AIML/AI/#introduction","title":"Introduction","text":""},{"location":"courses/AIML/AI/#agent","title":"Agent","text":""},{"location":"courses/AIML/AI/#problem-solving","title":"Problem Solving","text":""},{"location":"courses/AIML/AI/#knowledge-logic","title":"Knowledge &amp; Logic","text":""},{"location":"courses/AIML/AI/Agent/","title":"Agent","text":"<p>\u57fa\u672c\u5c5e\u6027</p> <ul> <li> <p>\u611f\u77e5\uff1a\u4efb\u4f55\u7ed9\u5b9a\u65f6\u523bAgent\u7684\u611f\u77e5\u8f93\u5165</p> </li> <li> <p>\u611f\u77e5\u5e8f\u5217\uff1a\u6240\u6709\u8f93\u5165\u6570\u636e\u7684\u5b8c\u6574\u5386\u53f2</p> </li> <li> <p>Agent\u51fd\u6570\uff1a\u5c06\u4efb\u610f\u7ed9\u5b9a\u611f\u77e5\u5e8f\u5217\u6620\u5c04\u4e3a\u884c\u52a8\uff0c\u53ef\u4ee5\u7531Agent\u7a0b\u5e8f\u5b9e\u73b0</p> </li> </ul> <p>\u80fd\u591f\u611f\u77e5\u548c\u52a8\u4f5c\u7684\u5b9e\u4f53\uff0c\u5177\u4f53\u6765\u8bf4\uff0c\u662f\u4e00\u4e2a\u4ece\u611f\u77e5\u5e8f\u5217\u5230\u52a8\u4f5c\u7684\u4e00\u4e2a\u51fd\u6570</p> \\[ f:P^{*}\\mapsto A \\]"},{"location":"courses/AIML/AI/Agent/#rational-agent","title":"\u7406\u6027\u667a\u80fd\u4f53 | Rational Agent","text":"<p>\u662f\u505a\u4e8b\u6b63\u786e\u7684\u667a\u80fd\u4f53\u3002</p> <ul> <li>\u6027\u80fd\u5ea6\u91cf</li> </ul> <p>\u7531\u8bbe\u8ba1\u8005\u7ed9\u51fa\uff0c\u6839\u636e\u5b9e\u9645\u5728\u6240\u5904\u7684\u73af\u5883\u4e2d\u5e0c\u671b\u5f97\u5230\u7684\u7ed3\u679c\u6765\u8bbe\u8ba1\u5ea6\u91cf\uff0c\u800c\u4e0d\u6839\u636e\u667a\u80fd\u4f53\u5e94\u8be5\u8868\u73b0\u7684\u884c\u4e3a\u3002</p> <ul> <li>\u7406\u6027\u7684\u5224\u65ad</li> </ul> <p>\u7406\u6027\u7684\u5224\u65ad</p> <ul> <li> <p>\u5b9a\u4e49\u6210\u529f\u6807\u51c6\u7684\u6027\u80fd\u5ea6\u91cf\uff08Performance measure\uff09</p> </li> <li> <p>Agent\u5bf9\u73af\u5883\u7684\u5148\u9a8c\u77e5\u8bc6\uff08Environment\uff09</p> </li> <li> <p>Agent\u53ef\u4ee5\u6267\u884c\u7684\u52a8\u4f5c\uff08Actuators\uff09</p> </li> <li> <p>Agent\u7684\u611f\u77e5\u5e8f\u5217\uff08Sensors\uff09</p> </li> </ul> <p>\u4e00\u4e2a\u7406\u6027\u7684Agent\uff0c\u5bf9\u4e8e\u6bcf\u4e00\u4e2a\u53ef\u80fd\u7684\u611f\u77e5\u5e8f\u5217\uff0c\u80fd\u6839\u636e\u5df2\u77e5\u7684\u611f\u77e5\u5e8f\u5217\u548c\u5185\u5efa\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u9009\u62e9\u80fd\u4f7f\u6027\u80fd\u6307\u6807\u7684\u671f\u671b\u503c\u6700\u5927\u5316\u7684\u52a8\u4f5c\u3002</p> <p>\u7406\u6027 vs \u5168\u77e5</p> <p>\u7406\u6027\u4e0d\u8981\u6c42\u5168\u77e5\u3002</p> <ul> <li> <p>\u5168\u77e5\u662f\u77e5\u9053\u5b83\u7684\u52a8\u4f5c\u4ea7\u751f\u7684\u5b9e\u9645\u6548\u679c</p> </li> <li> <p>\u7406\u6027\u5e94\u8be5\u7ecf\u5e38\u89c2\u5bdf\uff0c\u6709\u52a9\u4e8e\u6700\u5927\u5316\u671f\u671b\u6027\u80fd\u3002</p> </li> </ul> \u7406\u6027 \u5b8c\u7f8e \u4f7f\u671f\u671b\u7684\u6027\u80fd\u6700\u5927\u5316 \u4f7f\u5b9e\u9645\u7684\u6027\u80fd\u6700\u5927\u5316 <ul> <li>\u81ea\u4e3b\u6027</li> </ul> <p>\u4e0d\u5e94\u8be5\u4f9d\u8d56\u8bbe\u8ba1\u8005\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u5c3d\u53ef\u80fd\u5730\u5b66\u4e60</p>"},{"location":"courses/AIML/AI/Agent/#_1","title":"\u4efb\u52a1\u73af\u5883","text":"<p>\u5206\u7c7b</p> <ul> <li>\u5b8c\u5168\u53ef\u89c2\u5bdf\u548c\u90e8\u5206\u53ef\u89c2\u5bdf</li> </ul> <p>\u667a\u80fd\u4f53\u80fd\u591f\u83b7\u53d6\u73af\u5883\u7684\u5b8c\u6574\u72b6\u6001\uff0c\u4e0e\u4f20\u611f\u5668\u6709\u5173</p> <ul> <li>\u786e\u5b9a\u6027\uff1a\u73af\u5883\u7684\u4e0b\u4e00\u4e2a\u72b6\u6001\u5b8c\u5168\u53d6\u51b3\u4e8e\u5f53\u524d\u72b6\u6001\u548c\u667a\u80fd\u4f53\u7684\u884c\u52a8</li> </ul> <p>\u5982\u6b64\uff0c\u82e5\u65e0\u5176\u4ed6\u667a\u80fd\u4f53\u7684\u6d3b\u52a8\uff0c\u90a3\u4e48\u73af\u5883\u662f\u7b56\u7565\u7684</p> <ul> <li> <p>\u7247\u6bb5\u5f0f\u4e0e\u5ef6\u7eed\u5f0f\uff1a\u884c\u52a8\u7684\u9009\u62e9\u662f\u5426\u53d6\u51b3\u4e8e\u5f53\u524d\u7247\u6bb5\uff0c\u662f\u5426\u4f1a\u5bf9\u672a\u6765\u6709\u5f71\u54cd\u3002</p> </li> <li> <p>\u9759\u6001\u548c\u52a8\u6001\uff1a\u73af\u5883\u5728\u667a\u80fd\u4f53\u601d\u8003\u65f6\u662f\u5426\u53d8\u5316</p> </li> <li> <p>\u79bb\u6563\u4e0e\u8fde\u7eed</p> </li> </ul> <p>\u771f\u5b9e\u7684\u4e16\u754c\u662f\u90e8\u5206\u5ba2\u89c2\u3001\u968f\u673a\u7684\u3001\u5ef6\u7eed\u5f0f\u7684\u3001\u52a8\u6001\u7684\u3001\u8fde\u7eed\u7684\u3001\u591a\u667a\u80fd\u4f53\u7684\u3002</p>"},{"location":"courses/AIML/AI/Agent/#_2","title":"\u667a\u80fd\u4f53\u7684\u7ed3\u6784","text":"<p>AI\u7684\u76ee\u7684\u662f\u8bbe\u8ba1Agent\u7a0b\u5e8f\uff0c\u5b9e\u73b0\u628a\u611f\u77e5\u4fe1\u606f\u6620\u5c04\u5230\u884c\u52a8\u7684Agent\u51fd\u6570\u3002\u6ce8\u610f\uff0c\u6709\u4e9b\u51fd\u6570\u4e0d\u80fd\u88ab\u4efb\u4f55agent\u7a0b\u5e8f\u6784\u6210\uff0c\u5982\u56fe\u7075\u673a\u3002</p> <ul> <li>\u4f53\u7cfb\u7ed3\u6784\uff1a\u628a\u7a0b\u5e8f\u5728\u67d0\u4e2a\u5177\u5907\u7269\u7406\u4f20\u611f\u5668\u548c\u6267\u884c\u5668\u7684\u8ba1\u7b97\u88c5\u7f6e\u4e0a\u8fd0\u884c\u7684\u62bd\u8c61\u6574\u4f53</li> </ul> <p>\u5b83\u4e3a\u7a0b\u5e8f\u63d0\u4f9b\uff1a\u6765\u81ea\u4f20\u611f\u5668\u7684\u611f\u77e5\u4fe1\u606f\u3001\u8fd0\u884c\u7a0b\u5e8f\u3001\u628a\u7a0b\u5e8f\u4ea7\u751f\u7684\u884c\u52a8\u9001\u5230\u6267\u884c\u5668</p> \\[ \\text{Agent}=\\text{\u4f53\u7cfb\u7ed3\u6784}+\\text{\u7a0b\u5e8f} \\] <p>\u56db\u79cd\u57fa\u672c\u7684\u667a\u80fd\u4f53\u7ed3\u6784</p> <ul> <li>\u7b80\u5355\u53cd\u5c04\u578b: if-then \u7ed3\u6784(\u6761\u4ef6-\u884c\u4e3a\u89c4\u5219)</li> </ul> <p><p> </p></p> <p>\u7b80\u5355\u53cd\u5c04\u578b<pre><code>function Simple-Reflex-Agent\n    persistent: a set of condition-action rules.\n\n    state = interpret-input(percept)\n    rule = Rule-match(state, rules)\n    action = rule.ACTION\n    return action\n</code></pre> \u73af\u5883\u8981\u5b8c\u5168\u53ef\u89c2\u3002</p> <ul> <li>\u57fa\u4e8e\u6a21\u578b\u7684\u7b80\u5355\u53cd\u5c04\u578b</li> </ul> <p><p> </p></p> \u57fa\u4e8e\u6a21\u578b\u7684\u7b80\u5355\u53cd\u5c04\u578b<pre><code>function Model_Based-Agent\n    persistent: a set of condition-action rules,\n                state, the agent's current conception of the world state,\n                model, a description of how the next state depends on current state and action,\n                action, the most recent action\n\n\n    state = Undate-State(percept, state, action, model)\n    rule = Rule-match(state, rules)\n    action = rule.ACTION\n    return action\n</code></pre> <ul> <li>\u57fa\u4e8e\u76ee\u6807\u578b</li> </ul> <p><p> </p></p> <p>\u6548\u7387\u964d\u4f4e\uff0c\u7075\u6d3b\u5ea6\u589e\u52a0\uff0c\u529f\u80fd\u589e\u5f3a</p> <ul> <li>\u57fa\u4e8e\u6548\u7528\u578b(utility)</li> </ul> <p><p> </p></p> <ul> <li>\u5b66\u4e60\u578b</li> </ul> <p><p> </p></p> <p>\u5b66\u4e60\u5143\u4ef6(Learning element): \u6839\u636e\u53cd\u9988\u5bf9agent\u505a\u8bc4\u4ef7\uff0c\u4fee\u6539\u6267\u884c\u5143\u4ef6</p> <p>\u6267\u884c\u5143\u4ef6(Performance element)\uff1a\u4e4b\u524d\u7684\u6240\u6709agent</p> <p>\u8bc4\u8bba\u5143\u4ef6(Critic)\uff1a\u6839\u636e\u6027\u80fd\u6807\u51c6\u6765\u505a\u53cd\u9988</p> <p>\u95ee\u9898\u751f\u6210\u5668(Problem generator)\uff1a\u5efa\u8bae\u63a2\u7d22\u6027\u884c\u52a8\uff0c\u77ed\u671f\u6b21\u4f18\uff0c\u957f\u671f\u66f4\u597d</p>"},{"location":"courses/AIML/AI/Intro/","title":"Introduction","text":""},{"location":"courses/AIML/AI/ProblemSolving/","title":"Problem Solving","text":""},{"location":"courses/AIML/AI/ProblemSolving/#basic-search-method","title":"Basic Search Method","text":""},{"location":"courses/AIML/AI/ProblemSolving/#a-well-defined-problem","title":"\u826f\u5b9a\u4e49 | A well-defined problem","text":"<p>\u5047\u8bbe\u73af\u5883\u662f\u53ef\u89c2\u5bdf\u7684\uff08\u4e07\u80fd\u4f20\u611f\u5668\uff09\u3001\u79bb\u6563\u7684\u3001\u5df2\u77e5\u7684\uff08\u884c\u52a8\u9020\u6210\u786e\u5b9a\u7ed3\u679c\uff09\u3001\u786e\u5b9a\u7684\uff08\u884c\u52a8\u7ed3\u679c\u4e00\u4e00\u5bf9\u5e94\uff09</p> <p>\u8981\u7d20</p> <ul> <li> <p>\u72b6\u6001\u7a7a\u95f4\uff1a\u521d\u59cb\u72b6\u6001\u5230\u53ef\u4ee5\u8fbe\u5230\u7684\u6240\u6709\u72b6\u6001\u7684\u96c6\u5408</p> <ul> <li> <p>\u72b6\u6001\uff1astate \\(s\\), a node in search tree</p> </li> <li> <p>\u521d\u59cb\u72b6\u6001\uff1ainitial state \\(s_0\\)</p> </li> <li> <p>\u884c\u52a8\uff1aaction \\(a\\)</p> </li> <li> <p>\u8f6c\u79fb\u6a21\u578b(\u540e\u7ee7\u72b6\u6001)\uff1afunction(\\(s\\),\\(a\\))</p> </li> </ul> </li> <li> <p>\u76ee\u6807\u6d4b\u8bd5\uff1a\u786e\u5b9a\u5f53\u524d\u72b6\u6001\\(s\\)\u662f\u5426\u662f\u76ee\u6807\u72b6\u6001</p> </li> <li> <p>\u8def\u5f84\u8017\u6563\uff1a\u8003\u8651\u5355\u6b65\u8017\u6563</p> </li> </ul> <p>\u6027\u80fd\u6307\u6807</p> <ul> <li> <p>\u5b8c\u5907\u6027</p> </li> <li> <p>\u6700\u4f18\u6027</p> </li> <li> <p>\u65f6\u95f4\u590d\u6742\u5ea6</p> </li> <li> <p>\u7a7a\u95f4\u590d\u6742\u5ea6</p> </li> </ul>"},{"location":"courses/AIML/AI/ProblemSolving/#_1","title":"\u65e0\u4fe1\u606f\uff08\u76f2\u76ee\u5f0f\uff09\u641c\u7d22","text":"<p>\u533a\u522b\uff1a\u5982\u4f55\u62d3\u5c55\u5b50\u8282\u70b9\u3002</p> \u641c\u7d22\u7b97\u6cd5 \u7b97\u6cd5\u63cf\u8ff0 \u5b8c\u5907\u6027 \u6700\u4f18\u6027 time cost space cost \u5bbd\u5ea6\u4f18\u5148\u641c\u7d22 \u5148\u62d3\u5c55\u6839\u8282\u70b9\uff0c\u518d\u62d3\u5c55\u540e\u7ee7\u8282\u70b9\uff0c\u603b\u662f\u62d3\u5c55\u6df1\u5ea6\u6700\u6d45\u7684\u672a\u62d3\u5c55\u8282\u70b9\u3002\u7528\u961f\u5b9e\u73b0\u3002 Y Y \\(O(b^d)\\) \\(O(b^d)\\) \u4e00\u81f4\u4ee3\u4ef7\u641c\u7d22 \u62d3\u5c55\u7684\u662f\u8def\u5f84\u635f\u8017\\(g(n)\\)\u6700\u5c0f\u7684\u8282\u70b9\\(n\\).\u6700\u4f18\u6027\u6210\u7acb\uff0c\u5982\u679c\u6bcf\u4e00\u6b65\u4ee3\u4ef7\u5927\u4e8e\u67d0\u4e2a\u5c0f\u6b63\u6570\\(\\varepsilon\\). Y Y \\(O(b^{1+C/\\varepsilon})\\) \\(O(b^{1+C/\\varepsilon})\\) \u6df1\u5ea6\u4f18\u5148\u641c\u7d22 \u603b\u662f\u62d3\u5c55\u6811\u7684\u5f53\u524d\u8fb9\u7f18\u8282\u70b9\u96c6\u4e2d\u6700\u6df1\u7684\u8282\u70b9\u3002\u662f\u5b8c\u5907\u7684\uff0c\u4f46\u4e0d\u662f\u6700\u4f18\u7684\uff0c\u56e0\u4e3a\u9047\u5230\u89e3\u5c31\u4f1a\u8fd4\u56de\u3002 N N \\(O(bd)\\) \\(O(bd)\\) \u6df1\u5ea6\u53d7\u9650\u641c\u7d22 \u8bbe\u7f6e\u4e00\u4e2a\u6df1\u5ea6\u754c\u9650\\(l\\), \u82e5 \\(l&lt;d\\), \u4e0d\u5b8c\u5907\uff1b\u82e5 \\(l&gt;d\\), \u5219\u4e0d\u6700\u4f18 N/Y N \\(O(b^l)\\) \\(O(bl)\\) \u8fed\u4ee3\u52a0\u6df1\u7684\u6df1\u5ea6\u4f18\u5148\u641c\u7d22 \u4e0d\u65ad\u5730\u589e\u5927\u6df1\u5ea6\u9650\u5236\uff0c\u8fbe\u5230\u6700\u6d45\u7684\u76ee\u6807\u8282\u70b9\u6240\u5728\u6df1\u5ea6\\(d\\)\u65f6\uff0c\u5c31\u80fd\u627e\u5230\u76ee\u6807\u8282\u70b9\u3002 Y N/Y \\(O(b^d)\\) \\(O(bd)\\)"},{"location":"courses/AIML/AI/ProblemSolving/#_2","title":"\u6709\u4fe1\u606f\uff08\u542f\u53d1\u5f0f\uff09\u641c\u7d22","text":"<p>\u8282\u70b9\u662f\u57fa\u4e8e\u8bc4\u4ef7\u51fd\u6570\\(f(n)\\)\u88ab\u9009\u62e9\u62d3\u5c55\u7684\u3002</p> \u641c\u7d22\u7b97\u6cd5 \u7b97\u6cd5\u63cf\u8ff0 time cost space cost \u8d2a\u5a6a\u6700\u4f73\u4f18\u5148\u641c\u7d22 \u62d3\u5c55\u79bb\u76ee\u6807\u6700\u8fd1\u7684\u8282\u70b9\uff0c\u53ea\u662f\u7528\u542f\u53d1\u5f0f\u4fe1\u606f\uff0c\\(f(n)=h(n)\\)\uff0c\u4f46\u4e0d\u5b8c\u5907 \u6700\u574f\u60c5\u51b5\\(O(b^d)\\) A*\u641c\u7d22 \u5bf9\u8282\u70b9\u7684\u8bc4\u4f30\u5305\u542b\u4e86\u5230\u6b64\u8282\u70b9\u5df2\u6709\u7684\u4ee3\u4ef7\uff0c\u5373\\(f(n)=g(n)+h(n)\\)\uff0c\\(h(n)\\)\u6ee1\u8db3\u53ef\u91c7\u7eb3\u6027\u548c\u4e00\u81f4\u6027\uff0c\u5c31\u80fd\u4fdd\u8bc1\u95ee\u9898\u6c42\u89e3\u7684\u5b8c\u5907\u548c\u6700\u4f18\u6027 \u9012\u5f52\u6700\u4f73\u4f18\u5148\u641c\u7d22\uff08RBFS\uff09 \u4f7f\u7528f_limit\u6765\u8ddf\u8e2a\u4ece\u5f53\u524d\u8282\u70b9\u7684\u7956\u5148\u5f97\u5230\u7684\u6700\u4f73\\(f\\)\uff0c\u5982\u679c\u67d0\u8282\u70b9\u8d85\u8fc7\u6b64\u503c\uff0c\u5c31\u9012\u5f52\u56de\u6eaf\uff0c\u5bf9\u5f53\u524d\u8def\u5f84\u4e0a\u7684\u6bcf\u4e2a\u8282\u70b9\uff0c\u7528\u8be5\u8282\u70b9\u5b50\u8282\u70b9\u7684\u6700\u4f73\\(f\\)\u66f4\u65b0\u5176\\(f\\)\u503c <p>A*\u641c\u7d22\u7684\u5b8c\u5907\u548c\u6700\u4f18\u6027\u6761\u4ef6</p> <ul> <li>A*\u641c\u7d22\u7684\u53ef\u91c7\u7eb3\u6027</li> </ul> <p>\u4e13\u6ce8\u4e8e\u5bf9\u6811\u641c\u7d22\u8fdb\u884cA*\u641c\u7d22\uff0c\u8981\u6c42\\(h(n)\\)\u4e0d\u4f1a\u8fc7\u9ad8\u4f30\u8ba1\u5230\u8fbe\u76ee\u6807\u7684\u4ee3\u4ef7\uff0c\u4ece\u800c\\(f(n)\\)\u4e0d\u4f1a\u8d85\u8fc7\u7ecf\u8fc7\u8282\u70b9\\(n\\)\u7684\u89e3\u7684\u5b9e\u9645\u4ee3\u4ef7\u3002\u5728\u641c\u7d22\u4e2d\uff0c\u76f4\u7ebf\u8ddd\u79bb\u662f\u53ef\u91c7\u7eb3\u7684\u542f\u53d1\u5f0f\u3002</p> <ul> <li>A*\u641c\u7d22\u7684\u4e00\u81f4\u6027(\u5355\u8c03\u6027)</li> </ul> <p>\u4e13\u6ce8\u4e8e\u5bf9\u56fe\u641c\u7d22\u8fdb\u884cA*\u641c\u7d22, \u8981\u6c42\u6ee1\u8db3\u4e09\u89d2\u4e0d\u7b49\u5f0f\u3002\u5982\u679c\u5bf9\u4e8e\u6bcf\u4e2a\u8282\u70b9\\(n\\), \u901a\u8fc7\u4eba\u4ee5\u884c\u52a8\\(a\\)\u4ea7\u751f\u7684\\(n\\)\u7684\u6bcf\u4e2a\u540e\u7ee7\u8282\u70b9\\(n'\\), \u4ece\u8282\u70b9\\(n\\)\u5230\u8fbe\u76ee\u6807\u7684\u4f30\u8ba1\u4ee3\u4ef7\u4e0d\u5927\u4e8e\u4ece\\(n\\)\u5230\\(n'\\)\u7684\u5355\u6b65\u4ee3\u4ef7\u4e0e\u4ece\\(n'\\)\u5230\u8fbe\u76ee\u6807\u7684\u4f30\u8ba1\u4ee3\u4ef7\u4e4b\u548c</p> \\[ h(n)\\leq c(n,a,n')+h(n') \\] <p>\u53ef\u4ee5\u7406\u89e3\u6210\\(h(n)\\)\u662f\u5728\u56fe\u641c\u7d22\u4e2d\u4ece\\(n\\)\u5230\u76ee\u6807\u7684\u8def\u5f84\u635f\u8017\u7684\u4e0b\u754c\u3002\u4f30\u8ba1\u7684\u4e0b\u754c\u8d8a\u9760\u8fd1\u4e0b\u786e\u754c\uff0c\u641c\u7d22\u8d8a\u5feb\u3002</p> <p>\u6709\u4e0a\u5f0f\u53ef\u4ee5\u63a8\u5f97\\(f(n)\\)\u662f\u975e\u9012\u51cf\u7684\uff1a</p> \\[ f(n')=g(n')+h(n')=g(n)+c(n.a.n')+h(n')\\geq g(n)+h(n)=f(n) \\]"},{"location":"courses/AIML/AI/ProblemSolving/#_3","title":"\u5bf9\u6297\u641c\u7d22\uff08\u535a\u5f08\uff09","text":"<ul> <li>\u6781\u5927\u6781\u5c0f\u641c\u7d22</li> </ul> <p>\u4f7f\u7528\u4e86\u7b80\u5355\u7684\u9012\u5f52\u7b97\u6cd5\u3002\u7528\u6808\u5b9e\u73b0\uff0c\u6545\u662f\u6df1\u5ea6\u4f18\u5148\u641c\u7d22\u3002\u6545time cost \\(O(b^d)\\), space cost \\(O(bd)\\). \u65f6\u95f4\u5f00\u9500\u592a\u5927\u3002</p> Minimax Search<pre><code>action MiniMax_Decision(state)\n    return arg_max(Min_Value(Result(state,a)));\n\nvalue Max_Value(state)\n    if(terminal_test(state))return utility(state);//leaf node\n    double v=-INFTY; \n    for(auto&amp; a:actions(state))\n        v=max(v,Min_Value(result(state,a)));\n    return v;\n\nvalue Min_Value(state)\n    if(terminal_test(state))return utility(state);//leaf node\n    double v=INFTY; \n    for(auto&amp; a:actions(state))\n        v=min(v,Max_Value(result(state,a)));\n    return v;\n</code></pre> <ul> <li>\\(\\alpha-\\beta\\) \u526a\u679d</li> </ul> <p>\\(\\alpha\\) \u4e3a\u76ee\u524d\u4e3a\u6b62\u8def\u5f84\u4e0a\u53d1\u73b0\u7684MAX\u7684\u6700\u4f73\u9009\u62e9\uff08\u6781\u5927\u503c\uff09\uff0c\\(\\beta\\) \u4e3a\u76ee\u524d\u4e3a\u6b62\u8def\u5f84\u4e0a\u53d1\u73b0\u7684MIN\u7684\u6700\u4f73\u9009\u62e9\uff08\u6700\u5c0f\u503c\uff09\uff0c\u5e76\u4e0d\u65ad\u66f4\u65b0\u3002\u5f53\u67d0\u4e2a\u8282\u70b9\u7684\u503c\u6bd4\u76ee\u524d\u7684MAX\u7684\\(\\alpha\\)/MIN\u7684\\(\\beta\\)\u8981\u67e5\u65f6\u4fbf\u505a\u88c1\u526a\u3002</p> Alpha-Beta Search<pre><code>action Alpha_Beta_Search(state)\n    double v = max(state, -INFTY, +INFTY);\n    return actions.get(v)\n\nvalue Max_Value(state, alpha, beta)\n    if(terminal_test(state))return utility(state);//leaf node\n    double v=-INFTY; \n    for(auto&amp; a:actions(state))\n        v=max(v,Min_Value(result(state,a), alpha, beta));\n        if(v&gt;=beta) return v;\n        alpha = max(alpha, v)\n    return v;\n\nvalue Min_Value(state)\n    if(terminal_test(state))return utility(state);//leaf node\n    double v=INFTY; \n    for(auto&amp; a:actions(state))\n        v=min(v,Min_Value(result(state,a), alpha, beta));\n        if(v&lt;=alpha) return v;\n        beta = min(alpha, v)\n    return v;\n</code></pre>"},{"location":"courses/AIML/AI/logic/","title":"Knowledge &amp; Logic","text":""},{"location":"courses/AIML/AI/logic/#logic","title":"Logic","text":"<ul> <li>\u8bed\u4e49\uff1a\u5b9a\u4e49\u4e86\u8bed\u53e5\u5728\u6bcf\u4e2a\u53ef\u80fd\u4e16\u754c\uff08\u6a21\u578b\uff09\u7684\u771f\u503c</li> </ul> <p>\u5982\\(m\\)\u662f\\(\\alpha\\)\u7684\u4e00\u4e2a\u6a21\u578b\uff0c\u8868\u793a\u8bed\u53e5\\(\\alpha\\)\u5728\u6a21\u578b\\(m\\)\u4e2d\u4e3a\u771f\u3002\u7528\\(M(\\alpha)\\)\u8868\u793a\u6240\u6709\u7684\u6a21\u578b\u3002</p> <ul> <li>\u903b\u8f91\u8574\u6db5(entailment): \u67d0\u8bed\u53e5\u5728\u903b\u8f91\u4e0a\u8ddf\u968f\u53e6\u4e00\u4e2a\u8bed\u53e5. \u5982</li> </ul> \\[ \\alpha |=\\beta \\] <p>\u8868\u793a\u8bed\u53e5\\(\\alpha\\)\u8574\u6db5\u8bed\u53e5\\(\\beta\\)\u3002\u4e0a\u53e5\u5f53\u4e14\u4ec5\u5f53\uff0c\u5728\u4f7f\\(\\alpha\\)\u4e3a\u771f\u7684\u6bcf\u4e2a\u6a21\u578b\u4e2d\uff0c\\(\\beta\\)\u4e5f\u4e3a\u771f\u3002\u5373</p> \\[ \\alpha|=\\beta \\Leftrightarrow M(\\alpha)\\subset M(\\beta) \\] <p>\u6ce8\u610f\uff0c\u8fd9\u8868\u793a\\(\\alpha\\)\u662f\u6bd4\\(\\beta\\)\u66f4\u5f3a\u7684\u65ad\u8a00\uff0c\u6392\u9664\u4e86\u66f4\u591a\u53ef\u80fd\u6027\u3002</p> <p>\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u8574\u6db5\u6765\u5b9e\u73b0\u903b\u8f91\u63a8\u7406\u3002\u901a\u8fc7\u679a\u4e3e\u6240\u6709\u53ef\u80fd\u7684\u6a21\u578b\u6765\u68c0\u9a8cKB\u4e3a\u771f\u65f6\\(\\alpha\\)\u5747\u4e3a\u771f\u3002\u8fd9\u6837\u7684\u8fc7\u7a0b\u79f0\u4e3a\u63a8\u7406\u7b97\u6cd5\uff0c\u662f\u53ef\u9760\u7684\u3002</p>"},{"location":"courses/AIML/AI/logic/#_1","title":"\u547d\u9898\u903b\u8f91","text":"<p>\u7528\u5927\u5199\u5b57\u6bcd\u8868\u793a\u547d\u9898\u3002\u590d\u5408\u53e5\u7531\u7b80\u5355\u8bed\u53e5\u7528\u62ec\u53f7\u548c\u903b\u8f91\u8fde\u63a5\u8bcd\u6784\u9020\u800c\u6210\u3002</p> <p>Basic notation</p> <p>\\(\\land\\) \u8868\u793a\"\u4e0e\"\uff0c\u770b\u8d77\u6765\u548c\"And\"\u4e2d\u7684\"A\"\u5f88\u50cf\uff0c\\(W_{1,3}\\land P_{3,1}\\)\u88ab\u79f0\u4e3a\u5408\u53d6\u5f0f\u3002</p> <p>\\(\\lor\\) \u8868\u793a\"\u6216\"\uff0c\u6765\u6e90\u4e8e\u62c9\u4e01\u6587\"vel\"\uff0c\\(W_{1,3}\\lor P_{3,1}\\)\u88ab\u79f0\u4e3a\u6790\u53d6\u5f0f\u3002</p> <p>\\(\\Rightarrow\\) \u8868\u793a\u8574\u6db5(implication)\u3002\\(P\\Rightarrow Q\\) \u8868\u793a\uff0c\u5982\u679c\\(P\\)\u4e3a\u771f\uff0c\u90a3\u6211\u4e3b\u5f20\\(Q\\)\u4e3a\u771f\uff0c\u5426\u5219\u65e0\u53ef\u5949\u544a\u3002</p>"},{"location":"courses/AIML/AI/logic/#_2","title":"\u5b9a\u7406\u8bc1\u660e","text":"<ul> <li>\u903b\u8f91\u7b49\u4ef7: \\(\\equiv\\), \u5982 \\(P\\land Q\\equiv Q\\land P\\). \u4e5f\u53ef\u4ee5\u8868\u793a\u6210\u76f8\u4e92\u8574\u6db5</li> </ul> \\[ \\alpha\\equiv \\beta \\quad\\text{iff}\\quad (\\alpha|=\\beta) \\land (\\beta|=\\alpha) \\] <ul> <li> <p>\u6709\u6548\u6027\uff1a\u4e00\u4e2a\u8bed\u53e5\u662f\u6709\u6548\u7684\uff0c\u5982\u679c\u5728\u6240\u6709\u7684\u6a21\u578b\u4e2d\u90fd\u4e3a\u771f\u3002</p> </li> <li> <p>\u53ef\u6ee1\u8db3\u6027\uff1a\u4e00\u4e2a\u8bed\u53e5\u662f\u53ef\u6ee1\u8db3\u7684\uff0c\u5982\u679c\u5728\u67d0\u4e9b\u6a21\u578b\u4e2d\u4e3a\u771f\u3002\u53ef\u4ee5\u901a\u8fc7\u679a\u4e3e\u8fdb\u884c\uff0c\u662f\u4e00\u4e2aSAT\u95ee\u9898\uff0cNP\u5b8c\u5168\u95ee\u9898\u3002</p> </li> </ul> <p>\u4e24\u8005\u7684\u5173\u7cfb\u53ef\u4ee5\u63a8\u51fa\u53cd\u8bc1\u6cd5\u7684\u601d\u8def\u3002</p> \\[ \\alpha|=\\beta \\quad \\text{iff} \\quad (\\alpha\\land \\neg  \\beta)\\text{\u4e0d\u53ef\u6ee1\u8db3} \\] <ul> <li>\u63a8\u5bfc\u4e0e\u8bc1\u660e</li> </ul> <p>\u63a8\u7406\u89c4\u5219</p> <ul> <li>\u5047\u8a00\u63a8\u7406\u89c4\u5219(Modus Ponens)</li> </ul> <p>Given \\(\\alpha\\Rightarrow \\beta\\) and \\(\\alpha\\), we can deduce \\(\\beta\\).</p> \\[ \\frac{\\alpha\\Rightarrow \\beta,\\quad \\alpha}{\\beta} \\] <ul> <li>\u6d88\u53bb\u5408\u53d6\u8bcd</li> </ul> <p>extract subsentence from \u5408\u53d6\u5f0f</p> \\[ \\frac{\\alpha \\land \\beta}{\\alpha} \\] <ul> <li>\u5f52\u7ed3(resolution)</li> </ul> <p>types of Resolution</p> <ul> <li>\u5355\u5143\u5f52\u7ed3(unit resolution)</li> </ul> <p>if \\(m=\\neg l_i\\), then</p> \\[ \\frac{l_1\\lor \\cdots \\lor l_k, \\quad m}{l_1\\lor\\cdots \\lor l_{i-1}\\lor l_{i+1}\\lor \\cdots\\lor l_k} \\] <ul> <li>\u5168\u5f52\u7ed3(full reosolution)</li> </ul> <p>if \\(m_j=\\neg l_i\\), then</p> \\[ \\frac{l_1\\lor \\cdots \\lor l_k, \\quad m1\\lor\\cdots\\lor m_n}{l_1\\lor\\cdots \\lor l_{i-1}\\lor l_{i+1}\\lor \\cdots\\lor l_k\\lor m_1\\lor \\cdots\\lor m_{j-1}\\lor m_{j+1}\\lor\\cdots\\lor m_n} \\] <p>Q1. Given </p> \\[ \\begin{align*} &amp;R_1: \\quad\\neg P_{1,1}\\\\ &amp;R_2: \\quad B_{1,1}\\Leftrightarrow (P_{1,2}\\lor P_{2,1})\\\\ &amp;R_3: \\quad B_{2,1}\\Leftrightarrow (P_{1,1}\\lor P_{2,2} \\lor P_{3,1})\\\\ &amp;R_4: \\quad \\neg B_{1,1}\\\\ &amp;R_5: \\quad B_{2,1} \\end{align*} \\] <p>Simplify \\(R_1\\land R_2\\land R_3\\land R_4\\land R_5\\)</p> Answer <p>Notice that...</p> <p>\u4efb\u4f55\u57fa\u4e8e\u5f52\u7ed3\u7684\u5b9a\u7406\u8bc1\u660e\u5668\uff0c\u90fd\u80fd\u786e\u5b9a\\(\\alpha |=\\beta\\) \u662f\u5426\u6210\u7acb\u3002\u5176\u672c\u8d28\u5c31\u662f\u53cd\u8bc1\u6cd5\u3002\u82e5\u76ee\u6807\u547d\u9898\u4e3a\\(\\alpha\\), \u5df2\u77e5\u77e5\u8bc6\u5e93\u4e3a\\(KB\\), \u7136\u540e\u4f7f\u7528\\(KB\\land \\neg \\alpha\\)\uff0c\u82e5\u4e0d\u53ef\u6ee1\u8db3(\u5bf9\u6240\u6709\u6a21\u578b\u90fd\u4e3a\u5047)\uff0c\u5219\\(KB |=\\alpha\\).</p> <ul> <li>\u5408\u53d6\u8303\u5f0f: \u5b50\u53e5\u7684\u8fde\u63a5\u7b26\u662f\\(\\land\\)</li> </ul> <p>\u5bf9\u6240\u6709\u5b50\u53e5\u7684\u53ef\u80fd\u7684\u4e24\u4e24\u7ec4\u5408\uff08\u542b\u6709\u4e92\u8865\u8bed\u53e5\uff09\u8fdb\u884c\u5f52\u7ed3\uff0c\u4ea7\u751f\u65b0\u5b50\u53e5\u3002\u5982\u679c\u8be5\u5b50\u53e5\u6ca1\u6709\u51fa\u73b0\u8fc7\uff0c\u5219\u5c06\u4ed6\u88c5\u5165\u5b50\u53e5\u96c6\u3002\u76f4\u5230\uff1a</p> <p>\uff08i\uff09 \u6ca1\u6709\u65b0\u8bed\u53e5(new \\(\\subset\\) clauses), \u5219\\(KB \\land \\neg\\alpha\\) \u53ef\u6ee1\u8db3\uff0c</p> <p>\uff08ii\uff09 \u4ea7\u751f\u7a7a\u8bed\u53e5(like \\(P\\land \\neg P\\)), \u5219\\(KB \\land \\neg\\alpha\\) \u4e0d\u53ef\u6ee1\u8db3\uff0c\u539f\u547d\u9898\u53ef\u63a8\u51fa\\(\\alpha\\).</p> <p>This is the following theorem.</p> <p>Basic Resolution Theorem</p> <p>If the set \\(S\\) of sub clauses is not satisfiable, then the resolution closure \\(RC(S)\\) must contain empty clause.</p> Proof <p>By prove its converse-negative proposition. That is, if \\(RC(S)\\) does not contain empty clause, then \\(S\\) is satisfiable. ?</p>"},{"location":"courses/AIML/ML/","title":"Machine Learning","text":""},{"location":"courses/AIML/ML/#introduction","title":"Introduction","text":""},{"location":"courses/AIML/ML/#concept-learning","title":"Concept Learning","text":""},{"location":"courses/AIML/ML/#decision-tree","title":"Decision Tree","text":""},{"location":"courses/AIML/ML/Concept_Learning/","title":"Concept Learning","text":"<p>\u6982\u5ff5\u5b66\u4e60\u7684\u5b9a\u4e49</p> <p>\u7ed9\u5b9a\u7684\u67d0\u4e00\u7c7b\u522b\u7684\u82e5\u5e72\u6b63\u4f8b\u548c\u53cd\u4f8b\uff0c\u4ece\u4e2d\u83b7\u5f97\u8be5\u7c7b\u522b\u7684\u4e00\u822c\u5b9a\u4e49\u3002</p> <p>\u6982\u5ff5\u5b66\u4e60\u662f\u6307\u4ece\u6709\u5173\u67d0\u4e2a\u5e03\u5c14\u51fd\u6570\u7684\u8f93\u5165\u8f93\u51fa\u8bad\u7ec3\u6837\u4f8b\u4e2d\u63a8\u65ad\u51fa\u8be5\u5e03\u5c14\u51fd\u6570\u3002</p>"},{"location":"courses/AIML/ML/Concept_Learning/#_1","title":"\u6982\u5ff5\u5b66\u4e60\u4efb\u52a1","text":"<p>\u5f62\u5f0f\u5316\u8868\u8fbe</p> <p>\u6bcf\u4e2a\u5c5e\u6027\u53ef\u6709\u4e09\u79cd\u53d6\u503c</p> <ul> <li> <p>\"?\" \u8868\u793a\u4efb\u610f\u672c\u5c5e\u6027\u53ef\u63a5\u53d7\u7684\u503c\u3002</p> </li> <li> <p>\u660e\u786e\u7684\u5c5e\u6027</p> </li> <li> <p>\"\\(\\varnothing\\)\" \u8868\u793a\u4e0d\u63a5\u53d7\u4efb\u4f55\u503c\u3002</p> </li> </ul> <p>\u5982\u6700\u4e00\u822c\u7684\u5047\u8bbe\uff1a</p> \\[ &lt;?, ?, ?, ?, ?, ?&gt; \\] <p>\u6700\u7279\u6b8a\u7684\u5047\u8bbe\uff1a</p> \\[ &lt;\\varnothing, \\varnothing, \\varnothing, \\varnothing, \\varnothing, \\varnothing&gt; \\]"},{"location":"courses/AIML/ML/Concept_Learning/#_2","title":"\u672f\u8bed\u5b9a\u4e49","text":"<p>\u672f\u8bed\u5b9a\u4e49</p> <ul> <li> <p>\u6982\u5ff5\u5b9a\u4e49\u5728 \u5b9e\u4f8b\u96c6 \\(X\\) \u4e0a. </p> </li> <li> <p>\u5f85\u5b66\u4e60\u7684\u6982\u5ff5\u6216\u76ee\u6807\u51fd\u6570\u88ab\u79f0\u4e3a \u76ee\u6807\u6982\u5ff5(target concept, \\(c\\)). \u53ef\u4ee5\u662f\u5b9a\u4e49\u5728\u5b9e\u4f8b\u96c6\\(X\\)\u4e0a\u7684\u4efb\u610f\u5e03\u5c14\u51fd\u6570\uff0c\u5373</p> </li> </ul> \\[ c: X \\mapsto \\{0,1\\} \\] <ul> <li>\u8bad\u7ec3\u6837\u4f8b(training examples): \u6bcf\u4e2a\u6837\u4f8b\u4e3a \\(X\\) \u4e2d\u7684\u4e00\u4e2a\u5143\u7d20 \\(x\\) \u4ee5\u53ca\u5bf9\u5e94\u7684\u76ee\u6807\u6982\u5ff5 \\(c(x)\\)\uff0c\u8bb0\u4e3a \\(&lt;x, c(x)&gt;\\). \u82e5 \\(c(x)=1\\) \u8be5\u6837\u4f8b\u88ab\u79f0\u4e3a\u6b63\u4f8b\uff0c\u662f\u76ee\u6807\u6982\u5ff5\u7684\u6210\u5458\u3002</li> </ul> <p>\u7ed9\u5b9a\u8bad\u7ec3\u6837\u4f8b\u96c6\uff0c\u6211\u4eec\u53ef\u4ee5\u5b66\u4e60\u6216\u4f30\u8ba1\\(c\\)\u3002</p> <p>\u672f\u8bed\u5b9a\u4e49</p> <ul> <li> <p>\u6240\u6709\u53ef\u80fd\u5047\u8bbe\u96c6(all possible hypotheses) \\(H\\).</p> </li> <li> <p>\\(h\\) \u662f \\(H\\) \u4e2d\u7684\u4e00\u4e2a\u5143\u7d20\uff0c\u8868\u793a \\(X\\) \u4e0a\u5b9a\u4e49\u7684\u5e03\u5c14\u51fd\u6570 \\(h:X\\mapsto \\{0,1\\}\\). </p> </li> </ul> <p>\u5b66\u4e60\u7684\u76ee\u6807\u5c31\u662f\uff0c\u5bfb\u627e\u4e00\u4e2a \\(h\\)\uff0c\u4f7f\u5f97 \\(\\forall x\\in X\\), \u6709 \\(h(x)=c(x)\\).</p> <ul> <li>\u5f52\u7eb3\u5b66\u4e60\u5047\u8bbe: \u5bf9\u4e8e\u672a\u89c1\u5b9e\u4f8b\u6700\u597d\u7684\u5047\u8bbe\u5c31\u662f\u4e0e\u8bad\u7ec3\u6570\u636e\u6700\u4f73\u62df\u5408\u7684\u5047\u8bbe\u3002\u4efb\u4e00\u5047\u8bbe\u5982\u679c\u5728\u8db3\u591f\u5927\u7684\u8bad\u7ec3\u6837\u4f8b\u96c6\u4e2d\u5f88\u597d\u5730\u903c\u8fd1\u76ee\u6807\u51fd\u6570\uff0c\u5b83\u4e5f\u80fd\u5728\u672a\u89c1\u7684\u5b9e\u4f8b\u4e2d\u5f88\u597d\u5730\u903c\u8fd1\u76ee\u6807\u51fd\u6570\u3002</li> </ul>"},{"location":"courses/AIML/ML/Concept_Learning/#find-s","title":"\u7528\u641c\u7d22\u8fdb\u884c\u6982\u5ff5\u5b66\u4e60 | Find-S\u7b97\u6cd5","text":"<p>\u4ece \\(H\\) \u4e2d\u6700\u7279\u6b8a\u5047\u8bbe\u5f00\u59cb\uff0c\u7136\u540e\u5728\u5047\u8bbe\u8986\u76d6\u6b63\u4f8b\u65f6\u5c06\u5176\u4e00\u822c\u5316\u3002</p> <p>FIND-S\u7b97\u6cd5</p> <ol> <li> <p>\u5c06 \\(h\\) \u521d\u59cb\u5316\u4e3a \\(H\\) \u4e2d\u6700\u7279\u6b8a\u5047\u8bbe (\\(&lt;\\varnothing &gt;\\) \u7ec4\u6210)</p> </li> <li> <p>\u5bf9\u6bcf\u4e2a\u6b63\u4f8b \\(x\\) in \\(X\\)</p> <p>\u5bf9 \\(h\\) \u7684\u6bcf\u4e00\u4e2a\u5c5e\u6027\u7ea6\u675f \\(a_i\\)</p> Text Only<pre><code>if $x$ \u6ee1\u8db3\n\n    $a_i$ continue;\n\nelse\n\n    \u5c06 $h$ \u4e2d $a_i$ \u66ff\u6362\u4e3a $x$ \u6ee1\u8db3\u7684\u53e6\u4e00\u4e2a\u66f4\u4e00\u822c\u7ea6\u675f\n</code></pre> </li> <li> <p>\u8f93\u51fa \\(h\\).</p> </li> </ol> <p>\u5176\u5b9e\u662f\u5229\u7528 more_general_than \u7684\u504f\u5e8f\u6765\u641c\u7d22\u3002\u6bcf\u4e00\u6b65\u5f97\u5230\u7684\u5047\u8bbe\u90fd\u662f\u90a3\u4e00\u70b9\u4e0a\u4e0e\u8bad\u7ec3\u6837\u4f8b\u4e00\u81f4\u7684\u6700\u7279\u6b8a\u5047\u8bbe\u3002</p> <ul> <li>\u5b58\u5728\u95ee\u9898</li> </ul> <p>\u6536\u655b\u6027\uff1a \u65e0\u6cd5\u786e\u5b9a\u662f\u5426\u62df\u5408\u5230\u76ee\u6807\u6982\u5ff5 \\(c\\). \u5f97\u5230\u7684\u53ea\u662f\u80fd\u591f\u62df\u5408\u8bad\u7ec3\u6837\u4f8b\u7684\u591a\u4e2a\u5047\u8bbe\u7684\u4e00\u4e2a\u3002</p> <p>\u6ca1\u6709\u8003\u8651\u6ee1\u8db3\u6761\u4ef6\u7684\u6700\u4e00\u822c\u5047\u8bbe\u548c\u4ecb\u4e8e\u4e24\u8005\u4e4b\u95f4\u7684\u5176\u4ed6\u5047\u8bbe\u3002</p> <p>\u7b97\u6cd5\u4e0d\u591frobust\uff0c\u5f53\u6837\u4f8b\u4e2d\u51fa\u73b0\u9519\u8bef\u65f6\uff0c\u4f1a\u4e25\u91cd\u7834\u574f\u3002</p>"},{"location":"courses/AIML/ML/Concept_Learning/#candidate-elimination","title":"\u53d8\u5f62\u7a7a\u95f4\u548c\u5019\u9009\u6d88\u9664\u7b97\u6cd5(Candidate-Elimination)","text":"<p>\u8be5\u7b97\u6cd5\u5f97\u5230\u7684\u662f\u548c\u8bad\u7ec3\u6837\u4f8b\u4e00\u81f4\u7684\u6240\u6709\u5047\u8bbe\u7684\u96c6\u5408\u3002</p> <p>\u4f18\u70b9: \u4f9d\u8d56\u4e8e more_general_than \u504f\u5e8f\u7ed3\u6784\uff0c\u80fd\u591f\u5728\u63cf\u8ff0\u65f6\u4e0d\u7528\u5217\u4e3e\u6240\u6709\u6210\u5458\u3002</p> <ul> <li>\u53d8\u5f62\u7a7a\u95f4: \u4e0e\u8bad\u7ec3\u6837\u4f8b\u4e00\u81f4\u7684\u6240\u6709\u5047\u8bbe\u7684\u96c6\u5408\uff0c\u5373\u53d8\u5f62\u7a7a\u95f4\u4e2d\u4efb\u610f\u4e00\u4e2a\u5047\u8bbe\u90fd\u80fd\u8986\u76d6\u6240\u6709\u7684\u6b63\u4f8b\u5e76\u6392\u65a5\u6240\u6709\u7684\u53cd\u4f8b\u3002\u4e00\u822c\u4f7f\u7528\u4e00\u4e2a \u6781\u5927\u4e00\u822c\u5047\u8bbe(G) \u548c \u6781\u5927\u7279\u6b8a\u5047\u8bbe(S)\uff0c\u6765\u5f62\u6210\u4e00\u822c\u548c\u7279\u6b8a\u8fb9\u754c\uff0c\u5728\u6574\u4e2a\u504f\u5e8f\u7ed3\u6784\u4e2d\u5212\u5206\u51fa\u53d8\u5f62\u7a7a\u95f4\u3002</li> </ul> <p>\u5019\u9009\u6d88\u9664\u7b97\u6cd5</p> <ol> <li> <p>\u521d\u59cb\u5316 G \u4e3a \"?\", S \u4e3a \"\\(\\varnothing\\)\".</p> </li> <li> <p>\u904d\u5386\u8bad\u7ec3\u6837\u4f8b x in X:</p> <p>\u82e5\u4e3a\u6b63\u4f8b\uff0c\u5219\u4e00\u822c\u5316 S;  </p> <p>\u5426\u5219\u4e3a\u8d1f\u4f8b\uff0c\u7279\u6b8a\u5316 G</p> </li> </ol> <p>\u6ce8\u610f\u70b9</p> <ul> <li> <p>\u53ef\u4ee5\u7b5b\u9009\u6837\u4f8b\u4e2d\u7684\u9519\u8bef\uff1a \u7ed9\u5b9a\u8db3\u591f\u6837\u4f8b\uff0c\u4f1a\u6536\u655b\u5230\u7a7a\u7684\u53d8\u5f62\u7a7a\u95f4\u3002</p> </li> <li> <p>\u5019\u9009\u6d88\u9664\u7b97\u6cd5\u80fd\u591f\u6536\u655b\u5230\u6b63\u786e\u5047\u8bbe\u7684\u524d\u63d0: \u6837\u4f8b\u6ca1\u6709\u9519\u8bef\uff0c H \u4e2d\u786e\u5b9e\u5305\u542b\u63cf\u8ff0\u76ee\u6807\u6982\u5ff5\u7684\u6b63\u786e\u5047\u8bbe(\u76ee\u6807\u6982\u5ff5\u4e0d\u518d\u5047\u8bbe\u7a7a\u95f4\u5185\uff0c\u5373\u4e0d\u80fd\u591f\u7531\u51e0\u4e2a\u5c5e\u6027\u7684\u5408\u53d6\uff0c\u800c\u53ea\u80fd\u662f\u6790\u53d6)\u3002</p> </li> </ul>"},{"location":"courses/AIML/ML/Concept_Learning/#_3","title":"\u5f52\u7eb3\u504f\u7f6e","text":"<p>\u5982\u679c\u76ee\u6807\u6982\u5ff5\u4e0d\u5728\u5047\u8bbe\u7a7a\u95f4\uff0c\u90a3\u8be5\u5982\u4f55\u89e3\u51b3? \u5982\u4e24\u4e2a\u6b63\u4f8b\u5305\u542b\u5c5e\u6027<code>sky=sunny</code> \u548c <code>sky=cloudy</code>, \u8fd9\u65f6\u5019\u8bad\u7ec3\u5f97\u5230\u7684\u6781\u5927\u7279\u6b8a\u5047\u8bbe\u662f <code>sky=?</code>\uff0c\u8fd9\u6837\u5c31\u4f1a\u5c06 \u53cd\u4f8b<code>sky=rainy</code> \u5224\u6210\u6b63\u4f8b\u3002\u8fd9\u4e2a\u95ee\u9898\u51fa\u73b0\u7684\u539f\u56e0\u5728\u4e8e\u6211\u4eec\u7684\u5047\u8bbe\u7a7a\u95f4\u4e0d\u591f\u5927\uff0c\u4e0a\u8ff0\u7684\u6781\u5927\u7279\u6b8a\u5047\u8bbe\u8fd8\u662f\u592a\u4e00\u822c\u4e86\u3002</p> <p>\u663e\u7136\uff0c\u9700\u8981\u4e00\u4e2a\u5047\u8bbe\u7a7a\u95f4\uff0c\u80fd\u591f\u8868\u8fbe\u6240\u6709\u7684 \u53ef\u6559\u6388\u6982\u5ff5, \u5373\u80fd\u591f\u8868\u8fbe\u5b9e\u4f8b \\(X\\) \u6240\u6709\u53ef\u80fd\u7684\u5b50\u96c6\uff0c\u628a\u8fd9\u4e9b\u5b50\u96c6\u7ec4\u6210\u7684\u96c6\u5408\u79f0\u4e3a \\(X\\) \u7684\u5e42\u96c6(power set)\u3002</p> <p>\u8fd9\u5c31\u7528\u5230\u4e86\u4e0b\u9762\u7684 \u5f52\u7eb3\u504f\u7f6e\u3002</p> <p>\u5f53\u5b66\u4e60\u5668\u53bb\u9884\u5176\u672a\u9047\u5230\u8fc7\u7684\u8f93\u5165\uff0c\u4f1a\u505a\u51fa\u5047\u8bbe\uff0c\u5b66\u4e60\u7b97\u6cd5\u7684\u5f52\u7eb3\u504f\u7f6e\u662f\u8fd9\u4e9b\u5047\u8bbe\u7684\u96c6\u5408\u3002</p> <p>\u5f52\u7eb3\u504f\u7f6e</p> <ul> <li>FIND-S \u7b97\u6cd5\u7684\u5f52\u7eb3\u504f\u7f6e</li> </ul> <p>\u76ee\u6807\u6982\u5ff5\u5728\u5047\u8bbe\u7a7a\u95f4\u4e2d\uff1b\u5bf9\u4e8e\u4efb\u4f55\u5b9e\u4f8b\uff0c\u82e5\u4e0d\u7b26\u5408\u6700\u7279\u6b8a\u5047\u8bbe\uff0c\u9664\u975e\u5176\u4e3a\u6b63\u4f8b\uff1b</p> <ul> <li>\u5019\u9009\u6d88\u9664\u7b97\u6cd5\u7684\u5f52\u7eb3\u504f\u7f6e</li> </ul> <p>\u76ee\u6807\u6982\u5ff5\u5305\u542b\u5728\u7ed9\u5b9a\u7684\u5047\u8bbe\u7a7a\u95f4\u5185\u3002</p> <p>\u5bf9\u4e8e\u65b0\u5b9e\u4f8b\u505a\u5206\u7c7b\uff0c\u6216\u65e0\u6cd5\u5206\u7c7b\u3002</p> <p>\u4e00\u4e2a\u7b97\u6cd5\u6709\u504f\u6027\u8d8a\u5f3a\uff0c\u5219\u5f52\u7eb3\u80fd\u529b\u8d8a\u5f3a\uff0c\u53ef\u4ee5\u5206\u7c7b\u66f4\u591a\u7684\u672a\u89c1\u5b9e\u4f8b\u3002\u65e0\u504f\u7684\u5b66\u4e60\u5668\u65e0\u6cd5\u5bf9\u672a\u89c1\u6837\u4f8b\u8fdb\u884c\u5f52\u7eb3\u3002\u5982\u679c\u5047\u8bbe\u7a7a\u95f4\u88ab\u62d3\u5c55\uff0c\u5b9e\u4f8b\u96c6\u7684\u5e42\u96c6\u90fd\u6709\u4e00\u4e2a\u5047\u8bbe\uff0c\u5219\u5019\u9009\u6d88\u9664\u7b97\u6cd5\u7684\u5f52\u7eb3\u504f\u7f6e\u6d88\u5931\u3002</p>"},{"location":"courses/AIML/ML/Decision_Tree/","title":"Decision Tree","text":"<p>\u5f52\u7eb3\u504f\u7f6e: \u4f18\u5148\u9009\u62e9\u8f83\u5c0f\u7684\u6811</p> <p>\u51b3\u7b56\u6811</p> <p>\u6839\u5f00\u59cb\uff0c\u5230\u53f6\u5b50\u8282\u70b9\uff0c\u5bf9\u5b9e\u4f8b\u8fdb\u884c\u5206\u7c7b\u3002\u6811\u4e0a\u7684\u8282\u70b9\u5bf9\u5e94\u6bcf\u4e2a\u5b9e\u4f8b\u7684\u5c5e\u6027\u7684\u6d4b\u8bd5\uff0c\u540e\u7ee7\u5206\u652f\u4ee3\u8868\u8be5\u5c5e\u6027\u7684\u4e00\u4e2a\u53ef\u80fd\u503c\u3002</p> <p>\u672c\u8d28\u4e0a\uff0c\u5b83\u4ee3\u8868\u5b9e\u4f8b\u5c5e\u6027\u503c\u7684\u5408\u53d6(conjuction, \u4ece\u6839\u5230\u53f6\u5b50\u8282\u70b9)\u7684\u6790\u53d6\u5f0f(disjunction, \u4e00\u4e2a\u6839\u8282\u70b9\u7684\u4e0d\u540c\u7684\u53ef\u80fd\u6027)</p> <p>\u9002\u7528\u95ee\u9898\u7279\u5f81</p> <p>\u4e0b\u9762\u4e09\u70b9\u6bd4\u8f83\u663e\u7136\u3002</p> <ul> <li> <p>\u5b9e\u4f8b\u7531 \"\u5c5e\u6027-\u5bf9\" \u7ec4\u6210\uff0c\u5c5e\u6027\u5bf9\u5e94\u7684\u503c\u53ef\u4ee5\u5f0f\u79bb\u6563\u7684\uff0c\u4e5f\u53ef\u4ee5\u662f\u5206\u533a\u95f4\u8fde\u7eed\u7684\u3002</p> </li> <li> <p>\u76ee\u6807\u51fd\u6570\u6709\u79bb\u6563\u7684\u8f93\u51fa\u503c\uff0c\u5982\u5e03\u5c14\u578b\u5206\u7c7b\uff0c\u6216\u4e24\u4e2a\u4ee5\u4e0a\u8f93\u51fa\u503c\u7684\u5206\u7c7b\u5668\u3002</p> </li> <li> <p>\u9700\u8981\u6790\u53d6\u7684\u63cf\u8ff0</p> </li> </ul> <p>\u4e0b\u9762\u4e24\u70b9\u503c\u5f97\u63a2\u8ba8\u3002</p> <ul> <li> <p>\u5b9e\u4f8b\u96c6\u53ef\u4ee5\u5305\u542b\u9519\u8bef\uff1f\uff1f</p> </li> <li> <p>\u5b9e\u4f8b\u96c6\u53ef\u4ee5\u5305\u542b\u7f3a\u5c11\u5c5e\u6027\u503c\u7684\u5b9e\u4f8b\uff1f\uff1f</p> </li> </ul>"},{"location":"courses/AIML/ML/Decision_Tree/#id3","title":"ID3 \u7b97\u6cd5","text":"<p>\u4f7f\u7528\u8d2a\u5a6a\u7b97\u6cd5\u3002</p> <p>\u6838\u5fc3\u95ee\u9898\u662f\u9009\u62e9\u6bcf\u4e2a\u8282\u70b9\u8981\u6d4b\u8bd5\u7684\u5c5e\u6027\u3002\u5982\u4f55\u8861\u91cf\u4e00\u4e2a\u5c5e\u6027\u5177\u6709\u6700\u597d\u7684\u9009\u62e9\u80fd\u529b\uff1f\u8fd9\u91cc\u4f7f\u7528\u4fe1\u606f\u589e\u76ca\u3002</p>"},{"location":"courses/AIML/ML/Decision_Tree/#_1","title":"\u4fe1\u606f\u589e\u76ca","text":"<ul> <li>\u71b5</li> </ul> <p>\u7ed9\u5b9a\u4e00\u4e2a\u6837\u4f8b\u96c6\uff0c\u6709\u71b5</p> \\[ entropy(S)=\\sum_{i=1}^c -p_i\\log_2 p_i \\] <p>\u8fd9\u91cc \\(c\\) \u8868\u793a\u72b6\u6001\u6570\uff0c\\(p_i\\)\u8868\u793a\u67d0\u4e00\u5206\u7c7b\u503c\u5f97\u5b9e\u4f8b\u5360\u6bd4\u3002\u53ef\u77e5\u4e0a\u5f0f\u6700\u5927\u503c\u662f\\(\\log_2 c\\), \u5f53\u6bcf\u4e00\u4e2a\u72b6\u6001\u6570\u7684\u5b9e\u4f8b\u76f8\u7b49\u7684\u65f6\u5019\uff0c\u6700\u5c0f\u662f\\(0\\)\uff0c\u5f53\u5176\u4e2d\u4e00\u4e2a\u72b6\u6001\u5360\u6ee1\u3002</p> <ul> <li>\u4fe1\u606f\u589e\u76ca: \u662f\u7531\u4e8e\u4f7f\u7528\u8fd9\u4e2a\u5c5e\u6027\u5206\u5272\u6837\u4f8b\u96c6\u800c\u5bfc\u81f4\u671f\u671b\u71b5\u7684\u51cf\u5c0f</li> </ul> <p>\u8fd9\u91cc\u7528\u671f\u671b\uff0c\u8868\u660e\u4e00\u4e2a\u5c5e\u6027\u7684\u4e0d\u540c\u503c\u5bf9\u505a\u51fa\u6837\u4f8b\u96c6\u5212\u5206\u540e\uff0c\u8981\u52a0\u6743\u6c42\u5e73\u5747\u6765\u8868\u8fbe\u603b\u4f53\u7684\u71b5\u51cf\u60c5\u51b5\u3002</p> \\[ gain(S,A) = entropy(S) - \\sum_{v\\in value(A)}\\frac{|S_v|}{|S|}entropy(S_v) \\] <p>\u6ce8\u610f\u540e\u9762\u7684\u71b5\\(entropy(S_v)\\)\u662f\u6761\u4ef6\u71b5\uff0c\u57fa\u4e8e\\(S_v\\)\u6765\u8ba1\u7b97\u540e\u7eed\u522b\u7684\u5c5e\u6027\u7684\u71b5.</p> <p>\u9012\u5f52\u8c03\u7528\u3002\u5f53\u6837\u4f8b\u4e2a\u6570\u4e3a0\u6216\u71b5\u4e3a0\u65f6\uff0c\u786e\u5b9a\u53f6\u5b50\u8282\u70b9\u7684\u5c5e\u6027\u3002\u5df2\u88ab\u6811\u7684\u8f83\u9ad8\u8282\u70b9\u6d4b\u8bd5\u7684\u5c5e\u6027\u88ab\u6392\u9664\u5728\u5916\u3002</p> <p>ID3 \u7b97\u6cd5\u7684\u4f18\u52a3</p> \u4f18\u52bf \u52a3\u52bf \u5047\u8bbe\u7a7a\u95f4\u5305\u542b\u6240\u6709\u7684\u51b3\u7b56\u6811\uff0c\u65f6\u5173\u4e8e\u73b0\u6709\u5c5e\u6027\u7684\u6709\u9650\u79bb\u6563\u503c\u51fd\u6570\u7684\u4e00\u4e2a\u5b8c\u6574\u7a7a\u95f4\u3002 \u8bad\u7ec3\u7ed3\u675f\u540e\uff0cID3\u53ea\u7ef4\u62a4\u5355\u4e00\u7684\u5f53\u524d\u5047\u8bbe\u3002\u4e0d\u50cf\u53d8\u5f62\u7a7a\u95f4\u7684\u5019\u9009\u6d88\u9664\u7b97\u6cd5\u80fd\u591f\u7ef4\u62a4\u4e0e\u8bad\u7ec3\u6837\u4f8b\u4e00\u81f4\u7684\u6240\u6709\u5047\u8bbe \u6bcf\u4e00\u6b65\u90fd\u4f7f\u7528\u5f53\u524d\u7684\u6240\u6709\u8bad\u7ec3\u6837\u4f8b\uff0c\u57fa\u4e8e\u7edf\u8ba1\uff0c\u80fd\u591f\u964d\u4f4e\u5bf9\u4e2a\u522b\u8bad\u7ec3\u6837\u4f8b\u9519\u8bef\u7684\u654f\u611f\u6027\uff0c\u4e0d\u50cfFind_S\u7b97\u6cd5/\u5019\u9009\u6d88\u9664\u7b97\u6cd5\u5bf9\u4e2a\u522b\u6570\u636e\u7684\u9519\u8bef\u975e\u5e38\u654f\u611f \u641c\u7d22\u4e0d\u8fdb\u884c\u56de\u6eaf\uff0c\u5bb9\u6613\u6536\u655b\u5230\u5c40\u90e8\u6700\u4f18\u7b54\u6848\uff1f\uff1f"},{"location":"courses/AIML/ML/Decision_Tree/#_2","title":"\u5f52\u7eb3\u504f\u7f6e","text":"<p>\u8fd1\u4f3c\u7684\u5f52\u7eb3\u504f\u7f6e: \u8f83\u77ed\u7684\u6811\u6bd4\u8f83\u957f\u7684\u6811\u4f18\u5148\u3002\u4fe1\u606f\u589e\u76ca\u9ad8\u7684\u5c5e\u6027\u66f4\u9760\u8fd1\u6839\u8282\u70b9\u7684\u6811\u4f18\u5148\u3002</p> ID3\u7b97\u6cd5 \u5019\u9009\u6d88\u9664\u7b97\u6cd5 \u4f18\u9009\u504f\u7f6e\uff0c\u641c\u7d22\u504f\u7f6e\uff0c\u5bf9\u4e8e\u67d0\u79cd\u5047\u8bbe\u80dc\u8fc7\u5176\u4ed6\u5047\u8bbe\u7684\u4f18\u9009\uff0c\u5e76\u4e0d\u8003\u8651\u6700\u7ec8\u53ef\u5217\u4e3e\u7684\u5047\u8bbe\u79cd\u7c7b \u9650\u5b9a\u504f\u7f6e\uff0c\u8bed\u8a00\u504f\u7f6e\uff0c\u9650\u5b9a\u5047\u8bbe\u7684\u7a7a\u95f4"},{"location":"courses/AIML/ML/Decision_Tree/#occams-razor","title":"\u5965\u574e\u59c6\u5243\u5200(Occam's razor)","text":"<p>\u4f18\u5148\u9009\u62e9\u62df\u5408\u6570\u636e\u7684\u6700\u7b80\u5355\u7684\u5047\u8bbe\u3002</p> <p>ID3\u7b97\u6cd5\u4f18\u9009\u8f83\u77ed\u51b3\u7b56\u6811\u7684\u5f52\u7eb3\u504f\u7f6e\u662f\u6cdb\u5316\u7684\u4e00\u79cd\u4f9d\u636e\u3002</p>"},{"location":"courses/AIML/ML/Decision_Tree/#_3","title":"\u5e38\u89c1\u95ee\u9898: \u8fc7\u5ea6\u62df\u5408","text":"<ul> <li>\u566a\u58f0\u3001\u5de7\u5408\u4f1a\u5bfc\u81f4\u51b3\u7b56\u6811\u66f4\u52a0\u590d\u6742\u3002</li> </ul> <p>\u89e3\u51b3\u65b9\u6848</p> <ul> <li> <p>\u53ca\u65e9\u505c\u6b62\u6811\u589e\u957f: \u8f83\u4e3a\u56f0\u96be</p> </li> <li> <p>\u540e\u4fee\u526a\u6cd5: \u91c7\u7528\u4e00\u5b9a\u7684\u51c6\u5219\u8fdb\u884c\u4fee\u526a\u3002</p> </li> </ul> <p>||| |:---:|</p> <ul> <li>\u8bad\u7ec3\u548c\u9a8c\u8bc1\u96c6\u6cd5</li> </ul> <p>\u8bad\u7ec3\u96c6\u53ef\u80fd\u56e0\u566a\u58f0\u3001\u968f\u5373\u9519\u8bef\u548c\u5de7\u5408\u89c4\u5f8b\u6027\u8bef\u5bfc\uff0c\u4f46\u9a8c\u8bc1\u96c6\u4e0d\u592a\u53ef\u80fd\u8868\u73b0\u51fa\u540c\u6837\u7684\u6ce2\u52a8\u3002\u8fd9\u6837\u6c42\u9a8c\u8bc1\u96c6\u8981\u8db3\u591f\u5927\uff0c\u4e00\u822c\u2153\u7684\u6570\u636e\u7528\u4e8e\u9a8c\u8bc1\u3002</p> <p>\u540e\u4fee\u526a\u6cd5</p> <ul> <li> <p>\u9519\u8bef\u7387\u964d\u4f4e\u4fee\u526a</p> </li> <li> <p>\u89c4\u5219\u540e\u4fee\u526a</p> </li> </ul>"},{"location":"courses/AIML/ML/Intro/","title":"Introduction","text":""},{"location":"courses/AIML/ML/Intro/#_1","title":"\u795e\u7ecf\u7f51\u7edc","text":"<p>ouline</p> <p>\u795e\u7ecf\u7f51\u7edc\u5b9a\u4e49\u4e0e\u6982\u5ff5</p> <p>\u611f\u77e5\u5668\uff08perceptron\uff09</p> <p>\u7ebf\u6027\u5355\u5143\uff08linear unit\uff09</p> <p>\u8bad\u7ec3\u591a\u5c42\u7f51\u7edc\u7684\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5\uff08Delta\u6cd5\u5219\uff09</p> <p>ANN\u7684\u8868\u5f81\u80fd\u529b</p> <p>\u5047\u8bbe\u7a7a\u95f4\u641c\u7d22\u7684\u672c\u8d28\u7279\u5f81</p> <p>\u795e\u7ecf\u7f51\u7edc\u8fc7\u5ea6\u62df\u5408\u95ee\u9898</p> <ul> <li>\u795e\u7ecf\u7f51\u7edc\u5b9a\u4e49</li> </ul> <p>\u795e\u7ecf\u7f51\u7edc\u662f\u7531\u5177\u6709\u9002\u5e94\u6027\u7684\u7b80\u5355\u5355\u5143\u7ec4\u6210\u7684\u5e7f\u6cdb\u5e76\u884c\u4e92\u8054\u7684\u7f51\u7edc\uff0c\u5b83\u7684\u7ec4\u6210\u80fd\u591f\u6a21\u62df\u751f\u7269\u795e\u7ecf\u7cfb\u7edf\u5bf9\u771f\u5b9e\u4e16\u754c\u7269\u4f53\u6240\u4f5c\u51fa\u7684\u53cd\u5e94</p> <ul> <li>\u611f\u77e5\u673a</li> </ul> <p>\u7531\u4e24\u5c42\u795e\u7ecf\u5143\u7ec4\u6210\u8f93\u5165\u5c42\u63a5\u53d7\u5916\u754c\u4fe1\u53f7\u4f20\u9012\u7ed9\u8f93\u51fa\u5c42\uff0c\u8f93\u51fa\u5c42\u662fM-P\u795e\u7ecf\u5143\uff08\u9608\u503c\u903b\u8f91\u5355\u5143\uff09</p> <p>\u5b66\u4e60\u80fd\u529b</p> <ul> <li> <p>\u7ebf\u6027\u53ef\u5206 (\u4e0e\u6216\u975e)\uff0c\u5219\u4e00\u5b9a\u6536\u655b</p> </li> <li> <p>\u975e\u7ebf\u6027\u53ef\u5206\u95ee\u9898 (\u591a\u5c42\u611f\u77e5\u673a)</p> </li> </ul> <ul> <li>\u591a\u5c42\u524d\u9988\u795e\u7ecf\u7f51\u7edc</li> </ul> <p>\u5b66\u4e60\uff1a\u6839\u636e\u8bad\u7ec3\u6570\u636e\u6765\u8c03\u6574\u795e\u7ecf\u5143\u4e4b\u95f4\u7684\"\u8fde\u63a5\u6743\"\uff0c\u4ee5\u53ca\u6bcf\u4e2a\u529f\u80fd\u795e\u7ecf\u7684\"\u9608\u503c\"</p> <p>\u8868\u793a\u80fd\u529b: \u53ea\u8981\u5305\u542b\u8db3\u591f\u591a\u795e\u7ecf\u5143\u7684\u9690\u5c42\uff0c\u5c31\u80fd\u4ee5\u4efb\u610f\u7cbe\u5ea6\u903c\u8fd1\u4efb\u610f\u590d\u6742\u5ea6\u7684\u8fde\u7eed\u51fd\u6570</p> <ul> <li> <p>\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5</p> </li> <li> <p>\u7f13\u89e3\u8fc7\u62df\u5408\u7684\u7b56\u7565</p> </li> </ul> <p>\u7f13\u89e3\u8fc7\u62df\u5408\u7684\u7b56\u7565</p> <p>\u65e9\u505c: \u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u82e5\u8bad\u7ec3\u8bef\u5dee\u964d\u4f4e\uff0c\u9a8c\u8bc1\u8bef\u5dee\u589e\u52a0\uff0c\u5c31\u505c\u6b62\u8bad\u7ec3</p> <p>\u6b63\u5219\u5316: \u8bef\u5dee\u76ee\u6807\u51fd\u6570\u589e\u52a0\u4e00\u9879\u63cf\u8ff0\u7f51\u7edc\u590d\u6742\u7a0b\u5ea6\u7684\u90e8\u5206\uff0c\u5982\u8fde\u63a5\u6743\u503c\u548c\u9608\u503c\u7684\u5e73\u65b9\u548c</p> <p>\u4ea4\u53c9\u9a8c\u8bc1\u65b9\u6cd5\uff1aK-fold \u8f83\u5dee\u9a8c\u8bc1\u3002\u628a\u8bad\u7ec3\u6837\u4f8b\u5206\u6210k\u4efd\uff0c\u5176\u4e2d\u4e00\u4efd\u5206\u522b\u4f5c\u4e3a\u6d4b\u8bd5\u96c6\uff0c\u53e6\u5916\u7684\u6570\u636e\u4f5c\u4e3a\u8bad\u7ec3\u96c6\u3002\u9010\u6b21\u8f6e\u6362\uff0c\u8ba9\u6bcf\u4e00\u4efd\u6570\u636e\u90fd\u6210\u4e3a\u9a8c\u8bc1\u96c6\u8fdb\u884c\u8bad\u7ec3\uff0c\u8bb0\u5f55\u6bcf\u6b21\u8bad\u7ec3\u5728\u9a8c\u8bc1\u96c6\u4e0a\u8fbe\u5230\u6700\u4f73\u8bad\u7ec3\u6548\u679c(\u8bef\u5dee\u6700\u4f4e)\u7684\u8bad\u7ec3\u6b21\u6570i\uff0c\u5c06k\u4e2a\u4e0d\u540c\u7684i\u53d6\u5e73\u5747\uff0c\u4f5c\u4e3a \\(\\overline{i}\\)\u3002\u6700\u540e\u6240\u6709\u7684\u6570\u636e\u4f5c\u4e3a\u8bad\u7ec3\u96c6\uff0c\u8bad\u7ec3\\(\\overline{i}\\)\u540e\u505c\u6b62</p> <p>\u53ef\u4ee5\u5e94\u7528\u4ea4\u53c9\u9a8c\u8bc1\u65b9\u6cd5\u786e\u5b9a\u9690\u5c42\u5355\u5143\u6570</p>"},{"location":"courses/AIML/ML/Intro/#_2","title":"\u6982\u5ff5\u5b66\u4e60","text":"<p>Outline</p> <p>\u6982\u5ff5\u5b66\u4e60\u5b9a\u4e49</p> <p>\u6982\u5ff5\u5b66\u4e60\u4efb\u52a1</p> <p>\u5f52\u7eb3\u5b66\u4e60\u4e0e\u5f52\u7eb3\u5b66\u4e60\u5047\u8bbe</p> <p>Find-S: \u5bfb\u627e\u6781\u5927\u7279\u6b8a\u5047\u8bbe\uff0c\u504f\u5e8f</p> <p>\u53d8\u5f62\u7a7a\u95f4\u548c\u5019\u9009\u6d88\u9664\u7b97\u6cd5</p> <p>\u5f52\u7eb3\u504f\u7f6e</p>"},{"location":"courses/AIML/ML/Intro/#_3","title":"\u6982\u5ff5\u5b66\u4e60\u5b9a\u4e49","text":"<p>\u662f\u4ece\u6709\u5173\u67d0\u4e2a\u5e03\u5c14\u51fd\u6570\u7684\u8f93\u5165\u8f93\u51fa\u8bad\u7ec3\u6837\u4f8b\u4e2d\u63a8\u65ad\u51fa\u8be5\u5e03\u5c14\u51fd\u6570</p>"},{"location":"courses/AIML/ML/Intro/#_4","title":"\u6982\u5ff5\u5b66\u4e60\u7684\u4efb\u52a1","text":"<p>\u5bfb\u627e\u4e00\u4e2a\u5047\u8bbeh, \u4f7f\u5f97\u5bf9\u6240\u6709\u7684h\uff0c\u90fd\u6709h(x)=c(x)</p>"},{"location":"courses/AIML/ML/Intro/#_5","title":"\u5f52\u7eb3\u5b66\u4e60\u5b9a\u4e49","text":"<p>\u4ece\u7279\u6b8a\u7684\u6837\u4f8b\u5f97\u5230\u666e\u904d\u7684\u89c4\u5f8b</p>"},{"location":"courses/AIML/ML/Intro/#_6","title":"\u5f52\u7eb3\u5b66\u4e60\u5047\u8bbe","text":"<p>\u4efb\u4e00\u5047\u8bbe\uff0c\u5982\u679c\u5728\u8db3\u591f\u5927\u7684\u8bad\u7ec3\u6837\u4f8b\u4e0a\u80fd\u591f\u5f88\u597d\u5730\u903c\u8fd1\u76ee\u6807\u51fd\u6570(\u4e0e\u8bad\u7ec3\u6570\u636e\u6700\u4f73\u62df\u5408)\uff0c\u5b83\u4e5f\u80fd\u5728\u672a\u89c1\u5b9e\u4f8b\u4e2d\u5f88\u597d\u5730\u903c\u8fd1\u76ee\u6807\u51fd\u6570</p>"},{"location":"courses/AIML/ML/Intro/#find-s","title":"Find-S","text":""},{"location":"courses/AIML/ML/Intro/#_7","title":"\u5019\u9009\u6d88\u9664\u7b97\u6cd5","text":"<p>\u8f93\u51fa\u4e0e\u8bad\u7ec3\u6837\u4f8b\u4e00\u81f4\u5730\u6240\u6709\u5047\u8bbe\u7684\u96c6\u5408</p> <p>\u6b63\u4f8b-\u6781\u5c0f\u4e00\u822c\u5316</p> <p>\u5e94\u7528\uff1a\u5316\u5b66\u8d28\u8c31\u5206\u6790\u3001\u542f\u53d1\u5f0f\u641c\u7d22\u7684\u63a7\u5236\u89c4\u5219</p>"},{"location":"courses/AIML/ML/Intro/#_8","title":"\u53d8\u5f62\u7a7a\u95f4","text":"<p>\u4e0e\u8bad\u7ec3\u6837\u4f8b\u4e00\u81f4\u7684\u6240\u6709\u5047\u8bbe\u7ec4\u6210\u7684\u96c6\u5408\uff0c\u8868\u793a\u4e86\u76ee\u6807\u6982\u5ff5\u7684\u6240\u6709\u5408\u7406\u7684\u53d8\u578b</p> <p>\u88ab\u8868\u793a\u4e3a\u6781\u5927\u4e00\u822c(G)\u548c\u6781\u5927\u7279\u6b8a(S)\u7684\u6210\u5458</p> <p>\u6982\u5ff5\u5b66\u4e60\u7684\u6700\u4f18\u67e5\u8be2\u7b56\u7565\uff1a\u662f\u4ea7\u751f\u5b9e\u4f8b\u4ee5\u6ee1\u8db3\u5f53\u524d\u53d8\u578b\u7a7a\u95f4\u4e2d\u5927\u7ea6\u534a\u6570\u7684\u5047\u8bbe\u3002\u8fd9\u6837\uff0c\u53d8\u578b\u7a7a\u95f4\u7684\u5927\u5c0f\u53ef\u4ee5\u5728\u9047\u5230\u6bcf\u4e2a\u65b0\u6837\u4f8b\u65f6\u51cf\u534a</p>"},{"location":"courses/AIML/ML/Intro/#_9","title":"\u5f52\u7eb3\u504f\u7f6e","text":"<p>\u5f52\u7eb3\u5b66\u4e60\u9700\u8981\u7684\u9884\u5148\u5047\u5b9a</p>"},{"location":"courses/AIML/ML/Intro/#_10","title":"\u7b97\u6cd5\u8bc4\u4f30","text":"<p>outline</p> <p>\u5047\u8bbe\u7684\u8bc4\u4f30\u3001\u4e24\u4e2a\u5047\u8bbe\u7cbe\u5ea6\u7684\u6bd4\u8f83\u3001\u4e24\u4e2a\u5b66\u4e60\u7b97\u6cd5\u7cbe\u5ea6\u6bd4\u8f83</p> <p>\u6837\u672c\u9519\u8bef\u7387\u3001\u771f\u5b9e\u9519\u8bef\u7387</p> <p>\u4ea4\u53c9\u9a8c\u8bc1\u4e0et\u914d\u5bf9\u6d4b\u8bd5</p>"},{"location":"courses/AIML/ML/Intro/#_11","title":"\u8d1d\u53f6\u65af\u5b66\u4e60","text":"<p>success</p> <p>\u8d1d\u53f6\u65af\u7406\u8bba\u6982\u5ff5\u5b9a\u4e49</p> <p>\u6781\u5927\u4f3c\u7136\u5047\u8bbe\uff08ML\uff09\u548c\u6781\u5927\u540e\u9a8c\u6982\u7387\u5047\u8bbe\uff08MAP\uff09</p> <p>\u8d1d\u53f6\u65af\u7406\u8bba\u5bf9\u5176\u4ed6\u5b66\u4e60\u7b97\u6cd5\u7684\u89e3\u91ca</p> <p>\u8d1d\u53f6\u65af\u6700\u4f18\u5206\u7c7b\u5668</p> <p>Gibbs\u7b97\u6cd5 </p> <p>\u6734\u7d20\u8d1d\u53f6\u65af\u5206\u7c7b\u5668</p>"},{"location":"courses/AIML/ML/Intro/#_12","title":"\u8d1d\u53f6\u65af\u7406\u8bba","text":"<p>\u8d1d\u53f6\u65af\u7406\u8bba\u63d0\u4f9b\u4e86\u4e00\u79cd\u8ba1\u7b97\u5047\u8bbe\u6982\u7387\u7684\u65b9\u6cd5\uff0c\u57fa\u4e8e\u5047\u8bbe\u7684\u5148\u9a8c\u6982\u7387\u3001\u7ed9\u5b9a\u5047\u8bbe\u4e0b\u89c2\u5bdf\u5230\u4e0d\u540c\u6570\u636e\u7684\u6982\u7387\u4ee5\u53ca\u89c2\u5bdf\u5230\u7684\u6570\u636e\u672c\u8eab</p>"},{"location":"courses/Data_Structure%26Algorithm/","title":"Index","text":""},{"location":"courses/Data_Structure%26Algorithm/#data-structure-and-algorithm","title":"Data Structure and Algorithm","text":"<p>Reference</p> <p>Data structures and algorithm analysis in C++, Mark Allen Weiss</p>"},{"location":"courses/Data_Structure%26Algorithm/#outline","title":"Outline","text":""},{"location":"courses/Data_Structure%26Algorithm/#algorithm-analysis","title":"Algorithm Analysis","text":""},{"location":"courses/Data_Structure%26Algorithm/#lists-stacks-and-queues","title":"Lists, Stacks and Queues","text":""},{"location":"courses/Data_Structure%26Algorithm/#trees","title":"Trees","text":"<ul> <li>BST</li> </ul> <p>insert time cost: \\(O(\\log n)=O(h)\\)</p> <p>remove time cost: \\(O(h)\\)</p> <ul> <li>AVL tree</li> </ul>"},{"location":"courses/Data_Structure%26Algorithm/#hashing","title":"Hashing","text":""},{"location":"courses/Data_Structure%26Algorithm/#priority-queuesheaps","title":"Priority Queues(Heaps)","text":"<ul> <li>Heap-Order Properties</li> </ul>"},{"location":"courses/Data_Structure%26Algorithm/#sorting","title":"Sorting","text":""},{"location":"courses/Data_Structure%26Algorithm/#the-disjoint-sets-class","title":"The disjoint Sets Class","text":""},{"location":"courses/Data_Structure%26Algorithm/#graph-algorithm","title":"Graph Algorithm","text":""},{"location":"courses/Data_Structure%26Algorithm/AlgoAna/","title":"Algorithm Analysis","text":""},{"location":"courses/Data_Structure%26Algorithm/AlgoAna/#basic-notations","title":"Basic Notations","text":"<p>We must have some notation to better describe our algorithm.</p> <p>Definitions</p> <p>(i) We call \\(T(N)=O(f(N))\\), if there exists positive constants \\(c\\), \\(n_0\\), such that </p> \\[ T(N)\\leq cf(N), \\quad  \\forall N\\geq n_0 \\] <p>(ii) We call \\(T(N)=\\Omega(g(N))\\), if there exists positive constants \\(c\\), \\(n_0\\), such that</p> \\[ T(N)\\geq cg(N), \\quad  \\forall N\\geq n_0 \\] <p>(iii) we call \\(T(N)=\\Theta(h(N))\\), if and only if \\(T(N)=O(h(N))\\) and \\(T(N)=\\Omega(h(N))\\).</p> <p>(iv) we call \\(T(N)=o(p(N))\\), if forall constant \\(c&gt;0\\), there exists \\(n_0\\) such that </p> \\[ T(N)&lt;cp(N),\\quad \\forall N&gt;n_0 \\] <p>Or less formally, \\(T(N)=o(p(N))\\) if \\(T(N)=O(p(N))\\) and \\(T(N)\\neq \\Theta(p(N))\\). </p> <p>The following rules are usually really useful for algorithm analysis.</p> <p>Rules</p> <p>(i) If \\(T_1(N)=O(f(N))\\) and \\(T_2(N)=O(g(N))\\), then </p> \\[ \\begin{align*} T_1(N)+T_2(N)&amp;=O(f(N)+g(N))\\quad \\text{(steps summation)}\\\\ T_1(N)\\times T_2(N)&amp;=O(f(N)\\times g(N)) \\quad \\text{(steps multiplication)}. \\end{align*} \\] <p>(ii) If \\(T(N)\\) is a polynomial of degree \\(k\\), then </p> \\[ T(N)=\\Theta(N^k). \\] <p>(Proved by find \\(c_1\\), \\(c_2\\) to be its upper and lower bound)</p> <p>(iii) \\(\\log^kN=O(N)\\), \\(\\forall k\\).</p>"},{"location":"courses/Data_Structure%26Algorithm/AlgoAna/#recursions","title":"Recursions","text":"<p>We take Merge Sort as an example.</p> Merge<pre><code>Merge(A, p, q, r)\n    n1 = q - p + 1\n    n2 = r - q\n    L[1...(n1 + 1)]\n    R[1...(n2 + 1)]\n    for i = 1 to n1\n        L[i] = A[p + i - 1]\n    for j = 1 to n2\n        R[j] = A[q + j]\n    L(n1 + 1) = R(n2 + 1) = +inf\n    for k = p to r\n        if L[i] &lt;= R[j]\n            A[k] = L[i]\n            i = i + 1\n        else\n            A[k] = R[j]\n            j = j + 1\n</code></pre> <p>whose time cost is \\(O(N)\\), where \\(N=r+p-1\\).</p> MergeSort<pre><code>MergeSort(A, p, r)\n    if p &lt; r\n        q = (p + r) / 2\n        MergeSort(A, p, q)\n        MergeSort(A, q + 1, r)\n        Merge(A, p, q, r)\n</code></pre> <p>Assume the time cost of MergeSort is \\(T(N)\\), where \\(N=r+p-1\\), then we have</p> \\[ T(N)=\\begin{cases}O(1),\\quad N=1\\\\ 2T(N/2)+O(N),\\quad N&gt;1\\end{cases} \\] <p>It is apparent to draw a graph of tree for this recursion, get its time cost and then prove it using Induction.</p> <p>Generalize the above thoughts and we have the following theorem.</p> <p></p> <p>Master Theorem</p> <p>Assume \\(T(N)\\) is the time cost for a recursive algorithm, and satisfies</p> \\[ T(N)=\\begin{cases}O(1),\\quad N=1\\\\ aT(N/b)+f(N),\\quad N&gt;1\\end{cases} \\] <p>where \\(a\\geq 1\\), \\(b&gt;1\\) and \\(f(N)&gt;0\\) for sufficient large \\(N\\). </p> <p>(i) If there exists \\(\\varepsilon&gt;0\\) such that \\(f(N)=O(N^{\\log_b{a-\\varepsilon}})\\), then \\(T(N)=\\Theta(N^{\\log_ba})\\).</p> <p>(ii) If \\(f(N)=\\Theta(N^{\\log_ba})\\), then \\(T(N)=\\Theta(N^{\\log_b a}\\log N)\\).</p> <p>(iii) If there exists \\(\\varepsilon&gt;0\\), such that \\(f(N)=\\Omega(N^{\\log_b a+\\varepsilon})\\), and for sufficient large \\(N\\), \\(af(N/b)\\leq cf(N)\\), \\(c&lt;1\\), then \\(T(N)=\\Theta(f(N))\\).</p> <p>The above \\(\\varepsilon\\) in (i) and (iii) denotes the relationship on polynomial. And the latter condition in (iii) of the above theorem is called regularity condition.</p> <p>The focus is to compare the relative size between \\(N^{\\log_b a}\\) and \\(f(N)\\). If the former is larger, then time cost is determined by \\(T(N)=aT(N/b)\\), which is \\(O(N^{\\log_b a})\\), meaning the bifurcation of the tree is large enough such that the time cost of each layer can be ignored. However, on the other hand, if the latter is larger, then time cost is determined by \\(f(N)\\),  which is \\(O(f(N))\\).</p> <p>Q1. Solve for time cost \\(T(n)\\) in the following situation</p> <p>(i) \\(T(n)=9T(n/3)+n\\)</p> <p>(ii) \\(T(n)=T(2n/3)+1\\)</p> <p>(iii) \\(T(n)=3T(n/4)+n\\lg n\\)</p> Answer for (i)Answer for (ii)Answer for (iii) \\[ n^{\\log_b a}=n^{\\log_3 9}=n^2&gt;n \\] <p>so </p> \\[ f(n)=n=O(n^{\\log_b a}) \\] <p>which is (i) of Master Theorem. So \\(T(n)=\\Theta(n^2)\\).</p> \\[ n^{\\log_b a}=n^{\\log_{3/2} 1}=n^0=1 \\] <p>so</p> \\[ f(n)=1=\\Theta(1)=\\Theta(n^{\\log_b a}) \\] <p>which is (ii) of Master Theorem. So \\(T(n)=\\Theta(\\log n)\\).</p> \\[ n^{\\log_b a}=n^{\\log_4 3}&lt;n^1&lt;n\\lg n \\] <p>which does not satisfy (i), (ii) of Master Theorem, so we need to confirm if it satisfies the regularity condition of (iii) in Master Theorem. Notice that</p> \\[ af(n/b)=3{\\frac{n}{4}}\\lg \\left({\\frac{n}{4}}\\right) \\] <p>So if we choose \\(c=3/4\\), then </p> \\[ af(n/b)\\leq cf(n), \\forall n\\geq 1 \\] <p>So it satisfies (iii) of Master Theorem. Thus time cost \\(T(n)=\\Theta(n\\log n)\\)</p> <p>Notice that there exists gap between (i) and (ii) in Master Theorem, cause some function may not satisfy \\(\\varepsilon\\). There also exsits a gap  between (i) and (ii) in Master Theorem, cause the regularity condition.</p> <p> Q2. Solve for time cost \\(T(n)\\) in the following situation <p>(i) \\(T(n)=2T(n/2)+n\\lg n\\)</p> <p>(ii) \\(T(n)=T(n/2)+n(2-\\cos n\\pi)\\)</p> Answer <p>The above two could not be solved by Master Theorem. We have to use recursive tree.</p>"},{"location":"courses/Data_Structure%26Algorithm/AlgoAna/#divide-conquer","title":"Divide &amp; Conquer","text":"<ul> <li>\u4e8c\u5206\u641c\u7d22</li> </ul> <p>C++<pre><code>BinarySearch(A, x)\n    left, right = 0, len(A)-1\n\n    while left&lt;=right:\n        mid = (left+right)/2\n        if(x==A[mid]):\n            return mid;\n        else if (A[mid]&lt;x):\n            left = mid+1;\n        else if (A[mid]&gt;x):\n            right = mid-1;\n\n    return -1;\n</code></pre> time cost \\(O(log n)\\), space cost \\(O(1)\\).</p> C++<pre><code>BinarySearch(A, x, left, right)\n    if left&gt; right:\n        return -1;\n\n    mid = (left+right)/2;\n\n    if(A[mid]==x):\n        return mid;\n    else if (A[mid]&lt;x):\n        return BinarySearch(A,x, mid+1,right);\n    else if (A[mid]&gt;x):\n        return BinarySearch(A,x, left, mid-1);\n</code></pre> <p>time cost \\(O(log n)\\), space cost \\(O(log n)\\).</p>"},{"location":"courses/Data_Structure%26Algorithm/Dis_Sets_Class/","title":"The disjoint Sets Class","text":""},{"location":"courses/Data_Structure%26Algorithm/Dis_Sets_Class/#union-by-size","title":"Union-by-Size","text":"<p>change size when union</p>"},{"location":"courses/Data_Structure%26Algorithm/Dis_Sets_Class/#union-by-height","title":"Union-by-Height","text":"<p>does not need to change height when union, tree height only changes when two trees are of the same size,</p> <p>find is dynamic, let the found node points directly to the root node. However, we need to change the height.</p>"},{"location":"courses/Data_Structure%26Algorithm/Dis_Sets_Class/#union-by-rank","title":"Union-by-Rank","text":"<p>After find(), we do not change the height. So the height bacomes the rank.</p>"},{"location":"courses/Data_Structure%26Algorithm/Dis_Sets_Class/#generate-maze","title":"Generate Maze","text":""},{"location":"courses/Data_Structure%26Algorithm/Graph/","title":"Graph Algorithm","text":""},{"location":"courses/Data_Structure%26Algorithm/Graph/#preliminary-dynamic-programming","title":"Preliminary: Dynamic Programming","text":"<p>(Note: content of this part is different from dynamic programming in combination optimization)</p> <ul> <li>Longest common subsequence, LCS</li> </ul> <p>input: \\(x=\\{x_i\\}_{i=1}^m\\), \\(y=\\{y_i\\}_{i=1}^n\\),</p> <p>output: their LCS.</p> <p>For example, if we have two sequences </p> \\[ \\begin{cases} x=\\{A,B,C,B,D,A,B\\}\\\\ y=\\{B,D,C,A,B,A\\} \\end{cases} \\] <p>then LCS\\((x,y)=\\{BDAB,BCAB,BCBA\\}\\).</p> <ul> <li>solve the length of LCS.</li> </ul> Proof <ul> <li>\\(x[i]=y[j]\\), if \\(C[i,j]=k\\), denote one element \\(z[1,\\cdots,k]\\in\\) LCS\\([i,j]\\). then \\(z[k]=x[i]=y[j]\\). So \\(z[1,\\cdots,k-1]\\in LCS[i-1,j-1]\\).</li> </ul> <p>Assume \\(w\\in\\) LCS\\([i-1.j-1]\\) and \\(|w|&gt;k-1\\), then connect \\(w\\) and \\(z[k]\\), \\(w+z[k]\\) satisfies</p> \\[ \\begin{cases} w+z[k]\\in \\text{LCS}[i,j]\\\\ \\left|w+z[k]\\right|&gt;k \\end{cases} \\] <p>contradicts!</p> <ul> <li>optimal substructure</li> </ul> <p>\\(\\forall z[1,\\cdots,k]\\in\\) LCS\\([m,n]\\), then \\(z[1,\\cdots,s]\\), \\(s&lt; k\\) must be LCS of their prefix.</p> Natural Version for LCS<pre><code>LCS(x,y,i,j)\n    if(i==-1||j==-1)\n        return 0\n    else if (x[i]==y[j])\n        c[i,j]=LCS(x,y,i-1.j-1)+1\n    else \n        c[i,j]=max(LCS(x,y,i,j-1),LCS(x,y,i-1.j))\n    return c[i,j]\n</code></pre> <ul> <li>cut off </li> </ul> <p>write a note of LCS that has been calculated. So next time we encounter a same item, directly use it. In fact we use a 2 dimension array.</p> Better Version for LCS<pre><code>LCS(x,y,i,j)\n    if (c[i,j]==nil)\n        if (x[i]==y[j])\n            c[i,j]=LCS(x,y,i-1.j-1)+1\n        else \n            c[i,j]=max(LCS(x,y,i,j-1),LCS(x,y,i-1.j))\n    return c[i,j]\n</code></pre> <p>time cost \\(O(mn)\\).</p> <p>\u81ea\u9876\u5411\u4e0b top down.</p> <p>\u81ea\u5e95\u5411\u4e0a bottom up.</p>"},{"location":"courses/Data_Structure%26Algorithm/Graph/#gragh-theory","title":"Gragh theory","text":""},{"location":"courses/Data_Structure%26Algorithm/Graph/#definitions","title":"Definitions","text":"<p>A graph \\(G=(V,E)\\) consists a set of vertices, \\(V\\) and a set of edges \\(E\\). for \\(u,w\\in V\\), \\((u,w)\\in E\\).</p> <p>If \\((u,w)\\neq(w,u)\\), then we call the graph is directed, or digraph. Otherwise we call the graph undirected.</p> <p>We call \\(u,w\\in V\\) are adjacent iff \\((u,w)\\in E\\). A map from \\(E\\) to \\(\\mathbb{R}\\) is called weight, or cost.</p> <p>A path is a sequence of vertices \\(\\{w_1,w_2,\\cdots,w_N\\}\\), such that \\((w_i,w_{i+1})\\in E\\), \\(\\forall i\\in [1,N-1]\\). When there is not weight defined, the length of the path is \\(N-1\\).</p> <p>\u81ea\u73af\u8def\u5f84(Loop). </p> <p>A simple path is a path such that all vertices are distinct, except that the first and the last could be the same.</p> <p>A cycle path(\u5faa\u73af\u8def\u5f84) in a directed graph is a path of length at least 1 such that \\(w_1=w_N\\).  </p> <p>A directed graph is acyclic if it has no cycles.</p> <p>Directed Acyclic Graph(\u6709\u5411\u65e0\u73af\u56fe). </p> <p>An undirected graph is connected, if \\(\\forall u,w\\in V\\), \\((u,w)\\in E\\). A directed graph with this property is called strongly connected. </p>"},{"location":"courses/Data_Structure%26Algorithm/Graph/#representation-of-graph","title":"Representation of Graph","text":"<ul> <li>Use 2 dimension array. Adjacency matrix.</li> </ul> <p>If \\(|E|= \\Theta(|V|^2)\\), then the graph is dense.</p> <p>If \\(|E|= \\Theta(|V|)\\), then the graph is sparse.</p> <ul> <li>Use linked-list. Adjacency list.</li> </ul> <p>For each vertex, we keep a list of all adjacent vertices. </p> <p>Space cost \\(O(|V|+|E|)\\).</p>"},{"location":"courses/Data_Structure%26Algorithm/Graph/#topological-sort","title":"Topological Sort","text":"<p>Definition of Topological Sort</p> <ul> <li>\u62d3\u6251\u5e8f | Topological Sort</li> </ul> <p>\\(\\forall v_i,v_j\\in V\\), we say \\(v_i\\leq v_j\\), if there exists </p> <p>(i) a path from \\(v_i\\) to \\(v_j\\)</p> <p>(ii) \\(v_i\\) is before \\(v_j\\)</p> <ul> <li>\u62d3\u6251\u6392\u5e8f | Topological Sorting</li> </ul> <p>list \\(\\{v_i\\}_{i-1}^n\\) into a queue</p> \\[ v'_1, v'_2,\\cdots, v'_n \\] <p>such that, \\(\\forall v'_i\\neq v'_j\\), we have</p> <p>(i) \\(v_i\\leq v_j\\)</p> <p>(ii) there exists no path from \\(v_i\\) to \\(v_j\\).</p> <ul> <li>\u5165\u5ea6 | indegree</li> </ul> <p>there must exist a node with indegree = 0.</p> Version 1 for topological sort<pre><code>topsort()\n{\n    for(i=0;i&lt;n_vertex;i++)\n        v = FindNextVertexIndegreeZero() // actually we could only do this for the first time.\n        v.tp=i;\n        for each w adjacent to v\n            w.indegree --;\n}\n</code></pre> Better version for topological sort<pre><code>void Graph::topsort()\n{\n    Queue&lt;vertex&gt; q;\n    int counter = 0;\n    q.makeEmpty();\n    for each vertex in V:\n        if v.indegree==0:\n            q.enqueue(v); // O(|V|)\n\n    while(!q.empty()){\n        vertex v= q.dequeue();\n        v.tp=++counter;\n        for each vertex w adjacent to v:\n            if(--w.indegree==0):\n                q.enqueue(w); \n    } // O(|E|+|V|)\n\n    if(counter != NUM_VERTICES)\n        throw CYclwFoundException();\n}\n</code></pre>"},{"location":"courses/Data_Structure%26Algorithm/Graph/#shortest-path-algorithm","title":"Shortest-Path Algorithm","text":"<p>Input is a weighted Graph.</p> <p>Assume weight is positive.</p> <p>Theorem</p> <p>If \\(P_{(u,v)}\\) is the shortest path between u and v, then \\(\\forall x,y\\in P_{(u,v)}\\), \\(P_{(x,y)}\\subset P_{(u,v)}\\) is the shortest path bwtween x and y.</p> <p>Or triangle inequality. forall \\(x,y,z \\in V\\)</p> \\[ \\delta(x,y)\\leq \\delta(x,z)+\\delta(z,y) \\] Proof <p>By contradiction. If there exists another path \\(P'_{(x,y)}\\neq P_{(x,y)}\\) and </p> \\[ w(P'_{(x,y)})&lt; w(P_{(x,y)}) \\] <p>then </p> \\[ \\begin{align*} w{(P'_{(u,v)})} &amp;= w{(P_{(u,x)})} + w{(P'_{(x,y)})} + w{(P_{(y,v)})}\\\\ &amp;&lt; w{(P_{(u,x)})} + w{(P_{(x,y)})} + w{(P_{(y,v)})}\\\\ &amp;=w{(P_{(u,v)})} \\end{align*} \\] <p>that is, we find a path that is smaller than the shortest path, which contradicts!</p> <p>We also call the above problem Single-Source shortest path.</p> <p>The method is known as Dijlstra's Algorithm.</p>"},{"location":"courses/Data_Structure%26Algorithm/Graph/#unweighted-graph","title":"Unweighted graph","text":"<p>This is actually BFS. Unweighted graph<pre><code>void Graph::unweighted(Vertex s)\n{   \n    Queue&lt;Vertex&gt; q;\n    for each Vertex v\n    {\n        v.dist = INFTY;\n        v.known = false;\n    }\n\n    s.dist=0;\n    q.enqueue(s);\n\n    while(!q.isEmpty())\n    {\n        Vertex v = q.dequeue();\n\n        for each Vertex w adjacent to v:\n            if(w.dist == INFTY):\n            {\n                w.dist = v.dist + 1;\n                w.path = v;\n                q.enqueue(w);\n            }\n    }\n}\n</code></pre></p> <p>Cost: \\(O(|E|+|V|)\\)</p>"},{"location":"courses/Data_Structure%26Algorithm/Graph/#weighted-graph","title":"Weighted Graph","text":"<p>This time, we cannot say that the neighberhood is the shortest, which may be longer than some path with more vertices but less weight cost.</p> Weighted Graph<pre><code>s.d=0;\nfor each v in V-{s}\n    v.d=INFTY;\nQueue=V;\nwhile(!q.isEmpty()){\n    u = q.pop_min(); // smallest heap\n    for each v adjacent to u:\n        if v.d&gt;u.d+w(u,v):\n            v.d = u.d+w(u,v);\n            v.p = u;\n}\n</code></pre>"},{"location":"courses/Data_Structure%26Algorithm/Graph/#all-pair-shortest-path","title":"All-pair shortest path","text":"<p>use adjacent matrix \\(A=(a_{ij})_{n\\times n}\\), with \\(a_{ij}\\) denotes the weight from \\(v_i\\) to \\(v_j\\) and \\(a_{ii}=0\\).</p> <p>use \\(d_{ij}^{(m)}\\) to denote the shortest path weight from \\(v_i\\) to \\(v_j\\) with passing \\(m\\) edges.</p>"},{"location":"courses/Data_Structure%26Algorithm/Graph/#graph-with-negative-costs","title":"Graph with negative costs","text":"C++<pre><code>Bellman-Fond Algorithm\n</code></pre> <p>Theorem of Bellman-Fond Algorithm</p> <p>v.d is the shortest path from s to v.</p> Proof <p>v.d is homotonically decreasing and has lower bound.</p> \\[ v.d \\geq \\delta(s,v) \\]"},{"location":"courses/Data_Structure%26Algorithm/Graph/#bounded-graph","title":"bounded graph","text":"\\[ x_j-x_i\\leq w_{ij} \\leftrightarrow v_i\\overset{w_{ij}}{\\rightarrow}v_j \\]"},{"location":"courses/Data_Structure%26Algorithm/Graph/#solve-shortest-path-between-all-vertices","title":"solve shortest path between all vertices","text":"<p>Assume G is a dense graph.</p> <p>\\(a_{ij}\\) denotes the weight form i to j.</p> <p>\\(d_{ij}^{(m)}\\) denotes the shortest path with length \\(m\\) from i to j.</p> <p>(i) \\(d_{ij}^{(0)}=\\begin{cases}0,\\quad i=j\\\\ \\infty,\\quad \\text{else}\\end{cases}\\)</p> <p>(ii) \\(d_{ij}^{(k)}\\)</p>"},{"location":"courses/Data_Structure%26Algorithm/Graph/#floyd-worshall-algorithm","title":"FLoyd-Worshall Algorithm","text":"<p>define \\(c_{ij}^{(n)}\\) the shortest path with length at most \\(n-1\\) from i to j, and only passes vertices \\(\\{1,2,\\cdots, k\\}\\)</p> \\[ c_{ij}^{(k)} = \\min\\{c_{ij}^{(k-1)}, c_{ik}^{(k-1)} + c_{kj}^{(k-1)}\\} \\]"},{"location":"courses/Data_Structure%26Algorithm/Graph/#remapping","title":"Remapping","text":"\\[ h:V\\mapsto \\mathbb{R} \\] <p>then </p> \\[ w_h(u,v):=w(u,v) + h(u) - h(v),\\quad \\forall (u,v)\\in E \\] <p>Theorem</p> <p>\\(\\forall u,v \\in V\\), \\(P_{uv}\\) is the path, then </p> \\[ w_h(u,v)-w(u,v)=Const = h(u)-h(v) \\] <p>Corollary</p> <p>\\delta_h(u,v) = \\delta(u,v)+h(u)-h(v)</p> <p>How to get \\(h\\)?</p> <p>notice that \\(w_h(u,v)=w(u,v)+ h(u)-h(v)\\geq 0 \\Rightarrow\\)</p> \\[ h(v)-h(u)\\leq w(u,v) \\] <p>which is a difference bounded system. So we can use Bellman-Fond Algorithm. Then the cost of weight is all positive, so we can use Dijkstra Algorithm. So here comes Johnsen Algorithm.</p> <ul> <li>Johnsen Algorithm</li> </ul> <p>(i) find \\(h:V\\rightarrow \\mathb{R}\\), such that </p> \\[ w_h(u,v)=w(u,v)+ h(u)-h(v)\\geq 0 ,\\quad \\forall (u,v)\\in E \\] <p>use Bellman-Fond Algorithm to solve \\(h\\).</p> <p>(ii) use w_h to run Dijkstra Algorithm, get \\(\\delta_h(u,v)\\).</p> <p>(iii) </p>"},{"location":"courses/Data_Structure%26Algorithm/Graph/#minimum-spanning-trees","title":"Minimum spanning trees","text":"<p>Theorem</p> <p>delete one edge from a MST, denoted by T1, T2, and the subgraph of G denoted by G1, G2. Then T1, T2 is the MST of G1, G2 respectively.</p> Proof <p>Assume we delete (u,v), then</p> <p>w(T) = w(u,v) + w(T1)+w(T2)</p> <p>If we find another tree T1', s.t. w(T1')&lt;w(T1)</p> <p>then T'=T1'\\cup (u,v)\\cup T2</p> <p>which satisfies w(T')&lt;w(T), contracdicts!</p> <p>Convex!</p> <p>Theorem</p> <p>Assume T is the MST of G(V,E), A\\subset V, assume (u,v)\\in E is the smallest edge which can connect A and V\\ A, then </p> <p>(u,v)\\in T</p> Proof <p>Assume (u,v)\\nin T</p>"},{"location":"courses/Data_Structure%26Algorithm/Graph/#prims-algorithm","title":"Prim's Algorithm","text":"Text Only<pre><code>Q=V\nfor each v in V:\n    v-&gt;key = \\infty // initialization\n\ns\\in V, s-&gt;key = 0  root\nwhile(Q\\neq empty)\n    u = \\min_v Q key\n    for v\\in Adj[u]: // update neigborhood\n        if v\\in Q (not \u5f39\u51fa) and w(u,v)&lt;v-&gt;key\n            v-&gt;key = w(u,v) // update\n            v-&gt;parent = u\n</code></pre>"},{"location":"courses/Data_Structure%26Algorithm/Heaps/","title":"Heaps","text":""},{"location":"courses/Data_Structure%26Algorithm/outline/","title":"\u8003\u8bd5\u7eb2\u8981","text":"<p>\u6ca1\u6709 C++ \u4ee3\u7801\uff0c\u53ea\u6709\u7b97\u6cd5\u5206\u6790\u548c\u6570\u636e\u7ed3\u6784\u7684\u7406\u8bba\u77e5\u8bc6\uff0c\u6709\u4f2a\u4ee3\u7801\u3002</p> <ol> <li>\u7b97\u6cd5\u5206\u6790\uff0c\\(O\\) \u548c \\(\\varTheta\\) \u7b49\u6e10\u8fd1\u5206\u6790\u8bb0\u53f7\u7684\u4f7f\u7528\u3002\u5206\u6cbb\u7b56\u7565\uff0c\u4e3b\u5b9a\u7406\u548c\u5bf9\u5e94\u7684\u5f52\u7eb3\u6cd5\u8bc1\u660e\u3002</li> <li>\u57fa\u672c\u6570\u636e\u7ed3\u6784\uff0c\u6570\u7ec4\u3001\u94fe\u8868\u3001\u6808\u3001\u961f\u5217\u3001\u6811\u3001\u56fe\u7b49\u7684\u5b9a\u4e49\u548c\u5b9e\u73b0\u3002</li> <li>\u4e8c\u53c9\u641c\u7d22\u6811\u548c AVL \u6811\u7684\u5b9a\u4e49\uff0c\u63d2\u5165\u548c\u5220\u9664\u64cd\u4f5c\u7684\u590d\u6742\u5ea6\u5206\u6790\u3002(\u4e0d\u8981\u6c42\u4f38\u5c55\u6811\u7b49\u9ad8\u7ea7\u7ed3\u6784)</li> <li>\u6700\u5927\u5806\u3001\u6700\u5c0f\u5806\u548c\u5806\u6392\u5e8f\u7b97\u6cd5\u7684\u5b9a\u4e49\u548c\u5b9e\u73b0\u3002(\u4e0d\u8981\u6c42\u5de6\u5806\u3001\u659c\u5806\u7b49\u9ad8\u7ea7\u7ed3\u6784)</li> <li>\u6392\u5e8f\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u548c\u590d\u6742\u5ea6\u5206\u6790\uff0c\u5305\u62ec\u63d2\u5165\u6392\u5e8f\u3001\u5f52\u5e76\u6392\u5e8f\u3001\u5feb\u901f\u6392\u5e8f\u3001\u5806\u6392\u5e8f\u7b49\u3002</li> <li>\u5e76\u67e5\u96c6 (union-find) \u7684\u5b9a\u4e49\u548c\u5b9e\u73b0\u3002\u6ce8\u610f\u5e76\u67e5\u662f\u540c\u6b65\u53d1\u751f\u7684\u3002\u8981\u6c42\u5e76\u67e5\u96c6\u7684\u6548\u7387\u63d0\u5347\u6280\u5de7\u3002</li> <li>\u56fe\u7684\u8868\u793a\u65b9\u6cd5\uff0c\u90bb\u63a5\u77e9\u9635\u548c\u90bb\u63a5\u8868\u7684\u4f18\u7f3a\u70b9\u3002\u56fe\u7684\u904d\u5386\u7b97\u6cd5\uff0c\u6df1\u5ea6\u4f18\u5148\u641c\u7d22\u548c\u5e7f\u5ea6\u4f18\u5148\u641c\u7d22\u3002</li> <li>\u6700\u5c0f\u751f\u6210\u6811\u7b97\u6cd5\uff0c\u5bf9\u7b97\u6cd5\u7684\u7406\u89e3\uff0c\u5b9e\u73b0\u548c\u8bc1\u660e\u3002</li> <li>\u6700\u77ed\u8def\u5f84\u7b97\u6cd5\uff0c\u5bf9\u7b97\u6cd5\u7684\u7406\u89e3\u3002\u8981\u6c42\u5355\u6e90\u6700\u77ed\u8def\u5f84\uff0c\u6b63\u8fb9\u6743\u6700\u77ed\u8def\u5f84\u7b97\u6cd5\u3002</li> <li>\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\uff0c\u8981\u6c42\u7406\u89e3\uff0c\u8bc1\u660e\u548c\u5b9e\u73b0\u3002Quick Reply</li> </ol>"},{"location":"courses/Data_Structure%26Algorithm/outline/#recitation","title":"recitation","text":"<ul> <li> <p>\u5f52\u5e76\u6392\u5e8f\u7684\u4e24\u4e2a\u51fd\u6570</p> </li> <li> <p>\u62f7\u8d1d\u6784\u9020\u3001\u8d4b\u503c\u8fd0\u7b97\u3001\u79fb\u52a8\u6784\u9020</p> </li> <li> <p>list \u7684\u5217\u8868\u6784\u9020\uff0cfind\uff0cinsert</p> </li> <li> <p>\u8ba1\u7b97\u8868\u8fbe\u5f0f\u7684\u4e2d\u7f00\u3001\u540e\u7f00\u8868\u8fbe\uff0c\u4e92\u76f8\u8f6c\u6362\uff0c\u540e\u7f00\u8868\u8fbe\u5f0f\u8fdb\u884c\u8ba1\u7b97</p> </li> </ul> C++<pre><code>string infix2postfix(string infix){\n    std::stack&lt;char&gt; operators;\n    string postfix;\n\n    for(char c:infix){\n        if(isnum(c))postfix+=c;\n        else if (c=='(')operators.push(c);\n        else if (c==')'){\n            while(!operators.empty() &amp;&amp; operators.top()!='('){\n                prefix = operators.pop();\n            }\n            operators.pop();\n        }\n        else if(isoperator(c)){\n            while(!operators.empty() &amp;&amp; operators.top()!='(' &amp;&amp; precedence(operators.top())&gt;precedence(c))postfix+=operators.pop();// \u9ad8\u4f18\u5148\u7ea7\u7684\u6808\u5185\u5168\u90e8\u5f39\u51fa\n        }\n        operators.push(c);\n    }\n}\n</code></pre>"},{"location":"courses/Data_Structure%26Algorithm/sorting/","title":"Sorting","text":"<ul> <li> <p>\u6548\u7387 | Efficiency</p> </li> <li> <p>\u539f\u5730\u6027 | In Place</p> </li> <li> <p>\u7a33\u5b9a\u6027 | Stability</p> </li> </ul> <p>A sorting algorithm is stable if elements with equal elements are left in the same order as they occur in the input.</p> <p>A stable algorithm can be suitable for Multiple targets.</p>"},{"location":"courses/Data_Structure%26Algorithm/sorting/#insertion-sort","title":"Insertion Sort","text":""},{"location":"courses/Data_Structure%26Algorithm/sorting/#shell-sort","title":"Shell Sort","text":"<p>\\(h_t\\) = \\(\\lfloor N/2 \\rfloor\\), \\(h_k\\) = \\(\\lfloor h_{k+1}/2 \\rfloor\\).</p> <p>with Worst case \\(O(N^2)\\)</p> <p>Now we have better selection with worst case \\(O(N^{7/6})\\) with </p> \\[ h_k= \\begin{cases} 9\\cdot 4^i - 9\\cdot 2^i+1, \\quad i\\mod 2 == 0\\\\ 4^{i+1}-6 \\cdot 2^i+1, \\quad i\\mod 2 == 1 \\end{cases} \\]"},{"location":"courses/Data_Structure%26Algorithm/sorting/#heap-sort","title":"Heap Sort","text":""},{"location":"courses/Data_Structure%26Algorithm/sorting/#merge-sort","title":"Merge Sort","text":""},{"location":"courses/Data_Structure%26Algorithm/sorting/#quick-sort","title":"Quick Sort","text":"<ul> <li> <p>Picking the Pivot</p> </li> <li> <p>Partition</p> </li> </ul> Partition<pre><code>Partition(A, p, q)\n    x=A[p]\n    i=p+1\n    j=q\n    while(i!=j)\n        while(A[j]&gt;=x &amp;&amp; i&lt;j)\n            j-- // put larger element in place \n        while(A[i]&lt;=x &amp;&amp; i&lt;j)\n            i++ // put smaller element in place \n        if(i&lt;j)\n            swap(A[i], A[j])\n    swap(A[p], A[i])\n</code></pre> Another Partition<pre><code>Partition(A, p, q)\n    x=A[p]\n    i=p\n    for j=p+1 to q:\n        if(A[j]&lt;=x)\n            i=i+1\n            swap(A[i],A[j])\n    A[p]=A[i]\n    return i\n</code></pre> QuickSort<pre><code>QuickSort(A,p,q)\n    if(p&lt;q)\n        r=partition(A,p,q)\n        QuickSort(A,p,r-1)\n        QuickSort(A,r+1,q)\n</code></pre> <p>Time cost:</p> <ul> <li>Ordered sequence </li> </ul> \\[ \\begin{align*} T(n)=T(1)+T(n-1)+\\Theta(n) \\end{align*} \\] <ul> <li>Best sequence</li> </ul> \\[ T(n)=2T(n/2)+\\Theta(n) \\] <ul> <li>not Bad or Good</li> </ul> <p>Every Partition gives 1:9 sequence</p>"},{"location":"courses/Data_Structure%26Algorithm/sorting/#randomize-pivot","title":"Randomize pivot","text":"Text Only<pre><code>Randomized_Partition(A,p,q)\n    i=random(p,q)\n    swap(A[p],A[i])\n    return Partition(A,p,q)\n</code></pre> Text Only<pre><code>randomized_quicksort(A,p,q)\n    if(p&lt;q)\n        r=randomized_Partition(A,p,q+1)\n        randomized_quicksort(A,p,r-1)\n        randomized_quicksort(A,r+1,q)\n</code></pre>"},{"location":"courses/Data_Structure%26Algorithm/sorting/#some-details","title":"Some Details","text":"<ul> <li>Consider repetitive elements</li> </ul> Text Only<pre><code>Hoore_Partition(A,p,q)\n    x=A[p]\n    i=p, j=q+1\n    while(1):\n        do:\n            j=j-1\n        until(A[j]&lt;=x)\n        do:\n            i=i+1\n        until(A[i]&gt;=x)\n        if(i&lt;j)\n            swap(A[i],A[j])\n        else\n            swap(A[p],A[j])\n            return j\n</code></pre>"},{"location":"courses/Data_Structure%26Algorithm/sorting/#a-general-lower-bound-for-sorting","title":"A General Lower Bound for Sorting","text":"<p>Efficiency of sorting with comparing</p> <p>The best efficiency of sorting based on comparing is \\(\\Theta(n\\log{n})\\)</p> Proof <p>left tree denotes <code>true</code>, right tree denotes <code>false</code>, every leaf node represents a result of sorting. We have \\(n!\\) nodes. And let h denote the height of the decision tree. we have </p> \\[ n!\\leq 2^h \\] <p>\"=\" holds for complete binary decision tree. So</p> \\[ h\\geq \\log{n!} \\geq \\log{(n/e)^n} =\\Omega(n\\log{n}) \\]"},{"location":"courses/Intro2Visualization/","title":"Introduction to Visualization","text":"<p>The Project has been releaced on Conan Visualization.</p>"},{"location":"courses/Intro2Visualization/#conan-visualization","title":"Conan Visualization","text":"<p>Demo has been published on Bilibili, you are free to check it.</p> <p>Our main code lies in <code>./src</code>. </p>"},{"location":"courses/Intro2Visualization/#recommended-ide-setup","title":"Recommended IDE Setup","text":"<p>VSCode + Volar (and disable Vetur).</p>"},{"location":"courses/Intro2Visualization/#customize-configuration","title":"Customize configuration","text":"<p>See Vite Configuration Reference.</p>"},{"location":"courses/Intro2Visualization/#compile-and-hot-reload-for-development","title":"Compile and Hot-Reload for Development","text":"Bash<pre><code>npm run dev\n</code></pre>"},{"location":"courses/Intro2Visualization/#type-check-compile-and-minify-for-production","title":"Type-Check, Compile and Minify for Production","text":"Bash<pre><code>npm run build\n</code></pre>"},{"location":"courses/Numerical_Analysis/","title":"Numerical Analysis","text":"<p>Reference</p> <p>Numerical analysis, Richard L. Burden, J. Douglas Faires</p>"},{"location":"courses/Numerical_Analysis/#preliminary-errors","title":"\u8bef\u5dee | Preliminary: Errors","text":"<p>If a real number \\(x\\) is denoted as \\(0.d_1d_2d_3\\cdots \\times 10^{n}\\), then</p> <ul> <li>Truncation\uff08\u622a\u65ad\uff09 Error</li> </ul> <p>is induced when </p> \\[ \\hat{x}=0.d_1d_2d_3\\cdots d_k \\times 10^{n}  \\] <p>for some definite \\(k&lt;\\infty\\)</p> <ul> <li>Roundoff\uff08\u820d\u5165\uff09 Error</li> </ul> <p>is induced when </p> \\[ \\hat{x}=0. \\delta_1 \\delta_2 \\delta_3 \\cdots \\delta_k \\times 10^{n}  \\] <p>for some definite \\(k&lt;\\infty\\) </p> <p>where \\(\\delta_k &gt;d_k\\) if \\(d_{k+1}&gt;=5\\).</p>"},{"location":"courses/Numerical_Analysis/#t-significant-digits","title":"t significant digits","text":"<p>The number \\(p^*\\) is said to approximate p to \\(t\\) significant digits(or figures) if \\(t\\) is the largest nonnegative integer for which the relative error </p> \\[ e = \\frac{\\Delta p}{p}=\\frac{\\|p-p^*\\|}{\\|p\\|}&lt;5\\times 10^{-t}  \\] <p>where \\(p^*\\) is the approximate number of the exact number \\(p\\).</p> <ul> <li>for Chopping:</li> </ul> \\[  \\begin{align*} e &amp;= \\left|\\frac{0.d_{k+1}d_{k+2}\\cdots}{0.d_1d_2\\cdots}\\right| \\times 10^{-k} \\\\ &amp;\\leq \\frac{1}{0.1} \\times 10^{-k} \\quad \\text{\"=\" for } d_{k+1}d_{k+2}\\cdots\\rightarrow\\overline{9}\\text{ and }d_{1}d_{2}\\cdots\\rightarrow 0 \\\\ &amp;=10^{-k+1}  \\end{align*}  \\] <ul> <li>for rounding:</li> </ul> \\[ \\begin{align*} e &amp;\\leq \\frac{0.5}{0.1} \\times 10^{-k} \\quad \\text{\"=\" for } d_{k+1}d_{k+2}\\cdots\\rightarrow 5\\overline{0}\\text{ and }d_{1}d_{2}\\cdots\\rightarrow 0 \\\\  &amp;=0.5\\times 10^{-k+1} \\end{align*}  \\]"},{"location":"courses/Numerical_Analysis/#matrix-calculation","title":"\u6570\u503c\u4ee3\u6570\uff08\u77e9\u9635\u8ba1\u7b97\uff09 | Matrix Calculation","text":""},{"location":"courses/Numerical_Analysis/#numerical-approximation","title":"\u6570\u503c\u903c\u8fd1 | Numerical Approximation","text":""},{"location":"courses/Numerical_Analysis/#numerical-solution-of-differential-equations","title":"\u5fae\u5206\u65b9\u7a0b\u6570\u503c\u89e3 | Numerical Solution of Differential Equations","text":""},{"location":"courses/Numerical_Analysis/DE/","title":"Numerical solution of Differential Equation","text":"<p>Reference</p> <p>\u5fae\u5206\u65b9\u7a0b\u6570\u503c\u89e3\uff1a\u6709\u9650\u5dee\u5206\u7406\u8bba\u65b9\u6cd5\u4e0e\u6570\u503c\u8ba1\u7b97, \u5f20\u6587\u751f</p> <p>\u5fae\u5206\u65b9\u7a0b\u6570\u503c\u89e3, \u9648\u6587\u658c</p> <p>In ordinary differential equation</p> \\[ \\frac{du}{dt}f(t,u),\\quad u(0)=u_0 \\] <p>in region \\([a,b]\\). Note that it is hard to know the function, but we know its derivatives at all points on the region. That is, for any dicrete point \\(t_n\\in [a,b]\\), we have</p> \\[ \\frac{du}{dt}\\Bigg|_{t=t_n}=f(t_n,u(t_n)) \\] <p>The following part is focused on using there infomation to get a solution of ODE.</p>"},{"location":"courses/Numerical_Analysis/DE/#numerical-differentiation","title":"\u6570\u503c\u5fae\u5206 | Numerical Differentiation","text":"<p>Denote \\(h\\) as the time step forward, then according to Taylor expansion evaluated at \\(t_n\\) and take another forward point \\(t_{n+1}\\)</p> \\[ \\begin{equation} u(t_{n+1})=u(t_n)+h u'(t_n)+\\frac{h^2}{2!}u''(t_n)+\\frac{h^3}{3!}u'''(t_n)+o(h^4) \\label{forward} \\end{equation} \\] <p>backward at point \\(t_{n-1}\\), we substitute \\(h\\) with \\(-h\\) and get</p> \\[ \\begin{equation} u(t_{n-1})=u(t_n)-h u'(t_n)+\\frac{h^2}{2!}u''(t_n)-\\frac{h^3}{3!}u'''(t_n)+o(h^4)\\label{backward} \\end{equation} \\] <p>So we have an approximation of the first order derivative from equation \\(\\ref{forward}\\), that is, the forward form of divided difference</p> \\[ \\begin{align} u'(t_n)=\\frac{u(t_{n+1})-u(t_n)}{h}-\\frac{h}{2!}u''(t_n)-\\frac{h^2}{3!}u'''(t_n)+o(h^3) \\label{Forward} \\end{align} \\] <p>or from equation \\(\\ref{backward}\\), that is, the backward form of divided difference</p> \\[ \\begin{equation} u'(t_n)=\\frac{u(t_{n})-u(t_{n-1})}{h}+\\frac{h}{2!}u''(t_n)-\\frac{h^2}{3!}u'''(t_n)+o(h^3) \\label{Backward} \\end{equation} \\] <p>Combine equation \\(\\ref{Forward}\\) and equation \\(\\ref{Backward}\\) we have the center divided difference form</p> \\[ \\begin{equation} u'(t_n)=\\frac{u(t_{n+1})-u(t_{n-1})}{2h}-\\frac{h^2}{3!}u'''(t_n)+o(h^3) \\label{center} \\end{equation} \\] <p>which is more accurate. Now we can use the above equation to get Euler' method.</p> <p>Three Point Midpoint Formula</p> \\[ f'(x_0) = \\frac{1}{2h}[f(x_0+h)-f(x_0-h)] -\\frac{h^2}{6}f'''(\\xi) \\] <p>Use Lagrange Polynomial to approximate a function. Then the \\(n\\)th derivative of Lagrange Poly approximate the \\(n\\)th derivative of the original function.</p>"},{"location":"courses/Numerical_Analysis/DE/#eulers-method","title":"Euler's Method","text":"<p>Here we introduce Euler's one step method</p> \\[ \\begin{cases} \\omega_0=\\alpha,\\\\ \\omega_{i+1}=\\omega_i+hf(t_i,\\omega_i),\\quad i=0,1,\\cdots,n-1 \\end{cases} \\] <p>From forward form of divided difference \\(\\ref{Forward}\\) we have its rounding error </p> \\[ \\tau_n=\\frac{w(t_{n+1})-w(t_n)}{h}=\\frac{h}{2}f'(\\eta),\\quad t_n\\leq \\eta\\leq t_{n+1} \\] <p>First analyze its convergence.</p> <p>Convergence of Euler's Method</p> <p>Assume solution \\(\\phi(t)\\in C^2[t_0,b]\\), then the error of the solution \\(w_n\\) acquired from Euler's method satisfies</p> \\[ \\max_{a\\leq t_n \\leq b}|w(t_n)-w_n|\\leq e^{(b-t_0)L}|e_0|+\\frac{e^{(b-t_0)L}-1}{L}\\tau(h) \\] <p>where</p> \\[ \\tau(h)=\\frac{h}{2}\\|w''\\|_{\\infty},\\quad e_0=w(t_0)-w_0 \\] <p>If \\(h\\rightarrow 0\\), s.t.</p> \\[ |w(t_0)-w_0|\\leq c_1h,\\quad c_1&gt;0 \\] <p>then \\(\\exists c&gt;0\\), such that</p> \\[ \\max_{t_0\\leq t_n\\leq b}|w(t_n)-w_n|\\leq ch \\] <p>Then we analyze its asymptotic stability.</p> <p>Definition of asymptotic stability</p> <p>We call Euler's one step method is asymptotic stable, if \\(\\exists h_0&gt;0\\), \\(\\exists C&gt;0\\), such that \\(\\forall h\\in (0,h_0]\\), </p> \\[ |z_n-w_n|\\leq C\\varepsilon,\\quad \\forall 0\\leq n\\leq N(h) \\] <p>where \\(w_n\\) and \\(z_n\\) denotes the solution from before disturbance(initial value \\(w_0\\)) and after disturbance(initial value \\(z_0\\)) using Euler's one step method.</p> <p>The above shows that Euler's method is asymptotic stable.</p>"},{"location":"courses/Numerical_Analysis/DE/#implicite-eulers-method","title":"Implicite Euler's method","text":"<p>Implicite means we have to guess the number before iterating. Usually it is used after Explicite Euler's method to improve stability.</p> \\[ \\begin{cases} \\omega_0=\\alpha, \\\\ \\omega_{i+1}=\\omega_i+hf(t_i,\\omega_{i+1}),\\quad i=0,1,\\cdots,n-1 \\end{cases} \\] <p>From Backward form of Numerical Derivative(Divided Difference) \\(\\ref{Backward}\\), we can express its rounding error </p> \\[ \\tau_n=\\frac{w(t_{n+1})-w(t_{n})}{h}=-\\frac{h}{2}f'(\\eta),\\quad t_n\\leq \\eta\\leq t_{n+1} \\]"},{"location":"courses/Numerical_Analysis/DE/#modified-eulers-method","title":"Modified Euler's Method","text":"<p>So we can choose higher order items to improve accuracy.</p> <p>This method is also called Tranpezoid Method.</p> \\[ w(t_{n+1})=w(t_n)+\\frac{h}{2}\\left[f(t_n,w(t_n))+f(t_{n+1},w(t_{n+1}))\\right] \\] <p>We have this inspiration from tranpezoidal integration. </p> From Tranpezoidal integration <p>From integration the ODE on \\([t_n, t_{n+1}]\\) we have</p> \\[ w(t_{n+1})=w(t_n)+\\int_{t_n}^{t_{n+1}}f(\\tau, w(\\tau))d\\tau \\] <p>Using Tranpezoidal Integration Formula with its Error, we have</p> \\[ w(t_{n+1})=w(t_n)+\\frac{h}{2}\\left[f(t_n,w(t_n))+f(t_{n+1},w(t_{n+1}))\\right]-\\frac{h^3}{12}f'''(\\eta_n),\\quad t_n\\leq \\eta_n\\leq t_{n+1} \\] <p>which means a higher accuracy for method</p> \\[ w(t_{n+1})=w(t_n)+\\frac{h}{2}\\left[f(t_n,w(t_n))+f(t_{n+1},w(t_{n+1}))\\right] \\]"},{"location":"courses/Numerical_Analysis/DE/#double-step-method","title":"Double-step Method","text":"<p>We have this method inspired by the center form of divided difference. </p> <p>According to center form of divided difference \\(\\ref{center}\\) we have</p> \\[ \\begin{cases} \\omega_0=\\alpha,\\\\ \\omega_1=\\omega_0+hf(t_0,\\omega_0), \\\\ \\omega_{i+1}=\\omega_{i-1}+2hf(t_i,\\omega_i), \\quad i=1,2,\\cdots,N-1 \\end{cases} \\] <p>with its rounding error expressed by</p> \\[ \\tau_n=\\frac{w_{t_{n+1}}-w(t_n)}{h}=-\\frac{h^2}{3!}f''(\\eta), \\quad t_n\\leq \\eta\\leq t_{n+1} \\]"},{"location":"courses/Numerical_Analysis/DE/#taylor-method-of-order-n","title":"Taylor method of order n","text":"<p>To improve precision, we can choose more items of Taylor's expansion for the one order derivative.</p> \\[ \\begin{cases} \\omega_0=\\alpha, \\\\ \\omega_{i+1}=\\omega_i+hT^{(n)}(t_i,\\omega_i),\\quad i=0,1,\\cdots,N-1 \\end{cases} \\] <p>where</p> \\[ T^{(n)}(t_i,\\omega_i)=f(t_i,\\omega_i)+\\frac{h}{2}f'(t_i,\\omega_i)+\\cdots+\\frac{h^{n-1}}{n!}f^{(n-1)}(t_i,\\omega_i) \\] <p>Euler's method is Tarlor's method of order one.</p>"},{"location":"courses/Numerical_Analysis/DE/#runge-kutta-method","title":"Runge-Kutta Method","text":"<p>For Taylor's method of higher order, it is trivial to calculate high order derivatives, especially when \\(p&gt;3\\) and \\(f\\) has a complex expression. So the following Runge-Kutta Method aims to calculate function \\(f\\)'s values at different points to avoid high order derivatives, reducing calculation time.</p> <ul> <li>Runge-Kutta method with order 4.</li> </ul> \\[ \\begin{cases} w_{i+1}=w_i+\\frac{1}{6}(k_1+2K_2+2k_3+k_4) w_0=\\alpha \\end{cases} \\]"},{"location":"courses/Numerical_Analysis/DE/#multistep-methods","title":"Multistep Methods","text":"<p>Derive from integration</p> \\[ y(t_{i+1})-y(t_i)=\\int_{t_i}^{t_{i+1}}f(t,y)dt \\] <p>use interpolation polynomial to replace f(t,y).</p> <ul> <li>Adams-Bashforth Four-step Explicit Method</li> </ul> \\[ w_{i+1}=w_i + \\frac{h}{24}(55f_i-59f_{i-1}+37f_{i-2}-9f_{i -3}) \\] <ul> <li>Adams-Bashforth Three-step Implicit Method</li> </ul> \\[ w_{i+1}=w_i +\\frac{h}{24}(9f_{i+1}+19f_{i}-5f_{i-1}+f_{i-2}) \\] <p>Or we derive from Taylar expansion.</p>"},{"location":"courses/Numerical_Analysis/DE/#odes","title":"ODEs","text":""},{"location":"courses/Numerical_Analysis/MC/","title":"Matrix Calculation","text":""},{"location":"courses/Numerical_Analysis/MC/#direct-methods-for-solving-linear-systems","title":"Direct Methods for Solving Linear Systems","text":"<p>We focus on solving linear system </p> \\[ A\\vec{x} = \\vec{b} \\]"},{"location":"courses/Numerical_Analysis/MC/#gasussion-elimination","title":"\u9ad8\u65af\u6d88\u5143 | Gasussion Elimination","text":"<p>Reduce A into an upper-triangular matrix, and then solve for the unknowns by a backward-substitution process</p>"},{"location":"courses/Numerical_Analysis/MC/#pivoting-stratages","title":"\u4e3b\u5143\u9009\u62e9\u7b56\u7565 | Pivoting Stratages","text":"<p>This part is to reduce the error caused by rounding/Truncation error.</p> <p>We can show that the pivoting element is of great significance.</p> Partial PivotingScaled Partial PivotingComplete Pivoting <p>(also known for not changing the columns)</p> <p>Determine the smallest \\(p\\geq k\\) (in the same column of \\(a^{(k)}_{kk}\\))such that </p> \\[ |a_{ok}^{(k)}| = \\max_{k\\leq i \\leq n}{|a_{ik}^{(k)}|} \\] <p>and perform \\((E_k) \\leftrightarrow (E_p)\\).</p> <p>For row \\(i\\), let</p> \\[ s_i = \\max_{1\\leq j\\leq n}{|a_{ij}|} \\] <p>(if \\(\\exists i, s.t. s_i=0\\), then the system has no unique root. So we assume \\(\\forall i, s_i&gt;0\\))</p> <p>For each procedure of executing \\(E_k \\leftarrow E_k - m_{k,i}E_i\\) for \\(k=i+1, \\cdots, n\\), where \\(m_{k, i} = a_{ki}/{a_{ii}}\\). let </p> \\[ p = \\arg \\max_{i\\leq k \\leq n}{\\frac{|a_{ki}|}{s_k}} \\] <p>perform \\((E_i)\\leftrightarrow(E_p)\\)</p> <p>Incorporate the interchange of both rows and columns.</p>"},{"location":"courses/Numerical_Analysis/MC/#time-cost","title":"Time Cost","text":"<p>As we all know the time expense for Gaussion elimination is</p> \\[ O(n^3) \\]"},{"location":"courses/Numerical_Analysis/MC/#lu-lu-matrix-factorization","title":"LU\u5206\u89e3 | LU Matrix Factorization","text":"<p>The idea is encouraged by Gaussion Elimination. See that a matrix \\(A\\) can be transformed into an upper-trianglar matrix \\(U\\) by primary row operations:</p> \\[ \\begin{equation}  U = M_{n-1}\\cdots M_2M_1A  \\label{eq: LU} \\end{equation}  \\] <p>where \\(M_k (k=1,2,\\cdots n-1)\\) denotes a series of row operations. There are two perspetives.</p> Version 1Version 2 <p>\\(M_k (k=1, \\cdots, n-1)\\) can be interpreted that the \\(k+1\\) row has to make its column \\(1\\) to \\(k\\) to be \\(0\\). That is,</p> \\[ E_{k+1} \\leftarrow E_{k+1} - \\sum_{j=1}^{k} m_{k+1, j}E_j \\] <p>\\(M_k (k=1,\\cdots, n-1)\\) can be defined in another way as </p> \\[ E_j \\leftarrow E_j - \\sum\\limits_{k=j}^{n}m_{j,k}E_k \\quad \\text{for } j=k+1, \\cdots n \\] <p>which is also a lower-triangular matrix. To be proved by readers. </p> <p>And we can see \\(M_k\\) formed through the above two interpretations are the same.</p> <p>If we denote \\(L_k = M_k^{-1}\\), then apply \\(L = L_1L_2\\cdots L_{n-1}\\) left to both sides of the equation \\(\\ref{eq: LU}\\), then</p> \\[ LU = L_1L_2\\cdots L_{n-1} \\cdot M_{n-1}\\cdots M_2M_1A = A \\] <p>We know that matrix \\(L_k\\) and \\(M_k\\) are lower-triangular matrix(explaned by definition, to be proved by readers), so the product of matrix L is alao a lower-triangular matrix.</p> <p>So with the triangular matrix, it can be much quicker to solve the solution. See that</p> \\[  \\begin{align*} A\\vec{x} &amp;= \\vec{b} \\\\ LU\\vec{x} &amp;= \\vec{b} \\end{align*}  \\] <p>First solve \\(L \\vec{y} = \\vec{b}\\), then solve \\(U \\vec{x} = \\vec{y}\\).</p>"},{"location":"courses/Numerical_Analysis/MC/#time-cost_1","title":"time cost","text":"<p>Eliminate \\(0.5n^2\\) elements, it needs time \\(O(0.5n^3)\\).</p> <p>Solving \\(y\\) and \\(x\\), it needs time \\(O(2n^2)\\).</p>"},{"location":"courses/Numerical_Analysis/MC/#iterative-techniques-in-matrix-algebra","title":"\u77e9\u9635\u4ee3\u6570\u7684\u8fed\u4ee3\u6cd5 | Iterative Techniques in Matrix Algebra","text":"<p>This section, we introduce the iterative thoughts from Fixed-Point Iteration\uff08\u4e0d\u52a8\u70b9\u6cd5\uff09 to solve a linear system.</p> <p>We aim to find a iterative equation like equation \\(\\pmb{x}^{k} = f(\\pmb{x}^{k-1})\\). To be more specific, a linear iterative equation like</p> \\[ \\pmb{x}^{k} = T \\pmb{x}^{k-1} + \\pmb{c} \\] <p>and its corresponding convergent relation is</p> \\[ \\pmb{x}= T \\pmb{x} + \\pmb{c} \\]"},{"location":"courses/Numerical_Analysis/MC/#preliminary-knowledge-norm-of-vectors-and-matrixes","title":"\u8303\u6570 | Preliminary knowledge: Norm of Vectors and Matrixes","text":"<p>Definition of Norm of vectors</p> <p>A vector norm on \\(\\mathbb{R}^n\\) is a function, denoted as \\(\\Vert \\cdot \\Vert\\), mapping from \\(\\mathbb{R}^n\\) into \\(\\mathbb{R}\\) with the following properties for all \\(x, y \\in\\mathbb{R}^n\\) and \\(\\alpha \\in \\mathbb{C}\\).</p> <ul> <li>\u6b63\u6027 | positive</li> </ul> \\[ \\Vert \\vec{x} \\Vert\\geq 0 \\] <ul> <li>\u5b9a\u6027 | definite</li> </ul> \\[ \\Vert \\vec{x}\\Vert = 0 \\Leftrightarrow \\vec{x}=\\vec{0} \\] <ul> <li>\u9f50\u6027 | homogeneous</li> </ul> \\[ \\Vert \\alpha\\vec{x} \\Vert = |\\alpha|\\Vert \\vec{x} \\Vert \\] <ul> <li>\u4e09\u89d2\u4e0d\u7b49\u5f0f | triangle inequality</li> </ul> \\[ \\Vert \\vec{x}+\\vec{y} \\Vert \\leq \\Vert \\vec{x} \\Vert+\\Vert \\vec{y} \\Vert \\] <p>We usually use \\(p\\) norm</p> \\[ \\Vert \\vec{x} \\Vert_p = \\left(\\sum_{i=1}^n|x_i|^p\\right)^{1/p} \\] <p>with its common forms:</p> \\[ \\Vert \\vec{x} \\Vert_1 = \\sum_{i=1}^{n}|x_i|, \\quad \\Vert \\vec{x} \\Vert_2 = \\sqrt{\\sum_{i=1}^{n}|x_i|^2}, \\quad \\Vert \\vec{x} \\Vert_\\infty = \\max_{1\\leq i\\leq n}|x_i| \\] <p>Definition of Norm of Matrixes</p> <p>A matrix norm on the set of all matrices \\(R \\in \\mathbb{R}^{n\\times n}\\) is a real-valued function, denoted as \\(\\Vert \\cdot \\Vert\\), defined on this set, satisfying for all \\(A, B \\in \\mathbb{R}^{n\\times n}\\) and all \\(\\alpha \\in \\mathbb{C}\\):</p> <ul> <li>\u6b63\u6027 | positive</li> </ul> \\[ \\Vert \\mathbfit{A} \\Vert\\geq 0 \\] <ul> <li>\u5b9a\u6027 | definite</li> </ul> \\[ \\Vert \\mathbfit{A} \\Vert = 0 \\Leftrightarrow \\mathbfit{A}=\\mathbfit{0} \\] <ul> <li>\u9f50\u6027 | homogeneous</li> </ul> \\[ \\Vert \\alpha\\mathbfit{A} \\Vert = |\\alpha|\\Vert \\mathbfit{A} \\Vert \\] <ul> <li>\u4e09\u89d2\u4e0d\u7b49\u5f0f | triangle inequality</li> </ul> \\[ \\Vert \\mathbfit{A}+\\mathbfit{B} \\Vert \\leq \\Vert \\mathbfit{A} \\Vert+\\Vert \\mathbfit{B} \\Vert \\] <ul> <li>\u4e00\u81f4\u6027 | consistent</li> </ul> \\[ \\Vert \\mathbfit{A}\\mathbfit{B} \\Vert\\leq \\Vert \\mathbfit{A} \\Vert \\cdot \\Vert \\mathbfit{B} \\Vert \\] <p>Usually we use Natural Norm:</p> \\[ \\Vert \\mathbfit{A} \\Vert = \\max_{\\vec{x}\\neq 0}\\frac{\\Vert \\mathbfit{A}\\vec{x} \\Vert_p}{\\Vert \\vec{x} \\Vert_p} = \\max_{\\Vert \\vec{x} \\Vert_p =1}\\Vert \\mathbfit{A}\\vec{x} \\Vert \\] <p>with its common forms:</p> \\[ \\begin{align*} \\Vert \\mathbfit{A} \\Vert_1 &amp;= \\max_{1\\leq i\\leq n}\\sum_{j=1}^{n}|a_{ij}| \\quad\\text{the maximum of row summation}\\\\ \\Vert \\mathbfit{A} \\Vert_2 &amp;= \\sqrt{\\max \\rho(A^T A)} \\quad\\text{the maximum of spectrum radius}\\\\ \\Vert \\mathbfit{A} \\Vert_\\infty &amp;= \\max_{1\\leq j\\leq n}\\sum_{i=1}^{n}|a_{ij}| \\quad\\text{the maximum of column summation}\\\\ \\end{align*} \\] <p>for \\(p=2\\) norm of matrix, we have special expression for special matrix:</p> <p>Sepecial Expression of Norm 2 of matrix</p> <p>If matrix \\(A\\) is symetrical, then</p> \\[ \\Vert \\mathbfit{A} \\Vert_2 = \\sqrt{\\max \\rho(A)}. \\] <p>If matrix \\(A\\) is orthogonal(only rotate), then</p> \\[ \\Vert \\mathbfit{A} \\Vert_2 = 1. \\]"},{"location":"courses/Numerical_Analysis/MC/#jacobi-jacobis-method","title":"Jacobi\u65b9\u6cd5 | Jacobi's Method  <p>Here we denote \\(L\\) and \\(U\\) to be the lower-triangular and upper-triangular matrix of matrix \\(A\\) without its diagonal elements respectively. (different from \\(LU\\) factorization!) And then we denote \\(D\\) to be the diagonal elements of the matrix of \\(A\\). That is, </p> \\[ A = D - L -U \\] <p>Then</p> \\[  \\begin{align*} A\\pmb{x} &amp;= \\pmb{b} \\\\ (D-L-U)\\pmb{x} &amp;= \\pmb{b} \\\\ D\\pmb{x} &amp;= (L+U)\\pmb{x} + \\pmb{b} \\\\ \\pmb{x} &amp;= D^{-1}(L+U)\\pmb{x} + D^{-1}\\pmb{b}  \\end{align*}  \\] <p>which gives matrix form of the Jacobi iterative technique</p> \\[ \\pmb{x}^{k} = D^{-1}(L+U)\\pmb{x}^{k-1} + D^{-1}\\pmb{b} \\]","text":""},{"location":"courses/Numerical_Analysis/MC/#gauss-seidel-the-gauss-seidel-method","title":"Gauss-Seidel\u65b9\u6cd5 | The Gauss-Seidel Method <p>This method sees that a little slowness in Jacobi's Method. That is, for each itearion period(\\(\\pmb{x}^{k} \\leftarrow \\pmb{x}^{k-1}\\)), it makes use of the generated \\(\\pmb{x}^{k}_{i}\\) in the \\(i\\)th row of \\(\\pmb{x}^{k}\\) and use it to update the coressponding element in \\(\\pmb{x}^{k-1}\\).</p> <p>In matrix form, we have</p> \\[ D\\pmb{x}^{k} = U\\pmb{x}^{k-1} + L\\pmb{x}^{k}+ \\pmb{b} \\] <p>(to be proved by readers)</p> <p>then</p> \\[ \\pmb{x}^{k} = (D-L)^{-1}U\\pmb{x}^{k-1} + (D-L)^{-1}\\pmb{b} \\]","text":""},{"location":"courses/Numerical_Analysis/MC/#approximating-eigenvalues","title":"\u7279\u5f81\u503c\u903c\u8fd1 | Approximating Eigenvalues","text":""},{"location":"courses/Numerical_Analysis/MC/#the-power-method","title":"\u5e42\u6cd5 | The Power Method <p>Assume that \\(A\\) has eigenvalues \\(|\\lambda_1|&gt;|\\lambda_2|\\geq |\\lambda_3|\\geq \\cdots \\geq |\\lambda_n|\\geq 0\\), we can use the following method to make the largest \\(lambda_1\\) stand out.</p>  <p>\u5e42\u6cd5 | The Power Method</p> <p>Initialize randomly \\(\\vec{x}\\), which can be represented by \\(n\\) linearly irrelevant eigenvectors \\(\\vec{v}_1, \\vec{v}_2, \\cdots, \\vec{v}_n\\), with parameters \\(\\beta_1, \\beta_2, \\cdots, \\beta_n\\), such that</p> \\[ \\vec{x} = \\sum_{i=1}^{n}\\beta_i\\vec{v_i} \\] <p>multiply both sides by \\(A\\), according to \\(A\\vec{v_i}=\\lambda_i \\vec{v_i}\\), we get</p> \\[ A\\vec{x} = \\sum_{i=1}^{n}\\beta_i \\lambda_i \\vec{v_i} \\] <p>repeat this process for \\(n\\) times we get </p> \\[ A^n\\vec{x} = \\sum_{i=1}^{n}\\beta_i \\lambda_i^n \\vec{v_i} = \\lambda_1^n\\sum_{i=1}^{n}\\beta_i (\\frac{\\lambda_i}{\\lambda_1})^n \\vec{v_i}\\rightarrow \\lambda_1^n \\beta_1 \\vec{v_1} \\quad (n\\rightarrow \\infty) \\] <p>That is, we can neglect eigenvalues that are smaller than \\(\\lambda_1\\) through multiplying \\(A\\) and \"extract\" the biggest one.</p>  <p>To avoid divengence caused by \\(\\lambda_1&gt;0\\), we need to normalize \\(\\vec{x}^{k} = A\\vec{x}^{k-1}\\) each step after multiplying. Usually we choose \\(\\Vert\\  \\Vert_\\infty\\).</p> <p>To get the \\(\\lambda_1\\) out, we can use </p> \\[ \\frac{\\Vert\\vec{x}^{k}\\Vert}{\\Vert\\vec{x}^{k-1}\\Vert} \\approx \\lambda_1 \\quad (n\\rightarrow \\infty) \\] <p>to get \\(\\lambda_1\\).</p> <p>The next question is, naively, how about the speed of converging? Luckily, the question is easy to answer:</p> <p>rely on ratio \\(\\left|\\frac{\\lambda_2}{\\lambda_1}\\right|\\).</p>","text":""},{"location":"courses/Numerical_Analysis/MC/#inverse-power-method","title":"\u53cd\u5e42\u6cd5 | Inverse Power Method <p>This is a trick from the Power method. It comes from a question: what if we want to calculate the smallest eigenvalue of a matrix \\(A\\)?</p> <p>The answer is, by taking use of metrix inverse.</p>  <p>\u53cd\u5e42\u6cd5 | Inverse Power Method</p> <p>Matrix \\(A\\) has eigenvalues \\(|\\lambda_1| &lt; |\\lambda_2| \\leq \\cdots \\leq |\\lambda_n|\\), then matrix \\(A^{-1}\\) has eigenvalues </p> \\[ \\left|\\frac{1}{\\lambda_1}\\right| &gt; \\left|\\frac{1}{\\lambda_2}\\right| \\geq \\cdots \\geq \\left|\\frac{1}{\\lambda_n}\\right| \\] <p>Then use the power method to get \\(\\left|\\frac{1}{\\lambda_1}\\right|\\) out.</p>  <p>Actually, the above method is more often being used in situation where we have known an eigenvalue \\(\\lambda_1\\) (not neecssarily the largest or smallest) of \\(A\\) is close to a constant \\(q\\). That is, we can formulate matrix </p> \\[ (A-qI) \\] <p>which has eigenvalues </p> \\[ |\\lambda_1 - q| &lt; |\\lambda_2 - q| \\leq \\cdots \\leq \\left|\\lambda_n - q\\right| \\] <p>Then we can use the above method to get \\(\\lambda_1\\) out.</p>","text":""},{"location":"courses/Numerical_Analysis/NA/","title":"Index","text":""},{"location":"courses/Numerical_Analysis/NA/#numerical-approximation","title":"Numerical Approximation","text":"<p>Reference</p> <p>\u6570\u503c\u903c\u8fd1, \u848b\u5c14\u96c4 \u8d75\u98ce\u5149 \u82cf\u4ef0\u5cf0</p>"},{"location":"courses/Numerical_Analysis/NA/#solutions-of-equations-in-one-variables","title":"\u4e00\u5143\u51fd\u6570\u65b9\u7a0b\u6c42\u89e3 | Solutions of Equations in One Variables","text":""},{"location":"courses/Numerical_Analysis/NA/#interpolation-and-polynomial-approximation","title":"\u51fd\u6570\u63d2\u503c\u548c\u591a\u9879\u5f0f\u903c\u8fd1 | Interpolation and Polynomial Approximation","text":""},{"location":"courses/Numerical_Analysis/NA/#approximation-theory","title":"\u6700\u4f73\u903c\u8fd1\u7406\u8bba | Approximation Theory","text":""},{"location":"courses/Numerical_Analysis/NA/#numerical-integration","title":"\u6570\u503c\u79ef\u5206 | Numerical Integration","text":""},{"location":"courses/Numerical_Analysis/NA/Appro/","title":"Approximation Theory","text":""},{"location":"courses/Numerical_Analysis/NA/Appro/#best-square-approximation","title":"\u6700\u4f73\u5e73\u65b9\u903c\u8fd1 | Best Square Approximation","text":""},{"location":"courses/Numerical_Analysis/NA/Appro/#lead-in","title":"\u5f15\u5165 | Lead-in","text":"<p>We call a linear space with norm if there exists a function \\(\\|\\cdot\\|\\) that satisfies 3 properties(check here).</p> <p>So here we introduce</p> \\[ \\Delta(x,Y)=\\inf_{y\\in Y}\\|x-y\\| \\] <p>to be the best square approximation of element \\(x\\), where \\(x\\in X\\) is a linear space with norm 2 and \\(Y\\) is a subspace of \\(X\\).</p> <p>We can also introduce norm with the definition of inner product. In Euclid space, \\((\\cdot,\\cdot)\\) is defined as </p> \\[ (x,y)=x^Ty,\\quad x,y\\in \\mathbb{R}^n \\] <p>It is easy to see that \\(\\|x\\|_2=\\sqrt{(x,x)}\\). So we can give a more specific problem of inner product space. Assume \\(\\varphi_i\\), \\((i=1,2\\cdots,n)\\) are \\(n\\) linearly irrelevant elements in inner product space \\(X\\), choose \\(f\\in X\\), then subset </p> \\[ \\varPhi_n=\\text{span}(\\varphi_1,\\varphi_2,\\cdots,\\varphi_n) \\] <p>has a best approximation of \\(f\\), which is defined as </p> \\[ \\Delta(f,\\varPhi_n)=\\min_{\\varphi\\in \\varPhi_n}\\|f-\\varphi\\|_2 \\] <p>we call the \\(\\varphi\\) that enables the above equation to be the best square appromation element.</p>"},{"location":"courses/Numerical_Analysis/NA/Appro/#properties-of-best-square-appromation-element","title":"\u6700\u4f73\u5e73\u65b9\u903c\u8fd1\u5143 | Properties of Best Square Appromation Element","text":"<p>Sufficient and Necessary Condition for best square approximation element</p> <p>Assume \\(X\\) is an inner product space, \\(f\\in X\\), \\(\\varphi^*\\in \\varPhi_0\\) is the best square approximation element, if and only if</p> \\[ (f-\\varphi^*,\\varphi_i)=0, \\quad, i=1,2\\cdots,n. \\] Proof <p>\"\\(\\Leftarrow\\)\".</p> <p>\"\\(\\Rightarrow\\)\". </p> <p>The above theorem gives a general method to solve the best square approximation element. That is, we define</p> \\[ G=\\left[\\begin{array}{cccc} (\\varphi_1,\\varphi_1) &amp; (\\varphi_1,\\varphi_2) &amp; \\cdots &amp; (\\varphi_1,\\varphi_n)\\\\ (\\varphi_2,\\varphi_1) &amp; (\\varphi_2,\\varphi_2) &amp; \\cdots &amp; (\\varphi_2,\\varphi_n)\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ (\\varphi_n,\\varphi_1) &amp; (\\varphi_n,\\varphi_2) &amp; \\cdots &amp; (\\varphi_n,\\varphi_n)\\\\ \\end{array}\\right], \\quad \\pmb{\\alpha}^*=\\left[\\begin{array}{c} \\alpha_1^*\\\\ \\alpha_2^*\\\\ \\vdots\\\\ \\alpha_n^* \\end{array}\\right], \\quad \\pmb{\\beta}=\\left[\\begin{array}{c} (\\varphi_1,f)\\\\ (\\varphi_2,f)\\\\ \\vdots\\\\ (\\varphi_n,f) \\end{array}\\right] \\] <p>then we can solve \\(G\\pmb{\\alpha}^*=\\pmb{\\beta}\\) for the best square approximation element</p> \\[ \\varphi = \\sum_{i=1}^n\\alpha^*_i \\varphi_i \\] <p>The above theorem also tells us, if \\(\\{\\varphi_i\\}_{i=1}^n\\) are group of linearly irrelevant elements, then \\(G\\) has rank \\(n\\) and the parameters \\(\\{\\alpha_i\\}_{i=1}^n\\) is unique. And it is easy to give the error estimation form</p> \\[ \\begin{align*} \\|f-\\varphi^*\\|_2^2&amp;=(f-\\varphi^*,f-\\varphi^*)\\\\ &amp;=(f-\\varphi^*, f) - (f-\\varphi^*, \\varphi^*) \\\\ &amp;=(f-\\varphi^*, f) = (f,f) - (\\varphi^*,f)\\\\ &amp;=\\|f\\|_2^2-\\sum_{i=1}^n\\alpha^*_i (\\varphi_i, f) \\end{align*} \\] <p>The geometrical meaning is also clear, if we define \\(\\Delta=f-\\varphi^*\\) and then</p> \\[ \\begin{align*} \\|f-\\varphi^*\\|_2^2&amp;=(\\Delta+\\varphi^*,\\Delta+\\varphi^*)\\\\ &amp;=(\\Delta, \\Delta) +2 (\\Delta, \\varphi^*)+(\\varphi^*,\\varphi^*) \\\\ &amp;=(\\Delta, \\Delta) +2 (f-\\varphi^*, \\varphi^*)+(\\varphi^*,\\varphi^*) \\\\ &amp;= (\\Delta, \\Delta) - (\\varphi^*,\\varphi^*) \\quad \\text{(by property of BSPE)}\\\\ \\end{align*} \\] <p>which means \\(\\varphi^*\\) is the orthogonal projection of \\(f\\) on \\(\\varPhi_n\\).</p> <p>But actually it is not so easy to solve the above linear system, so we consider another way in specific situation.</p>"},{"location":"courses/Numerical_Analysis/NA/Appro/#best-square-approximation-on-l_rho2ab","title":"\u6709\u9650\u51fd\u6570\u7a7a\u95f4\u4e0a\u7684\u6700\u4f73\u5e73\u65b9\u903c\u8fd1 | Best Square Approximation on \\(L_\\rho^2[a,b]\\)","text":"<p>Weight Function</p> <p>Assume \\(\\rho(x)\\) is Lebesgue integrable on \\([a,b]\\) and is \\(0\\) on at most only one set of measure zero, then we call \\(\\rho(x)\\) the weight function.</p> <p>If \\(\\rho(x)\\) is the weight function on \\([a,b]\\) then we denote all the measurable function \\(\\rho(x)f^2(x)\\) that is Lebesgue integrable on \\([a,b]\\), by \\(L^2_\\rho[a,b]\\), which is a linear space. The inner product is defined by</p> \\[ (f,g)=\\int_{a}^b\\rho(x)f(x)g(x)dx, \\quad f,g\\in L^2_\\rho[a,b] \\] <p>which satisfies the 4 properties of inner product definition. So we can introduce natural norm </p> \\[ \\|f\\|_2=\\left[\\int_a^b\\rho(x)f^2(x)dx\\right] \\] <p>then \\(L^2_\\rho[a,b]\\) is a linear space with norm, so we can consider the best approximation problem.</p> <p>Because \\(L^2_\\rho[a,b]\\) is a special inner product space \\(X\\),  so the former discussion about the best square approximation element can be directly applied here. That is, if \\(\\{\\varphi_i\\}_{i=1}^n\\) is a group of linearly irrelevant function in \\(L^2_\\rho[a,b]\\), then the subset </p> \\[ \\varPhi_n=\\text{span}\\{\\varphi_1,\\varphi_2, \\cdots, \\varphi_n\\} \\] <p>is call the generalized polynomial space, whose element is called generalized polynomial, or polynomial for short. The best square approximation of \\(f\\) on \\(\\varPhi_n\\) is defined by</p> \\[ \\Delta(f,\\varPhi_n)=\\min_{\\varphi\\in \\Phi}\\|f-\\varphi\\|_2 \\] <p>and so does the best square approximation element \\(\\varphi^*\\) is called the best square approximation polynomial(BSAP).</p> ps <p>There is another way to get the BSAP.</p> <p>If we choose </p> \\[ E = (P-y,P-y)=\\|P-y\\|^2 \\] <p>we can easily solve the approximation polynomial \\(\\sum\\limits_{i=0}^\\infty a_ix^i\\) by letting \\(\\frac{\\partial E}{\\partial a_k}=0\\), and get</p> \\[ \\sum_{j=0}^n(\\varphi_k, \\varphi_j)a_j=(\\varphi_k,f),\\quad k=0,1,\\cdots,n \\]"},{"location":"courses/Numerical_Analysis/NA/Appro/#orthogonal-polynomials","title":"\u6b63\u4ea4\u591a\u9879\u5f0f | Orthogonal Polynomials","text":"<p>If we limit the base function of the subspace, we can simplify the matrix \\(G\\) and simplify the solving process. Actually, we are narrawing down the condition number of \\(G\\) by using orthogonal polynomials. Readers can use exactly the same statement from abstrat inner product space and its corresponding statement of orthogonal elements.</p> <p>Orthogonal Polynomials</p> <p>If \\(\\{\\varphi_i\\}_{i=1}^n \\subset L^2_\\rho[a,b]\\) satisfy</p> \\[ (\\varphi_i,\\varphi_j)=\\int_a^b\\rho(x)\\varphi_i(x)\\varphi_j(x)dx=\\begin{cases} 0, \\quad &amp;i=j,\\\\ \\sigma, \\quad &amp; i\\neq j. \\end{cases} \\] <p>where \\(\\sigma\\) is a non-zero number, then we call \\(\\{\\varphi_i\\}_{i=1}^n\\) are orthogonal on \\([a,b]\\) according to weight \\(\\rho(x)\\). Furthermore, if we limit </p> \\[ \\int_a^b\\rho(x)\\varphi_i^2(x)dx=1 \\] <p>then we call \\(\\{\\varphi_i\\}_{i=1}^n\\) are normalized orthogonal system.</p> <p>So from above definition we can get BSAP more easily with orthogonal functions \\(\\{\\varphi_i\\}_{i=1}^n\\)</p> \\[ \\varphi^*=\\sum_{i=1}^n\\frac{(\\varphi_i,f)}{(\\varphi_i,\\varphi_i)}\\varphi_i(x) \\] <p>and its corresponding error</p> \\[ \\|f-\\varphi^*\\|^2_2=\\|f\\|^2_2-\\sum_{i=0}^n\\frac{(\\varphi_i,f)^2}{(\\varphi_i,\\varphi_i)} \\] <p>Q1. Use \\(y=2^{ax+b}\\) to approximate the following 3 points.</p> \\(x_i\\) 0 1 4 \\(f(x_i)\\) 1 2 8 \\(w\\) 1 1 1"},{"location":"courses/Numerical_Analysis/NA/Appro/#discrete-least-squares-approximation","title":"\u79bb\u6563\u6700\u5c0f\u4e8c\u4e58\u6cd5 | Discrete Least Squares Approximation","text":"<p>This problem can be discribed as the best square approximation on Euclid space. That is, if \\(y\\in \\mathbb{R}^n\\), \\(\\{\\pmb{x}_i\\}_{i=1}^m\\subset \\mathbb{R}^n\\) is a group of linearly irrelevant vectors, then </p> \\[ V=\\text{span}\\{\\pmb{x}_1,\\pmb{x}_2,\\cdots,\\pmb{x}_m\\} \\] <p>is a subspace of \\(\\mathbb{R}^n\\). </p> <p>So we can consider the best square approximation problem</p> \\[ \\Delta(\\pmb{y},V)=\\min_{\\pmb{x}\\in V}\\|\\pmb{y}-\\pmb{x}\\|_2 \\] <p>where \\(\\|\\cdot\\|_2\\) is the Euclid norm, i.e. \\(\\|\\pmb{x}\\|_2=\\pmb{x}^T\\pmb{x}\\), \\(x\\in \\mathbb{R}^n\\)</p> <p>That is, we have to solve </p> \\[ \\left[\\begin{array}{cccc} (\\pmb{x}_1,\\pmb{x}_1) &amp; (\\pmb{x}_1,\\pmb{x}_2) &amp; \\cdots &amp; (\\pmb{x}_1,\\pmb{x}_n)\\\\ (\\pmb{x}_2,\\pmb{x}_1) &amp; (\\pmb{x}_2,\\pmb{x}_2) &amp; \\cdots &amp; (\\pmb{x}_2,\\pmb{x}_n)\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ (\\pmb{x}_n,\\pmb{x}_1) &amp; (\\pmb{x}_n,\\pmb{x}_2) &amp; \\cdots &amp; (\\pmb{x}_n,\\pmb{x}_n)\\\\ \\end{array}\\right] \\left[\\begin{array}{c} \\alpha_1^*\\\\ \\alpha_2^*\\\\ \\vdots\\\\ \\alpha_n^* \\end{array}\\right] = \\left[\\begin{array}{c} (\\pmb{x}_1,\\pmb{y})\\\\ (\\pmb{x}_2,\\pmb{y})\\\\ \\vdots\\\\ (\\pmb{x}_n,\\pmb{y}) \\end{array}\\right] \\] <p>If we denote \\(\\pmb{x}_i=(x_{1i},x_{2i},\\cdots,x_{ni})^T\\) and </p> \\[ A=[\\pmb{x}_1,\\pmb{x}_2,\\cdots,\\pmb{x}_m]=(x_{ij})_{n\\times m}=\\left[\\begin{array}{cccc} x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1m}\\\\ x_{21} &amp; x_{22} &amp; \\cdots &amp; x_{2m}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{n1} &amp; x_{n2} &amp; \\cdots &amp; x_{nm}\\\\ \\end{array}\\right] \\] <p>then it is not so hard to detect that the above equation can be rewritten as</p> \\[ A^TA\\pmb{\\alpha^*}=A^T\\pmb{y} \\] <p>Cause \\(A^TA\\) has full rank, we can get \\(\\pmb{\\alpha^*} =(A^TA)^{-1}A^T\\pmb{y}\\).</p>"},{"location":"courses/Numerical_Analysis/NA/Appro/#best-uniform-approximation","title":"\u6700\u4f73\u4e00\u81f4\u903c\u8fd1 | Best Uniform Approximation","text":""},{"location":"courses/Numerical_Analysis/NA/Appro/#lead-in_1","title":"\u5f15\u5165 | Lead-in","text":"<p>All the continuous function defined on \\([a,b]\\) compose a infinite-dimensional linear space, denoted by \\(C_{[a,b]}\\). To simplify our discription, we introduce norm \\(\\|\\cdot\\|_\\infty\\) to denote Chebyshev Norm</p> \\[ \\|\\cdot\\|_\\infty = \\max_{[a,b]}|f(x)|, \\quad \\forall f\\in C_{[a,b]}. \\] <p>With the following theorem explored by Weierstrass, we can find an algebraic polynomial to sufficiently approximate function \\(f\\) \\(\\in C_{[a,b]}\\).</p> <p>Weierstrass First Approximation Theorem</p> <p>For all given function \\(f(x)\\in C_{[a,b]}\\), \\(\\forall \\varepsilon&gt;0\\), \\(\\exists p(x)\\) which is an algebraic function, s.t.</p> \\[ \\|f(x)-p(x)\\|&lt; \\varepsilon \\] Hints <p>Prove it by Bernstein Polynomials.</p> <p>But what we care more about is whether we can find a polynomial of degree no more than \\(n\\) to approximate function \\(f(x)\\)?</p> <p>The answer to the above question results in Chebyshev Approximation.</p> <p>Consider set</p> \\[ P_n(x)=\\text{span}\\{1,x,\\cdots, x^n\\} \\] <p>It is not hard to see that \\(P_n\\) is a subspace of \\(C_{[a,b]}\\) with \\(n+1\\) dimension. And the goal is to get</p> \\[ \\Delta(f,P_n)=\\min_{p\\in P_n}\\|f(x)-p(x)\\|_\\infty \\] <p>which is called the best uniform approximation of \\(f(x)\\).</p>"},{"location":"courses/Numerical_Analysis/NA/Appro/#characteristics-of-best-uniform-approximation","title":"\u6700\u4f73\u4e00\u81f4\u903c\u8fd1\u7684\u7279\u5f81 | Characteristics of Best Uniform Approximation","text":"<p>We can represent the best uniform approximation with the introduction of deviation points.</p> <p>Deviation Point Set</p> <p>we define</p> \\[ \\begin{cases} E^+(f)=\\{x\\in [a,b]: f(x)=\\|f\\|_\\infty\\} \\\\ E^-(f)=\\{x\\in [a,b]: f(x)=-\\|f\\|_\\infty\\} \\end{cases} \\] <p>to be positive and negative deviation point set, and define \\(E(f)=E^+(f)\\cup E^-(f)\\) to be deviation point set.</p> <p>Recall that \\(f(x)\\in C_{[a,b]}\\), so \\(E^+(f)\\) and \\(E^-(f)\\) are both bounded closed set. </p> <p>Alternating Point Set</p> <p>If set \\(\\{x_1,x_2,\\cdots,x_k\\}\\) \\(\\subset E(f)\\) satisfies</p> \\[ \\begin{cases} a\\leq x_1&lt;x_2&lt;\\cdots &lt;x_k\\leq b  \\\\ f(x_j)=-f(x_{j+1}), \\quad j=1,2\\cdots,k-1 \\end{cases} \\] <p>then we call the above set Alternating Point Set. We call them them maximal alternating point set, if there does not exist an alternating point set of size larger than \\(k\\).</p> <p>We can construct maximal alternating point set by the following way.</p> <p>Construction of Maximal Alternating Point Set</p> <p>Let \\(S_1=\\inf E^+(f)\\cup E^-(f)\\), construct \\(x_k\\) recursively</p> \\[ x_k=\\inf S_k \\] <p>where</p> \\[ S_{k+1}=\\begin{cases} E^-(f)\\cap [x_k,b],\\quad x_k\\in E^+(f)\\\\ E^+(f)\\cap [x_k,b],\\quad x_k\\in E^-(f) \\end{cases} ,\\quad k=1,2 \\cdots \\] <p>We assure \\(x_k\\in S_k\\) when we let \\(x_k=\\inf S_k\\) because \\(S_k\\) is bounded closed set. The recursion assures \\(x_{k+1}\\) is the minimal deviation point with opposite sign to \\(x_k\\), which of course assures \\(\\{x_i\\}\\) is monotonically increasing. We can prove that, if \\(f\\neq 0\\), the above constructed set is maximal and finite(to be proved by readers).</p> <p>Property of best uniform approximation on \\(C_{[a,b]}\\)</p> <p>Assume \\(f\\in C_{[a,b]}\\) and \\(f\\notin P_n\\). If \\(p(x)\\) is the best uniform approximation polynomial of \\(f(x)\\) on \\([a,b]\\), then \\(f-p\\) has at least \\(n+2\\) alternating points.</p> Proof <p>By contradiction.</p> <p>Vall\u00e9e-Poisson Theorem</p> <p>Assume \\(f\\in C_{[a,b]}\\), if there exists polynomial \\(p\\in P_n\\), such that \\(f-p\\) has at least \\(n+2\\) points \\(x_1,x_2,\\cdots,x_{n+2} \\in [a,b]\\) alternated with positive and negative, then </p> \\[ \\Delta(f,P_n)\\geq \\mu=\\min_{1\\leq i\\leq n+2}|f(x_i)-g(x_i)| \\] Proof <p>By contradiction.</p> <p></p> <p>Chebyshev Theorem</p> <p>For all \\(f\\in C_{[a,b]}\\), \\(f\\notin P_n\\), \\(p\\in P_n\\) is the best uniform approximation of \\(f\\) if and only if, \\(f-p\\) has at least \\(n+2\\) alternating points on \\([a,b]\\).</p> Proof <p>By Sequence Theorem.</p> <p>Uniqueness of best uniform approximation</p> <p>If \\(f\\in C_{[a,b]}\\), then there exists a unique polynomial \\(p\\in P_n\\) that is the best uniform approximation of \\(f\\).</p> HintsProof <p>By prove two best uniform approximation polynomials \\(p_1,p_2\\) are the same.</p> <p>Prove Uniqueness.</p> <p>Assume \\(p_1,p_2\\in P_n\\) are both the best uniform approximation polynomials of \\(f\\),i.e. </p> \\[   \\|f-p_1\\|_\\infty = \\|f-p_2\\|_\\infty = \\min_{p\\in P_n}\\|f-p\\|_\\infty = \\Delta(f,P_n) \\] <p>then define \\(p_0=(p_1+p_2)/2\\), then</p> \\[ \\|f-p_0\\|_\\infty\\leq \\frac{1}{2}(\\|f-p_1\\|_\\infty+ \\|f-p_2\\|_\\infty) =\\Delta(f,P_n) \\] <p>which means \\(p_0\\) is also the best uniform approximation polynomial of \\(f\\). By Chebyshev Theorem, there exists \\(n+2\\) alternating points \\(x_0,x_1,\\cdots,x_{n+1}\\).</p> <p>Note that </p> \\[ \\Delta(f,P_n)=|f(x_k)-p_0(x_k)|\\leq \\frac{1}{2}(|f(x_k)-p_1(x_k)|+|f(x_k)-p_2(x_k)|)\\leq \\Delta(f,P_n),\\quad k=0,1,\\cdots,n+1 \\] <p>which means \"=\" must holds, i.e.</p> \\[ |f(x_k)-p_1(x_k)| = |f(x_k)-p_2(x_k)| = \\Delta(f,P_n),\\quad k=0,1,\\cdots,n+1 \\] <p>and </p> \\[ f(x_k)-p_1(x_k) = f(x_k)-p_2(x_k),\\quad k=0,1,\\cdots,n+1 \\quad \\text{(by Triangular inequation)} \\] <p>which means \\(p_1(x_k)=p_2(x_k)\\), that is, \\(p_1-p_2\\) has \\(n+2\\) roots, so \\(p_1=p_2\\).</p>"},{"location":"courses/Numerical_Analysis/NA/Appro/#chebyshev-first-class-chebyshev-polynomials","title":"\u7b2c\u4e00\u7c7bChebyshev\u591a\u9879\u5f0f | First Class Chebyshev Polynomials","text":"<p>Coonsider \\(n+1\\) extreme points of \\(cosn\\theta\\) on \\([0,\\pi]\\). If \\(p(x)\\) is the polynomial of best uniform  approximation of \\(f\\) on \\([a,b]\\), then \\(f-p\\) has at least \\(n+2\\) alternating points.</p> <p>We define \\(x = \\cos\\theta\\) and </p> \\[ \\begin{align*} T_n(x)&amp;=\\cos (n\\cos^{-1}x)\\\\ &amp;=\\cos(n \\theta)\\\\ &amp;=\\sum\\limits_{k=1}^n a_k(\\cos\\theta)^i\\\\ &amp;=\\sum\\limits_{k=1}^n a_k x^i \\end{align*} \\] <p>is a polynomial of degree \\(n\\), i.e. \\(T_n(x)\\in P_n\\). It is easy to see the resursive definition</p> \\[ \\begin{cases} T_0(x)=1, T_1(x)=x,\\\\  \\displaystyle T_{n+1}(x)=2xT_n(x)-T_{n-1}(x),\\quad n=1,2,\\cdots,n-1 \\end{cases} \\] <p>By definition, we have properties for first class Chebyshev Polynomials.</p> <p>Properties of First Class Chebyshev Polynomials</p> <p>(1) Prove the recursive form of definition.</p> <p>(2) the coefficient of item with the highest degree of \\(T_n(x)\\) is \\(1/2^{n-1}\\).</p> <p>(3) \\(|T_n(x)|\\leq 1\\), \\(\\forall |x|\\leq 1\\).</p> <p>(4) \\(T_n(x)\\) has \\(n\\) different real roots </p> \\[ \\cos\\left[\\frac{(2k-1)\\pi}{2n}\\right],\\quad k=1,2,\\cdots,n \\] <p>(5) \\(\\{\\cos(k\\pi/n)\\), \\(k=0,1,\\cdots,n\\}\\) are a maximal alternating point set of \\(T_n(x)\\) on \\([-1,1]\\).</p> <p>(6) \\(T_n(x)=(-1)^nT_n(-x)\\).</p> <p>(7)</p> \\[ \\int_{-1}^1\\frac{T_m(x)T_n(x)}{\\sqrt{1-x^2}}dx=\\begin{cases}\\pi, \\quad &amp;m=n=0;\\\\ \\pi/2, \\quad &amp;m=n\\neq 0;\\\\ 0,\\quad &amp;m\\neq n.\\end{cases} \\] HintsProof <p>(1) by combination property of triangular functions.</p> <p>(7) by the orthogonal property of triangular functions.</p> <p>With the above property, we can see the following theorem.</p> <p>\\(T_n(x)\\) is the is the best uniform approximation of 0</p> <p>\\(T_n(x)\\) is the best uniform approximation of function \\(f\\equiv 0\\), that is, \\(\\forall\\) Monicpolynomial \\(p\\in P_n\\)</p> \\[ \\|p\\|\\geq |T_n|/2^{n-1} = 2^{1-n} \\] <p>Use \\(n\\) Polynomial \\(P_n(x)\\) to approximate function \\(f\\) on region \\([-1,1]\\), its remainder </p> \\[ \\begin{align*} |P_n(x)-f(x)|=|R_n(x)|&amp;=\\left|\\frac{f^{(n+1)}(\\xi)}{(n+1)!}\\prod_{i=0}^n(x-x_i)\\right|\\\\ &amp;\\leq \\max\\limits_{x\\in [-1,1]}\\left|f^{(n+1)}(x)\\right|\\frac{1}{(n+1)!} \\left| \\prod_{i=0}^n(x-x_i)\\right|\\\\ &amp;\\leq \\max\\limits_{x\\in [-1,1]}\\left|f^{(n+1)}(x)\\right|\\frac{1}{(n+1)!}\\left| \\frac{T_{n+1}(x)}{2^{n}} \\right|\\\\ &amp;\\leq \\frac{1}{(n+1)!} \\frac{1}{2^{n}}\\max\\limits_{x\\in [-1,1]}\\left|f^{(n+1)}(x)\\right|  \\end{align*} \\] <ul> <li>Minimizing Approximation Error on Arbitrary Intervals</li> </ul> <p>The technique for choosing points to minimize the interpolating error is extended to a general closed interval \\([a, b]\\) by using the change of variables</p> \\[ \\tilde{x} = \\frac{1}{2}[(b-a)x+a+b] \\] <p>Q. Find the best approximating polynomial of  \\(f (x) = e^x\\) on \\([0, 1]\\) such that the absolute error is no larger than \\(0.5\\times 10^4\\).</p> Answer <p>\\(a=0\\), \\(b=1\\), so </p> \\[ x=\\frac{a+b}{2}+\\frac{b-a}{2}t = \\frac{1}{2}(t+1), \\quad t\\in [-1,1] \\] <p>So the actual function to be approximated is </p> \\[ g(t) = f\\left[\\frac{1}{2}(t+1)\\right] = e^{\\frac{1}{2}(t+1)}, \\quad t\\in [-1,1] \\] \\[ \\max\\limits_{x\\in [-1,1]}\\left|g^{(n+1)}(x)\\right| = \\max\\limits_{x\\in [-1,1]}\\left|\\frac{1}{2^{n+1}}e^{\\frac{1}{2}(t+1)}\\right|=\\frac{e}{2^{n+1}} \\] <p>So</p> \\[ \\begin{align*} |P_n(x)-f(x)|&amp;\\leq \\frac{1}{(n+1)!} \\frac{1}{2^{n}}\\max\\limits_{x\\in [-1,1]}\\left|g^{(n+1)}(x)\\right| \\\\ &amp;= \\frac{1}{(n+1)!} \\frac{1}{2^{n}}\\frac{e}{2^{n+1}} \\end{align*} \\]"},{"location":"courses/Numerical_Analysis/NA/Appro/#economization-of-power-series","title":"\u5e42\u7ea7\u6570\u7684\u964d\u7ef4 | Economization of Power series","text":"<p>This part is also called Reducing the Degree of Approximating Polynomials.</p> <p>Consider approximating an arbitrary \\(n\\)th-degree polynomial</p> \\[ P_n(x) = a_nx^n+a_{n-1}x^{n-1}+\\cdots+a_1x+a_0, \\quad x\\in [\u22121, 1] \\] <p>with a polynomial of degree at most \\(n \u2212 1\\).</p> <p>To let \\(\\max\\limits_{x\\in [-1,1]}|P_n(x)-P_{n-1}(x)|\\) to be mininal, equals to let it be</p> \\[ a_n\\tilde{T}_n(x) = a_n T_n(x)/2^{n-1} \\] <p></p>"},{"location":"courses/Numerical_Analysis/NA/Appro/#appendix-orthogonal-polynomials-on-l2_rhoab","title":"\u9644\u5f55: \u6709\u9650\u533a\u95f4\u4e0a\u7684\u6b63\u4ea4\u591a\u9879\u5f0f | Appendix: Orthogonal polynomials on \\(L^2_\\rho[a,b]\\)","text":""},{"location":"courses/Numerical_Analysis/NA/Appro/#properties","title":"\u6027\u8d28 | Properties","text":"<p>Properties</p> <p>If \\(\\omega_0(x), \\omega_1(x),\\cdots\\) are orthogonal polynomials on space \\(L^2_\\rho[a,b]\\) by orthogonalizing power series, then is must follow</p> <p>(i) \\(\\omega_n(x)\\) is a \\(n\\)th algebraic polynomial.</p> <p>(ii) \\(\\forall p \\in P_k\\), \\(k\\leq n\\), \\(p\\) can be represented as </p> \\[ p=\\sum_{i=0}^na_i\\omega_i(x) \\] <p>(iii) \\(\\omega_n(x)\\) is orthogonal to all polynomials whose degree is less than \\(n\\), that is, </p> \\[ \\int_{a}^b\\rho(x)\\omega_n(x)p_{n-1}(x)dx=0 \\]"},{"location":"courses/Numerical_Analysis/NA/Appro/#construction-of-monic-orthogonal-polynomials","title":"\u9996\u4e00\u6b63\u4ea4\u591a\u9879\u5f0f | Construction of Monic Orthogonal Polynomials","text":"<p>Construction of Monic Orthogonal Polynomials</p> <p>Assume \\(\\{\\overline{\\omega}_i(x)\\}_{i=0}^\\infty\\) are Monic Orthogonal Polynomials, then they satisfy the following recurrence relation</p> \\[ \\overline{\\omega}_{n+1}(x)=(x-B_n)\\overline{\\omega}_n(x)-C_n\\overline{\\omega}_{n-1}, \\quad n=1,2,\\cdots \\] <p>where </p> \\[ \\begin{align*} B_n &amp;= \\frac{(x\\overline{\\omega}_{n},\\overline{\\omega}_{n})}{(\\overline{\\omega}_{n},\\overline{\\omega}_{n})} \\\\ C_n&amp;=\\frac{( \\overline{\\omega}_{n}, \\overline{\\omega}_{n})}{(\\overline{\\omega}_{n-1},\\overline{\\omega}_{n-1})} \\end{align*} \\] HintsProof <p>By using the property of orthogonal polynomials.</p> <p>To simplify the notation, we temporarily use \\(\\omega_n(x)\\) to replace \\(\\overline{\\omega}_n(x)\\).</p> <p>We focus on \\(x\\omega_n(x)\\), which is a \\(n+1\\)th polynomial, so it can be represented by \\(\\omega_0,\\omega_1,\\cdots,\\omega_n\\), i.e.</p> \\[ \\begin{equation} x\\omega_n(x)=\\omega_{n+1}(x)+\\sum_{i=0}^nc_i\\omega_i(x) \\label{eq1} \\end{equation} \\] <p>where \\(c_i\\) are parameters to be determined.</p> <p>Now we notice that \\((\\omega_n,\\omega_s)=0\\), \\(s\\leq n-1\\), so we first employ inner product on both sides of \\(\\ref{eq1}\\) with \\(\\omega_s\\) (\\(s=0,1,\\cdots,n-2\\))</p> \\[ \\begin{equation} (x\\omega_n, \\omega_s)=(\\omega_{n+1},\\omega_s)+\\sum_{i=1}^nc_i(\\omega_i,\\omega_s)\\label{eq2} \\end{equation} \\] <p>Because we have an exact meaning of inner product, that is, integral form, so we have \\((x\\omega_n, \\omega_s)=(\\omega_n, x\\omega_s)\\), then</p> <p>for \\(s=0,1,\\cdots,n-2\\), we get </p> \\[ (x\\omega_n, \\omega_s)=0,\\quad (\\omega_{n+1},\\omega_s)=0 \\] <p>and </p> \\[ (\\omega_i,\\omega_s)=0,\\quad i\\geq s+1 \\text{ or }i\\leq s-1 \\] <p>So equation \\(\\ref{eq2}\\) becomes </p> \\[ 0=0+c_s(\\omega_s,\\omega_s) \\] <p>which means \\(c_s=0\\), \\(s=0,1,\\cdots n-2\\). Then we rewrite equation \\(\\ref{eq1}\\)</p> \\[ \\begin{equation} x\\omega_n(x)=\\omega_{n+1}(x)+c_n\\omega_n(x)+c_{n-1}\\omega_{n-1}(x) \\label{eq3} \\end{equation} \\] <p>In a similar way, we try employing inner product on both sides of the above equation \\(\\ref{eq3}\\) with \\(\\omega_{n-1}\\)</p> \\[ (x\\omega_n, \\omega_{n-1})=0+c_{n-1}(\\omega_{n-1},\\omega_{n-1}) \\] <p>which gives \\(c_{n-1}=(x\\omega_n, \\omega_{n-1})/(\\omega_{n-1},\\omega_{n-1})\\). Notice </p> \\[ (x\\omega_n, \\omega_{n-1})=(\\omega_n, x\\omega_{n-1})=(\\omega_n, \\omega_{n}) \\text{(by representing } x\\omega_{n-1} \\text{ again)} \\] <p>so \\(c_{n-1}=(\\omega_n, \\omega_{n})/(\\omega_{n-1},\\omega_{n-1})\\).</p> <p>In a similar way, employing inner product on both sides of the above equation \\(\\ref{eq3}\\) with \\(\\omega_{n}\\)</p> \\[ (x\\omega_n, \\omega_{n})=0+c_{n}(\\omega_{n},\\omega_{n}) \\] <p>which gives \\(c_{n}=(x\\omega_n, \\omega_{n})/(\\omega_{n},\\omega_{n})\\).</p> <p>Rewrite equation \\(\\ref{eq3}\\) and we prove the theorem.</p>"},{"location":"courses/Numerical_Analysis/NA/Appro/#roots-of-orthogonal-polynomials","title":"\u96f6\u70b9\u5206\u5e03 | Roots of Orthogonal Polynomials","text":"<p>Roots of Orthogonal Polynomials</p> <p>\\(n\\)th Orthogonal Polynomial \\(\\omega_n(x)\\) has \\(n\\) distinct roots on \\([a,b]\\).</p> Hints <p>By contradiction. First show that \\(\\omega_n(x)\\) must have root, and then show it is not multiple root. Finally show the number of roots must be equal to \\(n\\). All the proof can be done by making use of properties.</p>"},{"location":"courses/Numerical_Analysis/NA/Appro/#common-orthogonal-polynomials","title":"\u5e38\u89c1\u7684\u6b63\u4ea4\u591a\u9879\u5f0f | Common Orthogonal Polynomials","text":"<ul> <li> <p>Legendre Polynomial (on \\(L^2[-1,1]\\))</p> </li> <li> <p>First class Chebyshev Polynomial (on \\(L^2_\\rho[-1,1]\\) with \\(\\rho=1/\\sqrt{1-x^2}\\))</p> </li> <li> <p>Second class Chebyshev Polynomial (on \\(L^2_\\rho[-1,1]\\) with \\(\\rho=\\sqrt{1-x^2}\\))</p> </li> <li> <p>Laguerre Polynomial (on \\(L^2(0,\\infty)\\) with \\(\\rho=e^{-x}\\))</p> </li> <li> <p>Hermite Polynomial (on \\(L^2(\\infty,\\infty)\\) with \\(\\rho=e^{x^2}\\))</p> </li> </ul>"},{"location":"courses/Numerical_Analysis/NA/IntPo_Poly/","title":"Interpolation & Polynomial Approximation","text":""},{"location":"courses/Numerical_Analysis/NA/IntPo_Poly/#interpolating-polynomial","title":"\u63d2\u503c\u591a\u9879\u5f0f | Interpolating Polynomial","text":""},{"location":"courses/Numerical_Analysis/NA/IntPo_Poly/#lagrange-lagrange-interpolating-polynomial","title":"Lagrange\u63d2\u503c\u591a\u9879\u5f0f | Lagrange Interpolating Polynomial","text":"<p>Inspired by \u52a0\u6743\u5e73\u5747.</p> <p>There exists and only exists a \\(n\\)th Lagrange interpolating polynomial (\u62c9\u683c\u6717\u65e5\u57fa\u51fd\u6570) </p> \\[  L_n(x) = \\sum_{i=0}^{n}l_i(x)y_i = \\begin{bmatrix} l_0(x) &amp; l_1(x) &amp;l_2(x) &amp; \\cdots &amp; l_n(x)  \\end{bmatrix} \\begin{bmatrix} y_0 \\\\ y_1 \\\\ \\vdots \\\\ y_n  \\end{bmatrix} = \\Phi_n(x)\\vec{y}  \\] <p>such that for each pair of given points \\((x_i, y_i)\\), \\(i = 0, 1,2,\\cdots n\\), we have \\(y_i = L_n(x_i)\\).</p> <p>Here we can consider \\(l_i(x)\\) as a base of a linear space \\(\\mathcal{P}_n(x)\\), and it can be displayed by natural base \\(1, x, x^2, \\cdots x^n\\). To be more specific,</p> \\[ l_i(x) = \\prod_{j=0 \\atop j \\neq i }^{n}\\frac{(x - x_i)}{(x_i-x_j)} \\quad i=0,1,\\cdots n  \\] <p>readers can prove the above \\(n+1\\) polynomials are linearly irrelevant.</p> <p>Another form of \\(l_i(x)\\)</p> <p>If we denote \\(\\omega(x)=\\prod\\limits_{k=0}^{n}(x-x_k)\\), so the numerator of \\(l_i(x)\\) is </p> \\[ \\frac{\\omega(x)}{x-x_i} \\] <p>and its demunerator is </p> \\[ \\omega'(x)|_{x=x_i}=\\sum_{j=0}^{n}\\prod_{k=0 \\atop k\\neq j}^{n}(x-x_k)|_{x=x_i}=\\prod_{k=0\\atop k\\neq i}^{n}(x_i-x_k) \\] <p>so </p> \\[ l_i(x)=\\frac{\\omega(x)}{(x-x_i)\\omega'(x_i)} \\] <p>which satisfies </p> \\[ l_i(x_j)=\\begin{cases}1,\\quad &amp;j=i,\\\\ 0,\\quad &amp;j\\neq i.\\end{cases} \\] <p></p> <p>Q1. Calculate the Lagrange polynomial that interpolates the following 3 points.</p> \\(x_i\\) 1 2 4 \\(f(x_i)\\) 8 1 5 Answer \\[ \\begin{align*} p_2(x) &amp;= \\frac{(x-2)(x-4)}{(1-2)(1-4)}\\times 8 + \\frac{(x-1)(x-4)}{(2-1)(2-4)}\\times 1 + \\frac{(x-1)(x-2)}{(4-1)(4-2)}\\times 5\\\\ &amp;=\\frac{8}{3}(x^2-6x+8)-\\frac{1}{2}(x^2-5x+4)+\\frac{5}{6}(x^2-3x+2)\\\\ &amp;=3x^2-16x+21 \\end{align*} \\] <p>In fact, if we assume \\(P_n(x) = \\sum\\limits_{i=0}^{n}a_ix^i\\)(natural base), and to get the parameters \\(\\{a_i\\}\\) such that \\(P_n(x_i) = y_i\\), we have to solve the following linear system</p> \\[ \\begin{bmatrix} 1 &amp; x_0 &amp; x_0^2 &amp;\\cdots &amp; x_0^n \\\\  1 &amp; x_1 &amp; x_1^2 &amp;\\cdots &amp; x_1^n \\\\  \\vdots &amp; \\vdots &amp; \\vdots &amp; &amp;\\vdots \\\\ 1 &amp; x_n &amp; x_n^2 &amp;\\cdots &amp; x_n^n  \\end{bmatrix} \\begin{bmatrix}  a_0 \\\\ a_1 \\\\ \\vdots \\\\ a_n  \\end{bmatrix}=  \\begin{bmatrix} y_0 \\\\ y_1 \\\\ \\vdots \\\\ y_n  \\end{bmatrix}  \\] <p>which is a little tedious.</p>"},{"location":"courses/Numerical_Analysis/NA/IntPo_Poly/#neville-nevilles-method","title":"Neville\u65b9\u6cd5 | Neville's Method","text":"<p>There is another way to express polynomial, which is also a weighted average.</p> <p>An interpolation polynomial for a set of points \\(A =\\{x_0, x_1, \\cdots x_n \\}\\) can be expressed by two polynomials that interpolate \\(A\\) \\ \\(\\{ x_i\\}\\) and \\(A\\) \\ \\(\\{ x_j\\}\\). That is,</p> \\[ \\begin{align*} P_{0,1,\\cdots,n}(x) &amp;= \\frac{(x-x_i)P_{0,1,\\cdots,i-1,i+1,\\cdots,n}(x)-(x-x_j)P_{0,1,\\cdots,j-1,j+1,\\cdots,n}(x)}{(x_j-x_i)}\\\\ &amp;=\\frac{1}{(x_j-x_i)}\\left|\\begin{array}{cc} (x-x_i) &amp; P_{0,1,\\cdots,j-1,j+1,\\cdots,n}(x) \\\\ (x-x_j) &amp; P_{0,1,\\cdots,i-1,i+1,\\cdots,n}(x) \\end{array}\\right| \\end{align*} \\] <p>where \\(P_{0,1,\\cdots,i-1,i+1,\\cdots,n}(x)\\) and \\(P_{0,1,\\cdots,j-1,j+1,\\cdots,n}(x)\\) denotes the polynomial that interpolates \\(A\\) \\ \\(\\{ x_i\\}\\) and \\(A\\) \\ \\(\\{ x_j\\}\\) respectively.</p> <p>For the first item, we have</p> \\[ \\begin{align*} P_{0,1}(x) &amp;= \\frac{(x-x_0)\\times f(x_1)-(x-x_1)\\times f(x_0)}{(x_1-x_0)}\\\\ &amp;=\\frac{1}{(x_1-x_0)}\\left|\\begin{array}{cc} (x-x_0) &amp; f(x_0) \\\\ (x-x_1) &amp; f(x_1) \\end{array}\\right| \\end{align*} \\] <p>Q2. Get the polynomial \\(p_3(x)\\) that interpolates the following 4 points and estimate \\(f(5)\\) by calculate \\(p_3(5)\\).</p> \\(x_i\\) 2 4 6 8 \\(f(x_i)\\) -8 0 8 64 Answer <p>It is a little tedious if we write the form of the polynomial and then substitute \\(5\\) in. It is more suitable to list a table.</p> <p> \\(x\\) \\(f(x)\\) \\(p_1(5)\\) \\(p_2(5)\\) \\(p_3(5)\\) 2 -8 4 0 \\(\\frac{1}{(4-2)}\\left|\\begin{array}{cc}     (5-2) &amp; -8 \\\\     (5-4) &amp; 0     \\end{array}\\right|=4\\) 6 8 \\(\\frac{1}{(6-4)}\\left|\\begin{array}{cc}     (5-4) &amp; 0 \\\\     (5-6) &amp; 8     \\end{array}\\right|=4\\) \\(\\frac{1}{(6-2)}\\left|\\begin{array}{cc}     (5-2) &amp; 4 \\\\     (5-6) &amp; 4     \\end{array}\\right|=4\\) 8 64 \\(\\frac{1}{(8-6)}\\left|\\begin{array}{cc}     (5-6) &amp; -8 \\\\     (5-8) &amp; 64     \\end{array}\\right|=-20\\) \\(\\frac{1}{(8-4)}\\left|\\begin{array}{cc}     (5-4) &amp; 4 \\\\     (5-8) &amp; -20     \\end{array}\\right|=-2\\) \\(\\frac{1}{(8-2)}\\left|\\begin{array}{cc}     (5-2) &amp; 4 \\\\     (5-8) &amp; -2     \\end{array}\\right|=1\\) </p>"},{"location":"courses/Numerical_Analysis/NA/IntPo_Poly/#newton-newtons-divided-difference-formula","title":"Newton\u5dee\u5546\u8868\u8fbe\u5f0f | Newton's Divided Difference Formula","text":"<p>If we rewrite the \\(n\\)th Lagrange polynomial \\(P_n(x)\\) into another form:</p> \\[ P_n(x) = a_0+a_1(x-x_0)+a_2(x-x_0)(x-x_1)+\\cdots+a_n(x-x_0)\\cdots(x-x_n) \\] <p>By letting \\(x =x_0, x_1,\\cdots, x_n\\), we get</p> \\[ \\begin{align*} P_n(x_0) = &amp; a_0\\\\ P_n(x_1) = &amp; a_0 + a_1(x_1-x_0)\\\\ P_n(x_2) = &amp; a_0+a_1(x_2-x_0)+a_2(x_2-x_0)(x_2-x_1)\\\\ &amp;\\vdots\\\\ P_n(x_n) = &amp; a_0+a_1(x_n-x_0)+a_2(x_n-x_0)(x_n-x_1)+\\\\ &amp;\\cdots+a_n(x_n-x_0)\\cdots(x_n-x_{n-1}) \\end{align*} \\] <p>Then we can define:</p> \\[ \\begin{align*} f_n[x_0] &amp;\\overset{\\Delta}{=} f(x_0) = a_0\\\\ f_n[x_0, x_1] &amp;\\overset{\\Delta}{=} \\frac{f(x_1) - f(x_0)}{x_1-x_0} = a_1\\\\ f_n[x_0, x_1, x_2] &amp;\\overset{\\Delta}{=} \\frac{f[x_1,x_2]-f[x_0,x_1]}{x_2-x_0} \\\\&amp;= \\frac{\\frac{f(x_2)-f(x_1)}{x_2-x_1}-\\frac{f(x_1)-f(x_0)}{x_1-x_0} }{x_2-x_0} \\\\&amp;= \\frac{\\frac{f(x_2)-f(x_0)-(f(x_1)-f(x_0))}{x_2-x_1}-a_1}{x_2-x_0} \\\\ &amp;= \\frac{\\frac{f(x_2)-f(x_0)}{x_2-x_1} - \\frac{a_1(x_1-x_0)}{x_2-x_1}-\\frac{a_1(x_2-x_1)}{x_2-x_1}}{x_2-x_0} \\\\ &amp;= \\frac{\\frac{f(x_2)-f(x_0)}{x_2-x_1} - \\frac{a_1(x_2-x_0)}{x_2-x_1}}{x_2-x_0}\\\\ &amp;= \\frac{f(x_2)-a_0 - a_1(x_2-x_0)}{(x_2-x_0)(x_2-x_1)}=a_2\\\\ &amp;\\vdots\\\\ f[x_0,x_1,\\cdots, x_n] &amp;\\overset{\\Delta}{=} \\frac{f[x_1,x_2,\\cdots,x_n] - f[x_0,x_1,\\cdots,x_{n-1}]}{x_n-x_0} = a_n \\end{align*} \\] <p>We can prove the above definition \\(f[x_0,x_1\\cdots,x_n]\\), which is called divided difference, to be equal to \\(a_n\\) by induction.</p> <p>We can also show that \\(a_1\\) is the coefficient of the highest item of polynomial of degree \\(1\\) that interpolates \\(x_0,x_1\\), and \\(a_2\\) is the coefficient of the highest item of polynomial of degree \\(2\\) that interpolates \\(x_0,x_1,x_2\\) ...</p> <p>This iterative method is quite useful in determining the parameters of \\(n\\)th Lagrange polynomial.</p> \\[ P_n(x) = f[x_0] + \\sum_{i=1}^{n}\\left(f[x_0,x_1,\\cdots,x_i]\\prod_{j=0}^{i-1}(x-x_j)\\right) \\] <p>The following relation gives a slightly quicker way to compute \\(f[x_0,x_1,\\cdots,x_n]\\):</p> <p>\u8ba1\u7b97\u5dee\u5546 | Calculating divided difference</p> \\[ f[x_0,x_1,\\cdots,x_n] = \\sum_{k=0}^{n}\\frac{f(x_k)}{\\omega'_{n+1}(x_k)} \\] <p>where </p> \\[ \\omega_{n+1}(x) = \\prod_{i=0}^{n}(x-x_i),\\quad \\omega'_{n+1}(x_j) = \\prod_{i=0 \\atop i \\neq j}^{n}(x_j-x_i) \\] Proof <p>We know from Lagrange polynomial of degree \\(n\\)</p> \\[ P_n(x) = \\sum_{i=0}^{n}f(x_i)\\frac{\\omega(x)}{(x-x_i)\\omega'(x_i)} \\] <p>where </p> \\[ \\omega(x) = \\prod_{j=0}^n(x-x_j), \\quad \\omega'(x_i) = \\prod_{j=0\\atop j\\neq i}^n(x_i-x_j) \\] <p>So the divided difference equals to the coefficient of the highest item of \\(P_n(x)\\), and we are done.</p> <p>Now solve Question 1 in the above part of the article.</p> Answer <p> \\(x_i\\) \\(f(x_i)\\) \\(f[x_i,x_j]\\) \\(f[x_i,x_j,x_k]\\) 1  2  4 8  1  5 -7  2 3 </p> <p>So the interpolating polynomial is </p> \\[ P_2(x) = 8-7(x-1)+ 3(x-1)(x-2) \\] <p>Some properties are the followings.</p> <p>\u5dee\u5546\u4e0e\u5fae\u5206\u5747\u503c\u7684\u5173\u7cfb | Relationship of DD &amp; Mean Value Differentials</p> <p>Suppose that \\(f \\in C^n[a, b]\\) and \\(x_0, x_1, \\cdots, x_n\\) are distinct numbers in \\([a, b]\\). Then a number \\(\\xi\\) exists in \\((a, b)\\) with</p> \\[ f[x_0,x_1\\cdots,x_n] = \\frac{f^{(n)}(\\xi)}{n!} \\] <p>Specifically, we denote \\(a=\\min\\{x_i:i=0,1\\cdots,n\\}\\) and \\(b=\\max\\{x_i:i=0,1\\cdots,n\\}\\)</p> <p>It is like </p> \\[ f[x_0,x_1] = \\frac{f(x_1)-f(x_0)}{x_1-x_0} = f'(\\xi) \\] <p>but add \\(n!\\) to the denominator.</p> <p>The collary can be quite simple.</p> <p>Collary of the above</p> <p>If \\(f(x)\\) is a polynomial of degree \\(k\\), \\(k&lt;n\\), so the \\(n\\)th divided difference is \\(0\\).</p>"},{"location":"courses/Numerical_Analysis/NA/IntPo_Poly/#error-analysis-of-interpolation","title":"\u63d2\u503c\u8bef\u5dee\u5206\u6790 | Error Analysis of Interpolation","text":"<p>The following theorem gives the error bound.</p> <p>\u62c9\u683c\u6717\u65e5\u57fa\u51fd\u6570\u7684\u4f59\u9879 | The remainder of Lagrange interpolating polynomial</p> <p>Suppose \\(x_0, x_1, \\cdots , x_n\\) are distinct numbers in the interval \\([a, b]\\) and \\(f \\in C^{n+1}[a, b]\\). Then, for each \\(x \\in [a, b]\\), a number \\(\\xi(x)\\) (generally unknown) between \\(x_0, x_1, \\cdots , x_n\\), and hence in \\((a, b)\\), exists with</p> \\[ f(x) = P(x) + \\frac{f^{n+1}(\\xi(x))}{(n+1)!}\\prod_{i=0}^{n}(x - x_i) \\] <p>where \\(P(x)\\) is the Lagrange interpolating polynomial.</p> <p>Prove it.</p> HintsProof <ul> <li>Using a function </li> </ul> \\[ g(t) = f(t) - P(t) - [f(x)-P(x)]\\prod_{i=0}^{n}\\frac{(t-x_i)}{x-x_i} \\] <p>which is \\(C^{n+1}[a, b]\\). Here we see \\(x\\neq x_k\\) is a constant for variable \\(t\\). Note that \\(f(x_k)=0(k=0,1,\\cdots n)\\) and \\(f(x)=0\\) for \\(x \\in [a, b]\\)(Readers can prove them by substituting in). We can see that \\(n+1+1=n+2\\) zero points here(\\(x,x_0,\\cdots, x_n\\)). </p> <ul> <li>Using generalized Rolle's Theorem: There exists \\(\\xi(x) \\in [a, b]\\) such that \\(g^{n+1}(\\xi(x))=0\\)</li> </ul> <p>And make \\(n+1\\) derivatives to \\(g(t)\\)(with respect to \\(t\\)), we have</p> \\[ g^{(n+1)}(t)=f^{(n+1)}(t)-P^{(n+1)}(t)-\\frac{(n+1)![f(x)-P(x)]}{\\prod\\limits_{i=0}^n (x-x_i)} \\] <p>substitute in \\(\\xi\\) we have</p> \\[ 0=g^{(n+1)}(\\xi)=f^{(n+1)}(\\xi)- \\frac{(n+1)![f(\\xi)-P(\\xi)]}{\\prod\\limits_{i=0}^n (x-x_i)} \\] <p>cause \\(P(x)\\) is \\(n\\)th polynomial, so \\(P^{(n+1)}(\\xi)=0\\). So solve for \\(f(x)\\) we have</p> \\[ f(x)=P(x)+\\frac{f^{n+1}(\\xi(x))}{(n+1)!}\\prod_{i=0}^{n}(x - x_i) \\] <p>and we can get the result. </p> <p>Or by using the relationship between divided difference and mean value differentials.</p> <p>\u725b\u987f\u63d2\u503c\u516c\u5f0f\u7684\u4f59\u9879 | The remainder of Newton's interpolating Formula</p> <p>Suppose \\(x_0, x_1, \\cdots , x_n\\) are distinct numbers in the interval \\([a, b]\\) and \\(f \\in C^{n+1}[a, b]\\). Then, for each \\(x \\in [a, b]\\), we have</p> \\[ \\begin{align*} f(x)=&amp;f[x_0]+f[x_0,x_1](x-x_1)+\\cdots\\\\ &amp;+f[x_0,x_1,\\cdots, x_n]\\prod_{i=0}^{n-1}(x-x_i)\\\\ &amp;+f[x_0,x_1,\\cdots,x_n,x]\\prod_{i=0}^n(x-x_i) \\end{align*} \\] Proof <p>Similar to proof of Lagrange interpolation polynomial. We assume \\(z\\neq x_0,x_1,\\cdots,x_n\\), then consider \\(n+1\\)th polynomial \\(Q(x)\\) that interpolates these \\(n+2\\) points \\(\\{z,x_0,x_1,\\cdots,x_n\\}\\), so we have</p> \\[ Q(x)=f[x_0]+\\sum_{i=1}^{n}\\left(f[x_0,x_1,\\cdots,x_i]\\prod_{j=0}^{i-1}(x-x_j)\\right)+f[x_0,x_1,\\cdots,x_n,z]\\prod_{i=0}^n(x-x_i) \\] <p>which satisfies \\(f(x_i)=Q(x_i)(i=0,1,\\cdots,n)\\) and \\(f(z)=Q(z)\\), the latter one gives</p> \\[ f(z)= Q(z) = f[x_0]+\\sum_{i=1}^{n}\\left(f[x_0,x_1,\\cdots,x_i]\\prod_{j=0}^{i-1}(z-x_j)\\right)+f[x_0,x_1,\\cdots,x_n,z]\\prod_{i=0}^n(z-x_i) \\] <p>substitute \\(z\\) with \\(x\\) and we are done.</p> <p>There is still one step left. For \\(z=x_i\\), divided difference \\(f[x_0,x_1,\\cdots,x_n,z]\\) is not defined. So for \\(x_i\\), we consider a sequence \\(\\{y_k^{i}\\}_{k=1}^\\infty\\), where \\(y_k^i\\neq x_i\\) and \\(\\lim_{k\\rightarrow \\infty}y_k^i=x_i\\). First we have the above theorem holds on \\(y_k^i\\), i.e.</p> \\[ \\begin{align*} f(y_k^i) &amp;= f[x_0]+\\sum_{i=1}^{n}\\left(f[x_0,x_1,\\cdots,x_i]\\prod_{j=0}^{i-1}(y_k^i-x_j)\\right)+f[x_0,x_1,\\cdots,x_n,y_k^i]\\prod_{i=0}^n(y_k^i-x_i)\\\\ &amp;\\overset{\\Delta}{=}Q(y^i_k) + R(y^i_k) \\end{align*} \\] <p>we know that \\(f(x)\\) and \\(Q(x)\\)(\\(n\\)th polynomial) are continuous on \\([a,b]\\), so \\(\\lim_{k\\rightarrow \\infty}f(y_k^i)=f(x_i)\\) and \\(\\lim_{k\\rightarrow \\infty}Q(y_k^i)=Q(x_i)=f(x_i)\\), so</p> \\[ \\lim_{k\\rightarrow \\infty}R(y_k^i)=0 \\] <p>we could define \\(R(x_i)=0\\) and the above equation still holds.</p> <p>The above error item is useful in Numerical Integration.</p> <p>Compared to the remainder of Taylor's extension</p> \\[ R_n(x) = \\frac{f^{n+1}(\\xi(x))}{(n+1)!}(x-x_0)^{n+1} \\] <p>Because \\(\\xi(x)\\) is usually unknown, so we often use a number \\(x' \\in  [a, b]\\) such that </p> \\[ |f^{n+1}(\\xi(x))|\\leq |f^{n+1}(x')| \\]"},{"location":"courses/Numerical_Analysis/NA/IntPo_Poly/#interpolation-of-equidistant-points","title":"\u7b49\u8ddd\u63d2\u503c | Interpolation of Equidistant Points","text":"<p>In actual calculation in computer, we use difference to replace divided difference when we interpolate points with equal distance.</p> <p>Definition of Difference on Equidistant Points</p> <p>Difference of first order is defined by</p> \\[ \\Delta f(x_i)=f(x_{i+1})-f(x_i) \\] <p>Difference of second order is defined by</p> \\[ \\begin{align*} \\Delta^2 f(x_i)&amp;=\\Delta f(x_{i+1})-\\Delta f(x_i)\\\\ &amp;=f(x_{i+2})-2f(x_{i+1})+f(x_i) \\end{align*} \\] <p>By recursion, we define Difference of \\(n\\)th order</p> \\[ \\Delta^n f(x_i)=\\Delta^{n-1}f(x_{i+1})-\\Delta^{n-1}f(x_i) \\] <p>By induction, it is easy to see that</p> \\[ \\begin{align*} \\Delta^{n}f(x_i) = &amp;f(x_{i+n})-nf(x_{i+n-1})+C^1_{n}f(x_{i+n-2})\\\\ +&amp;\\cdots+(-1)^{n-1}nf(x_{i+1})+(-1)^nf(x_i)\\\\ =&amp;\\sum_{k=0}^n(-1)^k C_n^kf(x_{i+k}). \\end{align*} \\] <p>We usually use the recursive method for computing.</p> <p>In situation where points are equally distant, we have a relationship between divided difference and difference.</p> <p>relationship between divided difference and difference</p> <p>Under equidistant points, we have </p> \\[ f[x_0,x_1,\\cdots,x_n]=\\frac{\\Delta^{n}f(x_0)}{h^n n!} \\] <p>where \\(h\\) is the distance between two adjacent points.</p> Proof <p>By induction.</p>"},{"location":"courses/Numerical_Analysis/NA/IntPo_Poly/#hermite-hermite-polynomials","title":"Hermite\u591a\u9879\u5f0f | Hermite Polynomials","text":"<p>We want to consider the smoothness of interpolating polynomials, so we need to consider its derivatives, especially the first order derivative. </p> <p>Definition of Osculating Polynomial</p> <p>The osculating polynomial approximating \\(f\\) is the polynomial \\(P(x)\\) of least degree such that</p> \\[ \\frac{d^kP(x_i)}{dx^k} = \\frac{d^kf(x_i)}{dx^k},\\quad, \\forall i = 0,1,\\cdots, n, \\forall k = 0,1,\\cdots, m_i \\] <p>Where \\(n\\) is the total number of sampling points and \\(m_i\\) is the degree of smoothness at point \\(x_i\\).</p> <p>If \\(m_i=1\\) for all \\(i=0,1,\\cdots, n\\), then the above polynomial is called Hermite Polynomial.</p> <p>Composition of Hermite Polynomial</p> <p>If \\(f \\in C^1[a, b]\\) and \\(x_0, \\cdots , x_n \\in [a, b]\\) are distinct, the unique polynomial of least degree agreeing with \\(f\\) and \\(f'\\) at \\(x_0,\\cdots , x_n\\) is the Hermite polynomial of degree at most \\(2n + 1\\) given by</p> \\[ H_{2n+1}(x) = \\sum_{j=0}^{n}f(x_j)H_{n,j}(x) + \\sum_{j=0}^{n}f'(x_j)\\hat{H}_{n,j}(x) \\] <p>where, for \\(L_{n, j}(x)\\) denoting the \\(j\\)th Lagrange coefficient polynomial of degree \\(n\\), we have</p> \\[ H_{n,j}(x) = [1-2(x-x_j)L'_{n,j}(x_j)]L^2_{n,j}(x),\\quad \\hat{H}_{n,j}(x) = (x-x_j)L^2_{n,j}(x) \\] <p>The composition is usually a little tedious.</p>"},{"location":"courses/Numerical_Analysis/NA/IntPo_Poly/#cubic-spline-interpolation","title":"\u6837\u6761\u63d2\u503c | Cubic Spline Interpolation","text":"<p>We change \\(\\frac{d^kP(x_i)}{dx^k} = \\frac{d^kf(x_i)}{dx^k}\\) into equations between adjacent curves. Because we often do not know the derivatives of the point.</p> <p>Define \\(s(x)\\) piece-wisely with \\(s_i(x)\\in [x_i,x_{i+1}]\\)</p> \\[ s_i(x)=a_i+b_ix+c_ix^2+d_ix^3, \\quad i=0,1,\\cdots,n-1 \\] <p>which has 4 parameters to be determined. So the overall number of parameters to be determined is \\(4n\\). The condition they have to satisfy</p> \\[ s_i(x_i)=f(x_i), s_i(x_{i+1})=f(x_{i+1}), i=0,1,\\cdots,n-1 \\quad \\text{[$2n$ equations]} \\] \\[ s'_i(x_{i})=s'_{i-1}(x_{i}),i=1,2,\\cdots, n-1\\quad \\text{[$n-1$ equations]} \\] \\[ s''_i(x_{i})=s''_{i-1}(x_{i}),i=1,2,\\cdots, n-1\\quad \\text{[$n-1$ equations]} \\] <p>So the above necessary condition gives \\(4n-2\\) equations while we have \\(4n\\) parameters to be determined. So we need to add another two equations for computing. There are many ways while the followings are usually seen.</p> <p>Three possible dealings</p> <p>(i) provide the derivatives of \\(2\\) endpoints, i.e. \\(s'(a)=f'(a)\\), \\(s'(b)=f'(b)\\), which is called D1 cubic spline or Clamped Cubic Spline.</p> <p>(ii) provide the second derivatives of \\(2\\) endpoints, i.e. \\(s''(a)=f''(a)\\), \\(s''(b)=f''(b)\\), which is called D1 cubic spline. Specially, if \\(s''(a)=s''(b)=0\\), it is called Natural Spline.</p> <p>(iii) take \\(s(a)=f(a)\\) or \\(s(b)=f(b)\\) out and provide extra \\(3\\) equations \\(s(a)=s(b)\\), \\(s'(a)=s'(b)\\), \\(s''(a)=s''(b)\\), which is called periodical cubic spline.</p> <p>The solution for solving parameters by given condition.</p> <p>Calculation of parameters</p> <p>Consider \\(s(x)\\) on \\([x_i,x_{i+1}]\\), if we denote \\(s''(x_i)=M_i\\), then the overall equation becomes</p> \\[ \\mu_i M_{i-1}+2M_i+\\lambda_i M_{i+1} = 6f[x_{i-1},x_i,x_{i+1}], \\quad i=1,2,\\cdots,n-1 \\] <p>where </p> \\[ \\mu_i=\\frac{x_i-x_{i-1}}{x_{i+1}-x_{i-1}}, \\lambda_i=\\frac{x_{i+1}-x_i}{x_{i+1}-x_{i-1}} \\] <p>for \\(i=0\\) and \\(i=n\\) two situation, we deduce different equations for different condition.</p> <p>(i) D1 cubic spline. we have</p> \\[ \\begin{cases} 2M_0+M_1=6f[x_0,x_0,x_1]\\\\ M_{n-2}+2M_{n-1}=6f[x_{n-2},x_{n-1}] \\end{cases} \\] <p> Q3. Interpolate the following points with natural cubic spline. \\(x_i\\) 0 1 2 \\(f(x_i)\\) 1 2 6 Answer <p>Define \\(s_0(x)=a_0+b_0x+c_0x^2+d_0x^3\\) and \\(s_1(x)=a_1+b_1(x-1)+c_1(x-1)^2+d_1(x-1)^3\\), then </p> \\[ \\begin{cases} b_0+2c_0+3d_0=b_1 \\quad &amp;\\text{derivative}\\\\ 2c_0+6d_0=2c_1\\quad &amp;\\text{second derivative}\\\\ 2c_0=0 \\quad &amp;s''(a)=0\\\\ 2c_1+6d_1=0 \\quad &amp; s''(b)=0 \\end{cases} \\] <p>with condition that satisfies passing data points</p> \\[ \\begin{cases} a_0=1\\\\  a_0+b_0+c_0+d_0=2\\\\ a_1=2\\\\ a_1+b_1+c_1+d_1=6 \\end{cases} \\] <p>So represent all the parameters with \\(d_1\\) or \\(d_0\\), we get</p> \\[ \\begin{cases} a_0=1\\\\ b_0=\\frac{1}{4}\\\\ c_0=0\\\\ d_0=\\frac{3}{4} \\end{cases} ,\\quad  \\begin{cases} a_1=2\\\\ b_1=\\frac{5}{2}\\\\ c_1=\\frac{9}{4}\\\\ d_1=-\\frac{3}{4}\\\\ \\end{cases} \\]"},{"location":"courses/Numerical_Analysis/NA/NI/","title":"Numerical Integration","text":"<p>Generally speaking, we encounter an integration problem </p> \\[ I(f) =\\int_a^b\\rho(x)f(x)dx \\] <p>which cannot be solved by indefinite integral, so we have to define a numerical formula to approximate the integration. Typical quadrature formula (\u6c42\u79ef\u516c\u5f0f) can be written as</p> \\[ \\begin{equation} I_n(f)=\\sum_{k=1}^nA_kf(x_k)\\label{I-genaral} \\end{equation} \\] <p>where the subscript \\(n\\) denotes sampling on \\(n\\) points, \\(x_k(k=1,2,\\cdots,n)\\) are nodes on \\([a,b]\\) and \\(A_k\\) are coefficients, whose value are only related with \\([a,b]\\), \\(\\rho(x)\\) and the sampling points, rather than the formula of \\(f\\) itself.</p> <p>Naively, we get error</p> \\[ E_n(f)= I(f)-I_n(f) \\] <p>which is obviously hard to get because we do not know the exact integration value. So we transfer to a concept called  Algebraic Precision.</p>"},{"location":"courses/Numerical_Analysis/NA/NI/#algebraic-precision","title":"\u4ee3\u6570\u7cbe\u5ea6 | Algebraic Precision","text":"<p>The degree of accuracy, or precision, of a quadrature formula is the largest positive integer \\(m\\) such that the formula is exact for \\(x^k\\) , for each \\(k = 0, 1,\\cdots, m\\). That is, we call a formula has \\(m\\) algebraic precision if there exists \\(m\\in \\mathbb{N}^+\\), s.t.</p> \\[ E_n(x^k)=0, k=0,1,\\cdots,m,\\quad E_n(x^{m+1})\\neq 0. \\] <p>It is easy to see that for all polynomial \\(p(x)\\) of degree no more than \\(m\\), we have \\(E_n(p)=0\\).</p> <p>If we find a simpler function \\(p(x)\\) that approximates \\(f(x)\\) on \\([a,b]\\), and \\(I(p)\\) is easy to get, then we can use \\(I(p)\\) to approximate \\(I(f)\\). That is, if </p> \\[ \\|f-p\\|=\\max_{x\\in [a,b]}|f(x)-p(x)| &lt;\\varepsilon \\] <p>then </p> \\[ |I(f)-I(p)|=\\left|\\int_a^b\\rho(x)[f(x)-p(x)]dx\\right| &lt;\\varepsilon (b-a) \\] <p>which is also small enough.</p>"},{"location":"courses/Numerical_Analysis/NA/NI/#newton-cotes-newton-cotes-formulas","title":"Newton-Cotes \u516c\u5f0f | Newton-Cotes Formulas","text":"<p>The simplest function is polynomials. So we try to find a polynomial \\(p(x)\\) to approximate \\(f(x)\\) and then use \\(I(p)\\) to approximate \\(I(f)\\). The following formulas are really natural if readers have been familiar with Interpolating Polynomial.</p> <p>Given \\(n+1\\) points \\(x_0&lt;x_1&lt;\\cdots&lt;x_n\\), we have a Lagrange Polynomial</p> \\[ p_n(x)=\\sum_{i=0}^n\\prod_{j=0\\atop j\\neq i}^n\\frac{(x-x_j)}{x_i-x_j}f(x_i) \\] <p>use the above polynomial as an approximation for integration. That is, </p> \\[ \\begin{equation} \\begin{cases} \\displaystyle I_{n+1}(p)=I(p_n)=\\sum\\limits_{i=0}^nA^n_if(x_i)\\\\ \\displaystyle A_i^n=\\int_a^b\\prod\\limits_{j=0\\atop j\\neq i}^n \\frac{(x-x_j)}{(x_i-x_j)}dx, i=0,1,\\cdots, n. \\end{cases}\\label{quadrature} \\end{equation} \\]"},{"location":"courses/Numerical_Analysis/NA/NI/#deduction","title":"\u63a8\u5bfc | Deduction","text":"<p>In equation \\(\\ref{quadrature}\\), if we let \\(\\rho(x)\\equiv1\\) and choose equidistant points:</p> \\[ x_k=a+kh, \\quad h=\\frac{b-a}{n},\\quad  k=0,1,\\cdots,n \\] <p>then the corresponding quadrature formula is called Newton-Cotes Formula. In fact, if we let \\(x=a+th\\), \\(t\\in [0,n]\\), then we have</p> \\[ \\begin{align*} A_i^n&amp;=\\int_0^n\\prod_{j=0\\atop j\\neq i}^n \\frac{(th-jh)}{(ih-jh)}d(a+th)\\\\ &amp;=h\\int_0^n\\prod_{j=0\\atop j\\neq i}^n\\frac{(t-j)}{(i-j)}dt \\end{align*} \\] <p>common rule of quadrature formula</p> <p>(i) Trapezoidal Rule</p> <p>If we let \\(n=1\\) and get Trapezoidal Rule of quadrature</p> \\[ I_2(f)=I(p_1)=\\frac{b-a}{2}[f(a)+f(b)] \\] <p>\\(f\\) is approximated by a linear expression.</p> <p>(ii) Simpson\u2019s Rule(Commonly used)</p> <p>If we let \\(n=2\\) and get Simpson\u2019s Rule of quadrature</p> \\[ I_3(f)=I(p_2)=\\frac{b-a}{6}\\left[f(a)+4f\\left(\\frac{a+b}{2}\\right)+f(b)\\right] \\] <p>\\(f\\) is approximated by a parabola expression.</p> <p>(iii) Cotes' Rule</p> <p>If we let \\(n=4\\) and get Cotes' rule of quadrature</p> \\[ I_5(f)=I(p_4)=\\frac{b-a}{90}\\left[7f(x_0)+32f(x_1)+12f(x_2)+32f(x_3)+7f(x_4)\\right] \\] <p>where \\(x_i=a+(b-a)/4\\cdot i\\), \\((i=0,1,2,3,4)\\).</p>"},{"location":"courses/Numerical_Analysis/NA/NI/#error-analysis","title":"\u8bef\u5dee\u5206\u6790 | Error analysis","text":"<p>Assume \\(x_i(i=0,1\\cdots,n)\\) are equidistant(\\(h\\) apart), then we introduce</p> \\[ \\omega_n(x)=\\prod_{i=0}^n(x-x_i) \\] <p>which have some properties.</p> <p>Properties of \\(\\omega_n(x)\\)</p> <p>(i) \\(\\omega_n((a+b)/2+\\xi)=(-1)^{n+1}\\omega_n((a+b)/2-\\xi)\\)</p> <p>(ii) For \\(\\xi\\neq x_i(i=0,1,\\cdots,n)\\), the following statements hold.</p> \\[ \\begin{align*} &amp;|\\omega_n(\\xi+h)|&lt;|\\omega_n(\\xi)|,\\quad \\forall a&lt;\\xi+h\\leq(a+b)/2\\\\ &amp;|\\omega_n(\\xi)|&lt;|\\omega_n(\\xi+h)|,\\quad \\forall (a+b)/2\\leq\\xi&lt;b \\end{align*} \\] <p>Look at the graph, in which \\(n=5,6\\).</p> <p><p> </p></p> <p>Then we define an integral of \\(\\omega_n(x)\\)</p> \\[ \\Omega_n(x)=\\int_a^x \\omega_n(s)ds, n=1,2,\\cdots \\] <p>which has a great property.</p> <p>Properties of \\(\\Omega_n(x)\\)</p> <p>When \\(n\\) is en even number, then</p> <p>(i) \\(\\Omega_n(a)=\\Omega_n(b)=0\\)</p> <p>(ii) \\(\\Omega_n(x)&gt;0, \\quad\\forall x\\in(a,b)\\)</p> <p>Then we can prove the error equation for Newton-Cotes Formula.</p> <p>Theorem of Error Analysis for Newton-Cotes Formula</p> <p>(i) If \\(n\\) (number of intervals) is even, and \\(f\\in C^{n+2}[a,b]\\), then we have</p> \\[ E_{n+1}(f)=\\frac{k_n}{(n+2)!}f^{(n+2)}(\\eta),\\quad a&lt;\\eta&lt;b \\] <p>where </p> \\[ k_n=\\int_a^bx\\omega_n(x)dx&lt;0 \\] <p>(ii) If \\(n\\) is odd, and \\(f\\in C^{n+1}[a,b]\\), then the error expression is</p> \\[ E_{n+1}(f)=\\frac{k_n}{(n+1)!}f^{(n+1)}(\\eta),\\quad a&lt;\\eta&lt;b \\] <p>where</p> \\[ k_n=\\int_a^b\\omega_n(x)dx&lt;0 \\] Proof <p>Using remainder of interpolation polynomial in Newton's divided difference formula.</p> <p></p> <p>Corollary of Error Analysis</p> <p>(i) For Tranpezoidal Formula, the rounding error is</p> \\[ E_2(f)=-\\frac{(b-a)^3}{12}f''(\\eta),\\quad a&lt;\\eta&lt;b \\] <p>(ii) For Simpson Formula, the rounding error is </p> \\[ E_2(f)=-\\frac{(b-a)^5}{2880}f^{(4)}(\\eta),\\quad a&lt;\\eta&lt;b \\]"},{"location":"courses/Numerical_Analysis/NA/NI/#numerical-stability","title":"\u6570\u503c\u7a33\u5b9a\u6027 | Numerical Stability","text":"<p>Actually, it is usually hard to get an accurate value of \\(f(x_k)\\), so we introduce error here, which may influence afterwards computation. Now we quantify this effect. </p> <p>Assume we use \\(\\tilde{f}(x_k)\\) to replace \\(f(x_k)\\), denote \\(\\varepsilon_k=f(x_k)-\\tilde{f}(x_k)\\), then the integral error</p> \\[ \\begin{align*} |I_n(f)-I_n(\\tilde{f})|&amp;=\\left|\\sum_{k=0}^n A_k f(x_k)-\\sum_{k=0}A_k \\tilde{f}(x_k)\\right|\\\\ &amp;\\leq \\sum_{k=0}^n |A_k||\\varepsilon_k|\\\\ &amp;&lt;\\sum_{k=0}^n |A_k|\\max_{0\\leq k\\leq n}{|\\varepsilon_k|} \\end{align*}\\] <p>So if \\(\\sum_{k=0}^n |A_k|\\) is bounded, then the error can be bounded. However, we could prove that this item would go to infinity as \\(n\\rightarrow \\infty\\), so higher order of Newton-Cotes formula is impractical.</p>"},{"location":"courses/Numerical_Analysis/NA/NI/#composite-numerical-integration","title":"\u590d\u5316\u79ef\u5206 | Composite Numerical Integration","text":"<p>We could use low order Newton-Cotes formula with Narrowing down \\(h\\). That is, use Simpson's formula multiple times on little interval of length \\(h\\). </p>"},{"location":"courses/Numerical_Analysis/NA/NI/#romberg-romberg-integration","title":"Romberg \u79ef\u5206 | Romberg Integration","text":"<p>Here we have another way to improve precision of integration, which is Richardson's Extrapolation. This is a general method.</p> <ul> <li>Richardson's Extrapolation</li> </ul> \\[ \\begin{equation} T_0(h) - I=\\alpha_1 h + \\alpha_2 h^2 +\\cdots \\label{richardson} \\end{equation} \\] <p>Let</p> \\[ \\begin{equation} T_0(h/2)-I=\\alpha_1 h/2 + \\alpha_2 (h/2)^2 + \\cdots \\label{richardson-2} \\end{equation} \\] <p>multiply \\(2\\) to both sides of equation \\(\\ref{richardson-2}\\) and Subtract equation \\(\\ref{richardson}\\), get</p> \\[ \\frac{2T_0(h/2)-T_0(h)}{2-1}-I=-\\frac{1}{2}\\alpha_2 h^2 +\\cdots \\] <p>So </p> \\[ \\begin{align*} T_1(h) &amp;= \\frac{2T_0(h/2)-T_0(h)}{2-1} = I + \\beta_1 h^2+\\beta_2 h^3+\\cdots \\\\ T_2(h) &amp;= \\frac{2^2T_1(h/2)-T_1(h)}{2^2-1} = I + \\gamma_1 h^3 +\\cdots \\\\ &amp;\\vdots\\\\ T_n(h) &amp;= \\frac{2^nT_{n-1}(h)-T_{n-1}(h)}{2^n-1} = I + \\xi_1 h^{n+1} +\\cdots  \\end{align*} \\]"},{"location":"courses/Numerical_Analysis/NA/NI/#adaptive-quadrature-methods","title":"\u81ea\u9002\u5e94\u6c42\u79ef\u65b9\u6cd5 | Adaptive Quadrature Methods","text":""},{"location":"courses/Numerical_Analysis/NA/NI/#non-equidistant-quadrature-formula","title":"\u975e\u7b49\u8ddd\u6c42\u79ef\u516c\u5f0f | Non-equidistant Quadrature formula","text":"<p>Could we free some limits of the above quadrature, and get some more accurate formula? To be more specific, could we let the coefficient of the quadrature to vary, and do not limit the node to be integrated, and then obtain a better result? </p> <p>Let us analyze that, if a quadrature has \\(m\\) algebraic precision, then its coefficients and nodes of formula \\(\\ref{I-genaral}\\) must follow the system</p> \\[ \\begin{cases} A_1+A_2+\\cdots+A_n&amp;=\\int_{a}^b\\rho(x)dx\\\\ A_1x_1+A_2x_2+\\cdots+A_nx_n&amp;=\\int_{a}^b\\rho(x)xdx\\\\ \\cdots&amp;\\\\ A_1x_1^m+A_2x_2^m+\\cdots+A_nx_n^m&amp;=\\int_{a}^b\\rho(x)x^mdx\\\\ \\end{cases} \\]"},{"location":"courses/Numerical_Analysis/NA/NI/#uniform-coefficient-formula","title":"\u4e00\u81f4\u7cfb\u6570\u516c\u5f0f | Uniform Coefficient Formula","text":"<p>Given coefficients \\(A_1,A_2,\\cdots,A_n\\), the above non-linear system of variables \\(x_1,x_2,\\cdots,x_n\\) might have solution, with at least \\(m\\) algebraic precision.</p> <p>Assume \\(\\rho(x)\\equiv 1\\), \\(A_1=A_2=\\cdots=A_n\\), then it must follow</p> \\[ I_n(f)=A_n\\sum_{i=1}^nf(x_i) \\] <p>and</p> \\[ A_n\\sum_{i=1}^nx_i^k=\\frac{1}{k+1}(b^{k+1}-a^{k+1}),\\quad k=1,2\\cdots,n \\] <p>Common expression for uniform coefficient formula</p> <p>(i) \\(n=1\\), then \\(A_1=b-a\\), \\(x_1=(a+b)/2\\), so </p> \\[ I_1(f)=(b-a)f\\left(\\frac{a+b}{2}\\right) \\] <p>(ii) \\(n=2\\), then \\(A_2=(b-a)/2\\), so</p> \\[ I_2=\\frac{b-a}{2}\\left[f\\left(\\frac{b+a}{2}-\\frac{b-2}{2\\sqrt{3}}\\right) +f\\left(\\frac{b+a}{2}+\\frac{b-2}{2\\sqrt{3}}\\right)  \\right] \\] <p>Notice that here nodes are central symmetry about \\(\\frac{(a+b)}{2}\\).</p>"},{"location":"courses/Numerical_Analysis/NA/NI/#gaussian-gaussian-quadrature","title":"Gaussian \u6c42\u79ef\u516c\u5f0f | Gaussian Quadrature","text":"<p>Given \\(n\\) unknown nodes \\(x_1&lt;x_2&lt;\\cdots&lt;x_n\\), and denote</p> \\[ \\omega_n(x)=\\prod_{i=1}^n(x-x_i) \\] <p>If we let </p> \\[ A_i=\\int_a^b\\rho(x)\\frac{\\omega_n(x)}{(x-x_i)\\omega_n'(x)}dx, \\quad i=1,2,\\cdots,n \\] <p>then the above quadrature must have at least \\(n-1\\) algebraic precision(readers can check this because it is Lagrange interpolation polynomial, but non-equidistant nodes). That is, when using \\(n\\) nodes to interpolate and then integrate, we must get a quadrature formula with \\(n-1\\) algebraic precision. Now we analyze where the upper bound is for this quadrature formula if we choose a proper position of there \\(n\\) nodes.</p> <p>Interpolation a function has a similar error formula like in Newton-Cotes Formula</p> \\[ E_n(f)=\\int_a^b\\rho(x)f[x_1,x_2,\\cdots,x_n,x]\\omega_n(x)dx \\] <p>Notice that the degree of \\(f[x_1,x_2,\\cdots,x_n,x]\\) declines \\(n\\)(Because \\(x\\) is \\(n\\) times divided), and assume \\(f(x)\\) is a polynomial of degree larger than \\(n-1\\). If we want to let the quadrature formula have \\(m(m&gt;n)\\) algebraic precision, then if and only if for all polynomial \\(q(x)\\) of degree no more then \\(m-n\\), we have</p> \\[ \\int_{a}^b\\rho(x)q(x)\\omega_n(x)dx=0 \\] <p>Apparently, \\(m-n\\) needs to be less than \\(n\\), for \\((\\omega_n,\\omega_n)&gt;0\\)(inner product), which means \\(m\\) could be at most \\(2n-1\\). If \\(\\omega_n(x)\\) is a \\(n\\)th orthogonal polynomial, it can satisfy the above condition. So if we choose the roots of this polynomial as nodes to be integrated, then the corresponding quadrature formula has \\(2n-1\\) algebraic precision.</p> <p>Theoretically speaking, Gaussian quadrature formula always exists, but in practice it is not easy to solve. Here we use method of undetermined coefficients.</p> <p>Using different orthogonal polynomials, we can directly deduce some Gaussian quadrature.</p> <p>Examples of Gaussian Quadrature</p> <p>(i) \\(\\rho(x)\\equiv 1\\) integrated on region \\([-1,1]\\), the corresponding polynomial is Legendre Polynomial </p> \\[ P_n(x)=\\frac{1}{2^n n!}\\frac{d^n{(x^2-1)}^n}{dx^n} \\] <p>So the coefficients are </p> \\[ A_i=\\int_{-1}^1 \\frac{P_n(x)dx}{(x-x_i)P'_n(x_i)}=\\frac{2}{(1-x_i^2)P'_n(x_i)^2} \\] <p>with error</p> \\[ E_n(f)=\\frac{2^{2n+1}(n!)^4}{[(2n)!]^3(2n+1)}f^{(2n)}(\\eta),\\quad \\eta\\in(-1,1) \\] <p>(ii) \\(\\rho(x)=\\sqrt{1-x^2}\\), integrated on region \\([-1,1]\\), the corresponding polynomial is Second class Chebyshev polynomial</p> \\[ U_n(x)=\\frac{\\sin[(n+1)\\arccos x]}{\\sqrt{1-x^2}} \\] <p>using its roots as integration nodes we get</p> \\[ \\int_{-1}^1\\sqrt{1-x^2}f(x)ds\\approx \\sum_{k=1}^n\\frac{\\pi}{n+1}\\sin^2 \\frac{k\\pi}{n+1}f\\left(\\cos \\frac{kx}{n+1}\\right) \\] <p>with error</p> \\[ E_n(f)=\\frac{\\pi}{2^{2n+1}(2n)!}f^{(2n)}(\\eta),\\quad \\eta\\in(-1,1) \\]"},{"location":"courses/Numerical_Analysis/NA/Solve_Equ/","title":"Solutions of Equations in One Variables","text":""},{"location":"courses/Numerical_Analysis/NA/Solve_Equ/#basic-ideas-for-solving-equation","title":"\u57fa\u672c\u60f3\u6cd5 | Basic ideas for Solving Equation","text":"<p>To find the solution of an equation, we hope to have an iteration method which takes good advantage of Computer resources like</p> \\[ \\begin{equation}  \\pmb{x}^{k} = f(\\pmb{x}^{k-1}) \\label{eq: iterative eq}  \\end{equation} \\] <p>for \\(k = 1, 2, \\cdots n\\). We use \\(\\pmb{x}\\) instead of \\(x\\) because the above iteration method also applies to solving linear system.</p> <p>Hopefully, if the above equation converges, that is, for \\(k \\rightarrow \\infty\\), \\(\\pmb{x}^{k-1}\\rightarrow \\pmb{x}^*\\), \\(\\pmb{x}^{k}\\rightarrow \\pmb{x}^*\\), and the equation becomes</p> \\[ \\pmb{x}^* = f(\\pmb{x}^*) \\] <p>where \\(\\pmb{x^*}\\) is the sulution of the equation to be solved.</p> <p>If the above thought gets right, then we can consider the converging speed of the iterative process, which makes great sense in practical applications. That is, in a given definition of distence,</p> \\[ \\frac{\\|\\pmb{x}^{k+1} - \\pmb{x}^*\\|}{\\|\\pmb{x}^{k}-\\pmb{x}^*\\|^\\alpha}  \\] <p>to be small as much as possible for each \\(k\\).</p>"},{"location":"courses/Numerical_Analysis/NA/Solve_Equ/#the-bisection-method","title":"\u4e8c\u5206\u6cd5 | the Bisection Method","text":"<p>This method is quite intuitive. By choosing two end points \\(a, b\\), we get another point (Mid-point here)</p> \\[ p = a+\\frac{b-a}{2} \\] <p>Then update \\(a, b\\) by evaluating whether \\(f(p)&gt;0\\) or not to narrow down the interval.</p> <p>What is interesting is the stopping procedure. Readers can see the following question if interested.</p> <p>When we calculate the new point \\(p\\) based on \\(a, b\\), we need to judge whether \\(p\\) is an appropriate answer. Apart from \\(f(p)=0\\), which condition do you think is the best?</p> <ol> <li>\\((b-a)/{|\\min{(a, b)}|}&lt;\\epsilon\\)</li> <li>\\(|p-p_{prev}|=(b-a)/2 &lt; \\epsilon\\)</li> <li>\\(f(p)&lt;\\epsilon\\)</li> </ol> Choose an answerAnwser <ul> <li>1</li> <li>2</li> <li>3</li> </ul> <p>Choose 1, which is close to relative error, currently the best.</p> <p>2: consider \\(\\{p_n\\}=\\sum\\limits_{i=1}^{n}\\frac{1}{k}\\).</p> <p>3: easy to see.</p> <p>The converging speed can discribed as the following:</p> \\[ |x_n- x^*| &lt; \\frac{(b-a)}{2^{n}} \\]"},{"location":"courses/Numerical_Analysis/NA/Solve_Equ/#Fixed-Point-Iteration","title":"\u4e0d\u52a8\u70b9\u6cd5 | Fixed-Point Iteration","text":"<p>As we said previously in Basic ideas for solving equation, we hope to find an iterative relation such that the converging point \\(x^*\\) is exactly what we want, which in this case, means that </p> \\[ f(x^*) = 0 \\] <p>So intuitively, we ask: Whether can we derive a relation from </p> \\[ \\begin{equation} f(x) = 0 \\label{zero-equation} \\end{equation} \\] <p>to </p> \\[ x = g(x) \\] <p>for iterative method?</p> <p>The answer is, of course, YES!</p> <p>One of the easist way to transform is adding \\(x\\) to both sides of equation \\(\\ref{zero-equation}\\), but in most cases this does not work. Because we rely on \\(f(x)\\) ifself for the convergence! </p> <p>Thus, it is necessary to find the condition for \\(x = g(x)\\) to converge. The following theorem <p></p> gives a Sufficient condition.</p> <p>\u4e0d\u52a8\u70b9\u5b58\u5728\u5b9a\u7406 | Fixed-Point Theorom</p> <p>Let \\(g \\in C[a, b]\\) be such that \\(g(x) \\in [a, b]\\), for all \\(x\\) in \\([a, b]\\). Suppose, in addition, that \\(g'\\) exists on \\((a, b)\\) and that a constant \\(0 &lt; k &lt; 1\\) exists with</p> \\[ |g'(x)|\\leq k \\quad \\forall x \\in (a, b) \\] <p>Then for any initial number \\(p_0 \\in [a, b]\\), the sequence \\(\\{p_n\\}_{n=0}^{\\infty}\\) defined by </p> \\[ p_n = g( p_{n\u22121}) \\quad n \\geq 1 \\] <p>converges to the unique fixed point \\(p \\in [a, b]\\).</p> <p>Proving it is easily.</p> HintsProof <ul> <li>using the differential mean value theorem.</li> </ul> <p>\\(\\forall n \\geq 1, \\exists \\zeta_n \\in (p_{n-1}, p) \\subset (a, b)\\), we have</p> \\[ |p_n-p| = |g(p_{n-1}) - g(p)| = g'(\\zeta_n)|p_{n-1}-p|\\leq k|p_{n-1}-p| \\] <p>by induction, we have</p> \\[ |p_n-p|\\leq k^{n}|p_0-p| \\] <p>Let \\(n \\rightarrow \\infty\\), \\(|p_n-p| \\rightarrow 0\\), that is, \\(p_n\\) converges to \\(p\\). </p> <p>What we use in the proof will benefit us in identifying the speed of converging process.</p>"},{"location":"courses/Numerical_Analysis/NA/Solve_Equ/#newtons-method","title":"\u725b\u987f\u6cd5 | Newton's Method","text":"<p>This method is also a fixed-point method. There are two perspectives to get the inspirations.</p> HintsVersion 1Version 2 <ul> <li>version1: shrink the derivative of the iterative function \\(g(x)\\).</li> <li>version2: using Taylor's expansion.</li> </ul> <p>We can know that given a random function \\(f(x)\\), it may not be convergent to some point \\(x^*\\) for \\(x^{k} = f(x^{k-1}) + x^{k-1}\\) in a given interval. So the queation is, can we formulate a function \\(g(x)\\) such that \\(x^{k} = g(x^{k-1})\\) is convergent?</p> <p>The answer is, again, YES!</p> <p>The following content tells us we can formulate \\(g(x)= x - f(x)/(f'(x))\\) such that \\(g'(x) &lt; 1\\) in a given interval.</p> <p>Readers can easily see that </p> \\[ \\begin{align*} g'(x) &amp;= 1 - \\frac{f'^2(x)-f''(x)f(x)}{f'^2(x)}\\\\ &amp;=\\frac{f''(x)f(x)}{f'^2(x)} \\end{align*} \\] <p>if we add some constrictions, it can be easy to make \\(g'(x)&lt;1\\).</p> <p>Here we make use of the Taylor's expansion(or the derivatives) of the goal function. </p> <p>Suppose that \\(f \\in C^2[a, b]\\), Let \\(x_0 \\in [a, b]\\) be an approximation to \\(x^*\\) such that \\(f(x^*) \\neq 0\\) and \\(|x_0-x^*|\\) is \"small\". Consider the first Taylor polynomial for \\(f(x)\\) expanded at \\(x_0\\):</p> \\[  f(x) = f(x_0) + (x-x_0)f'(x_0) + \\frac{(x-x_0)^2}{2}f''(\\zeta). \\] <p>If we let \\(x = x^*\\), and according to \\(f(x^*)=0\\), we get </p> \\[ 0 = f(x_0) + (x^*-x_0)f'(x_0) + \\frac{(x^*-x_0)^2}{2}f''(\\zeta) \\] <p>neglecting the square item, we get </p> \\[ 0 \\approx f(x_0) + (x^*-x_0)f'(x_0) \\] <p>to represent \\(x^*\\), we get</p> \\[ x^* = x_0 - \\frac{f(x_0)}{f'(x_0)} \\] <p>Then we can define the iterative relation as</p> \\[  x_n = x_{n-1} - \\frac{f(x_{n-1})}{f'(x_{n-1})}  \\] <p>The following statement guarrantees the convergence of the above iterative method.</p> <p>\u725b\u987f\u6cd5\u6536\u655b\u6761\u4ef6 | conditions for convergence of Newton's method</p> <p>Let \\(f \\in C^2[a, b]\\). If \\(p \\in (a, b)\\) is such that \\(f (p) = 0\\) and \\(f'( p) \\neq 0\\), then there exists a \\(\\delta &gt; 0\\) such that Newton\u2019s method generates a sequence \\(\\{p_n\\}_{n=1}^{\\infty}\\) converging to \\(p\\) for any initial approximation \\(p_0 \\in [p \u2212 \\delta, p + \\delta]\\).</p> <p>Prove it.</p> HintsProof <ul> <li>make use of the condition \\(f(p) = 0\\) and \\(f'(p)\\neq 0\\)</li> </ul> <p>for \\(x \\in (a, b)\\), we aim to find a narrower interval \\((x^*-\\delta, x^*+\\delta)\\) to have \\(g(x)\\) map into itself. That is, </p> \\[ g(x)\\leq k, \\forall k\\in (0,1) \\] <p>Firstly, \\(f'(p)\\neq 0\\) implies that \\(\\exists \\delta_1 &gt;0\\) such that \\(f'(x)\\neq 0, \\forall x \\in [x^* - \\delta, x^*+\\delta]\\subset [a, b]\\).</p> <p>THus, we have</p> \\[ g'(x) = \\frac{f(x)f''(x)}{(f'^2(x))} \\] <p>capable of dividing non-zero numbers.</p> <p>Since \\(f\\in C^2[a,b]\\), we have \\(g' \\in C^1[x^*-\\delta_1, x^*+\\delta_1]\\) for the exact solution \\(x^*\\), we have \\(f(x^*)=0\\), so </p> \\[ g'(x^*) = 0 \\] <p>which implies that \\(\\exists 0&lt;\\delta &lt; \\delta_1\\), such that </p> \\[ g'(x)\\leq k, \\forall k \\in [x^*-delta, x^*+\\delta] \\] <p>By differential Mean Value Theorem, for \\(x \\in [x^*-delta, x^*+\\delta], \\exists \\zeta \\in [x, x^*]\\) such that </p> \\[ |g(x)-g(x^*)|=g'(\\zeta)|x - x^*|\\leq k|x-x^*|&lt;|x-x^*| \\] <p>which means that \\(g\\) maps into itself. By Fixed-Point Theorom, the sequence defined by Newton's method converges. </p> <ul> <li>Secant Method It may not be easy to find derivarive of function \\(f\\), so we can use difference instead. That is, we have to store two adjacent points for calculating differnce</li> </ul> \\[ f'(x^{k}) \\approx \\frac{f(x^{k}) - f(x^{k-1})}{x^{k}-x^{k-1}} \\] <p>generate \\(p_{k+1}\\) using the above approximation and iterate.</p>"},{"location":"courses/Numerical_Analysis/NA/Solve_Equ/#order-of-convergence","title":"\u6536\u655b\u9636\u6570 | Order of Convergence","text":"<p>So how to identify the speed of convergence? The following definition gives a glimpse.</p> <p>Suppose \\(\\{p_n\\}_{n=1}^{\\infty}\\) is a sequence that converges to \\(p\\), with \\(p_n \\neq p (\\forall n)\\). If positive constants \\(\\lambda\\) and \\(\\alpha\\) exist with</p> \\[ \\lim_{n\\rightarrow \\infty}{\\frac{|p_{n+1}-p|}{|p_n-p|^{\\alpha}}}=\\lambda \\] <p>then \\(\\{p_n\\}_{n=0}^{\\infty}\\) converges to \\(p\\) of order \\(\\alpha\\), with asymptotic error constant \\(\\lambda\\).</p> <ul> <li>(i) If \\(\\alpha=1 (\\lambda&lt;1)\\), the sequence is linearly convergent.</li> <li>(ii) If \\(\\alpha=2\\), the sequence is quadratically convergent.</li> </ul> <p>The following theorem gives a sufficient condition for linear convergence.</p> <p>\u7ebf\u6027\u6536\u655b\u7684\u5145\u5206\u6761\u4ef6 | sufficient condition of linear convergence</p> <p>Let \\(g \\in C[a, b]\\) be such that \\(g(x) \\in [a, b], \\forall x \\in [a, b]\\). Suppose, in addition, that \\(g\\) is continuous on \\((a, b)\\) and a positive constant \\(k &lt; 1\\) exists with</p> \\[ |g'(x)|\\leq k \\quad \\forall x \\in (a, b) \\] <p>If \\(g'(p) \\neq 0\\), then for any number \\(p_0=p\\) in \\([a, b]\\), the sequence </p> \\[ p_n=g(p_{n-1}) \\quad \\forall n \\geq 1 \\] <p>converges only linearly to the unique fixed point \\(p\\) in \\([a, b]\\).</p> <p>Prove it.</p> Hints <p>Prove that linear convergence represents \\(\\exists alpha=1, lambda\\), such that \\(\\lim\\limits_{n\\leftarrow \\infty}\\frac{|p_{n+1}-p^*|}{|p_{n}-p^*|} = \\lambda\\).</p> <ul> <li>multiple roots We see that the speed of convergence is limited by multiple roots.</li> </ul> <p>Here we have modified Newton's Method:</p> \\[ g(x)= x - \\frac{f(x)f'(x)}{f'^2(x)-f(x)f''(x)} \\]"},{"location":"courses/Numerical_Analysis/NA/Solve_Equ/#accelerating-convergence","title":"\u52a0\u901f\u6536\u655b | Accelerating convergence","text":"<ul> <li>Aitken's \\(\\Delta^2\\) Method</li> </ul> <p>Suppose \\(\\{p_n\\}_{n=0}^{\\infty}\\) is a linearly convergent sequence with limit \\(p\\). To motivate the construction of a sequence \\(\\{\\hat{p}_n\\}_{n=1}^{\\infty}\\) that converges more rapidly to \\(p\\) than does \\(\\{p_n\\}_{n=0}^{\\infty}\\), let us first assume that the signs of \\(p_n-p\\), \\(p_{n+1}-p\\) and \\(p_{n+2}-p\\) agree and that \\(n\\) is sufficiently large that </p> \\[ \\frac{p_{n+1}-p}{p_n-p} \\approx \\frac{p_{n+2}-p}{p_{n+1}-p} \\] <p>Then solving for \\(p\\) gives</p> \\[ p \\approx \\frac{p_{n+2}p_n-p_{n+1}^2}{p_{n+2}-2p_{n+1}+p_n} \\] <p>And to get \\(p_n\\) out gives</p> \\[ \\begin{align*} p &amp;\\approx p_n - \\frac{(p_{n+1}-p_n)^2}{p_{n+2} - 2p_{n+1} + p_n}\\\\ \\Rightarrow \\hat{p}_n &amp;= p_n - \\frac{(\\Delta p_n)^2}{\\Delta p_{n+1}-\\Delta p_{n}} \\quad \\text{(denote $\\Delta p_n = p_{n+1} - p_n$)}\\\\ &amp;= p_n - \\frac{(\\Delta p_n)^2}{\\Delta^2 P_{n}}  \\end{align*}  \\] <ul> <li>Steffensen's Method</li> </ul> <p>The following thought is based on that the generated sequence \\(\\hat{p}\\) is a better approximation to true \\(p^*\\). We make use of the constructed sequence \\(\\{\\hat{p}_n\\}\\) to update the original sequence \\(\\{p_n\\}\\). That is, after generating a new \\(\\hat{p}\\), we can update \\(p_0 \\leftarrow p\\).</p>"},{"location":"courses/cpp/","title":"Index","text":""},{"location":"courses/cpp/#c","title":"C++","text":"<p>Reference</p> <p>C++ Primer Plus, sixth edition, Stephen Prata. </p>"},{"location":"courses/cpp/#elementary","title":"Elementary","text":""},{"location":"courses/cpp/#class-object","title":"Class &amp; Object","text":""},{"location":"courses/cpp/#inheritance","title":"Inheritance","text":""},{"location":"courses/cpp/#dynamic-memory-allocation","title":"Dynamic Memory Allocation","text":""},{"location":"courses/cpp/#exception","title":"Exception","text":""},{"location":"courses/cpp/#streams","title":"Streams","text":""},{"location":"courses/cpp/#appendix","title":"Appendix | \u9644\u5f55","text":""},{"location":"courses/cpp/#template","title":"Template | \u5143\u4ee3\u7801","text":"<p>Reuse source code. It generates code for compiling.</p> <ul> <li>Generic programming(\u6cdb\u578b, universal type)vs \u8303\u5f62 model shape. Use type as parameters in class or function definitions.</li> </ul> <p>Function Template | \u51fd\u6570\u6a21\u677f</p> C++<pre><code>void myswap(int &amp;x, int &amp;y) // only calls when passing in \"int\"\n{\n  int temp = x;\n  x = y;\n  y = temp;\n}\n\n// T is tyep parameter class\ntemplate &lt;class T&gt;\nvoid myswap(T &amp;x, T&amp;y)\n{\n  T temp = x;\n  x = y;\n  y = temp;\n}\n\nint main()\n{\n  int a=6,a=5;\n  double c=1,d=2;\n  myswap(a.b); // right\n  myswap(a,c); // error, parameters a,c must be the same type\n}\n\n// but if we do this\ntemplate &lt;class T1,class T2&gt;\nvoid myswap(T1 &amp;x, T2&amp;y)\n{\n  T1 temp = x;\n  x = y;\n  y = temp;\n}\n</code></pre> <p>compiler need to know the type <code>T</code> if we do not explicitly give varible of type.</p> C++<pre><code>template&lt;class T&gt;\nvoid f()\n{\n  T a;\n}\n\nint main()\n{\n  f&lt;double&gt;();\n}\n</code></pre> <p>Class Template | \u7c7b\u6a21\u677f</p> <p>A class declaration.</p> <p>All the functions in the template are function template.</p> C++<pre><code>template &lt;class T&gt;\nclass vector{\n  public:\n  vector(int s)size(s){}\n  T&amp; operator[](int s);\n};\n\n// definition\ntemplate &lt;class T&gt;\nT&amp; vector&lt;T&gt;::operator[](int s){\n  return 0;\n}\n\nTemplate nest:\n\n```c++\nvector&lt; vector&lt;double*&gt; &gt;\n</code></pre> <p>Template arguments can be constant expresstions, Non-type parameters with a default argument.</p> C++<pre><code>template&lt;class T, int bounds=100&gt;\nclass FixedVector{\npublic:\n  FixedVector();\n  T &amp; operator[](int);\nprivate:\n  T elements[bounds]; \n};\n\ntemplate&lt;class T, int bounds=100&gt;\nT &amp; FixedVector&lt;T, bounds&gt;::operator[](int i){\n  return elements[i];\n}\n\nint main()\n{\n  FixedVector&lt;int, 10&gt; v1;\n  FixedVector&lt;int&gt; v2; // use default parameter 100\n\n}\n</code></pre> <p>All the content of template should be put in <code>.h</code> file for they are only declarations. Remember we also have to put inline function and static member variables in <code>.h</code> file.</p>"},{"location":"courses/cpp/#stlstandard-template-library","title":"STL(standard template library)","text":"<p>All the following identifiers in library are in <code>std</code> namespace.</p> <p>This is also called Container(lowercase).</p> <ul> <li>Sequential </li> </ul> <p>Sequential classes</p> <ul> <li>vector(variable array)</li> </ul> <p>It is easy to use index to search and save memory</p> C++<pre><code>vector&lt;typeName&gt; vt(n_ele);\nvector&lt;int&gt; x;\nx.push_back(1);\n</code></pre> <p>reload p++ </p> <p>C++<pre><code>vector&lt;int&gt;::iterator p;\nfor(p=x.begin(); p&lt;x.end(); p++)\n  cout &lt;&lt; *p &lt;&lt; \u201c \u201c;\n</code></pre> or  C++<pre><code>for(auto k: x)\ncout &lt;&lt; k &lt;&lt; \u201c \u201c;\n</code></pre></p> <p><code>auto</code> means the compiler can identify the type of the variable itself.</p> <p>for \u4ec5\u7528\u4e8e\u904d\u5386\u5168\u90e8(range-based for loops)</p> <p>\u653e\u8fdb\u5bb9\u5668\u91cc\u7684\u5185\u5bb9\u662fclone</p> <p>\u76f4\u63a5\u4f7f\u7528\u4e0b\u6807\uff0c\u4e0d\u4f7f\u7528<code>push_back</code>/<code>pop</code> \u662f\u4e0d\u4f1a\u6539\u53d8size\u7684\u3002</p> C++<pre><code>x[999] = 9; // no error but invalid\n</code></pre> <ul> <li> <p>array</p> </li> <li> <p>fixed length, use stack.</p> </li> </ul> <p>C++<pre><code>array&lt;typeName, n_ele&gt; arr;\n</code></pre> <code>n_ele</code> should be constant</p> <ul> <li>list(double-linked-list) It can insert/delete items very quickly.</li> </ul> <p>C++<pre><code>list&lt;int&gt; L;\nlist&lt;int&gt;::iterator li;\nli = L.begin();\nL.erase();\n++li; //wrong! li has been removed.\n</code></pre> right:</p> C++<pre><code>li = L.erase(li)\nli now points to nest node\n</code></pre> <ul> <li>others</li> </ul> <p>Deque(double ended queue), forward_list, maps(HashMap)</p> C++<pre><code>map&lt;string, float&gt; price;\nprice[\u201csnapple\u201d] =0.75\n</code></pre>"},{"location":"courses/cpp/#cast-operators","title":"Cast Operators","text":"<ul> <li>static_cast</li> </ul> <p>It is used in basic type/class conversion, (void *) to goal type pointer conversion, child to parent conversion. And it takes place in compiling.</p> C++<pre><code>int main(){\n  int a=10;\n  double b = static_cast&lt;double&gt;(a); // int to double\n\n  class Base {};\n  class Derived: public Base {};\n  Derived d;\n  Base* basePtr = static_cast&lt;Base*&gt;(&amp;d); // child to parent\n\n  void* voidPtr = &amp;a;\n  int* intPtr = static_cast&lt;int *&gt;(voidPtr); // void * to int *\n}\n</code></pre> <ul> <li>dynamic_cast</li> </ul> <p>usually used in down cast(see parent as child) in polymorphism.</p> C++<pre><code>class Base {\n  virtual void foo{}\n};\nclass Derived : public Base {\n};\n\nint main(){\n  Base* basePtr = new Derived();\n  Derived * derivedPtr = dynamic_cast&lt;Derived*&gt;(basePtr); // check when running program, better for locate error\n  if(derivedPtr){\n    cout &lt;&lt; \"cast succeed.\\n\";\n  }else{\n    cout&lt;&lt; \"cast failed.\\n\";\n  }\n  delete basePtr;\n}\n</code></pre> <ul> <li>const_cast</li> </ul> <p><code>volatile</code> means the variable is changable and should not be optimized by compiler.</p> <p><code>const_cast</code> remove a const variable into non-const.</p> C++<pre><code>int main(){\n  const int a=10;\n  int * b = const_cast&lt;int *&gt;(&amp;a); // remove const \n  *b = 20; \n}\n</code></pre> <ul> <li>reinterpret_cast</li> </ul> <p>used in low class conversion, which is dangerous.</p> C++<pre><code>int main(){\n  int a=65;\n  char* chPtr = reinterpret_cast&lt;char*&gt; (&amp;a);\n  cout&lt;&lt; *chPtr &lt;&lt; endl; // output 'A', whose ASCII is 65.\n}\n</code></pre>"},{"location":"courses/cpp/Class_Object/","title":"Class &amp; Object","text":""},{"location":"courses/cpp/Class_Object/#classstruct","title":"Class(Struct)","text":"<p>Intuitively, we put a function into a <code>struct</code> and it bacome <code>class</code>.(we can use functional point in C) act like a type.</p> <ul> <li>Definition We all know the principle of designing:</li> </ul> <p>separated .h &amp;.cpp are used to define one class</p> <ul> <li> <p>Header file(.h): class declaration&amp;prototype</p> </li> <li> <p>Source file(.cpp): all the bodies of functions</p> </li> </ul> <p>Hidden parameter: <code>this</code>, which is a pointer to the variable.</p> Publicprivate C++<pre><code>struct point{\n  float x;\n  float y;\n  void init(int x, int y){\n    this-&gt;x=x;\n    this-&gt;y=y;\n  }\n  void print(){\n    cout &lt;&lt; x &lt;&lt; \", \" &lt;&lt; y &lt;&lt;endl;\n  }\n}\n</code></pre> C++<pre><code>class point{\nprivate:\n  float x;\n  float y; //the above data is protected.\n\npublic://the followings can be accessed from outside\n  void init(int x, int y){\n    this-&gt;x=x;\n    this-&gt;y=y;\n  }\n  void print(){\n    cout &lt;&lt; x &lt;&lt; \", \" &lt;&lt; y &lt;&lt;endl;\n  }\n}\n</code></pre>"},{"location":"courses/cpp/Class_Object/#object-an-instance-of-class","title":"Object | an instance of class","text":""},{"location":"courses/cpp/Class_Object/#ctor-constructor","title":"C\u2019tor (constructor)","text":"<ul> <li>Initialization List</li> </ul> <p>C++<pre><code>class A{\n  private:\n    int i;\n    int j= i;\n\n  public:\n    A():i(11){}\n}\n</code></pre> Initialization versus Assignment</p> <p>C++<pre><code>Stu::Stu(string s):name(s){} // better to use\nStu::Stu(string s){name=s;}\n</code></pre> Equivalent:</p> C++<pre><code>string place(\u201cHangzhou\u201d);\nstring place = \u201cHangzhou\u201d;\n\nint i = 6;\nint i(6); \n</code></pre> <p>A constructor function:</p> <p>C++<pre><code>point::point(int x, int y){\n  this-&gt;x=x;\n  this-&gt;y=y;\n}\n</code></pre> then use it:</p> <p>C++<pre><code>point a(1,2);\n</code></pre> And if the constructor function only takes in one parameter, like </p> <p>C++<pre><code>point::point(int dep){\n  this-&gt;x=this-&gt;y=dep;\n}\n</code></pre> then we can use it to initialize:</p> <p>C++<pre><code>point a(1);\npoint a=10;\n</code></pre> we can not initialize a point like we do in struct:</p> C++<pre><code>point a ={1,2}; // this can succeed only when the class does not have a constructor function and the parameter is public.\n</code></pre> <ul> <li>Default Constructor</li> </ul> <p>It is a function that can be called with no arguments input.</p> <p>If we don\u2019t give any constructor function, then the system can give one that does nothing.</p> <p>If we offer a constructor function that takes input of more than one parameter, we have no default constructor function.</p> <p>C++<pre><code>struct Y{\n  float y;\n  int i;\n  Y(int a);\n}\n</code></pre> then:</p> C++<pre><code>Y y1[] = {Y(1), Y(2)}; // right\nY y2[2] {Y(1)}; // wrong\nY y3[7]; //wrong\nY y4;  //wrong\n</code></pre>"},{"location":"courses/cpp/Class_Object/#dtordestructor","title":"D\u2019tor(destructor)","text":"<p>This function would be implemented before the memory is recycled.</p> <p>To design it, please add tilde <code>~</code> before the name of the class. The function have no input and output.</p> <p>It will delete local objects in a reverse manner.(caused by stack action)</p> C++<pre><code>class point{\nprivate:\n  float x;\n  float y;\n//the above data is protected.\n\npublic://can be accessed from outside\n  point(int dep); // reload\n  point(int x, int y);\n  ~point();\n  void print();\n}\n</code></pre>"},{"location":"courses/cpp/Class_Object/#using-class","title":"Using Class","text":""},{"location":"courses/cpp/Class_Object/#function-overloading","title":"Function overloading","text":"<p>For function with the same name, compiler will choose from different function according to different input/labels.</p> <p>Pay attention to primitive type input</p> C++<pre><code>void f(int i){}\nvoid f(double d){}\n\nint main() {\n  f(\u2018a\u2019); //despite smaller than int, it can be transformed\n  f(2); // ambiguous\n  f(2L); // ambiguous\n  f(3.2); // ok\n}\n</code></pre>"},{"location":"courses/cpp/Class_Object/#default-argument","title":"Default argument","text":"<p>we should give default argument from right to left:</p> C++<pre><code>int harpo(int n, int m, int j=5);\nint chico(int n, int m=6, int j); //illegal\n</code></pre> <p>default argument must be written in declaration, i.e. in \".h\" file. We can not write it in definition, but always in calling back.</p> C++<pre><code>void fun(int a, int b = 1, char c = 'a'); // \u58f0\u660e\u4e2d\u6307\u5b9a\u9ed8\u8ba4\u53c2\u6570\n\nvoid fun(int a, int b, char c) { // \u5b9a\u4e49\u4e2d\u4e0d\u80fd\u518d\u6307\u5b9a\u9ed8\u8ba4\u53c2\u6570\n    ...// \u51fd\u6570\u5b9e\u73b0\n}\n</code></pre>"},{"location":"courses/cpp/Class_Object/#friend","title":"Friend","text":"<p>A declaration, which cannot append.</p> <p>Two classes can be a friend relationship when they cannot be \"is-a\"(public inheritance) or \"has-a\"(embedding), like Tv and remote.</p> C++<pre><code>class Tv\n{\n  friend class Remote; // a class Remote can access all the private member functions/variables of Tv.\n}\n</code></pre> <p>In fact, a class can use member functions of another class to achieve some usage, not by accessing directly its member variables. Only some functions must access these member variables.</p> C++<pre><code>class Tv\n{\n  friend void Remote::set_chan(Tv &amp;t, int c); // here only this function can affect private member variables of Tv.\n  // compiler need to first know Remote class.\n};\n</code></pre> <p>So </p> C++<pre><code>// this is right:\nclass Tv; //forward declaration\nclass Remote{...}; // Remote::set_chan declaration\nclass Tv{...};\n\n// this is wrong\nclass Remote; // forward declaration\nclass Tv{...}; // Tv declaration in which we have set_chan which has not been declared.\nclass Remote{...};\n</code></pre> <p>But if <code>Remote</code> has inline function which calls a function of <code>Tv</code>, the above right forward declaration does not work. So the solution becomes </p> C++<pre><code>class Tv;\nclass Remote{...}; // Tv0using methods declaration without definition.\nclass Tv{...};\n</code></pre>"},{"location":"courses/cpp/Class_Object/#inline-function","title":"inline function | \u5185\u8054","text":"<p>It can check the type, which is better than Macro(\u5b8f)!</p> <p>C++<pre><code>inline double square(double x);\ninline double square(double x){ return x*x;}\n</code></pre> is only a declaration instead of a definition, so the compiler must see the body of function!</p> <ul> <li>compiler must see body so it can compile!(not just write down as a declaration)</li> </ul> <p>Body of inline function must be put in \".h\" files so it can be used in another file!</p> <ul> <li>if you write a inline function in a \".cpp\" file, you mean the function should only be used locally.</li> </ul> <p>Function that can be used <code>inline</code></p> <p>Function that can be used <code>inline</code>:</p> <ul> <li>small function</li> <li>frequently called function</li> </ul> <p>others that cannot be used <code>inline</code>:</p> <ul> <li>long function</li> <li>recursive function</li> </ul> <p>\u7c7b\u7684\u5185\u8054\u51fd\u6570\u53ef\u4ee5\u5728\u7c7b\u4f53\u5185\u5b9a\u4e49\uff0c\u6b64\u65f6\u4e0d\u9700\u8981\u663e\u5f0f\u4f7f\u7528 inline \u5173\u952e\u5b57\u3002\u5982\u679c\u9009\u62e9\u5728\u7c7b\u4f53\u5916\u5b9a\u4e49\uff0c\u5219\u9700\u8981\u4f7f\u7528 inline \u5173\u952e\u5b57\u3002</p>"},{"location":"courses/cpp/Class_Object/#const","title":"Const","text":"<p>constants are variables -    (instant \u7acb\u5373\u6570)</p> C++<pre><code>const int a = 6; \n</code></pre> <ul> <li>literal -&gt; 6</li> <li>constant -&gt; a</li> </ul> <p>Distinguish:</p> C++<pre><code>String p1(\u201cFred\u201d);\nconst string * p = &amp;p1; //(1)\nstring const * p = &amp;p1; //(2)\nstring * const p = &amp;p1; //(3)\n</code></pre> HintsAnswer <p>Const only restrict one variable.</p> <p>(1)(2) are the same: <code>(*p)</code> can not change; That is, we cannot change <code>p1</code> through <code>(*p)</code>.</p> <p>(3) means the pointer <code>p</code> itself cannot change but p1 itself can still change.</p> C++<pre><code>int i; \nconst int ci = 3;\n\nint *ip = &amp;i; \nint *ip = &amp;ci; // illegal, that is, a changeable pointer now points a non-changeable variable, which is illegal in compiler.\n\nconst int * cip = &amp;i;\nconst int * cip = &amp;ci;\n</code></pre> <ul> <li>Passing &amp; returning by const value</li> </ul> <p>We do this in case that we change some value in a function. So we let compiler check.</p> <ul> <li>Const object </li> </ul> <p>in a function, we pass a pointer instead of a copy!</p> <p>(1) public</p> <p>(2) change value by inner function</p> C++<pre><code>int get_day(void) const;\nint get_day(void) const {return day;}\n\nconst A a; // must provide a an initial value!(or constructor) because later we cannot change it! Like below:\n\nconst int i=1; // we cannot do this before C11\n\npublic:\n  A(int k): i(k){}\n</code></pre> <ul> <li>constant i cannot change during execution, but need a value in initialization.</li> </ul> <p>C++<pre><code>class A {\nprivate:\n  int i=0;\npublic:\n  void f() {i=10;\n    cout &lt;&lt; \u201cA::f()\u201d&lt;&lt; end;\n  }\n  void f() const {\n    cout &lt;&lt; \u201cA::f() const\u201d&lt;&lt; end;\n  }\n}\n</code></pre> The above means:</p> C++<pre><code>public:\n  void f(A *this) {i=10;\n    cout &lt;&lt; \u201cA::f()\u201d&lt;&lt; end;\n  }\n  void f(const A *this) const {\n    cout &lt;&lt; \u201cA::f() const\u201d&lt;&lt; end;\n  }\n</code></pre> <p>So:</p> <p>C++<pre><code>const A a;\nA b;\na.f(); // &lt;&lt;\u201cA::f() const\u201d\nb.f(); // &lt;&lt;\u201cA::f()\u201d\n</code></pre> The above code is using overload, and there are two different f() that have been distinguished by \u201cconst\u201d.</p> <p>So a const object can only use function attached to \"const\" and cannot use function with no \"const\".</p>"},{"location":"courses/cpp/Class_Object/#static","title":"Static","text":"<p>on members which are </p> <ul> <li>Hidden</li> <li>Persistent</li> </ul> <p>static variable is actually global variable.</p> <p>static function can only access static variable!</p> <p>C++<pre><code>static int i; // can be accessed by all the objects of same class\n</code></pre> we must define the static variable (global variable) before main!</p> <p>C++<pre><code>int A::i = 0;\n</code></pre> Note: without static!</p> <p>C++<pre><code>static void sf(){\n  i++;\n}\n</code></pre> we can call static function without creating an object! Just use class!</p> C++<pre><code>int main(){\n  A::sf();\n}\n</code></pre> <p>We can eliminate global variable because we can limit it in class, which can prevent arbitrary changes!</p>"},{"location":"courses/cpp/Class_Object/#overloaded-operator","title":"Overloaded Operator","text":"<p>operators of primitive class cannot be changed. C++<pre><code>Integer x(1), y(2);\nz = x + y // x.operator+(y)\nz = x + 3 // x.operator+(Interger(3))\nz = 3 + x // not allowed\n</code></pre></p> <p>Global operator. C++<pre><code>z = 3 + 7; // pass 10 to initialize z\n</code></pre></p> <p><code>= () [] -&gt; -&gt;*</code> must be members and all other binary operators(\u53cc\u76ee) should be non-members.</p> <p>Will the operator change the operation number\uff1f</p> <ul> <li>If not, use <code>const</code>.</li> </ul> Common Operators<pre><code>// + - * / % ^ &amp; | ~\nconst T operatorX(const T&amp;I, const T&amp;L)\n\n// ! &amp;&amp; || &lt; &lt;= == &gt;= &gt;\nbool operatorX(const T&amp;I, const T&amp;L)\n\n// []\nE&amp; T::operator[](int index);\n\n// prefix ++ -- e.g. ++a\nconst Integer&amp; Integer::operator++(){\n  *this +=1;\n  return *this; // reference for old one, without copying new\n}\n\n// postfix ++ -- e.g. a++\nconst Integer&amp; Integer::operator++(int){\n  Integer old(*this);\n  ++(*this);\n  return old;\n}\n\n// Relational operators\nbool Integer::operator==(const Integer &amp; rhs) const{\n  return i == rhs.i;\n}\n\nbool Integer::operator!=(const Integer &amp; rhs) const{\n  return !(*this == rhs);\n}\n\nbool Integer::operator&lt;(const Integer &amp; rhs) const{\n  return !(i &lt; rhs.i);\n}\n\nbool Integer::operator&gt;(const Integer &amp; rhs) const{\n  return rhs &lt; *this;\n}\n\nbool Integer::operator&lt;=(const Integer &amp; rhs) const{\n  return !(rhs &lt; *this);\n}\n\nbool Integer::operator&gt;=(const Integer &amp; rhs) const{\n  return !(*this &lt; rhs);\n}\n\n// operator [] must be member function, single argument\n</code></pre> <ul> <li>Extractor &amp; Inserter</li> </ul> Extractor &amp; Inserter<pre><code>// stream extractor cin &gt;&gt;: global function\noperator&gt;&gt;(istream &amp;is, T&amp; obj){\n  // ...\n  return is; // always this\n}\n\ncin &gt;&gt; a &gt;&gt; b // ((cin &gt;&gt; a ) &gt;&gt; b)\n\n// stream inserter cout &lt;&lt;: global function\noperator&lt;&lt;(ostream&amp; os, const T&amp; obj){\n  // ...\n  return os;\n}\n</code></pre> <ul> <li>Assignment <code>=</code></li> </ul> =<pre><code>// member function\n// usually before calling \"=\" the object being assigned already has had sth. \nT &amp; T::operator=(const T &amp; rhs){\n  // check for self assignment\n  if(*this != ths) // otherwise will cause error (you will delete allocated memory and new one according to the deleted memory)\n  {\n    // ...\n  }\n  return *this;\n}\n</code></pre>"},{"location":"courses/cpp/Dynamic_Alloc/","title":"Dynamic memory allocation","text":""},{"location":"courses/cpp/Dynamic_Alloc/#new-delete","title":"<code>new</code> &amp; <code>delete</code>","text":"<p><code>new</code> (operator) has 2 steps:</p> <ul> <li>allocate space</li> <li>call the constructor function</li> </ul> C++<pre><code>int *p1 = new int // malloc(sizeof(int)); + constructor\nint *p2 = new int [10] // continuous space allocated\n</code></pre> <p><code>delete</code> + pointer</p> <p>If delete a constructed type, it will implement <code>D\u2019tor</code> function</p> <p>C++<pre><code>p2 = new student [10];\ndelete p2; // remove the first one\ndelete[] p2; // remove whole 10 objects\n</code></pre> it is safe to delete a <code>NULL</code>.</p>"},{"location":"courses/cpp/Dynamic_Alloc/#copy-constructor","title":"Copy constructor","text":"<p>has a unique signature</p> <p>C++<pre><code>T::T(const T&amp;)\n</code></pre> call by reference</p> <p>compiler (in-line) would do it automatically.</p> <p>But how?</p> <ul> <li>member-wise (versus bit-wise)</li> </ul> <p>if it has a class defined, it will iteratively call its copy function.</p> <p>It is neccesary to define a copy constructor when you have pointer member or what you don't want to be copied in your class.</p> C++<pre><code>A b(a);\nA c = a;\n\n// define function passing in value\nvoid f(A aa)\n{\n\n}\n\n// return value\nA f()\n{\n  A aa(19);\n  return aa;\n}\n</code></pre> <p>Assignment: can be done alot of time. Ctor can only be called once.</p>"},{"location":"courses/cpp/Elementary/","title":"Elementary","text":""},{"location":"courses/cpp/Elementary/#basic-ideas","title":"Basic Ideas","text":"<ul> <li>object-oriented | \u7269\u4ef6\u5bfc\u5411-\u9762\u5411\u5bf9\u8c61</li> </ul> <p>An object, or entity(either visible or invisible), is a variable in programming languages, made up of two primary components:</p> <ul> <li>Attibutes, or Data, representing the object's properties and status</li> <li>Services, or Operations, refered to as functions in programming.</li> </ul> <p>C++ Focuses on things instead of operations.</p>"},{"location":"courses/cpp/Elementary/#key-words","title":"Key words","text":"<ul> <li>interface</li> <li>communications</li> <li>protection</li> <li>the hidden implementation</li> <li>encapsulation</li> </ul>"},{"location":"courses/cpp/Elementary/#oop-characteristics","title":"OOP characteristics","text":"<ul> <li>Everything is an object.</li> <li>A program is a bunch of objects telling each other what to do/(not how to do) by sending messages</li> <li>Each object has its own memory made up of other objects.</li> <li>Every object has a type.</li> <li>All objects of a particular type can receive the same messages. (Using the method to distinguish between different types or classes)</li> </ul>"},{"location":"courses/cpp/Elementary/#header-files","title":"Header Files","text":"<p>To prevent defining repeatedly:</p> <p>x.h<pre><code># pragma once //\u53ea\u5305\u542b\u8fd9\u4e2a\u5934\u6587\u4ef6\u4e00\u6b21\n</code></pre> or the same as C language:</p> log.h<pre><code># ifndef _LOG_H\n# define _LOG_H\n//...//\n# endif\n</code></pre>"},{"location":"courses/cpp/Elementary/#declaration","title":"declaration","text":"<ul> <li>external variables</li> <li>function prototypes</li> <li>class/struct declarations</li> </ul>"},{"location":"courses/cpp/Elementary/#resolver","title":":: resolver","text":"::<pre><code>&lt;Class Name&gt;::&lt;function name&gt;//not free\n::&lt;function name&gt;\n\nvoid S::f(){\n  ::f();//would be recursive otherwise\n  ::a++;//select the global a\n  a\u2014;//select the partial a\n}\n</code></pre>"},{"location":"courses/cpp/Elementary/#string","title":"String","text":"C++<pre><code>#include&lt;string&gt;\nString age, name; \ncin &gt;&gt; age &gt;&gt; name;\ncout &lt;&lt; name; \n</code></pre> <ul> <li>A class type, not a primitive type.</li> <li>Initially <code>name</code> is all zero. No matter it is static or global.</li> <li>No <code>\\0</code> at the end of the string.</li> </ul>"},{"location":"courses/cpp/Elementary/#reference","title":"Reference","text":"<p>Reference make use of the idea of a pointer, but is used as a normal variable rather than pointer. It is widely used in passing variables into a function.</p> C++<pre><code>char c;\nchar &amp; r = c;// a reference to c;\n</code></pre> <p>Some notes on reference:</p> <ul> <li>cannot be <code>NULL</code>.</li> <li>cannot calculate.</li> <li>No reference to reference</li> </ul> C++<pre><code>int &amp;* r; // No pointer to reference \nint *&amp; r; // We have reference to pointer\n</code></pre>"},{"location":"courses/cpp/Exception/","title":"Exception","text":"<ul> <li>Exception type</li> </ul> C++<pre><code>class VectorIndexError\n{\npublic:\n  VectorIndexError(int v):m_badValue(v){}\n  VectorIndexError({}\n  void diagnostic(){\n    cout&lt;&lt;\"index\"&lt;&lt; m_badValue &lt;&lt;\"out of range!\";\n  }\nprivate:\n  int m_badValue;\n}\n</code></pre> <p>C++<pre><code>throw &lt;&lt;something&gt;&gt;\n</code></pre> <code>throw</code> raises exception.</p> <ul> <li>Try blocks can select type of exceptions</li> </ul> C++<pre><code>try {...}\n  catch {...}\n</code></pre> C++<pre><code>try{\n  func();\n} catch(VIE v){ // take a single argument\n  cout&lt;&lt; \"8\\n\";\n} catch (...){ // others\n  cout&lt;&lt; \"7\\n\";\n}\n</code></pre> <p>it can re-raise exceptions</p> <ul> <li>Hierarchy of exception types</li> </ul> C++<pre><code>class MathErr{\n  ...\n  virtual void diagnostic();\n};\n\nclass OverflowErr : public MathErr {...}\nclass UnderflowErr : public MathErr {...}\n</code></pre> <p>a catch of parent Exception Type can catch its child type.</p> C++<pre><code>try{\n  throw VIEE(idx);\n}catch (VIE v){\n  cout&lt;&lt; \"7\\n\";\n}catch (...){\n  cout&lt;&lt; \"6\\n\";\n}\n// output 7\n\ntry{\n  throw VIEE(idx);\n}catch (VIEE v){\n  cout&lt;&lt; \"8\\n\";\n}catch (VIE v){\n  cout&lt;&lt; \"7\\n\";\n}catch (...){\n  cout&lt;&lt; \"6\\n\";\n}\n// output 8\n\ntry{\n  throw VIEE(idx);\n}catch (VIE v){\n  cout&lt;&lt; \"8\\n\";\n}catch (VIEE v){ // this expression is useless\n  cout&lt;&lt; \"7\\n\";\n}catch (...){\n  cout&lt;&lt; \"6\\n\";\n}\n// output 8\n</code></pre> <ul> <li> <p>Standard Library Exception</p> </li> <li> <p>Failure in C'tor</p> </li> </ul>"},{"location":"courses/cpp/Inheritance/","title":"Inheritance","text":"<p>Allow sharing of design for</p> <ul> <li>Member data</li> <li>Member functions</li> <li>Interfaces</li> </ul> <p>Advantages:</p> <ul> <li>extendable</li> <li>avoid code duplication</li> <li>code reuse </li> </ul> <p>B is a A:</p> <ul> <li>A: Base/super/parent class</li> <li>B: derived/sub/child class</li> </ul> employee.h<pre><code>class Employee\n{\npublic: \n  Employee(const string&amp; _name, const string &amp; _ssn): name(_name), ssn(_ssn){}\n\n  void print() const\n{\n  cout &lt;&lt; name &lt;&lt; endl;\n  cout &lt;&lt; ssn &lt;&lt; endl;\n}\n\n  void print(const string &amp; msg) const {\n  cout &lt;&lt; msg &lt;&lt; endl;\n  print(); // we rewrite the print function, then the child cannot access print from parent!\n// Name Hide!\n}\n\nprotected:\n// private: // can not access from child class\n  const string name;\n  const string ssn;\n}\n</code></pre> manager.h<pre><code>class Manager: public Employee\n{\npublic:\n  Manager(const string &amp; _name, const string &amp; _ssn, const string &amp; _title): Employee(_name, _ssn), title(_title) {}\n\n  const string &amp; getTitle() const\n{\n  return title;\n}\n\n  void print() const\n{\n  Employee::print();\n  cout &lt;&lt; title &lt;&lt; end;\n}\n\nprotected:\n  const string title;\n}\n</code></pre>"},{"location":"courses/cpp/Inheritance/#polymorphism","title":"Polymorphism","text":"<p>If a child class wants to act different with parent class in the same member function, i.e. it depends on the object that calls the function, so we have to introduce some more mechanisms. That is, </p> <ul> <li> <p>redefine function in child function.</p> </li> <li> <p>use virtual function.</p> </li> </ul> <p>Here comes an example.</p> Brass &amp; Brassplus<pre><code>class Brass\n{\nprivate:\n    std::string fullName;\n    long acctNum;\n    double balance;\n\npublic:\n    Brass(const std::string &amp; s = \"Nullbody\", long an = -1, double bal = 0.0);\n    void Deposit(double amt);\n    virtual void Withdraw(double amt);\n    double Balance() const;\n    virtual void ViewAcct() const;\n    virtual ~Brass() {}\n};\n\n// Brass Plus Account Class\nclass BrassPlus : public Brass\n{\nprivate:\n    double maxLoan;\n    double rate;\n    double owesBank;\n\npublic:\n    BrassPlus(const std::string &amp; s = \"Nullbody\", long an = -1, double bal = 0.0, double ml = 500, double r = 0.11125);\n    BrassPlus(const Brass &amp; ba, double ml = 500, double r = 0.11125);\n    virtual void ViewAcct() const;\n    virtual void Withdraw(double amt);\n    void ResetMax(double m) { maxLoan = m; }\n    void ResetRate(double r) { rate = r; }\n    void ResetOwes() { owesBank = 0; }\n};\n</code></pre> <p>Now the following executable program</p> C++<pre><code>// create an object\nBrass dom(\"DominicBanker\", 11224, 4183.45);\nBrassPlus dot(\"DorothyBanker\", 12118, 2592.00);\n\n// call member function for a real object\ndom.ViewAcct(); // use Brass::ViewAcct();\ndot.ViewAcct(); // use Brassplus::ViewAcct();\n\nBrass &amp; b1_ref = dom;\nBrass &amp; b2_ref = dot;\n</code></pre> <p>Then which function does the following call?</p> C++<pre><code>b1_ref.ViewAcct();\nb2_ref.ViewAcct();\n</code></pre> <p>Compare the difference.</p> Without <code>virtual</code>With <code>virtual</code> C++<pre><code>b1_ref.ViewAcct(); // use refernce type Brass\nb2_ref.ViewAcct(); // use refernce type Brass\n</code></pre> C++<pre><code>b1_ref.ViewAcct(); // use pbject type Brass\nb2_ref.ViewAcct(); // use object type Brassplus\n</code></pre> <p>It is good to declare member function <code>virtual</code> and destructor function <code>virtual</code> in base class.</p> <p>There are two ways of calling functions.</p>"},{"location":"courses/cpp/Inheritance/#up-casting","title":"up-casting (*&amp;=) | \u9020\u578b","text":"<p>We know cast(\u7c7b\u578b\u8f6c\u6362), like</p> C++<pre><code>int i = (int)3.62;\n</code></pre> <p>But up-casting is</p> <ul> <li> <p>is the act of converting from a derived reference or pointer to a base class reference or pointer.</p> </li> <li> <p>take an object of a derived class as an object of the base one.</p> </li> </ul> C++<pre><code>C2* pC2 = new C2();  // pC2 \u662f\u6d3e\u751f\u7c7b C2 \u7684\u6307\u9488\nC1* pC1 = pC2;       // Up-casting\uff1a\u5c06 C2* \u8f6c\u6362\u4e3a C1*\n</code></pre> C++<pre><code>C2 objC2;            // objC2 \u662f\u6d3e\u751f\u7c7b C2 \u7684\u5bf9\u8c61\nC1&amp; refC1 = objC2;   // Up-casting\uff1a\u5c06 C2 \u5bf9\u8c61\u8f6c\u6362\u4e3a C1 \u5f15\u7528\n</code></pre> <p>(\u6539\u53d8\u4e86\u773c\u5149\uff0c\u4e0d\u6539\u53d8\u5185\u5bb9)</p> <p>Other Items</p> <ul> <li>encapsulation \u5c01\u88c5\uff1b\u5305\u88c5\uff1b</li> <li>capsulation \u5c01\u88c5\uff1b[\u9ad8\u5206\u5b50] \u5305\u56ca\u5316\u4f5c\u7528\uff1b</li> <li>bonding \u90a6\u5b9a</li> </ul> C++<pre><code>void func(C1 obj);   // \u51fd\u6570\u63a5\u53d7\u57fa\u7c7b C1 \u7684\u5bf9\u8c61\n\nC2 objC2;            // objC2 \u662f\u6d3e\u751f\u7c7b C2 \u7684\u5bf9\u8c61\nfunc(objC2);         // Up-casting\uff1a\u5c06 C2 \u5bf9\u8c61\u4f5c\u4e3a C1 \u5bf9\u8c61\u4f20\u9012, and slice-off will happen.\n</code></pre>"},{"location":"courses/cpp/Inheritance/#dynamic-binding-virtual-function","title":"Dynamic Binding &amp; virtual function | \u7ed1\u5b9a\u548c\u865a\u51fd\u6570","text":"<p>Two ways of binding</p> <p>Binding: which function to be called.</p> <ul> <li> <p>Static binding: call the function as the code declared, only taking effect on  Non-virtual function. This is known at compiling, which is fast.</p> </li> <li> <p>Dynamic binding: call the function according to the actual object, only taking effect on Virtual function. This is only known at running time, which needs a virtual table to achieve this.</p> </li> </ul> <p>a class which has a virtual function: vtable -&gt; table of the address of virtual functions(8 \u4f4dfor 64)(this is formed while compiling)</p> <p>Notes on virtual function</p> <ul> <li> <p>constructor function could not be <code>virtual</code>. </p> </li> <li> <p>destructor function should be <code>virtual</code>, for derived class may have special object the need to be free and destructed.</p> </li> <li> <p>If derived class does not redefine virtual function, then the real pointer to derived class object would call function of base.</p> </li> <li> <p>redefine function parameters would cause hiding method. That is, overloaded function defined in derived class would cause virtual function of base to be hidden. See the following code as an example.</p> </li> </ul> C++<pre><code>// define class of base\nclass Dwelling {\n    public:\n        virtual void showperks(int a) const;\n    };\n// define derived class\nclass Howel : public Dwelling {\n    public:\n        virtual void showperks() const;\n    };\n\n// use function\nHovel trump;\ntrump.showperks(); // valid\ntrump.showperks(2); // invalid, cause function of base has been hidden\n</code></pre> <ul> <li>redefine return type of virtual function would cause covariance of return type, which is an exception but valid for polymorphism. See the following code as an example.</li> </ul> C++<pre><code>class Dwelling {\n  public:\n      virtual Dwelling&amp; build(int n);\n  };\n\nclass Hove1 : public Dwelling {\n    public:\n        virtual Hove1&amp; build(int n); // same function signature\n    };\n</code></pre> <ul> <li>If virtual function is overloaded in base class, then derived class must redefine all the overloaded function, otherwise would cause non-defined overloaded function to be hidden. See the following code as an example.</li> </ul> C++<pre><code>class Dwelling {\n    public:\n        // three overloaded showperks function\n        virtual void showperks(int a) const;\n        virtual void showperks(double x) const;\n        virtual void showperks() const;\n    };\n\nclass Hove1 : public Dwelling {\n    public:\n        // redefine three overloaded showperks function\n        virtual void showperks(int a) const;\n        virtual void showperks(double x) const;\n        virtual void showperks() const;\n    };\n</code></pre> <p>If you does not need to modify a virtual function, then just use it as base function, like <code>void Hovel::showperks() const {Dwelling::showperks();}</code></p> C++<pre><code>B b;\nA * p = &amp;b;\nlong long **vp (long long **)p;\n// vp is a pointer to *long long, that is, to a pointer of type long long.\n\nvoid (*pf)() = (void (*)())(**vp);\n// pf is a function pointer, which matches the definition of f()\n\npf(); // \u201cB::f()\u201d\n</code></pre> <p>Initialize: A will create A\u2019s vtable but B will than create B\u2019s vtable and change the point to the vtable.</p> <p> </p> C++<pre><code>Manager Pete(\u201cPete\u201d,\u201d4\u201d, \u201cBakery\u201d);\nEmployee* ep = &amp;pete;\nEmployee &amp; er = pete;\n</code></pre> <p>See the following code.</p> C++<pre><code>void fr(Brass &amp; rb) { rb.ViewAcct();}\n\nvoid fp(Brass * pb) { pb-&gt;ViewAcct();}\n\nvoid fv(Brass b) { b.ViewAcct();}\n\nint main() {\n    Brass b(\"Billy Bee\", 123432, 10000.0);\n    BrassPlus bp(\"Betty Beep\", 232313, 12345.0);\n\n    // dynamic binding\n    fr(b);  // uses Brass::ViewAcct()\n    fr(bp); // uses BrassPlus::ViewAcct()\n\n    // dynamic binding\n    fp(&amp;b);  // uses Brass::ViewAcct()\n    fp(&amp;bp); // uses BrassPlus::ViewAcct()\n\n    // up-casting, caused by static binding\n    fv(b);  // uses Brass::ViewAcct()\n    fv(bp); // uses Brass::ViewAcct()\n\n    return 0;\n}\n</code></pre>"},{"location":"courses/cpp/Inheritance/#override","title":"override | \u8986\u76d6","text":"C++<pre><code>class A\n{\nprotected:\n  int i;\npublic:\n  A() {i=10;}\n  virtual void f() {cout&lt;&lt; \u201cA::f()\u201d&lt;&lt; endl;}\n}\n\nclass B: public A\n{\npublic:\n  int i;\npublic: \n  B(){ i=20; cout&lt;&lt; \u201cB::i\u201d &lt;&lt; i &lt;&lt;endl;}\n  void f() {cout&lt;&lt; \u201cB::f()\u201d&lt;&lt; endl;}\n}\n\ncout &lt;&lt; sizeof(A) &lt;&lt;\u201c, \u201c &lt;&lt; sizeof(B) &lt;&lt; endl;\n// not just 4, 8 but a complex one!\n</code></pre> <ul> <li>polymorphic variable(* &amp;)</li> </ul> <p>static type  dynamic type</p>"},{"location":"courses/cpp/Inheritance/#slice-off-obj","title":"Slice off (obj=) | \u5207\u7247","text":"<ul> <li>will copy the content of child object to the parent object while ignoring the extra thins of the child.</li> </ul> <p>C++<pre><code>a = b;\np = &amp;a;\np-&gt;f(); // still execute A\u2019s func\n</code></pre> -   Never redefine an inherited non-virtual function.</p> <p>The vptr is ignored.</p> <ul> <li>Never redefine an inherited default parameter value.</li> </ul> C++<pre><code>virtual void f() =0;\n</code></pre> <ul> <li>Multiple inheritance</li> </ul>"},{"location":"courses/cpp/Inheritance/#abstract-base-class-abc","title":"Abstract Base Class | \u62bd\u8c61\u57fa\u7c7b(ABC)","text":"<p>Abstract base classes:</p> <ul> <li>has pure virtual functions</li> <li>Cannot be instantiated</li> </ul> <p>WE take Ellipse and Circle as an example. Circle is an ellipse, so we can use inheritance, but this could lead to redundency. Because circle is an ellipse if and only if the semimajor and semiminor axis of an ellipse are equal. That is, member data <code>semimajor</code> and <code>semiminor</code> is useless in circle. </p> <p>It is better to think in terms of data. What data does ellipse and circle both have? Could we create an abstract class the holds the public member data and functions needed for circle and ellipse?</p> <p>Check the following code.</p> C++<pre><code>class BaseEllipse // Abstract base class\n{\nprivate:\n    double x; // x-coordinate of the center\n    double y; // y-coordinate of the center\n\npublic:\n    // Constructor with default values for x and y coordinates\n    BaseEllipse(double x0 = 0, double y0 = 0) : x(x0), y(y0) {}\n\n    // Virtual destructor to ensure proper cleanup of derived class objects\n    virtual ~BaseEllipse() {}\n\n    // Function to move the center to new coordinates (nx, ny)\n    // same for circle and ellipse\n    void Move(int nx, int ny) { x = nx; y = ny; }\n\n    // Pure virtual function to calculate the area (must be overridden by derived classes)\n    // difference for circle and ellipse\n    virtual double Area() const = 0; // notation for pure virtual function\n};\n</code></pre> <p>Note that, when a base class has a pure virtual function, it could not be instantiated. Because we might not give a definition in base class, it depends.</p>"},{"location":"courses/cpp/Streams/","title":"Streams","text":"<p>Easy to locate in a one-demension line, which flows in a single direction.</p>"},{"location":"courses/cpp/Streams/#kinds-of-streams","title":"Kinds of streams","text":"<p>Kinds of streams</p> <p>(i) text streams</p> <ul> <li> <p>readable</p> </li> <li> <p>organized in lines</p> </li> </ul> <p>(ii) binary streams</p> C++<pre><code>cout&lt;&lt;\"Hello\\n\";\ncerr&lt;&lt;\"Byebye\\n\";\n</code></pre> <p>if we do</p> C++<pre><code>./a.out &gt; 1 // hello in 1, Byebye in terminal\n./a.out 2&gt; 1 // Byebye in 1, hello in termial\n./a.out &gt;1 2&gt; 2 // hello in 1, Byebye in 2\n</code></pre> <p><code>int get()</code>, a member function, returns the next character in the stream, EOF if no character left. <code>istream&amp; get(char&amp; ch)</code>, a manipulator.</p> C++<pre><code>// copy input to output\nint ch;\nwhile(ch=cin.get()!=EOF){\n  cout.put(ch);\n}\n</code></pre> <p>use free function instead of object</p> C++<pre><code>// recommended\nistream&amp; getline(istream&amp; is, string&amp; str, char delim='\\n') // delim is the end sign\n\n// not recommended, you need to create an array, \ncin.getline(char *, int size)\n</code></pre> <p><code>ignore(int limit=1, int delim=EOF)</code>, a member function, skip over <code>limit</code> number character or until <code>delim</code>. Like skipping until the end of line, or let wrong string to be read, such that program could continue.</p> <p><code>int gcount()</code> returns number of characters just read.</p> Text Only<pre><code>string buffer;\ngetline(cin, buffer);\ncuot&lt;&lt; \"read \"&lt;&lt; cin.gcount()&lt;&lt;\" characters\"\n</code></pre> <p><code>cin.putback(char)</code> pushes a single character into a stream</p> <p><code>char peek()</code> examines next character without reading it.</p> <p><code>flush</code> forces content in ostream to output.</p>"},{"location":"courses/cpp/Streams/#manipulators","title":"Manipulators","text":"<p>Manipulators modify the state of the stream.</p> Manipulator effect type dec, hex, oct set numberic conversion I,O endl insert new line and flush O flush flush stream O setw(int) set field width I,O setfill(char) change fill character(like <code>#</code>) I,O setbase(int) set number base(like 2) I,O ws skip whitespace I setprecision(int) set floating point precision O C++<pre><code>#include &lt;iomanip&gt;\nint n;\ncout &lt;&lt; \"enter number in hexadecimal\" &lt;&lt; flush; // only takes effect once\ncin &gt;&gt; hex &gt;&gt; n; // effexts hold\n</code></pre> C++<pre><code>#include &lt;iostream&gt;\n#include &lt;ismanip&gt;\nmain(){\n  cout &lt;&lt; setprecision(2) &lt;&lt; 1000.243 &lt;&lt; endl;\n  cout &lt;&lt; setw(10) &lt;&lt; \"OK!\";\n}\n\n// returns\n1e03\n          OK!\n</code></pre> <p>we can create our own manipulator.</p> C++<pre><code>ostream&amp; manip(ostream&amp; out){\n  ...\n  return out;\n}\n\nostream&amp; tab (ostraam&amp; out){\n  return out &lt;&lt; '\\t';\n}\n\ncout &lt;&lt; \"Hello\" &lt;&lt; tab &lt;&lt; \"world\"&lt;&lt; endl;\n</code></pre>"},{"location":"courses/cpp/Streams/#stream-flags","title":"Stream flags","text":"C++<pre><code>// set amnd reset using manipulators\nsetiosflags(flags) // set '1'\nresetiosflag(flags) // reset '0'\n\n// using `stream` member function\ncin.setf(flags)\ncout.unsetf(flags)\n</code></pre> C++<pre><code>main(){\n  // add two functions\n  cout.setf(ios::showpos | ios::scientific);\n  cout &lt;&lt; 123 &lt;&lt; \" \" &lt;&lt; 456.78 &lt;&lt;endl;\n  cout &lt;&lt; resetiosflags(ios::showpos) &lt;&lt; 123;\n  return 0;\n}\n\n// output\n+123 +4.567800e+02\n123\n</code></pre> <p><code>clear()</code> returns error stream to GOOD, which is useful for reading failure.</p> <p>checking status: <code>good()</code>, <code>eof</code>, <code>fail()</code>, <code>bad()</code> return true or false.</p> C++<pre><code>int n;\nwhile(cin.good()){// skip if eof or bad\n  cin &gt;&gt; n;\n  if(cin){ // overload operator bool() cin, cin.put\n    cin,ignore(INT_MAX, '\\n');\n    break;\n  }\n  if(cin.fail()){\n    cin.clear();\n    cin,ignore(INT_MAX, '\\n');\n    cout &lt;&lt; \"No good, try again!\" &lt;&lt; flush;\n  }\n}\n</code></pre>"},{"location":"courses/cpp/Streams/#file-streams","title":"File streams","text":"<p>In <code>&lt;fstream&gt;</code>, <code>ifstream</code>, <code>ofstream</code> connects files to streams.</p> <p>Open modes specify how to create files using flags.</p> modes purpose ios::app append ios::ate position at the end ios::binary do binary I/O ios::in open for input ios::out open for output C++<pre><code>#include &lt;iostream&gt;\n#include &lt;fstream&gt;\nint main(int argc, char *argv[]){\n  if(argc !=3){\n    cerr &lt;&lt; \"Usage: copy file1 file2\" &lt;&lt;endl;\n    exit(1);\n  }\n  ifstream in(argv[1]);\n  if(! in){\n    cerr &lt;&lt; \"Unable to open file \" &lt;&lt; argv[1];\n    exit(2);\n  }\n  ofstream out(argv[2]);\n  if(! out){\n    cerr &lt;&lt; \"Unable to open file \" &lt;&lt; argv[];\n    exit(2);\n  } \n}\n</code></pre> <p>more stream operations like <code>open(const char *, int flags, int)</code> to open a specified file, <code>close()</code> to close a stream.</p> C++<pre><code>ifstream inputS;\ninputS.open(\"somefile\", ios::in);\nif(!inputS){\n  cerr &lt;&lt; \"Unable to open somefile\";\n}\n</code></pre>"},{"location":"courses/mpm/","title":"Material Point Method","text":"<p>Reference</p> <p>The Material Point Method for Simulating Continuum Materials, Chenfanfu Jiang, SIGGRAPH 2016 Course Notes Version 1 (May 2016).</p>"},{"location":"courses/mpm/#preliminary","title":"Preliminary","text":""},{"location":"courses/mpm/#contimuum-motion","title":"Contimuum Motion","text":"<p>Here \\(d\\) denotes dimension of the space, usually \\(d=2,3\\).</p> <p>We denote \\(\\pmb{x}\\in \\Omega^t\\subset \\mathbb{R}^d\\) world (deformed) space(coordinates), current position. Physically, we focus on a fixed position in the space and measurethe velocity of whichever particla that is passing by the position.</p> <p>Then we denote \\(\\pmb{X}\\in \\Omega^0\\subset \\mathbb{R}^d\\) material (undeformed) space(coordinates), or initial position. Physically, we measure velocity from a fixed particle, which has its mass and occupies some volume since the beginning.</p> <p>Define </p> \\[ \\pmb{x}=\\phi(\\pmb{X},t) \\] <p>which is usually a bijection. This is associated with the assumption that no two different particles of material ever occupy the same space at the same time. So </p> \\[ \\forall \\pmb{x}\\in \\Omega^t, \\exists ! \\pmb{X}\\in \\Omega^0, \\text{ s.t. } \\phi(\\pmb{X},t)=\\pmb{x}. \\] <p></p> <p>Example</p> <p>For a given initial position \\(\\pmb{X}\\), we have some common function of \\(\\pmb{x}\\).</p> <p>(i) \\(\\pmb{x}=\\pmb{X}+tv(t)\\vec{N}\\)</p> <p>discrete form at time \\(n\\):</p> \\[ \\pmb{x}^{(n)}=\\pmb{x}^{(n-1)}+\\Delta t v^{(n)} \\vec{N}^{(n)} \\] <p>(ii) \\(\\pmb{x}=R\\pmb{X}+\\vec{b}\\)</p> <p>discrete form at time \\(n\\):</p> \\[ \\pmb{x}^{(n)}=R^{(n-1)}\\pmb{x}^{(n-1)}+\\vec{b}^{(n)} \\] <p>The velocity of a given material point \\(\\pmb{X}\\) at time \\(t\\) is a mapping \\(V(\\cdot,t):\\Omega^0\\rightarrow \\mathbb{R}^d\\) defined by</p> \\[ \\pmb{V}(\\pmb{X},t)\\overset{\\Delta}{=}\\frac{\\partial \\phi(\\pmb{X},t)}{\\partial t} \\] <p>and acceleration \\(A(\\cdot,t):\\Omega^0\\rightarrow \\mathbb{R}^d\\)</p> \\[ \\begin{align*} \\pmb{A}(\\pmb{X},t)&amp;\\overset{\\Delta}{=}\\frac{\\partial \\pmb{V}(\\pmb{X},t)}{\\partial t}\\\\ &amp;=\\frac{\\partial^2 \\phi(\\pmb{X},t)}{\\partial t^2} \\end{align*} \\]"},{"location":"courses/mpm/#deformation","title":"Deformation","text":"<p>For every small unit in a material, it has a deformation mapping :</p> \\[ \\pmb{F}(\\pmb{X},t)\\overset{\\Delta}{=}\\frac{\\partial \\pmb{x}}{\\partial \\pmb{X}} \\in \\mathbb{R}^{d\\times d} \\] <p>Note that \\(F\\) is also related with both \\(\\pmb{X}\\) and \\(t\\). And \\(J=\\det{(F)}\\) characterizes infinitesimal volume change.</p> <p>In Example we have \\(\\pmb{F}=I\\) (i) and \\(\\pmb{F}=R\\) (ii).</p> <p>The above is rigid transformation, and local.</p>"},{"location":"courses/mpm/#push-forward-and-push-back","title":"Push forward and Push back","text":"<p>Push forward of a function, is often referred to as Eulerian (a function of \\(\\pmb{x}\\)). That is, given \\(G: \\Omega^0\\rightarrow \\mathbb{R}\\), the push forward \\(g(\\cdot, t):\\Omega^t\\rightarrow \\mathbb{R}\\) is defined</p> \\[ g(\\pmb{x})=G(\\phi^{-1}(\\pmb{x}),t) \\] <p>Push back function is often referred to as Lagrangian (a function of \\(\\pmb{X}\\)). That is, given \\(g:\\Omega^t\\rightarrow \\mathbb{R}\\), the push back \\(G(\\cdot,t):\\Omega^0\\rightarrow \\mathbb{R}\\) is defined </p> \\[ G(\\pmb{X})=g(\\phi(\\pmb{x}), t) \\] <p>But we usually do not use push back but push forward more frequently since we can always have access to the grid velocity and acceleration based on the particles around it.</p> <p>So it is useful to define Eulerian counterparts. The velocity</p> \\[ \\pmb{v}(\\pmb{x},t)=\\pmb{V}(\\phi^{-1}(\\pmb{x},t),t) \\] <p>and the acceleration</p> \\[ \\pmb{a}(\\pmb{x},t)=\\pmb{A}(\\phi^{-1}(\\pmb{x},t),t) \\] <p>The following result is really important:</p> \\[ \\begin{align*} a_i(\\pmb{x},t)&amp;=A_i(\\phi^{-1}(\\pmb{x},t), t)\\\\ &amp;=\\frac{\\partial V_i}{\\partial t}(\\phi^{-1}(\\pmb{X}, t),t)\\\\ &amp;=\\frac{\\partial v_i}{\\partial t}(\\pmb{x}, t)+\\sum_{j=1}^d\\frac{\\partial v_i}{\\partial x_j}(\\pmb{x},t)v_j(\\pmb{x},t)\\\\ &amp;\\overset{\\Delta}{=}\\frac{D}{Dt}v_i(\\pmb{x},t) \\end{align*} \\] <p>So </p> \\[ \\pmb{a}=\\frac{D}{Dt}\\pmb{v} \\] <p>More generally, for a general Eulerian function \\(f(\\cdot, t):\\Omega^t\\rightarrow \\mathbb{R}\\), we can use this same notation to mean</p> \\[ \\frac{D}{Dt}f(\\pmb{x},t)=\\frac{\\partial f}{\\partial t}(\\pmb{x}, t)+\\sum_{j=1}^d\\frac{\\partial f}{\\partial x_j}(\\pmb{x},t)v_j(\\pmb{x},t) \\] <p>which is called material derivative, and the first item of derivative is called local rate of change. The above one is also the push forward of \\(\\partial F/\\partial t\\) where \\(F\\) is a Lagrangian function with \\(F(\\cdot,t):\\Omega^0\\rightarrow \\mathbb{R}\\).</p>"},{"location":"courses/mpm/#relationship-between-two-deformation-gradients","title":"Relationship between two Deformation Gradients","text":"<p>For deformation gradient, most of the time in the physics of a material, the Lagrangian view is the dominant one. There is however a useful evolution of the Eulerian (push forward) of \\(\\pmb{F}(\\cdot,t):\\Omega^0\\rightarrow \\mathbb{R}^{d\\times d}\\). If we denote \\(\\pmb{f}(\\cdot, t):\\Omega^t\\rightarrow \\mathbb{R}^{d\\times d}\\) be the push forward of \\(\\pmb{F}\\), then</p> \\[ \\frac{D}{Dt}\\pmb{f}(\\pmb{x},t) = \\frac{\\partial \\pmb{v}}{\\partial \\pmb{x}}\\pmb{f} \\] <p>or </p> \\[ \\dot{\\pmb{F}}=(\\nabla \\pmb{v})\\pmb{F} \\] <p>cause we have</p> \\[ \\begin{align*} \\frac{\\partial}{\\partial t}F_{ij}(\\pmb{X}, t)&amp;=\\frac{1}{\\partial t }\\frac{\\partial \\phi_i}{\\partial X_j}(\\pmb{X},t)\\\\ &amp;=\\frac{1}{\\partial X_j}\\frac{\\partial \\phi_i}{ \\partial t }(\\pmb{X},t)\\\\ &amp;=\\frac{\\partial V_i}{\\partial X_j}(\\pmb{X},t)\\\\ &amp;=\\frac{\\partial v_i}{\\partial X_j}(\\phi(\\pmb{X},t),t)\\\\ &amp;=\\sum_{k=1}^d\\frac{\\partial v_i}{\\partial x_k}(\\phi(\\pmb{x},t),t)\\cdot \\frac{\\partial \\phi_k}{\\partial X_j}(\\pmb{X},t)\\\\ &amp;=\\sum_{k=1}^d\\frac{\\partial v_i}{\\partial x_k}(\\phi(\\pmb{x},t),t)\\cdot F_{kj}(\\pmb{X},t) \\end{align*} \\] <p>The above equation will play an important role in deriving the discretized deformation gradient \\(F\\) update on each MPM particle.</p>"},{"location":"courses/mpm/#volume-and-area-change","title":"Volume and Area Change","text":"<ul> <li>Volume</li> </ul> <p>Consider \\(dV\\) being defined over the standard basis vectors \\(\\vec{e}_i\\), \\(i=1.2.3\\), with \\(dV=dL_1\\vec{e}_1\\cdot (dL_2\\vec{e}_2 \\times dL_3\\vec{e}_3)\\). If we denote \\(d\\pmb{L}_i=dL_i\\vec{e}\\), then </p> \\[ dV=dL_1dL_2dL_3, \\quad  \\] <p>cause we have \\(d\\pmb{l}_i=\\pmb{F}d\\pmb{L}_i\\) (like \\((\\pmb{x}_2-\\pmb{x}_1)=\\pmb{F}(\\pmb{X_2}-\\pmb{X}_1)\\)), so it can be shown that \\(dl_1dl_2dl_3=JdL_1dL_2dL_3\\) or </p> \\[ dv=JdV. \\] <p>Based on the above property we can have a great weapon in proof. Given function \\(G(\\pmb{X})\\) or \\(g(\\pmb{x},t)\\) we have </p> \\[ \\int_{B^t}g(\\pmb{x})d\\pmb{x}=\\int_{B^0}G(\\pmb{X})J(\\pmb{X},t)d\\pmb{X} \\] <p>Actually it is the variable substitution formula of multiple integral.</p> <ul> <li>Areas</li> </ul> <p>And similar analysis can be done for areas.</p> \\[ \\begin{align*} dv&amp;=JdV\\\\ \\pmb{n}ds\\cdot d\\pmb{l}&amp;=J\\pmb{N}dS\\cdot d\\pmb{L}\\\\ \\pmb{n}ds\\cdot \\pmb{F}d\\pmb{L}&amp;=J\\pmb{N}dS\\cdot d\\pmb{L}\\quad \\text{using $d\\pmb{l}=\\pmb{F}d\\pmb{L}$}\\\\ \\pmb{n}ds\\cdot \\pmb{F}&amp;=J\\pmb{N}dS \\end{align*} \\] <p>So </p> \\[ \\begin{equation} \\pmb{n}ds=J\\pmb{F}^{-T}\\pmb{N}dS.\\label{eq-area} \\end{equation} \\] <p>which is also called Nansom's formula.</p>"},{"location":"courses/mpm/#piola-kirchhoff-stress","title":"Piola-Kirchhoff Stress","text":"<p>Consider a vector element of surface in the material world(Lagrangian), \\(\\pmb{N}dS\\) and after deformation, the material particles making up this area now occupy the element defined by \\(\\pmb{n}ds\\), where \\(ds\\) is the area and \\(\\pmb{n}\\) is the normal vector in the current configuration(Eulerian).</p> <p>Then by definition of the Cauchy stress</p> \\[ d\\pmb{f}=\\pmb{\\sigma}\\pmb{n}ds \\] <p>The first Piola-Kirchhoff stress tensor \\(\\pmb{P}\\) (PK1 stress, Nominal Stress tensor) is defined by</p> \\[ d\\pmb{f}=\\pmb{P}\\pmb{N}dS \\] <p>which relates the force acting in the current configuration to the surface element in the reference configuration.</p> <p>So similarly with (Cauchy) traction vector was defined by</p> \\[ \\pmb{t}=\\frac{d\\pmb{f}}{ds} \\] <p>we can introduce a PK1 traction vector</p> \\[ \\pmb{T}=\\frac{d\\pmb{f}}{dS}, \\quad \\pmb{T}=\\pmb{P}\\pmb{N} \\] <p>which is a fictitious quatity.</p> <p>Note that \\(d\\pmb{f}=\\pmb{t}ds=\\pmb{T}dS\\), so \\(\\pmb{t}\\) and \\(\\pmb{T}\\) at the same area has the same direction but different magnitudes.</p> <p> </p> <ul> <li>Relation between the Cauchy and PK1 Stresses</li> </ul> <p>From the above definition,</p> \\[ \\begin{align*} \\pmb{P}\\pmb{N}dS&amp;=\\pmb{\\sigma}\\pmb{n}\\\\ \\pmb{P}\\pmb{N}dS&amp;=\\pmb{\\sigma}J\\pmb{F}^{-T}\\pmb{N}dS\\quad\\text{using equation $\\ref{eq-area}$}\\\\ \\pmb{P}&amp;=J\\pmb{\\sigma}\\pmb{F}^{-T}\\\\ \\pmb{\\sigma}&amp;=J^{-1}\\pmb{P}\\pmb{F}^T \\end{align*} \\]"},{"location":"courses/mpm/#hyperelasticity","title":"Hyperelasticity","text":"<p>Stress is related to strain(Deformation gradient \\(F\\)) through \"Constitutive Relationship\". </p> <p>For perfect hyperelastic materials, the relation is defined through the potential energy, which increases with non-rigid deformation from the initial state. That is, The elastic solids whose first Piola-Kirchoff stress \\(\\pmb{P}\\) can be derived from an strain energy density function \\(\\Psi(\\pmb{F})\\) (a scalar function) via</p> \\[ \\pmb{P}=\\frac{\\partial \\Psi}{\\partial \\pmb{F}} \\] <p>and Cauchy stress with respect to \\(\\Psi(\\pmb{F})\\)</p> \\[ \\sigma=\\frac{1}{J}\\pmb{P}\\pmb{F}^T=\\frac{1}{J}\\frac{\\partial \\Psi}{\\partial \\pmb{F}}\\pmb{F}^T \\]"},{"location":"courses/mpm/#energy-density-function","title":"Energy density function","text":"<ul> <li>Neo-Hookean</li> </ul> \\[ \\Psi(\\pmb{F})=\\frac{\\mu}{2}(\\text{tr}{(\\pmb{F}^T\\pmb{F})}-d)-\\mu\\log(J)+\\frac{\\lambda}{2}\\log^2(J) \\] <p>where \\(d\\) denoted dimension, \\(2\\) or \\(3\\) in practice and </p> \\[ \\mu=\\frac{E}{2(1+v)},\\quad \\lambda=\\frac{Ev}{(1+v)(1-2v)} \\] <p>in which \\(E\\) is Young's modulus and \\(v\\) is Poisson's ratio.</p> <ul> <li>Fixed corotated model</li> </ul> <p>This is derived from the Singular Value Decomposition (SVD). Assuming the polar SVD \\(\\pmb{F}=U\\Sigma V^T\\), then the energy for fixed corotated model is </p> \\[ \\Psi(\\pmb{F})=\\hat{\\Psi}(\\Sigma(\\pmb{F}))=\\mu\\sum_{i=1}^d(\\sigma_i-1)^2+\\frac{\\lambda}{2}(J-1)^2 \\] <p>where \\(J=\\prod\\limits_{i=1}^d\\sigma_i\\) and </p> \\[ \\pmb{P}(\\pmb{F})=\\frac{\\partial\\Psi}{\\partial \\pmb{F}}(\\pmb{F})=2\\mu(\\pmb{F}-\\pmb{R})+\\lambda(J-1)J\\pmb{F}^{-T}. \\]"},{"location":"courses/mpm/#governing-equations","title":"Governing Equations","text":"<p>Let </p> \\[ \\pmb{V}(\\pmb{X},t)=\\frac{\\partial \\phi(\\pmb{X},t)}{\\partial t}=\\frac{\\partial{\\pmb{x}}}{\\partial t} \\] <p>be the velocity defined over \\(X\\). Then from Lagrangian view of point, the equations are</p> \\[ \\begin{cases} \\displaystyle R(\\pmb{X},t)J(\\pmb{X},t)=R(\\pmb{X},0) \\quad &amp;\\text{Conservation of mass},\\\\ \\displaystyle R(\\pmb{X},t)\\frac{\\partial \\pmb{V}}{\\partial t}=\\nabla^{\\pmb{X}}\\cdot \\pmb{P}+R(\\pmb{X},0)g \\quad &amp;\\text{Conservation of momentum}. \\end{cases} \\] <p>where \\(R\\) is the Lagrangian mass density which is related to the more commonly used Eulerian mass density \\(\\rho\\). Note that mass conservation can also be written as </p> \\[ \\frac{\\partial }{\\partial t}[R(\\pmb{X},t)J(\\pmb{X},t)]=0 \\] <p>In Eulerian view, the governing equations are</p> \\[ \\begin{cases} \\displaystyle \\frac{D}{Dt}\\rho(\\pmb{x},t)+\\rho(\\pmb{x},t)\\nabla^{\\pmb{x}}\\cdot \\pmb{v}(\\pmb{x},t)=0,\\quad &amp;\\text{Conservation of mass},\\\\ \\displaystyle \\rho(\\pmb{x},t)\\frac{D\\pmb{v}}{Dt}=\\nabla^{\\pmb{x}}\\cdot \\sigma +\\rho(\\pmb{x},t)g, \\quad &amp;\\text{Conservation of momentum}.\\\\ \\end{cases} \\] <p>where </p> \\[ \\frac{D}{Dt}=\\frac{\\partial}{\\partial t}+\\pmb{v}\\cdot \\nabla^{\\pmb{x}} \\]"},{"location":"courses/mpm/#conservation-of-mass","title":"Conservation of Mass","text":"<p>To be more specific, we have two ways to get the Continuity Equation.</p> HintsFrom global viewFrom material view <p>By using multiple integral for density.</p> <p>Notice that </p> \\[ \\rho(\\pmb{x},t)=\\lim_{\\varepsilon\\rightarrow 0^+}\\frac{\\text{mass}(B_{\\varepsilon}^t)}{\\int_{B_{\\varepsilon}^t}d\\pmb{x}} \\] <p>We choose a fixed volume, in which the rate of increase of mass must equal the rate at which mass is flowing into the volume through its bounding surface.</p> <p>The rate of increase mass in a fixed volume \\(v\\) is</p> \\[ \\frac{\\partial m}{\\partial t}=\\frac{\\partial}{\\partial t}\\int_{v}\\rho(\\pmb{x},t)dv=\\int_v \\frac{\\rho(\\pmb{x},t)}{\\partial t}dv \\] <p>while the mass flux out through the surface is given by</p> \\[ \\int_s \\rho \\pmb{v}\\cdot \\pmb{n} ds \\] <p>here \\(s\\) can denote the overall outer surface of the material(we consider in and out).</p> <p>so combine the above two we get</p> \\[ \\int_v \\frac{\\rho(\\pmb{x},t)}{\\partial t}dv + \\int_s \\rho \\pmb{v}\\cdot \\pmb{n} ds=0 \\] <p>using divergence theorem, we get</p> \\[ \\begin{align*} \\int_v \\frac{\\rho(\\pmb{x},t)}{\\partial t}dv + \\int_v \\nabla^{\\pmb{x}}\\cdot(\\rho \\pmb{v}) dv&amp;=0\\\\ \\frac{\\rho(\\pmb{x},t)}{\\partial t}+ \\nabla^{\\pmb{x}}\\cdot(\\rho \\pmb{v}) &amp;=0\\\\ \\frac{\\partial \\rho(\\pmb{x},t)}{\\partial t}+ (\\nabla^{\\pmb{x}}\\cdot\\rho)\\pmb{v} +(\\nabla^{\\pmb{x}}\\cdot\\pmb{v})\\rho &amp;=0\\\\ \\frac{D\\rho}{Dt} +\\rho \\nabla^{\\pmb{x}}\\cdot(\\pmb{v})&amp;=0 \\end{align*} \\] <p>We check the fixed volume in terms of material point. Note \\(R(\\pmb{X},t)=\\rho(\\phi(\\pmb{X},t),t)\\), then we see the mass from initial time to time \\(t\\) must be the same:</p> \\[ \\begin{align*} \\text{mass}(v)&amp;=\\text{mass}V_0\\\\ \\int_{v}\\rho(\\pmb{x},t)d\\pmb{x}&amp;=\\int_{V}R(\\pmb{X},0)d\\pmb{X}\\\\ \\Rightarrow \\int_{V}R(\\pmb{X},t)J(\\pmb{X},t)d\\pmb{X}&amp;=\\int_{V}R(\\pmb{X},0)d\\pmb{X} \\end{align*} \\] <p>So </p> \\[ R(\\pmb{X},t)J(\\pmb{X},t)=R(\\pmb{X},0), \\quad \\forall \\pmb{X}\\in \\Omega^0, t\\geq 0 \\] <p>Note that \\(J(\\pmb{X},0)=1\\), so \\(R(\\pmb{X},t)J(\\pmb{X},t)=R(\\pmb{X},0)J(\\pmb{X},0)\\), that is, </p> \\[ \\frac{\\partial }{\\partial t}[R(\\pmb{X},t)J(\\pmb{X},t)]=0. \\]"},{"location":"courses/mpm/#conservation-of-momentum","title":"Conservation of Momentum","text":"<p>Also, we have two ways to consider.</p> HintsFrom glabal viewFrom material view <p>Use the similar logic in proof of mass conservation.</p> <p>This is also the spatial form.</p> <p>We know that</p> \\[ \\pmb{t}(\\pmb{x},\\pmb{n},t)=\\pmb{\\sigma}(\\pmb{x},t)\\pmb{n} \\] <p>We consider a fixed mass, so the space occupied by this matter may change over time.</p> <p>If \\(\\pmb{v}\\) denotes the Eulerian velocity, then the linear momentum of Euler can be denoted as</p> \\[ \\pmb{L}(t)=\\int_{B_{\\varepsilon}^t} \\rho\\pmb{v}d\\pmb{x}, \\] <p>where </p> <p>then by \\(\\pmb{f}^{ext}=\\frac{d(m\\pmb{v})}{dt}\\) formulated by Euler,</p> \\[ \\begin{align*} \\int_{\\partial B_{\\varepsilon}^t}\\pmb{\\sigma}\\pmb{n}ds+\\int_{B_{\\varepsilon}^t}\\pmb{f}^{ext}d\\pmb{x}&amp;=\\frac{d}{dt}\\int_{B_{\\varepsilon}^t}\\rho \\pmb{v}d\\pmb{x}\\\\ \\int_{B_{\\varepsilon}^t}\\nabla^{\\pmb{x}}\\cdot \\sigma+\\pmb{f}^{ext} d\\pmb{x}&amp;=\\frac{d}{dt}\\int_{B_{\\varepsilon}^0}R\\pmb{V}Jd\\pmb{X}\\\\ &amp;=\\int_{B_{\\varepsilon}^0}R\\pmb{A}Jd\\pmb{X}\\\\ &amp;=\\int_{B_{\\varepsilon}^t}\\rho\\pmb{a}d\\pmb{x} \\end{align*} \\] <p>Note \\(\\pmb{V}(\\pmb{X},t)=\\pmb{v}(\\phi(\\pmb{X},t),t)\\), so we have a fixed volume \\(V\\) with momentum </p> \\[ \\pmb{L}(t) = \\int_V R(\\pmb{X},t)V(\\pmb{X},t)dV \\] <p>So </p> \\[ \\begin{align*} \\frac{\\partial}{\\partial t}\\int_V R(\\pmb{X},t)V(\\pmb{X},t)dV &amp;=\\int_S\\pmb{T}dS +\\int_V \\pmb{F}dV\\\\ \\int_V R(\\pmb{X},t)\\frac{\\partial}{\\partial t}V(\\pmb{X},t)dV&amp;= \\int_S \\pmb{P}\\cdot\\pmb{N}dS+\\int_V\\pmb{F} dV\\\\ \\int_V R(\\pmb{X},t)\\frac{\\partial}{\\partial t}V(\\pmb{X},t)dV&amp;= \\int_V \\nabla^{\\pmb{X}}\\cdot \\pmb{P}+\\pmb{F} dV\\\\ \\Rightarrow R(\\pmb{X},t)\\frac{\\partial}{\\partial t}V(\\pmb{X},t) &amp;=\\nabla^{\\pmb{X}}\\cdot \\pmb{P}+\\pmb{F} \\end{align*} \\]"},{"location":"courses/mpm/#material-particles","title":"Material particles","text":"<p>Recall that the material point method is Lagrangian in the sense that we track actual particles of material. That is we keep track of mass (\\(m_p\\)), velocity (\\(v_p\\)) and position (\\(x_p\\)) for a collection of material particles \\(p\\).</p> <p>However, all stress based forces are computed on the Eulerian grid, so we have to transfer the material state to the Eulerian configuration to incorporate the effects of material forces. </p> <p>Then, we transfer these effects back to the material particles and move them in the normal Lagrangian way. The Lagrangian nature makes advection very trivial compared to pure Eulerian methods (such as grid-based fluid simulation).</p>"},{"location":"courses/mpm/#eulerian-interpolating-functions","title":"Eulerian Interpolating Functions","text":"<p>We can denote the interpolation function at grid node \\(\\pmb{i}=(i,j,k)\\) evaluated at a particle location \\(\\pmb{x}_p\\) with </p> \\[ N_{\\pmb{i}}(\\pmb{x}_p)=N\\left(\\frac{1}{h}(\\pmb{x}_p-\\pmb{x}_{\\pmb{i}})\\right)N\\left(\\frac{1}{h}(\\pmb{y}_p-\\pmb{y}_{\\pmb{i}})\\right)N\\left(\\frac{1}{h}(\\pmb{z}_p-\\pmb{z}_{\\pmb{i}})\\right) \\] <p>where \\(h\\) is the grid spacing. We can define diffenrent kernel \\(N:\\mathbb{R}\\rightarrow \\mathbb{R}\\).</p> <p>Common Kernel \\(N\\)</p> <ul> <li>cubic kernel. It is more expensive but provide wider coverage, thus less sensitive to numerical errors.</li> </ul> \\[ N(x)=\\begin{cases} \\displaystyle \\frac{1}{2}|x|^3-|x|^2+\\frac{2}{3}, \\quad &amp;0\\leq |x| &lt; 1\\\\ \\displaystyle \\frac{1}{6}(2-|x|)^3,\\quad &amp;1\\leq |x|&lt; 2\\\\ \\displaystyle 0,\\quad &amp;2\\leq|x| \\end{cases} \\] <ul> <li>quadratic kernel. It is more computational efficient and memory saving.</li> </ul> \\[ N(x)=\\begin{cases} \\displaystyle \\frac{3}{4}-|x|^2,\\quad &amp; \\displaystyle 0\\leq |x| &lt; \\frac{1}{2}\\\\ \\displaystyle \\frac{1}{2}\\left(\\frac{3}{2}-|x|\\right)^2,\\quad &amp;\\displaystyle \\frac{1}{2}\\leq |x|&lt;\\frac{3}{2}\\\\ \\displaystyle 0,\\quad &amp;\\displaystyle \\frac{3}{2}\\leq |x| \\end{cases} \\] <p>Then the gradient of function \\(N_{\\pmb{i}}(\\pmb{x}_p)\\) is </p> \\[ \\nabla N_{\\pmb{i}}(\\pmb{x}_p) =\\sum_{k=1}^d\\left[N'\\left(\\frac{1}{h}(x_k-x_{\\pmb{i}})\\right)\\prod_{j=1\\atop j\\neq k}^d N\\left(\\frac{1}{h}(x_{j}-x_{\\pmb{i}})\\right)\\right] \\] <p>where \\(x_k\\), \\(x_j\\) denote the component index of \\(\\pmb{x}_p\\), i.e. \\(x_k, x_j\\in\\{x_p,y_p,z_p\\}\\).</p>"},{"location":"courses/mpm/#eulerianlagrangian-mass","title":"Eulerian/Lagrangian Mass","text":"<p>Mass of the particle</p> \\[ m_p^n=\\int_{B_{\\Delta x,p}^{t^n}}\\rho(\\pmb{x},t^n)d\\pmb{x} \\] <p>and define the mass from Eulerian perspective</p> \\[ m_{\\pmb{i}}=\\sum_pm_p\\cdot N_{\\pmb{i}}(\\pmb{x}_p) \\] <p>Easy to see that </p> \\[ \\sum_{\\pmb{i}}m_{\\pmb{i}}=\\sum_p m_p \\] <p>since the weight function \\(N_\\pmb{i}(\\pmb{x}_p)\\) is normalized to \\(1\\).</p>"},{"location":"courses/mpm/#eulerian-momentum","title":"Eulerian Momentum","text":"<p>Similarly, we transfer particle monentum \\(m_p\\pmb{v}_p\\) to the grid</p> \\[ (m\\pmb{v})_{\\pmb{i}}=\\sum_p m_p \\pmb{v}_p N_{\\pmb{i}}(\\pmb{x}_p) \\] <p>Also easy to that </p> \\[ \\sum_{\\pmb{i}}(m\\pmb{v})_{\\pmb{i}}=\\sum_{p}m_p\\pmb{v}_p \\] <p>and the Eulerian velocity \\(\\pmb{v}_\\pmb{i}\\) is defined as </p> \\[ \\pmb{v}_{\\pmb{i}}=\\frac{(m\\pmb{v})_{\\pmb{i}}}{m_{\\pmb{i}}} \\]"},{"location":"courses/mpm/#eulerian-to-lagrangian-transfer","title":"Eulerian to Lagrangian Transfer","text":"<p>We do not need to transfer mass from the grid to the particles since Lagrangian particle mass never changes. But the Velocity is simply interpolated as</p> \\[ \\pmb{v}_p =\\sum_{\\pmb{i}}\\pmb{v}_{\\pmb{i}}N_{\\pmb{i}}(\\pmb{x}_p) \\] <p>Easy to see that</p> \\[ \\sum_p m_p \\pmb{v}_p=\\sum_{\\pmb{i}}m_i\\pmb{v}_{\\pmb{i}} \\]"},{"location":"courses/mpm/#discretization","title":"Discretization","text":""},{"location":"courses/mpm/#explicite-time-integration","title":"Explicite time Integration","text":""},{"location":"courses/mpm/#apic-transfers","title":"APIC Transfers","text":"<p>Transfer from particle to grid</p> \\[ \\begin{cases} m_i=\\sum_p w_{ip}m_p\\\\ m_i\\pmb{v}_i=\\sum_p w_{ip}m_p(\\pmb{v}_p+\\pmb{B}_p(\\pmb{D}_p)^{-1}(\\pmb{x}_i-\\pmb{x}_p)) \\end{cases} \\] <p>where \\(\\pmb{B}_p\\) is a matrix quatity stored at each particle(like mass, position and velocity), \\(\\pmb{D}_p\\) is given by </p> \\[ \\pmb{D}_p=\\sum_i w_{ip}(\\pmb{x}_i-\\pmb{x}_p)(\\pmb{x}_i-\\pmb{x}_p)^T \\] <p>which has a simple form \\(\\frac{1}{4}\\Delta x^2\\pmb{I}\\) for quadratic and \\(\\frac{1}{3}\\Delta x^2\\pmb{I}\\) for cubic interpolation stencils.</p> <p>Then from grid to particle</p> \\[ \\begin{cases} \\pmb{v}_p=\\sum_i w_{ip}\\pmb{v}_i\\\\ \\pmb{B}_p=\\sum_i w_{ip}\\pmb{v}_i (\\pmb{x}_i-\\pmb{x}_p)^T \\end{cases} \\]"},{"location":"courses/mpm/#deformation-gradient-update","title":"Deformation Gradient Update","text":"<p>Given \\(f_p\\), we can update the position and velocity of the grid</p> \\[ \\begin{cases} \\pmb{v}_i^{n+1}=\\pmb{v}_i^n+\\Delta t f_i(\\pmb{x}_i^n)/m_i\\\\ \\pmb{x}_{i}^{n+1}=\\pmb{x}_i^n+\\Delta t \\pmb{v}_i^{n+1} \\end{cases} \\] <p>Given a grid velocity field \\(\\pmb{v}_i^{n+1}\\), we can update \\(F\\) as</p> \\[ F_p^{n+1}=\\left(I+\\Delta t \\sum_i \\pmb{v}_i^{n+1}(\\nabla w_{ip}^n)^T\\right) F^n_p \\]"},{"location":"courses/mpm/#forces","title":"Forces","text":"<p>MPM Forces are defined on grid nodes. If we assume a deformation gradient based hyperelastic energy density, then the total elastic potential energy is then </p> \\[ e=\\sum_p V_p^0\\Psi_p(F_p) \\] <p>where \\(V_p^0\\) is the material space volume of particle.</p> <p>Nodal elastic force is the negative gradient of the total potential energy evaluated at nodal positions. So the MPM spatial discretization of the stress-based forces is given as </p> \\[ f_i(x_i^n)=-\\frac{\\partial e}{\\partial x_i}(x)=-\\sum_p V_p^0\\left(\\frac{\\partial \\Psi_p}{\\partial F}(F_p(x_i^n))\\right)(F^n_p)^T\\nabla w_{ip}^n \\] <p>which fully depends on the existing particle/grid weights and particle attributes.</p>"},{"location":"courses/mpm/preliminary/","title":"Preliminary","text":"<p>Reference</p> <p>Mechanics Lecture Notes Part III: Foundations of Continuum Mechanics, pa.kelly@auckland.ac.nz.</p> <p>Website</p>"},{"location":"courses/mpm/preliminary/#tensors","title":"Tensors","text":"<p>A tensor of order zero is simply another name for a scalar \\(\\alpha\\).</p> <p>A first-order tensor is simply another name for a vector \\(\\pmb{u}\\). </p> <p>We use uppercase bold-face Latin letters to denote second order tensor.</p> <p>A second-order tensor \\(\\pmb{T}\\) may be defined as an operator that acts on a vector \\(\\pmb{u}\\) generating another vector \\(\\pmb{v}\\), such that </p> \\[ \\pmb{T}(\\pmb{u})=\\pmb{v} \\] <p>which is a linear operator. </p> <ul> <li>dyad(tensor product)</li> </ul> <p>the tensor product of two vectors \\(\\pmb{u}\\) and \\(\\pmb{v}\\)</p> \\[ \\pmb{u}\\otimes \\pmb{v} \\] <p>is defined by</p> \\[ (\\pmb{u}\\otimes \\pmb{v} ) \\pmb{w} = \\pmb{u}(\\pmb{v}\\cdot \\pmb{w}) \\] <p>Properties</p> <p>(i)</p> \\[ (\\pmb{u}\\otimes \\pmb{v})(\\pmb{w}\\otimes \\pmb{x})=(\\pmb{v}\\cdot\\pmb{w})(\\pmb{u}\\otimes \\pmb{x}) \\] <p>cause</p> \\[ \\begin{align*} (\\pmb{u}\\otimes \\pmb{v})(\\pmb{w}\\otimes \\pmb{x})\\pmb{y}&amp;=(\\pmb{u}\\otimes \\pmb{v})(\\pmb{x}\\cdot\\pmb{y})\\pmb{w}\\\\ &amp;=(\\pmb{x}\\cdot\\pmb{y})(\\pmb{u}\\otimes \\pmb{v})\\pmb{w}\\\\ &amp;=(\\pmb{x}\\cdot\\pmb{y})(\\pmb{v}\\cdot\\pmb{w})\\pmb{u}\\\\ &amp;=(\\pmb{v}\\cdot\\pmb{w})(\\pmb{x}\\cdot\\pmb{y})\\pmb{u}\\\\ &amp;=(\\pmb{v}\\cdot\\pmb{w})(\\pmb{u}\\otimes \\pmb{x})\\pmb{y} \\end{align*} \\] <p>(ii)</p> \\[ \\pmb{u}(\\pmb{v}\\otimes \\pmb{w})=(\\pmb{u}\\cdot \\pmb{v})\\pmb{w} \\] <p>cause</p> \\[ \\begin{align*} (\\pmb{y}\\otimes \\pmb{u})(\\pmb{v}\\otimes \\pmb{w})&amp;=(\\pmb{u}\\cdot \\pmb{v})(\\pmb{y}\\otimes \\pmb{w})\\\\ &amp;=\\pmb{y} \\otimes [(\\pmb{u}\\cdot \\pmb{v})\\pmb{w}] \\end{align*} \\] <p>Some example</p> <ul> <li>Projection Tensor \\((\\pmb{e}\\otimes \\pmb{e})\\) </li> </ul> <p>So </p> \\[ (\\pmb{e}\\otimes \\pmb{e})\\pmb{u} = (\\pmb{e}\\cdot \\pmb{u})\\pmb{e} \\] <p>is the vector projection of \\(\\pmb{u}\\) on \\(\\pmb{e}\\), denoted by \\(\\pmb{P}\\).</p> <p>A dyadic is a linear combination of dyads (with scalar coefficients).</p> <p>In the following discussion, we can treat \\(\\pmb{T}\\) as a matrix.    </p>"},{"location":"courses/mpm/preliminary/#cartesian-tensors","title":"Cartesian Tensors","text":"<p>A second order tensor and the the vector it operates on can be described in terms of Cartesian components.</p> <p>Example</p> <ul> <li>Identity tensor/(or unit tensor).</li> </ul> \\[ \\pmb{I}=\\sum_{i=1}^d\\pmb{e}_i\\otimes \\pmb{e}_i \\] <p>cause it follows</p> \\[ \\begin{align*} \\pmb{I} \\pmb{u}&amp;=\\sum_{i=1}^d(\\pmb{e}_i\\otimes \\pmb{e}_i) \\pmb{u}\\\\ &amp;=\\sum_{i=1}^d(\\pmb{e}_i \\cdot \\pmb{u})\\pmb{e}_i \\\\ &amp;=\\sum_{i=1}^d u_i\\pmb{e}_i \\\\ &amp;=\\pmb{u} \\end{align*} \\] <p>Or identity tensor can be written as</p> \\[ \\pmb{I} = \\sum_{i,j=1}^d\\delta_{ij}(\\pmb{e}_i\\otimes \\pmb{e}_j) \\] <p>Second order tensor as a Dyadic</p> <p>Every second order tensor can always be written as a dyadic involving the Cartesian base vectors \\(\\pmb{e}_i\\), that is, if we denote </p> \\[ \\pmb{E}_i=\\pmb{T}\\left(\\pmb{e}_i\\right) \\] <p>then </p> \\[ \\pmb{T}= \\sum_{i=1}^d \\left(\\pmb{E}_i\\otimes \\pmb{e}_i\\right) \\] Proof \\[ \\begin{align*} \\pmb{b}&amp;=\\pmb{T}(\\pmb{a})\\\\ &amp;=\\pmb{T}\\left(\\sum_{i=1}^d a_i\\pmb{e}_i\\right)\\\\ &amp;=\\sum_{i=1}^d a_i \\pmb{T}(\\pmb{e}_i)\\\\ \\end{align*} \\] <p>Denote \\(\\pmb{T}(\\pmb{e}_i)\\) to be \\(\\pmb{E}_i\\), then</p> \\[ \\begin{align*} \\pmb{b}&amp;=\\sum_{i=1}^d a_i \\pmb{E}_i\\\\ &amp;=\\sum_{i=1}^d (\\pmb{a}\\cdot \\pmb{e}_i )\\pmb{E}_i\\\\ &amp;=\\sum_{i=1}^d (\\pmb{E}_i\\otimes \\pmb{e}_i) \\pmb{a} \\end{align*} \\] <p>So </p> \\[ \\pmb{T}= \\sum_{i=1}^d (\\pmb{E}_i\\otimes \\pmb{e}_i) \\] <p>If we write \\(\\pmb{E}_i\\) with base vectors like</p> \\[ \\pmb{E}_i=\\sum_{j=1}^d E_{ij}\\pmb{e}_j, \\quad i=1,\\cdots, d \\] <p>Then </p> \\[ \\begin{align*} (\\pmb{E}_i\\otimes \\pmb{e}_i)&amp;=\\left(\\sum_{j=1}^d E_{ij}\\pmb{e}_j\\right)\\otimes \\pmb{e}_i\\\\ &amp;=\\sum_{j=1}^d E_{ij} (\\pmb{e}_j\\otimes \\pmb{e}_i), \\quad i=1,\\cdots, d \\end{align*} \\] <p>Thus</p> \\[ \\pmb{T}=\\sum_{i=1}^d\\sum_{j=1}^d E_{ij} (\\pmb{e}_j\\otimes \\pmb{e}_i) \\] <p>Introduce 9 scalars \\(T_{ij}=E_{ji}\\), then </p> \\[ \\begin{align*} \\pmb{T}&amp;=\\sum_{i=1}^d\\sum_{j=1}^d T_{ji} (\\pmb{e}_j\\otimes \\pmb{e}_i)\\\\ &amp;=\\sum_{j=1}^d\\sum_{i=1}^d T_{ji} (\\pmb{e}_j\\otimes \\pmb{e}_i) \\quad \\text{switch summation turn}\\\\ &amp;=\\sum_{i=1}^d\\sum_{j=1}^d T_{ij} (\\pmb{e}_i\\otimes \\pmb{e}_j)\\quad \\text{switch $i$ and $j$} \\end{align*} \\] <p>We can see that 9 dyads \\(\\{\\pmb{e}_i\\otimes \\pmb{e}_j\\}_{i,j=1}^3\\) forms a basis for the space of second order tensors.</p> <p>Recall that </p> \\[ \\begin{align*} T_{ij}&amp;=E_{ji}\\\\ &amp;=\\pmb{E}_j \\cdot \\pmb{e}_i \\\\ &amp;=T(\\pmb{e}_j) \\cdot (\\pmb{e}_i)\\\\ &amp;=(\\pmb{e}_i) \\cdot T(\\pmb{e}_j)\\\\ \\end{align*} \\] <p>So we can get the component of a tensor by the above way.</p>"},{"location":"courses/mpm/preliminary/#cauchy-stress-tensor","title":"Cauchy Stress Tensor","text":"<p>The traction vector, the limiting value of the ratio of force over area, that is,</p> \\[ \\pmb{t}^{\\pmb{n}}=\\lim_{\\Delta_s\\rightarrow 0}\\frac{\\Delta F}{\\Delta S} \\] <p>where \\(\\pmb{n}\\) denotes normal vector to the surface.</p> <p>The stress \\(\\pmb{\\sigma}\\), a second order tensor which maps \\(\\pmb{n}\\) onto \\(\\pmb{t}\\)</p> \\[ \\pmb{t}=\\pmb{\\sigma}\\pmb{n} \\] <p>If we consider a coordinate system with base vectors \\(\\pmb{e}_i\\), then \\(\\pmb{\\sigma}=\\sum\\limits_{i,j=1}^d(\\sigma_{ij}\\pmb{e}_i \\otimes \\pmb{e}_j)\\) </p> \\[ \\pmb{\\sigma}=(\\sigma_{ij}) \\] <p>So </p> \\[ t_i \\pmb{e_i} = \\sum_{j=1}^3\\sigma_{ij}n_{j} \\pmb{e}_i \\] <p> </p> <p>For example, </p> \\[ \\pmb{\\sigma}\\pmb{e}_j=\\sum_{i=1}^3\\sigma_{ij}\\pmb{e}_i  \\] <p>which denotes the summation of the \\(j\\)th column of matrix \\(\\pmb{\\sigma}\\).</p> <p>So the components \\(\\sigma_{11}, \\sigma_{21}, \\sigma_{31}\\) of the stress tensor are the three components of the traction vector which acts on the plane with normal \\(\\pmb{e}_1\\).</p>"},{"location":"courses/mpm/preliminary/#hamilton-operator","title":"Hamilton Operator","text":"<p>First we want to introduce the operator </p> \\[ \\nabla=\\pmb{i}\\frac{\\partial }{\\partial x}+\\pmb{j}\\frac{\\partial }{\\partial y}+\\pmb{k}\\frac{\\partial }{\\partial z} \\] <p>then </p> \\[ \\nabla f= \\pmb{i}\\frac{\\partial f}{\\partial x}+\\pmb{j}\\frac{\\partial f}{\\partial y}+\\pmb{k}\\frac{\\partial f}{\\partial z}=\\text{grad} f \\] <p>we also have inner product </p> \\[ \\begin{align*} \\nabla\\cdot \\pmb{a}&amp;=\\left(\\pmb{i}\\frac{\\partial }{\\partial x}+\\pmb{j}\\frac{\\partial }{\\partial y}+\\pmb{k}\\frac{\\partial }{\\partial z}\\right)\\cdot (P\\pmb{i}+Q\\pmb{j}+R\\pmb{k})\\\\ &amp;=\\frac{\\partial P}{\\partial x}+\\frac{\\partial Q} {\\partial y}+\\frac{\\partial R}{\\partial z}\\\\ &amp;=\\text{div} \\pmb{a} \\end{align*} \\] <p>and cross product</p> \\[ \\begin{align*} \\nabla\\times \\pmb{a}&amp;=\\left(\\pmb{i}\\frac{\\partial }{\\partial x}+\\pmb{j}\\frac{\\partial }{\\partial y}+\\pmb{k}\\frac{\\partial }{\\partial z}\\right)\\times (P\\pmb{i}+Q\\pmb{j}+R\\pmb{k})\\\\ &amp;=\\left|\\begin{array}{ccc} \\pmb{i}&amp;\\pmb{j}&amp;\\pmb{k}\\\\ \\displaystyle \\frac{\\partial }{\\partial x}&amp;\\displaystyle \\frac{\\partial }{\\partial y} &amp; \\displaystyle \\frac{\\partial }{\\partial z}\\\\ P &amp; Q &amp; R\\\\ \\end{array} \\right|\\\\ &amp;=\\left( \\frac{\\partial R}{\\partial y}- \\frac{\\partial Q}{\\partial z}\\right)\\pmb{i}+\\left( \\frac{\\partial R}{\\partial x}- \\frac{\\partial P}{\\partial z}\\right)\\pmb{j}+\\left( \\frac{\\partial Q}{\\partial x}- \\frac{\\partial P}{\\partial y}\\right)\\pmb{k}\\\\ &amp;=\\text{rot} \\pmb{a} \\end{align*} \\] <p>Then Gauss Formula can be expressed by</p> \\[ \\iint_{\\partial\\Omega}\\pmb{a}d\\pmb{S}=\\iiint_{\\Omega}\\nabla\\cdot \\pmb{a}dV \\] <p>Stokes Formula can be expressed by</p> \\[ \\int_{\\partial \\Sigma}\\pmb{a}d\\pmb{s}=\\iint_{\\Sigma}(\\nabla\\times \\pmb{a})\\cdot d\\pmb{S} \\]"},{"location":"tag/","title":"Tag","text":"<p>Hey!</p>"}]}
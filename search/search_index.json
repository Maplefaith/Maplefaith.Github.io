{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Keep in low-key wind","text":"<p>Here you can catch a glimpse of notes of courses at ZJU from a student majoring in CSE.</p> <p>Also, I share some of my experience in Blog.</p> <p>Wish you a good reading time.</p> <ul> <li> <p> \u5fc3\u8a00\u5fc3\u8bed</p> <ul> <li>May Day Holiday</li> </ul> </li> </ul> <ul> <li> <p> \u7a7a\u8bf4\u65e0\u51ed</p> <ul> <li>Numerical Analysis</li> <li>Ordinary Differential Equation</li> <li>Modern Control Theory</li> </ul> </li> </ul>"},{"location":"friends/","title":"Welcome to Join my Friend Zone!","text":"Maythics Little Mouse SyncrnzdClk Medium Cat Little_Whale Great Whale LastingWind Giant in Stu'Union"},{"location":"blog/","title":"Blog","text":"<p>In this part, I hope you can stand my childish thoughts and points.</p>"},{"location":"blog/Travel/","title":"Preface","text":"<p>Write down the life before it passes by...</p>"},{"location":"blog/Travel/5th_05_24/","title":"\u5c0f\u957f\u5047\u51fa\u6e38","text":"<p>\u5047\u671f\u76ee\u6807\uff1a\u4ece\u5404\u7c7b\u4e8b\u9879\u4e2d\u89e3\u653e\u51fa\u6765\uff0c\u601d\u8003\u603b\u7ed3\u524d\u534a\u4e2a\u5b66\u671f\u7684\u751f\u5b58\u72b6\u6001\u3002</p> <p>\u884c\u52a8\u539f\u5219\uff1a\u6162\u4e0b\u6765\uff0c\u6162\u4e0b\u6765\u3002</p>"},{"location":"blog/Travel/5th_05_24/#430-52","title":"4.30-5.2","text":"<p>\u9996\u5148\u611f\u8c22\u591c\u5bb5\u7ec4\u4f19\u4f34\u7684\u5b89\u6392\uff0c\u7279\u522b\u611f\u8c22\u5c0f\ud83d\udc1f\u7684\u7edf\u7b79\uff0c\u5c0f\ud83c\udf43\u7684\u666f\u5fb7\u9547\u8ba1\u5212\uff0c\u8ba9\u6211\u4eec\u7684\u6574\u4e2a\u884c\u7a0b\u90fd\u975e\u5e38\u987a\u5229\u3002</p> <p>\u4e00\u5927\u65e9\u5c31\u5f00\u59cb\u7fd8\u8bfe\u51fa\u884c\uff0c\u6211\u4eec\u5148\u5750\u9ad8\u94c1\u5230\u8fbe\u5a7a\u6e90\uff0c\u542f\u52a8\u81ea\u9a7e\u6a21\u5f0f\uff0c\u4e09\u5929\u53bb\u4e86\u7bc1\u5cad\u3001\u4e09\u6e05\u5c71\u548c\u666f\u5fb7\u9547\u3002</p>"},{"location":"blog/Travel/5th_05_24/#430","title":"4.30","text":"<p>\u7b2c\u4e00\u5929\u7684\u7bc1\u5cad\u7740\u5b9e\u8ba9\u4eba\u773c\u524d\u4e00\u4eae\uff0c\u5f88\u591a\u7167\u7247\uff0c\u65e0\u8bba\u662f\u98ce\u666f\u8fd8\u662f\u4eba\u7269\uff0c\u90fd\u5f88\u4e0a\u955c\u3002\u4ece\u73bb\u7483\u6808\u9053\uff08\u5792\u5fc3\u6865\uff09\u5230\u508d\u5c71\u5c0f\u9547\uff08\u82b1\u6eaa\u6c34\u8857\uff09\uff0c\u4ece\u675c\u9e43\u56ed\u5230\u98d8\u96ea\u6c11\u56fd\u9986\uff0c\u7edd\u7f8e\u7684\u5c71\u95f4\u98ce\u5149\u3001\u7eaf\u51c0\u7684\u5c71\u95f4\u7a7a\u6c14\uff0c\u8ba9\u4e00\u4e2a\u957f\u671f\u751f\u6d3b\u5728\u95ed\u585e\u6821\u56ed\u91cc\u7684\u4eba\u611f\u5230\u65e2\u964c\u751f\u53c8\u4eb2\u5207\u3002</p>"},{"location":"blog/Travel/5th_05_24/#51","title":"5.1","text":"<p>\u65e9\u6668\u9192\u6765\uff0c\u521a\u53d1\u751f\u7684\u5e7f\u4e1c\u6885\u5927\u9ad8\u901f\u584c\u9677\u8ba9\u6211\u5403\u4e86\u4e00\u5927\u60ca\u3002\u540e\u9762\u4ece\u4e09\u6e05\u5c71\u56de\u6765\u5f00\u9ad8\u901f\u7684\u65f6\u5019\uff0c\u5176\u5b9e\u6709\u4e00\u70b9\u5bd2\u98a4\u3002</p> <p>\u4e09\u6e05\u5c71\u6211\u66fe\u7ecf\u53bb\u8fc7\u4e00\u6b21\uff0c\u8bb0\u5fc6\u4e2d\u5370\u8c61\u975e\u5e38\u4e0d\u9519\uff0c\u4e91\u96fe\u7f2d\u7ed5\uff0c\u4eba\u4eec\u508d\u5c71\u800c\u884c\uff0c\u5982\u4e34\u4ed9\u5883\u3002\u7136\u800c\uff0c\u8fd9\u6b21\u975e\u5e38\u7684\u4e0d\u884c\uff0c\u4e91\u96fe\u8fc7\u4e8e\u6d53\u91cd\uff0c\u5927\u90e8\u5206\u7684\u5c71\u95f4\u7f8e\u666f\u90fd\u65e0\u6cd5\u6e05\u65b0\u7528\u773c\u770b\u89c1\uff08\u6216\u8bb8\u53ef\u4ee5\u8bd5\u8bd5\u5176\u4ed6\u6ce2\u6bb5\u7684\u5149hh\uff09\u3002</p> <p>\u5728\u80fd\u89c1\u5ea6\u4e0d\u523010m\u7684\u5c71\u95f4\uff0c\u62cd\u7167\u51e0\u4e4e\u662f\u4e00\u4ef6\u5403\u529b\u4e0d\u8ba8\u597d\u7684\u4e8b\u60c5\u3002\u597d\u5728\u6211\u4eec\u6700\u540e\u4e5f\u627e\u5230\u4e86\u8bb8\u591a\u7684\u62cd\u7167\u70b9\uff0c\u8d4b\u4e88\u6d53\u96fe\u91cc\u7684\u5c71\u548c\u6211\u4eec\u4e00\u70b9\u7279\u522b\u7684\u610f\u4e49\u3002</p> <p>\u53e6\u5916\uff0c\u4e94\u4e00\u4eba\u6570\u8fc7\u591a\uff0c\u4e0b\u5c71\u6392\u961f\u7b49\u5019\u8d85\u8fc7\u4e00\u4e2a\u534a\u5c0f\u65f6\uff0c\u4e5f\u662f\u4e00\u4e2a\u51cf\u5206\u9879\u3002</p>"},{"location":"blog/Travel/5th_05_24/#52","title":"5.2","text":"<p>\u6211\u4eec\u53bb\u5230\u4e86\u666f\u5fb7\u9547\uff0c\u4f53\u4f1a\u4e86\u4e00\u4e0b\u4e2d\u56fd\u74f7\u90fd\u7684\u97f5\u5473\u3002\u6211\u672c\u4eba\u5bf9\u4e8e\u74f7\u5668\u5e76\u65e0\u592a\u5927\u611f\u53d7\uff0c\u74f7\u5668\u53ca\u5176\u4ecb\u7ecd\u4e5f\u53ea\u662f\u8d70\u9a6c\u89c2\u82b1\u5306\u5306\u800c\u8fc7\u3002\u4f46\u662f\uff0c\u7ec6\u7ec6\u8d70\u6765\uff0c\u53d1\u73b0\u5176\u5236\u4f5c\u6d41\u7a0b\u7684\u7cbe\u7ec6\u3001\u7e41\u6742\uff0c\u5f97\u5230\u7684\u74f7\u5668\u8d28\u91cf\u4e0a\u4e58\u3001\u4ef7\u503c\u9ad8\u6602\uff0c\u5185\u542b\u5927\u5bb6\u5bf9\u5320\u5fc3\u7684\u8ffd\u6c42\u3002</p> <p>\u665a\u4e0a\u56de\u6765\uff0c\u53c2\u4e0e\u4e86\u5927\u8868\u54e5\u7684\u5a5a\u793c\u3002\u4e0d\u77e5\u4e3a\u4f55\uff0c\u6211\u4e00\u8fb9\u4e3a\u4ed6\u4eec\u611f\u5230\u5f88\u5f00\u5fc3\uff0c\u4e00\u8fb9\u611f\u53d7\u5230\u65f6\u5149\u98de\u901d\uff0c\u6bcf\u4e2a\u4eba\u90fd\u5728\u5f80\u524d\u8d70\u3002\u6211\u5bf9\u672a\u6765\u6709\u671f\u5f85\uff0c\u4e5f\u6709\u7126\u8651\u3002</p>"},{"location":"blog/Travel/5th_05_24/#53-55","title":"5.3-5.5","text":"<p>\u56de\u5230\u6e29\u5dde\uff0c\u9664\u4e86\u4e45\u8fdd\u7684\u6d77\u9c9c\u4e0d\u65ad\u6ee1\u8db3\u6211\u81ea\u5df1\u4e4b\u5916\uff0c\u6211\u8fd8\u5c06\u81ea\u5df1\u7684\u751f\u6d3b\u6162\u4e86\u4e0b\u6765\u3002\u6211\u548c\u5bb6\u91cc\u4eba\u90fd\u76f8\u5904\u4e86\u4e00\u4e0b\uff0c\u6211\u660e\u767d\u4e4b\u540e\u53ef\u80fd\u6ca1\u6709\u5f88\u591a\u7684\u65f6\u95f4\u966a\u4f34\u5bb6\u4eba\u3002</p> <p>\u6211\u5237\u4e86\u4e00\u4e0b\u8001\u53cb\u8bb0\uff0c\u5176\u5b9e\u8fd9\u4e2a\u662f\u53ef\u4ee5\u4e0a\u763e\u7684\uff01\u770b\u5267\u4e0a\u763e\u7684\u673a\u5236\uff0c\u6e90\u81ea\u4e8e\u5267\u60c5\u5bf9\u4eba\u7684\u5174\u8da3\u7684\u4e0d\u65ad\u6311\u9017\uff0c\u4eba\u770b\u5267\u65f6\u80fd\u591f\u6709\u7684\u677e\u5f1b\u611f\u548c\u5fd8\u6211\u611f\u3002</p> <p>\u5b9e\u9645\u4e0a\uff0c\u8fd9\u4e2a\u4e1c\u897f\u662f\u53ef\u4ee5\u8c03\u63a7\u7684\uff0c\u56e0\u4e3a\u5f88\u591a\u79ef\u6781\u7684\u4e8b\u60c5\uff0c\u90fd\u5728\u52aa\u529b\u4e2d\uff0c\u53d8\u5f97\u4e0a\u763e\u3002</p> <p>\u9ebb\u5c06\u5f53\u7136\u4e5f\u662f\u4e00\u6b21\u805a\u4f1a\u7684\u597d\u65f6\u673a\u3002\u5927\u5bb6\u90fd\u80fd\u591f\u804a\u804a\u5404\u81ea\u7684\u751f\u6d3b\uff0c\u4e0d\u540c\u7684\u4eba\u4e0d\u540c\u7684\u5730\u65b9\uff0c\u6709\u7740\u76f8\u4f3c\u7684\u60c5\u611f\u3002</p> <p>\u6700\u91cd\u8981\u7684\u662f\uff0c\u6211\u80fd\u591f\u4e3a\u6211\u7684\u5bb6\u5ead\u628a\u628a\u8109\uff0c\u770b\u770b\u6709\u6ca1\u6709\u4ec0\u4e48\u6bdb\u75c5\uff0c\u63a5\u4e0b\u6765\u79bb\u5bb6\u540e\uff0c\u6211\u662f\u4e0d\u662f\u8981\u505a\u4e00\u4e9b\u8c03\u6574\u3002\u6700\u91cd\u8981\u7684\u662f\u6211\u548c\u7238\u5988\u7684\u8ddd\u79bb\u53d8\u5f97\u66f4\u52a0\u9065\u8fdc\uff0c\u6211\u53d1\u73b0\u4e4b\u524d\u65e5\u5e38\u7684\u4e2d\u9910\u5e76\u4e0d\u80fd\u5f88\u597d\u5730\u89e3\u51b3\u95ee\u9898\u3002\u6211\u9700\u8981\u591a\u53d1\u4e00\u4e9b\u7167\u7247\uff0c\u5173\u4e8e\u6211\u7684\u751f\u6d3b\uff0c\u6211\u7684\u5468\u672b\uff0c\u8ba9\u4ed6\u4eec\u591a\u770b\u770b\u6211\u5230\u5e95\u662f\u600e\u4e48\u505a\u7684\u3002</p> <p>\u5047\u671f\u7ed3\u5c3e\u6211\u7adf\u7136\u80fd\u591f\u8bfb\u5230\u4e00\u53e5\u597d\u6587\uff1a \u4e09\u8054\u751f\u6d3b\u5468\u520a<pre><code>\u4e0d\u8981\u8bd5\u56fe\u5bfb\u627e\u4e00\u79cd\u201c\u4e0d\u7126\u8651\u201d\u7684\u72b6\u6001\uff0c\u56e0\u4e3a\u7126\u8651\u4e0e\u4eba\u7684\u9009\u62e9\u548c\u751f\u5b58\u76f8\u4f34\u968f\uff0c\u4e0d\u5982\u5b66\u4f1a\u4e00\u4e9b\u5c0f\u7684\u6280\u5de7\uff0c\u8ba9\u81ea\u5df1\u80fd\u548c\u7126\u8651\u5171\u5904\u3002\n</code></pre></p> <p>\u5b89\u6170\u4e86\u66fe\u7ecf\u52aa\u529b\u7684\u81ea\u5df1\uff0c\u6e05\u6670\u4e86\u5bf9\u672a\u6765\u7684\u52aa\u529b\u3002</p> <p>\u4ee5\u524d\u63a2\u7d22\u7684\u5404\u79cd\u6709\u76ca\u7684\u65b9\u5f0f\uff0c\u90fd\u80fd\u591f\u548c\u7126\u8651\u4e32\u8054\u8d77\u6765\uff0c\u9a7e\u9a6d\u7126\u8651\uff0c\u4e3a\u6211\u6240\u7528\u3002</p> <p>\u611f\u89c9\u81ea\u5df1\u7684\u5f88\u591a\u53d1\u8a00\u90fd\u6709\u529f\u5229\u5b9e\u7528\u4e3b\u4e49\u7684\u8272\u5f69\uff0c\u65e5\u6e10\u957f\u5927\u7684\u81ea\u5df1\u5f80\u5f80\u90fd\u662f\u5bf9\u7406\u60f3\u5931\u53bb\u8010\u5fc3\u548c\u4fe1\u5fc3\uff0c\u8d70\u4e00\u6b65\u770b\u4e00\u6b65\uff0c\u800c\u603b\u662f\u8981\u56de\u5934\u770b\u770b\uff0c\u81ea\u5df1\u4e3a\u4ec0\u4e48\u51fa\u53d1\u3002</p> <p>\u6211\u89c9\u5f97\u5bf9\u6211\u6765\u8bf4\uff0c\u6211\u9700\u8981\u4e0d\u65ad\u6253\u5f00\u81ea\u5df1\u3002\u6211\u80fd\u591f\u653e\u4e0b\u5f88\u591a\u4ee5\u524d\u6240\u8ba4\u4e3a\u7684\u7981\u5fcc\uff0c\u53bb\u505a\u5f88\u591a\u5b9e\u9645\u4e0a\u90fd\u80fd\u591f\u505a\u7684\u4e8b\u60c5\u3002</p> <p>\u5176\u5b9e\u5f88\u591a\u4e1c\u897f\u90fd\u662f\u4f1a\u4e0a\u763e\u7684\u3002</p> <p>\u5b66\u4f1a\u628a\u4e0d\u65ad\u7684\u4e0a\u763e\u53d8\u6210\u81ea\u5df1\u6700\u559c\u6b22\u7684\u6a21\u6837\u3002</p>"},{"location":"courses/","title":"Content","text":"<p>The following contains some notes of courses at ZJU.</p>"},{"location":"courses/#c-plus-plus","title":"C Plus Plus","text":"<p>To be continued...</p>"},{"location":"courses/#material-point-method","title":"Material Point Method","text":"<p>To be continued...</p>"},{"location":"courses/#modern-control-theory","title":"Modern Control Theory","text":"<p>completed.</p>"},{"location":"courses/#numerical-analysis","title":"Numerical Analysis","text":"<p>To be continued...</p>"},{"location":"courses/#ordinary-differential-equation","title":"Ordinary Differential Equation","text":"<p>To be continued...</p>"},{"location":"courses/#sensing-detection","title":"Sensing &amp; Detection","text":"<p>Maybe will only present the midterm exam.</p>"},{"location":"courses/Data_Structure%26Algorithm/","title":"Index","text":""},{"location":"courses/Data_Structure%26Algorithm/#data-structure-and-algorithm","title":"Data Structure and Algorithm","text":"<p>Reference</p> <p>Data structures and algorithm analysis in C++, Mark Allen Weiss</p>"},{"location":"courses/Data_Structure%26Algorithm/#algorithm-analysis","title":"Algorithm Analysis","text":""},{"location":"courses/Data_Structure%26Algorithm/#lists-stacks-and-queues","title":"Lists, Stacks and Queues","text":""},{"location":"courses/Data_Structure%26Algorithm/#trees","title":"Trees","text":""},{"location":"courses/Data_Structure%26Algorithm/#hashing","title":"Hashing","text":""},{"location":"courses/Data_Structure%26Algorithm/#priority-queuesheaps","title":"Priority Queues(Heaps)","text":""},{"location":"courses/Data_Structure%26Algorithm/#sorting","title":"Sorting","text":""},{"location":"courses/Data_Structure%26Algorithm/#the-disjoint-sets-class","title":"The disjoint Sets Class","text":""},{"location":"courses/Data_Structure%26Algorithm/#graph-algorithm","title":"Graph Algorithm","text":""},{"location":"courses/Data_Structure%26Algorithm/Dis_Sets_Class/","title":"The disjoint Sets Class","text":""},{"location":"courses/Data_Structure%26Algorithm/Dis_Sets_Class/#union-by-size","title":"Union-by-Size","text":"<p>change size when union</p>"},{"location":"courses/Data_Structure%26Algorithm/Dis_Sets_Class/#union-by-height","title":"Union-by-Height","text":"<p>does not need to change height when union, tree height only changes when two trees are of the same size,</p> <p>find is dynamic, let the found node points directly to the root node. However, we need to change the height.</p>"},{"location":"courses/Data_Structure%26Algorithm/Dis_Sets_Class/#union-by-rank","title":"Union-by-Rank","text":"<p>After find(), we do not change the height. So the height bacomes the rank.</p>"},{"location":"courses/Data_Structure%26Algorithm/Dis_Sets_Class/#generate-maze","title":"Generate Maze","text":""},{"location":"courses/Data_Structure%26Algorithm/Graph/","title":"Graph Algorithm","text":""},{"location":"courses/Data_Structure%26Algorithm/Graph/#preliminary-dynamic-programming","title":"Preliminary: Dynamic Programming","text":"<p>(Note: content of this part is different from dynamic programming in combination optimization)</p> <ul> <li>Longest common subsequence, LCS</li> </ul> <p>input: \\(x=\\{x_i\\}_{i=1}^m\\), \\(y=\\{y_i\\}_{i=1}^n\\),</p> <p>output: their LCS.</p> <p>For example, if we have two sequences </p> \\[ \\begin{cases} x=\\{A,B,C,B,D,A,B\\}\\\\ y=\\{B,D,C,A,B,A\\} \\end{cases} \\] <p>then LCS\\((x,y)=\\{BDAB,BCAB,BCBA\\}\\).</p> <ul> <li>solve the length of LCS.</li> </ul> Proof <ul> <li>\\(x[i]=y[j]\\), if \\(C[i,j]=k\\), denote one element \\(z[1,\\cdots,k]\\in\\) LCS\\([i,j]\\). then \\(z[k]=x[i]=y[j]\\). So \\(z[1,\\cdots,k-1]\\in LCS[i-1,j-1]\\).</li> </ul> <p>Assume \\(w\\in\\) LCS\\([i-1.j-1]\\) and \\(|w|&gt;k-1\\), then connect \\(w\\) and \\(z[k]\\), \\(w+z[k]\\) satisfies</p> \\[ \\begin{cases} w+z[k]\\in \\text{LCS}[i,j]\\\\ \\left|w+z[k]\\right|&gt;k \\end{cases} \\] <p>contradicts!</p> <ul> <li>optimal substructure</li> </ul> <p>\\(\\forall z[1,\\cdots,k]\\in\\) LCS\\([m,n]\\), then \\(z[1,\\cdots,s]\\), \\(s&lt; k\\) must be LCS of their prefix.</p> Natural Version for LCS<pre><code>LCS(x,y,i,j)\n    if(i==-1||j==-1)\n        return 0\n    else if (x[i]==y[j])\n        c[i,j]=LCS(x,y,i-1.j-1)+1\n    else \n        c[i,j]=max(LCS(x,y,i,j-1),LCS(x,y,i-1.j))\n    return c[i,j]\n</code></pre> <ul> <li>cut off </li> </ul> <p>write a note of LCS that has been calculated. So next time we encounter a same item, directly use it. In fact we use a 2 dimension array.</p> Better Version for LCS<pre><code>LCS(x,y,i,j)\n    if (c[i,j]==nil)\n        if (x[i]==y[j])\n            c[i,j]=LCS(x,y,i-1.j-1)+1\n        else \n            c[i,j]=max(LCS(x,y,i,j-1),LCS(x,y,i-1.j))\n    return c[i,j]\n</code></pre> <p>time cost \\(O(mn)\\).</p> <p>\u81ea\u9876\u5411\u4e0b top down.</p> <p>\u81ea\u5e95\u5411\u4e0a bottom up.</p>"},{"location":"courses/Data_Structure%26Algorithm/Graph/#gragh-theory","title":"Gragh theory","text":""},{"location":"courses/Data_Structure%26Algorithm/Graph/#definitions","title":"Definitions","text":"<p>A graph \\(G=(V,E)\\) consists a set of vertices, \\(V\\) and a set of edges \\(E\\). for \\(u,w\\in V\\), \\((u,w)\\in E\\).</p> <p>If \\((u,w)\\neq(w,u)\\), then we call the graph is directed, or digraph. Otherwise we call the graph undirected.</p> <p>We call \\(u,w\\in V\\) are adjacent iff \\((u,w)\\in E\\). A map from \\(E\\) to \\(\\mathbb{R}\\) is called weight, or cost.</p> <p>A path is a sequence of vertices \\(\\{w_1,w_2,\\cdots,w_N\\}\\), such that \\((w_i,w_{i+1})\\in E\\), \\(\\forall i\\in [1,N-1]\\). When there is not weight defined, the length of the path is \\(N-1\\).</p> <p>\u81ea\u73af\u8def\u5f84(Loop). </p> <p>A simple path is a path such that all vertices are distinct, except that the first and the last could be the same.</p> <p>A cycle path(\u5faa\u73af\u8def\u5f84) in a directed graph is a path of length at least 1 such that \\(w_1=w_N\\).  </p> <p>A directed graph is acyclic if it has no cycles.</p> <p>Directed Acyclic Graph(\u6709\u5411\u65e0\u73af\u56fe). </p> <p>An undirected graph is connected, if \\(\\forall u,w\\in V\\), \\((u,w)\\in E\\). A directed graph with this property is called strongly connected. </p>"},{"location":"courses/Data_Structure%26Algorithm/Graph/#representation-of-graph","title":"Representation of Graph","text":"<ul> <li>Use 2 dimension array. Adjacency matrix.</li> </ul> <p>If \\(|E|= \\Theta(|V|^2)\\), then the graph is dense.</p> <p>If \\(|E|= \\Theta(|V|)\\), then the graph is sparse.</p> <ul> <li>Use linked-list. Adjacency list.</li> </ul> <p>For each vertex, we keep a list of all adjacent vertices. </p> <p>Space cost \\(O(|V|+|E|)\\).</p>"},{"location":"courses/Data_Structure%26Algorithm/Graph/#topological-sort","title":"Topological Sort","text":"<p>Definition of Topological Sort</p> <ul> <li>\u62d3\u6251\u5e8f | Topological Sort</li> </ul> <p>\\(\\forall v_i,v_j\\in V\\), we say \\(v_i\\leq v_j\\), if there exists </p> <p>(i) a path from \\(v_i\\) to \\(v_j\\)</p> <p>(ii) \\(v_i\\) is before \\(v_j\\)</p> <ul> <li>\u62d3\u6251\u6392\u5e8f | Topological Sorting</li> </ul> <p>list \\(\\{v_i\\}_{i-1}^n\\) into a queue</p> \\[ v'_1, v'_2,\\cdots, v'_n \\] <p>such that, \\(\\forall v'_i\\neq v'_j\\), we have</p> <p>(i) \\(v_i\\leq v_j\\)</p> <p>(ii) there exists no path from \\(v_i\\) to \\(v_j\\).</p> <ul> <li>\u5165\u5ea6 | indegree</li> </ul> <p>there must exist a node with indegree = 0.</p> Version 1 for topological sort<pre><code>topsort()\n{\n    for(i=0;i&lt;n_vertex;i++)\n        v = FindNextVertexIndegreeZero() // actually we could only do this for the first time.\n        v.tp=i;\n        for each w adjacent to v\n            w.indegree --;\n}\n</code></pre> Better version for topological sort<pre><code>void Graph::topsort()\n{\n    Queue&lt;vertex&gt; q;\n    int counter = 0;\n    q.makeEmpty();\n    for each vertex in V:\n        if v.indegree==0:\n            q.enqueue(v); // O(|V|)\n\n    while(!q.empty()){\n        vertex v= q.dequeue();\n        v.tp=++counter;\n        for each vertex w adjacent to v:\n            if(--w.indegree==0):\n                q.enqueue(w); \n    } // O(|E|+|V|)\n\n    if(counter != NUM_VERTICES)\n        throw CYclwFoundException();\n}\n</code></pre>"},{"location":"courses/Data_Structure%26Algorithm/Graph/#shortest-path-algorithm","title":"Shortest-Path Algorithm","text":"<p>Input is a weighted Graph.</p> <p>Assume weight is positive.</p> <p>Theorem</p> <p>If \\(P_{(u,v)}\\) is the shortest path between u and v, then \\(\\forall x,y\\in P_{(u,v)}\\), \\(P_{(x,y)}\\subset P_{(u,v)}\\) is the shortest path bwtween x and y.</p> <p>Or triangle inequality. forall \\(x,y,z \\in V\\)</p> \\[ \\delta(x,y)\\leq \\delta(x,z)+\\delta(z,y) \\] Proof <p>By contradiction. If there exists another path \\(P'_{(x,y)}\\neq P_{(x,y)}\\) and </p> \\[ w(P'_{(x,y)})&lt; w(P_{(x,y)}) \\] <p>then </p> \\[ \\begin{align*} w{(P'_{(u,v)})} &amp;= w{(P_{(u,x)})} + w{(P'_{(x,y)})} + w{(P_{(y,v)})}\\\\ &amp;&lt; w{(P_{(u,x)})} + w{(P_{(x,y)})} + w{(P_{(y,v)})}\\\\ &amp;=w{(P_{(u,v)})} \\end{align*} \\] <p>that is, we find a path that is smaller than the shortest path, which contradicts!</p> <p>We also call the above problem Single-Source shortest path.</p> <p>The method is known as Dijlstra's Algorithm.</p>"},{"location":"courses/Data_Structure%26Algorithm/Graph/#unweighted-graph","title":"Unweighted graph","text":"<p>This is actually BFS. Unweighted graph<pre><code>void Graph::unweighted(Vertex s)\n{   \n    Queue&lt;Vertex&gt; q;\n    for each Vertex v\n    {\n        v.dist = INFTY;\n        v.known = false;\n    }\n\n    s.dist=0;\n    q.enqueue(s);\n\n    while(!q.isEmpty())\n    {\n        Vertex v = q.dequeue();\n\n        for each Vertex w adjacent to v:\n            if(w.dist == INFTY):\n            {\n                w.dist = v.dist + 1;\n                w.path = v;\n                q.enqueue(w);\n            }\n    }\n}\n</code></pre></p> <p>Cost: \\(O(|E|+|V|)\\)</p>"},{"location":"courses/Data_Structure%26Algorithm/Graph/#weighted-graph","title":"Weighted Graph","text":"<p>This time, we cannot say that the neighberhood is the shortest, which may be longer than some path with more vertices but less weight cost.</p> Weighted Graph<pre><code>s.d=0;\nfor each v in V-{s}\n    v.d=INFTY;\nQueue=V;\nwhile(!q.isEmpty()){\n    u = q.pop_min(); // smallest heap\n    for each v adjacent to u:\n        if v.d&gt;u.d+w(u,v):\n            v.d = u.d+w(u,v);\n            v.p = u;\n}\n</code></pre>"},{"location":"courses/Data_Structure%26Algorithm/Graph/#all-pair-shortest-path","title":"All-pair shortest path","text":"<p>use adjacent matrix \\(A=(a_{ij})_{n\\times n}\\), with \\(a_{ij}\\) denotes the weight from \\(v_i\\) to \\(v_j\\) and \\(a_{ii}=0\\).</p> <p>use \\(d_{ij}^{(m)}\\) to denote the shortest path weight from \\(v_i\\) to \\(v_j\\) with passing \\(m\\) edges.</p>"},{"location":"courses/Data_Structure%26Algorithm/Heaps/","title":"Heaps","text":""},{"location":"courses/Data_Structure%26Algorithm/sorting/","title":"Sorting","text":"<ul> <li> <p>\u6548\u7387 | Efficiency</p> </li> <li> <p>\u539f\u5730\u6027 | In Place</p> </li> <li> <p>\u7a33\u5b9a\u6027 | Stability</p> </li> </ul> <p>A sorting algorithm is stable if elements with equal elements are left in the same order as they occur in the input.</p> <p>A stable algorithm can be suitable for Multiple targets.</p>"},{"location":"courses/Data_Structure%26Algorithm/sorting/#insertion-sort","title":"Insertion Sort","text":""},{"location":"courses/Data_Structure%26Algorithm/sorting/#shell-sort","title":"Shell Sort","text":"<p>\\(h_t\\) = \\(\\lfloor N/2 \\rfloor\\), \\(h_k\\) = \\(\\lfloor h_{k+1}/2 \\rfloor\\).</p> <p>with Worst case \\(O(N^2)\\)</p> <p>Now we have better selection with worst case \\(O(N^{7/6})\\) with </p> \\[ h_k= \\begin{cases} 9\\cdot 4^i - 9\\cdot 2^i+1, \\quad i\\mod 2 == 0\\\\ 4^{i+1}-6 \\cdot 2^i+1, \\quad i\\mod 2 == 1 \\end{cases} \\]"},{"location":"courses/Data_Structure%26Algorithm/sorting/#heap-sort","title":"Heap Sort","text":""},{"location":"courses/Data_Structure%26Algorithm/sorting/#merge-sort","title":"Merge Sort","text":""},{"location":"courses/Data_Structure%26Algorithm/sorting/#quick-sort","title":"Quick Sort","text":"<ul> <li> <p>Picking the Pivot</p> </li> <li> <p>Partition</p> </li> </ul> Partition<pre><code>Partition(A, p, q)\n    x=A[p]\n    i=p+1\n    j=q\n    while(i!=j)\n        while(A[j]&gt;=x &amp;&amp; i&lt;j)\n            j-- // put larger element in place \n        while(A[i]&lt;=x &amp;&amp; i&lt;j)\n            i++ // put smaller element in place \n        if(i&lt;j)\n            swap(A[i], A[j])\n    swap(A[p], A[i])\n</code></pre> Another Partition<pre><code>Partition(A, p, q)\n    x=A[p]\n    i=p\n    for j=p+1 to q:\n        if(A[j]&lt;=x)\n            i=i+1\n            swap(A[i],A[j])\n    A[p]=A[i]\n    return i\n</code></pre> QuickSort<pre><code>QuickSort(A,p,q)\n    if(p&lt;q)\n        r=partition(A,p,q)\n        QuickSort(A,p,r-1)\n        QuickSort(A,r+1,q)\n</code></pre> <p>Time cost:</p> <ul> <li>Ordered sequence </li> </ul> \\[ \\begin{align*} T(n)=T(1)+T(n-1)+\\Theta(n) \\end{align*} \\] <ul> <li>Best sequence</li> </ul> \\[ T(n)=2T(n/2)+\\Theta(n) \\] <ul> <li>not Bad or Good</li> </ul> <p>Every Partition gives 1:9 sequence</p>"},{"location":"courses/Data_Structure%26Algorithm/sorting/#randomize-pivot","title":"Randomize pivot","text":"Text Only<pre><code>Randomized_Partition(A,p,q)\n    i=random(p,q)\n    swap(A[p],A[i])\n    return Partition(A,p,q)\n</code></pre> Text Only<pre><code>randomized_quicksort(A,p,q)\n    if(p&lt;q)\n        r=randomized_Partition(A,p,q+1)\n        randomized_quicksort(A,p,r-1)\n        randomized_quicksort(A,r+1,q)\n</code></pre>"},{"location":"courses/Data_Structure%26Algorithm/sorting/#some-details","title":"Some Details","text":"<ul> <li>Consider repetitive elements</li> </ul> Text Only<pre><code>Hoore_Partition(A,p,q)\n    x=A[p]\n    i=p, j=q+1\n    while(1):\n        do:\n            j=j-1\n        until(A[j]&lt;=x)\n        do:\n            i=i+1\n        until(A[i]&gt;=x)\n        if(i&lt;j)\n            swap(A[i],A[j])\n        else\n            swap(A[p],A[j])\n            return j\n</code></pre>"},{"location":"courses/Data_Structure%26Algorithm/sorting/#a-general-lower-bound-for-sorting","title":"A General Lower Bound for Sorting","text":"<p>Efficiency of sorting with comparing</p> <p>The best efficiency of sorting based on comparing is \\(\\Theta(n\\log{n})\\)</p> Proof <p>left tree denotes <code>true</code>, right tree denotes <code>false</code>, every leaf node represents a result of sorting. We have \\(n!\\) nodes. And let h denote the height of the decision tree. we have </p> \\[ n!\\leq 2^h \\] <p>\"=\" holds for complete binary decision tree. So</p> \\[ h\\geq \\log{n!} \\geq \\log{(n/e)^n} =\\Omega(n\\log{n}) \\]"},{"location":"courses/Intro2Visualization/","title":"Introduction to Visualization","text":""},{"location":"courses/Intro2Visualization/#midterm-doc","title":"Midterm doc","text":""},{"location":"courses/Intro2Visualization/midterm/","title":"\u9879\u76ee\u80cc\u666f","text":"<p>\u300a\u540d\u4fa6\u63a2\u67ef\u5357\u300b\u4f5c\u4e3a\u63a8\u7406\u52a8\u6f2b\u7684\u7ecf\u5178\u4ee3\u8868\uff0c\u81ea1996\u5e74\u5f00\u64ad\u4ee5\u6765\uff0c\u4ee5\u4e30\u5bcc\u7684\u6848\u4ef6\u7c7b\u578b\u3001\u590d\u6742\u7684\u89d2\u8272\u5173\u7cfb\u548c\u6301\u7eed\u63a8\u8fdb\u7684\u4e3b\u7ebf\u5267\u60c5\uff0c\u5438\u5f15\u4e86\u5168\u7403\u89c2\u4f17\u3002\u5176\u5185\u5bb9\u4f53\u7cfb\u6db5\u76d6\u5343\u96c6\u52a8\u753b\u3001\u6570\u5341\u90e8\u5267\u573a\u7248\u7535\u5f71\uff0c\u4ee5\u53ca\u5927\u91cf\u6848\u4ef6\u4fe1\u606f\u548c\u7ec4\u7ec7\u4e92\u52a8\uff0c\u5c55\u73b0\u51fa\u6781\u9ad8\u7684\u6587\u5316\u4ef7\u503c\u548c\u6570\u636e\u5206\u6790\u6f5c\u529b\u3002 \u672c\u9879\u76ee\u65e8\u5728\u6316\u6398\u300a\u540d\u4fa6\u63a2\u67ef\u5357\u300b\u7684\u6570\u636e\uff0c\u901a\u8fc7\u53ef\u89c6\u5316\u5c55\u793a\u6848\u4ef6\u5206\u5e03\u3001\u89d2\u8272\u5173\u7cfb\u7f51\u7edc\u548c\u65f6\u95f4\u7ebf\u53d1\u5c55\u89c4\u5f8b\u7b49\uff0c\u5e76\u7ed3\u5408\u7528\u6237\u4ea4\u4e92\u529f\u80fd\uff0c\u6253\u9020\u591a\u7ef4\u5ea6\u7684\u63a2\u7d22\u4f53\u9a8c\u3002\u8fd9\u4e0d\u4ec5\u4e3a\u7c89\u4e1d\u6df1\u5165\u7406\u89e3\u4f5c\u54c1\u63d0\u4f9b\u5de5\u5177\uff0c\u4e5f\u80fd\u4e3a\u63a8\u7406\u7c7b\u521b\u4f5c\u63d0\u4f9b\u6570\u636e\u652f\u6301\u548c\u7075\u611f\u542f\u53d1\u3002</p>"},{"location":"courses/Intro2Visualization/midterm/#_2","title":"\u8bbe\u8ba1\u65b9\u6848","text":"<ul> <li> <p>\u72af\u6848\u5730\u70b9\uff0c\u7edf\u8ba1\u67ef\u5357\u5404\u7c7b\u4e8b\u4ef6\u53d1\u751f\u7684\u5730\u70b9\uff0c\u7528\u65e5\u672c\u5730\u56fe\uff0c\u505a\u5730\u56fe\u53ef\u89c6\u5316</p> </li> <li> <p>\u753b\u4e00\u5f20\u4eba\u7269\u5173\u7cfb\u7f51\u7edc\uff0c\u5404\u7c7b\u4eba\u7269\u663e\u793a\u53ef\u4ee5\u6309\u7167\u65f6\u95f4\u987a\u5e8f\u6765\u51fa\u73b0\u3002\u5e76\u6dfb\u52a0\u4ea4\u4e92\u3002\u4ea4\u4e92\u70b9\u5f00\u6709\u4ea4\u4e92\u96c6\u6570\uff0c\u4eba\u7269\u521d\u767b\u573a\u96c6\u6570</p> </li> <li> <p>\u5267\u573a\u7248\u7968\u623f\u3001\u53e3\u7891\u7684\u5750\u6807\u56fe\uff0c\u5404\u4e2a\u5267\u573a\u7248\u53ef\u4ee5\u6309\u7167\u65f6\u95f4\u987a\u5e8f\uff0c\u9010\u4e2a\u63d2\u5165\u3002\u5267\u573a\u7248\u5173\u952e\u8bcd\uff0c\u70ed\u641c\u8bcd\u7684\u8bcd\u4e91\u56fe\uff0c</p> </li> <li> <p>\u52a8\u6001\u65f6\u95f4\u8f74\uff0c\u589e\u52a0\u9752\u5c71\u7684\u66f4\u65b0\u8bb0\u5f55\u56fe\uff0c\u4ee5\u53ca\u4e3b\u7ebf\u91cd\u5927\u4e8b\u4ef6\u53d1\u751f\u7684\u65f6\u95f4\uff08\u67ef\u5357\u65f6\u95f4\u548c\u73b0\u5b9e\u53d1\u8868\u65f6\u95f4\uff09</p> </li> </ul>"},{"location":"courses/Intro2Visualization/midterm/#_3","title":"\u72af\u6848\u5730\u70b9\u53ef\u89c6\u5316","text":"<ul> <li>\u53ef\u89c6\u5316\u6548\u679c\uff1a</li> </ul> <p>\u5730\u56fe\u5c42\u6b21\uff1a\u4f7f\u7528\u65e5\u672c\u5730\u56fe\u6807\u8bb0\u6bcf\u4e2a\u6848\u4ef6\u53d1\u751f\u7684\u5730\u70b9\uff0c\u6848\u4ef6\u53d1\u751f\u7684\u5bc6\u96c6\u533a\u57df\u4f7f\u7528\u989c\u8272\u52a0\u6df1\u6216\u901a\u8fc7\u70ed\u529b\u56fe\u663e\u793a\u3002\u6bcf\u4e2a\u6848\u4ef6\u7684\u5730\u70b9\u53ef\u4ee5\u901a\u8fc7\u4e0d\u540c\u7684\u56fe\u6807\u6807\u6ce8\u51fa\u6765\uff0c\u4f8b\u5982\u5bb6\u3001\u516c\u53f8\u3001\u5b66\u6821\u3001\u516c\u5171\u573a\u6240\u7b49\u3002</p> <p>\u65f6\u95f4\u8f74\u8fc7\u6ee4\uff1a\u53ef\u4ee5\u589e\u52a0\u65f6\u95f4\u8f74\u6ed1\u52a8\u6761\uff0c\u7528\u6237\u53ef\u4ee5\u9009\u62e9\u67e5\u770b\u4e0d\u540c\u65f6\u95f4\u6bb5\u5185\u53d1\u751f\u7684\u6848\u4ef6\uff08\u4f8b\u5982\u6bcf\u4e2a\u5b63\u8282\u6216\u5e74\u5ea6\u7684\u6848\u4ef6\u5206\u5e03\uff09\u3002</p> <p>\u6848\u4ef6\u7c7b\u578b\u8fc7\u6ee4\uff1a\u5bf9\u6848\u4ef6\u7c7b\u578b\uff08\u4f8b\u5982\u8c0b\u6740\u6848\u3001\u76d7\u7a83\u6848\u7b49\uff09\u8fdb\u884c\u8fc7\u6ee4\u663e\u793a\uff0c\u6216\u8005\u901a\u8fc7\u6848\u4ef6\u7c7b\u578b\u7684\u4e0d\u540c\u6807\u8bb0\u56fe\u6807\u533a\u5206\u3002</p> <p>\u4ea4\u4e92\u529f\u80fd\uff1a\u70b9\u51fb\u67d0\u4e00\u5730\u70b9\u6216\u6848\u4ef6\u540e\uff0c\u663e\u793a\u6848\u4ef6\u7684\u8be6\u7ec6\u4fe1\u606f\uff0c\u5305\u62ec\u6848\u53d1\u7684\u5177\u4f53\u96c6\u6570\u3001\u6848\u4ef6\u7c7b\u578b\u3001\u6848\u4ef6\u7ed3\u679c\u7b49\u3002</p> <p>\u6570\u636e\u6765\u6e90\uff1a\u53ef\u4ee5\u4ece\u67ef\u5357\u7684\u6570\u636e\u5e93\uff08\u5982\u67ef\u5357wiki\u3001Fandom\u7b49\uff09\u4e2d\u63d0\u53d6\u6bcf\u4e2a\u6848\u4ef6\u7684\u53d1\u751f\u5730\u70b9\u548c\u65f6\u95f4\u7b49\u6570\u636e\uff0c\u8fdb\u884c\u6574\u7406\u548c\u6c47\u603b\u3002</p> <ul> <li>\u5206\u6790\u89d2\u5ea6\uff1a</li> </ul> <p>\u5730\u57df\u5206\u5e03\uff1a\u4e0d\u540c\u5730\u533a\u6848\u4ef6\u7684\u53d1\u751f\u9891\u7387\uff0c\u53ef\u4ee5\u5206\u6790\u54ea\u4e9b\u5730\u533a\u7684\u6848\u4ef6\u53d1\u751f\u9891\u7e41\uff0c\u8fd9\u53ef\u4ee5\u53cd\u6620\u67ef\u5357\u5267\u60c5\u4e2d\u7684\u5730\u57df\u6027\u7279\u5f81\u3002</p> <p>\u6848\u4ef6\u5bc6\u5ea6\u4e0e\u5267\u60c5\u53d1\u5c55\uff1a\u8003\u5bdf\u6848\u4ef6\u5206\u5e03\u7684\u53d8\u5316\u8d8b\u52bf\uff0c\u4f8b\u5982\uff0c\u662f\u5426\u6709\u67d0\u4e00\u5730\u533a\u6848\u4ef6\u7a81\u7136\u589e\u591a\uff0c\u53ef\u80fd\u4e0e\u5267\u60c5\u7684\u53d8\u5316\u6216\u67d0\u4e9b\u89d2\u8272\u7684\u52a0\u5165\u6709\u5173\u3002</p>"},{"location":"courses/Intro2Visualization/midterm/#_4","title":"\u4eba\u7269\u5173\u7cfb\u7f51\u7edc","text":"<ul> <li>\u53ef\u89c6\u5316\u6548\u679c\uff1a</li> </ul> <p>\u65f6\u95f4\u5e8f\u5217\u5173\u7cfb\u56fe\uff1a\u6784\u5efa\u4e00\u4e2a\u52a8\u6001\u7684\u4eba\u7269\u5173\u7cfb\u7f51\u7edc\uff0c\u53ef\u4ee5\u5c06\u4eba\u7269\u7684\u767b\u573a\u65f6\u95f4\u4e0e\u5176\u5173\u7cfb\u5c55\u793a\u5728\u4e00\u4e2a\u65f6\u95f4\u8f74\u4e0a\uff0c\u70b9\u51fb\u65f6\u95f4\u8f74\u4e0a\u67d0\u4e2a\u8282\u70b9\u65f6\uff0c\u5c55\u793a\u8be5\u65f6\u6bb5\u5185\u65b0\u589e\u89d2\u8272\u4e0e\u5df2\u6709\u89d2\u8272\u7684\u5173\u7cfb\u53d8\u5316\u3002</p> <p>\u8282\u70b9\u989c\u8272\uff1a\u6839\u636e\u4eba\u7269\u7684\u5173\u7cfb\u6027\u8d28\uff08\u4f8b\u5982\u4fa6\u63a2\u3001\u9ed1\u8863\u7ec4\u7ec7\u6210\u5458\u3001\u8b66\u5bdf\u3001\u5acc\u7591\u4eba\u7b49\uff09\u7ed9\u4e0d\u540c\u4eba\u7269\u89d2\u8272\u8bbe\u7f6e\u4e0d\u540c\u7684\u989c\u8272\u3002</p> <p>\u8fb9\u7684\u6743\u91cd\uff1a\u4eba\u7269\u95f4\u7684\u5173\u7cfb\u5f3a\u5ea6\uff08\u4f8b\u5982\u670b\u53cb\u3001\u5bf9\u624b\u3001\u5408\u4f5c\u4f19\u4f34\u7b49\uff09\u53ef\u4ee5\u7528\u4e0d\u540c\u7c97\u7ec6\u7684\u8fb9\u8868\u793a\uff0c\u70b9\u51fb\u8fb9\u663e\u793a\u8be5\u5173\u7cfb\u7684\u5177\u4f53\u60c5\u8282\u3002</p> <p>\u4ea4\u4e92\u6027\uff1a\u7528\u6237\u53ef\u4ee5\u70b9\u51fb\u4eba\u7269\u8282\u70b9\u67e5\u770b\u5176\u9996\u6b21\u51fa\u73b0\u7684\u96c6\u6570\u4ee5\u53ca\u76f8\u5173\u7684\u5267\u60c5\u6458\u8981\u3002\u8fd8\u53ef\u4ee5\u8fc7\u6ee4\u65f6\u95f4\u6bb5\u6216\u6309\u7c7b\u578b\u67e5\u770b\u4e0d\u540c\u4eba\u7269\u7684\u5173\u7cfb\u7f51\uff08\u4f8b\u5982\u4e3b\u89d2\u3001\u53cd\u6d3e\u3001\u914d\u89d2\u7b49\uff09\u3002</p> <p>\u6570\u636e\u6765\u6e90\uff1a\u53ef\u4ee5\u4f9d\u8d56\u300a\u540d\u4fa6\u63a2\u67ef\u5357\u300b\u5b98\u65b9\u8d44\u6599\u3001\u5404\u7c7b\u52a8\u6f2b\u6570\u636e\u5e93\uff08\u5982MyAnimeList\uff0cFandom\uff09\u4ee5\u53ca\u901a\u8fc7\u5267\u60c5\u68b3\u7406\u81ea\u5b9a\u4e49\u4eba\u7269\u51fa\u73b0\u548c\u5173\u7cfb\u7684\u6570\u636e\u96c6\u3002</p> <ul> <li>\u5206\u6790\u89d2\u5ea6\uff1a</li> </ul> <p>\u4eba\u7269\u53d1\u5c55\u8f68\u8ff9\uff1a\u89c2\u5bdf\u5404\u4e2a\u89d2\u8272\u7684\u51fa\u73b0\u987a\u5e8f\u3001\u5173\u7cfb\u53d1\u5c55\u4e0e\u4e3b\u8981\u5267\u60c5\u7684\u5173\u7cfb\u3002\u4f8b\u5982\uff0c\u67ef\u5357\u3001\u7070\u539f\u3001\u5149\u5f66\u3001\u6b65\u7f8e\u7b49\u4e3b\u8981\u4eba\u7269\u7684\u767b\u573a\u4e0e\u6545\u4e8b\u7684\u5173\u8054\u3002</p> <p>\u89d2\u8272\u4e4b\u95f4\u7684\u4e92\u52a8\uff1a\u5206\u6790\u89d2\u8272\u95f4\u5173\u7cfb\u7684\u53d8\u5316\uff0c\u4f8b\u5982\u4ece\u654c\u5bf9\u5230\u5408\u4f5c\uff0c\u6216\u8005\u4ece\u670b\u53cb\u5230\u964c\u751f\u4eba\u7b49\u3002</p>"},{"location":"courses/Intro2Visualization/midterm/#_5","title":"\u5267\u573a\u7248\u7968\u623f\u548c\u53e3\u7891\u7684\u5750\u6807\u56fe","text":"<ul> <li>\u53ef\u89c6\u5316\u6548\u679c\uff1a</li> </ul> <p>\u5750\u6807\u8f74\uff1a\u6a2a\u8f74\u4ee3\u8868\u5267\u573a\u7248\u4e0a\u6620\u7684\u65f6\u95f4\uff08\u5e74\u5ea6\uff09\uff0c\u7eb5\u8f74\u4ee3\u8868\u7968\u623f\u6536\u5165\u6216\u8bc4\u5206\uff08IMDb\u3001\u8c46\u74e3\u3001Google\u7528\u6237\u8bc4\u5206\u7b49\uff09\u3002</p> <p>\u5267\u573a\u7248\u6807\u8bb0\uff1a\u6bcf\u4e2a\u5267\u573a\u7248\u53ef\u4ee5\u7528\u4e00\u4e2a\u70b9\u5728\u5750\u6807\u56fe\u4e2d\u8868\u793a\uff0c\u70b9\u7684\u5927\u5c0f\u53ef\u4ee5\u6839\u636e\u7968\u623f\u6216\u53e3\u7891\u8bc4\u5206\u7684\u9ad8\u4f4e\u8c03\u6574\u3002</p> <p>\u52a8\u6001\u66f4\u65b0\uff1a\u968f\u7740\u65b0\u7684\u5267\u573a\u7248\u4e0a\u6620\uff0c\u9010\u4e2a\u63d2\u5165\u5e76\u66f4\u65b0\u56fe\u8868\uff0c\u53ef\u4ee5\u9010\u5e74\u5c55\u793a\u5267\u573a\u7248\u7684\u7968\u623f\u548c\u53e3\u7891\u8d8b\u52bf\u3002</p> <p>\u6570\u636e\u6765\u6e90\uff1a\u7968\u623f\u6570\u636e\u53ef\u4ee5\u4eceBox Office Mojo\u7b49\u7f51\u7ad9\u83b7\u53d6\uff0c\u53e3\u7891\u6570\u636e\u53ef\u4ee5\u901a\u8fc7IMDb\u3001\u8c46\u74e3\u3001Rotten Tomatoes\u7b49\u5e73\u53f0\u6293\u53d6\u3002</p> <ul> <li>\u5206\u6790\u89d2\u5ea6\uff1a</li> </ul> <p>\u7968\u623f\u4e0e\u53e3\u7891\u7684\u5173\u7cfb\uff1a\u63a2\u7d22\u7968\u623f\u548c\u53e3\u7891\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4f8b\u5982\u53e3\u7891\u4f18\u79c0\u7684\u5267\u573a\u7248\u662f\u5426\u90fd\u80fd\u83b7\u5f97\u8f83\u9ad8\u7968\u623f\u3002 \u8d8b\u52bf\u5206\u6790\uff1a\u5206\u6790\u5267\u573a\u7248\u53e3\u7891\u4e0e\u7968\u623f\u662f\u5426\u6709\u5468\u671f\u6027\u6ce2\u52a8\uff0c\u662f\u5426\u6709\u5267\u573a\u7248\u56e0\u7279\u6b8a\u539f\u56e0\u83b7\u5f97\u7206\u53d1\u5f0f\u7684\u7968\u623f\u3002</p>"},{"location":"courses/Intro2Visualization/midterm/#_6","title":"\u5267\u573a\u7248\u5173\u952e\u8bcd\u70ed\u641c\u8bcd\u7684\u8bcd\u4e91\u56fe","text":"<ul> <li>\u53ef\u89c6\u5316\u6548\u679c\uff1a</li> </ul> <p>\u8bcd\u4e91\u56fe\uff1a\u6839\u636e\u5267\u573a\u7248\u76f8\u5173\u7684\u70ed\u641c\u8bcd\u3001\u89c2\u4f17\u8ba8\u8bba\u7684\u5173\u952e\u8bcd\u751f\u6210\u52a8\u6001\u8bcd\u4e91\u56fe\uff0c\u8bcd\u4e91\u56fe\u53ef\u4ee5\u901a\u8fc7\u989c\u8272\u3001\u5927\u5c0f\u3001\u900f\u660e\u5ea6\u7b49\u65b9\u5f0f\u7a81\u51fa\u70ed\u5ea6\u6700\u9ad8\u7684\u5173\u952e\u8bcd\u3002</p> <p>\u4e92\u52a8\u529f\u80fd\uff1a\u70b9\u51fb\u67d0\u4e2a\u5173\u952e\u8bcd\uff0c\u53ef\u4ee5\u663e\u793a\u8be5\u8bcd\u5173\u8054\u7684\u5267\u573a\u7248\u3001\u5267\u60c5\u80cc\u666f\u4ee5\u53ca\u89c2\u4f17\u7684\u8ba8\u8bba\u70b9\u3002</p> <p>\u52a8\u6001\u66f4\u65b0\uff1a\u968f\u7740\u6bcf\u4e2a\u5267\u573a\u7248\u7684\u63a8\u51fa\uff0c\u70ed\u641c\u8bcd\u7684\u8bcd\u4e91\u53ef\u4ee5\u6839\u636e\u5b9e\u65f6\u7684\u6570\u636e\u81ea\u52a8\u66f4\u65b0\uff0c\u5c55\u793a\u5f53\u524d\u5267\u573a\u7248\u7684\u8ba8\u8bba\u70ed\u5ea6\u548c\u5173\u952e\u8bcd\u7684\u53d8\u5316\u3002</p> <p>\u6570\u636e\u6765\u6e90\uff1a\u53ef\u4ee5\u4ece\u793e\u4ea4\u5a92\u4f53\uff08Twitter\u3001Weibo\uff09\u3001\u7535\u5f71\u8bc4\u8bba\u7f51\u7ad9\uff08\u8c46\u74e3\u3001IMDb\uff09\u53ca\u5176\u4ed6\u7f51\u53cb\u7684\u8bc4\u8bba\u5e73\u53f0\u6293\u53d6\u5173\u952e\u8bcd\u6570\u636e\u3002</p> <ul> <li>\u5206\u6790\u89d2\u5ea6\uff1a</li> </ul> <p>\u70ed\u641c\u8bcd\u7684\u53d8\u5316\uff1a\u901a\u8fc7\u8bcd\u4e91\u89c2\u5bdf\u300a\u540d\u4fa6\u63a2\u67ef\u5357\u300b\u5267\u573a\u7248\u8bdd\u9898\u548c\u89c2\u4f17\u5173\u6ce8\u70b9\u7684\u53d8\u5316\uff0c\u4e86\u89e3\u54ea\u4e9b\u89d2\u8272\u3001\u4e8b\u4ef6\u6216\u4e3b\u9898\u5728\u89c2\u4f17\u4e2d\u5f15\u8d77\u4e86\u5e7f\u6cdb\u8ba8\u8bba\u3002</p> <p>\u5267\u573a\u7248\u4e0e\u89c2\u4f17\u504f\u597d\u7684\u5173\u7cfb\uff1a\u4f8b\u5982\u67d0\u4e9b\u5267\u573a\u7248\u662f\u5426\u56e0\u4e3a\u67d0\u4e2a\u89d2\u8272\u3001\u67d0\u79cd\u60c5\u8282\u800c\u83b7\u5f97\u70ed\u5ea6\u3002</p>"},{"location":"courses/Intro2Visualization/midterm/#_7","title":"\u52a8\u6001\u65f6\u95f4\u8f74\uff08\u5305\u542b\u9752\u5c71\u7684\u66f4\u65b0\u8bb0\u5f55\u4e0e\u4e3b\u7ebf\u91cd\u5927\u4e8b\u4ef6\uff09","text":"<ul> <li>\u53ef\u89c6\u5316\u6548\u679c\uff1a</li> </ul> <p>\u65f6\u95f4\u8f74\uff1a\u6784\u5efa\u4e00\u4e2a\u4ece\u300a\u540d\u4fa6\u63a2\u67ef\u5357\u300b\u9996\u64ad\u5230\u5f53\u524d\u65f6\u95f4\u7684\u52a8\u6001\u65f6\u95f4\u8f74\uff0c\u6807\u51fa\u9752\u5c71\u521a\u5fd7\u7684\u6f2b\u753b\u66f4\u65b0\u8bb0\u5f55\uff08\u4f8b\u5982\u6f2b\u753b\u7ae0\u8282\u53d1\u5e03\u65e5\u671f\u3001\u5267\u96c6\u66f4\u65b0\u65f6\u95f4\uff09\u4ee5\u53ca\u4e0e\u5267\u60c5\u76f8\u5173\u7684\u91cd\u5927\u4e8b\u4ef6\uff08\u4f8b\u5982\u91cd\u8981\u6848\u4ef6\u7684\u89e3\u51b3\u3001\u5173\u952e\u89d2\u8272\u7684\u8f6c\u53d8\u7b49\uff09\u3002</p> <p>\u4e8b\u4ef6\u6807\u6ce8\uff1a\u65f6\u95f4\u8f74\u4e0a\u7684\u4e8b\u4ef6\u53ef\u4ee5\u6839\u636e\u7c7b\u578b\uff08\u4f8b\u5982\u6f2b\u753b\u66f4\u65b0\u3001\u5267\u96c6\u64ad\u653e\u3001\u91cd\u8981\u5267\u60c5\u53d1\u5c55\u7b49\uff09\u7528\u4e0d\u540c\u989c\u8272\u6216\u5f62\u72b6\u7684\u6807\u8bb0\u8fdb\u884c\u533a\u5206\uff0c\u9f20\u6807\u60ac\u505c\u6216\u70b9\u51fb\u4e8b\u4ef6\u6807\u8bb0\u53ef\u4ee5\u663e\u793a\u5177\u4f53\u7684\u7ec6\u8282\u4fe1\u606f\u3002</p> <p>\u52a8\u6001\u66f4\u65b0\uff1a\u968f\u7740\u65b0\u6f2b\u753b\u7ae0\u8282\u6216\u65b0\u5267\u96c6\u7684\u53d1\u5e03\uff0c\u65f6\u95f4\u8f74\u4f1a\u5b9e\u65f6\u66f4\u65b0\uff0c\u4fdd\u6301\u6700\u65b0\u7684\u5267\u60c5\u548c\u66f4\u65b0\u8bb0\u5f55\u3002</p> <p>\u6570\u636e\u6765\u6e90\uff1a\u9752\u5c71\u521a\u5fd7\u7684\u6f2b\u753b\u66f4\u65b0\u53ef\u4ee5\u53c2\u8003\u9752\u5c71\u7684\u5b98\u65b9\u793e\u4ea4\u5e73\u53f0\u3001\u6f2b\u753b\u51fa\u7248\u5e73\u53f0\uff08\u5982\u5468\u520a\u5c11\u5e74Sunday\uff09\u3002\u91cd\u5927\u5267\u60c5\u4e8b\u4ef6\u548c\u52a8\u753b\u96c6\u6570\u53ef\u4ee5\u4ece\u67ef\u5357\u7684\u5267\u60c5\u8d44\u6599\u5e93\u6216\u76f8\u5173\u7f51\u7ad9\u83b7\u53d6\u3002</p> <ul> <li>\u5206\u6790\u89d2\u5ea6\uff1a</li> </ul> <p>\u6f2b\u753b\u4e0e\u52a8\u753b\u7684\u540c\u6b65\u53d1\u5c55\uff1a\u53ef\u4ee5\u901a\u8fc7\u65f6\u95f4\u8f74\u89c2\u5bdf\u300a\u540d\u4fa6\u63a2\u67ef\u5357\u300b\u6f2b\u753b\u66f4\u65b0\u548c\u52a8\u753b\u66f4\u65b0\u7684\u8282\u594f\u5dee\u5f02\uff0c\u6216\u8005\u6f2b\u753b\u5267\u60c5\u5982\u4f55\u5f71\u54cd\u52a8\u753b\u5236\u4f5c\u3002</p> <p>\u5267\u60c5\u7684\u91cd\u5927\u8f6c\u6298\u70b9\uff1a\u901a\u8fc7\u6807\u8bb0\u4e3b\u8981\u7684\u5267\u60c5\u53d1\u5c55\u8282\u70b9\uff0c\u53ef\u4ee5\u63ed\u793a\u300a\u540d\u4fa6\u63a2\u67ef\u5357\u300b\u4e2d\u7684\u5173\u952e\u60c5\u8282\u8f6c\u6298\u3002</p>"},{"location":"courses/Intro2Visualization/midterm/#_8","title":"\u76ee\u524d\u5b9e\u73b0\u8fdb\u5c55","text":"<ul> <li>\u7968\u623f\u548c\u53e3\u7891\u7684\u5750\u6807\u56fe\u5df2\u7ecf\u5b9e\u73b0</li> </ul> <p>\u5982\u4e0b\u6240\u793a\u3002</p> <p> </p>"},{"location":"courses/Intro2Visualization/midterm/#_9","title":"\u5f85\u5b8c\u6210\u7684\u5de5\u4f5c","text":""},{"location":"courses/Intro2Visualization/midterm/#_10","title":"\u6570\u636e\u83b7\u53d6","text":"<ul> <li>\u4ecebilibili\u5404\u4e2a\u9014\u5f84\u83b7\u53d6\u67ef\u5357\u7684\u76f8\u5173\u6570\u636e</li> </ul>"},{"location":"courses/Intro2Visualization/midterm/#_11","title":"\u53ef\u89c6\u5316\u5448\u73b0","text":"<ul> <li>\u5b9e\u73b0\u4e00\u4e2a\u4e3b\u9875\u7f51\u7ad9</li> <li>\u5b9e\u73b0\u4eba\u7269\u5173\u7cfb\u7f51\u7edc\u7684\u52a8\u6001\u56fe</li> <li>\u5b9e\u73b0\u72af\u6848\u5730\u70b9\u7684\u70ed\u529b\u56fe</li> <li>\u5b9e\u73b0\u52a8\u6001\u65f6\u95f4\u8f74</li> </ul>"},{"location":"courses/Modern_Control_Theory/MCT/","title":"Modern Control Theory","text":"<p>I show the outline of the course for the coming exam.</p> {\"url\": \"../MCT_review.pdf\"}"},{"location":"courses/Numerical_Analysis/","title":"Numerical Analysis","text":"<p>Reference</p> <p>Numerical analysis, Richard L. Burden, J. Douglas Faires</p>"},{"location":"courses/Numerical_Analysis/#preliminary-errors","title":"\u8bef\u5dee | Preliminary: Errors","text":"<p>If a real number \\(x\\) is denoted as \\(0.d_1d_2d_3\\cdots \\times 10^{n}\\), then</p> <ul> <li>Truncation\uff08\u622a\u65ad\uff09 Error</li> </ul> <p>is induced when </p> \\[ \\hat{x}=0.d_1d_2d_3\\cdots d_k \\times 10^{n}  \\] <p>for some definite \\(k&lt;\\infty\\)</p> <ul> <li>Roundoff\uff08\u820d\u5165\uff09 Error</li> </ul> <p>is induced when </p> \\[ \\hat{x}=0. \\delta_1 \\delta_2 \\delta_3 \\cdots \\delta_k \\times 10^{n}  \\] <p>for some definite \\(k&lt;\\infty\\) </p> <p>where \\(\\delta_k &gt;d_k\\) if \\(d_{k+1}&gt;=5\\).</p>"},{"location":"courses/Numerical_Analysis/#t-significant-digits","title":"t significant digits","text":"<p>The number \\(p^*\\) is said to approximate p to \\(t\\) significant digits(or figures) if \\(t\\) is the largest nonnegative integer for which the relative error </p> \\[ e = \\frac{\\Delta p}{p}=\\frac{\\|p-p^*\\|}{\\|p\\|}&lt;5\\times 10^{-t}  \\] <p>where \\(p^*\\) is the approximate number of the exact number \\(p\\).</p> <ul> <li>for Chopping:</li> </ul> \\[  \\begin{align*} e &amp;= \\left|\\frac{0.d_{k+1}d_{k+2}\\cdots}{0.d_1d_2\\cdots}\\right| \\times 10^{-k} \\\\ &amp;\\leq \\frac{1}{0.1} \\times 10^{-k} \\quad \\text{\"=\" for } d_{k+1}d_{k+2}\\cdots\\rightarrow\\overline{9}\\text{ and }d_{1}d_{2}\\cdots\\rightarrow 0 \\\\ &amp;=10^{-k+1}  \\end{align*}  \\] <ul> <li>for rounding:</li> </ul> \\[ \\begin{align*} e &amp;\\leq \\frac{0.5}{0.1} \\times 10^{-k} \\quad \\text{\"=\" for } d_{k+1}d_{k+2}\\cdots\\rightarrow 5\\overline{0}\\text{ and }d_{1}d_{2}\\cdots\\rightarrow 0 \\\\  &amp;=0.5\\times 10^{-k+1} \\end{align*}  \\]"},{"location":"courses/Numerical_Analysis/#matrix-calculation","title":"\u6570\u503c\u4ee3\u6570\uff08\u77e9\u9635\u8ba1\u7b97\uff09 | Matrix Calculation","text":""},{"location":"courses/Numerical_Analysis/#numerical-approximation","title":"\u6570\u503c\u903c\u8fd1 | Numerical Approximation","text":""},{"location":"courses/Numerical_Analysis/#numerical-solution-of-differential-equations","title":"\u5fae\u5206\u65b9\u7a0b\u6570\u503c\u89e3 | Numerical Solution of Differential Equations","text":""},{"location":"courses/Numerical_Analysis/DE/","title":"Numerical Solution of Differential Equations","text":""},{"location":"courses/Numerical_Analysis/DE/#eulers-method","title":"Euler's Method","text":"<p>error accumulates.</p> \\[ \\begin{cases} \\omega_0=\\alpha,\\\\ \\omega_{i+1}=\\omega_i+hf(t_i,\\omega_i),\\quad i=0,1,\\cdots,n-1 \\end{cases} \\] <ul> <li>local trucation error.</li> </ul> <p>we consider error, the difference between true value and estimated value divided by \\(h\\) with assuming \\(\\omega_i=y_i\\).</p> \\[ \\begin{align*} \\tau_{i+1}&amp;=\\frac{y_i-\\omega_i}{h}\\\\ &amp;=\\frac{[y_i+hy'(t_i)+h^2/2\\times h''(\\xi_i)]-[y_i+hy'(t_i)]}{h}\\\\ &amp;=h/2\\times h''(\\xi_i) \\end{align*} \\] <p>So we can choose higher order items.</p>"},{"location":"courses/Numerical_Analysis/DE/#taylor-method-of-order-n","title":"Taylor method of order n","text":"\\[ \\begin{cases} \\omega_0=\\alpha, \\\\ \\omega_{i+1}=\\omega_i+hT^{(n)}(t_i,\\omega_i),\\quad i=0,1,\\cdots,N-1 \\end{cases} \\] <p>where</p> \\[ T^{(n)}(t_i,\\omega_i)=f(t_i,\\omega_i)+\\frac{h}{2}f'(t_i,\\omega_i)+\\cdots+\\frac{h^{n-1}}{n!}f^{(n-1)}(t_i,\\omega_i) \\] <p>Euler's method is Tarlor's method of order one.</p> <ul> <li>Implicite Euler's method</li> </ul> \\[ \\begin{cases} \\omega_0=\\alpha, \\\\ \\omega_{i+1}=\\omega_i+hf(t_i,\\omega_{i+1}),\\quad i=0,1,\\cdots,n-1 \\end{cases} \\] <p>which is more stable.</p> <ul> <li> <p>Trapezoidal Method</p> </li> <li> <p>Double-step Method</p> </li> </ul> <p>use the mid point of the two points. According to numerical differential</p> \\[ y'(t_0)=\\frac{1}{2h}[y(t_0+h)-y(t_0-h)]-\\frac{h^3}{6}y'''(\\xi) \\] <p>So we have</p> \\[ \\begin{cases} \\omega_0=\\alpha,\\\\ \\omega_1=\\omega_0+hf(t_0,\\omega_0), \\\\ \\omega_{i+1}=\\omega_{i-1}+2hf(t_i,\\omega_i), \\quad i=1,2,\\cdots,N-1 \\end{cases} \\]"},{"location":"courses/Numerical_Analysis/DE/#runge-kutta-method","title":"Runge-Kutta Method","text":""},{"location":"courses/Numerical_Analysis/MC/","title":"Matrix Calculation","text":""},{"location":"courses/Numerical_Analysis/MC/#direct-methods-for-solving-linear-systems","title":"Direct Methods for Solving Linear Systems","text":"<p>We focus on solving linear system </p> \\[ A\\vec{x} = \\vec{b} \\]"},{"location":"courses/Numerical_Analysis/MC/#gasussion-elimination","title":"\u9ad8\u65af\u6d88\u5143 | Gasussion Elimination","text":"<p>Reduce A into an upper-triangular matrix, and then solve for the unknowns by a backward-substitution process</p>"},{"location":"courses/Numerical_Analysis/MC/#pivoting-stratages","title":"\u4e3b\u5143\u9009\u62e9\u7b56\u7565 | Pivoting Stratages","text":"<p>This part is to reduce the error caused by rounding/Truncation error.</p> <p>We can show that the pivoting element is of great significance.</p> Partial PivotingScaled Partial PivotingComplete Pivoting <p>(also known for not changing the columns)</p> <p>Determine the smallest \\(p\\geq k\\) (in the same column of \\(a^{(k)}_{kk}\\))such that </p> \\[ |a_{ok}^{(k)}| = \\max_{k\\leq i \\leq n}{|a_{ik}^{(k)}|} \\] <p>and perform \\((E_k) \\leftrightarrow (E_p)\\).</p> <p>For row \\(i\\), let</p> \\[ s_i = \\max_{1\\leq j\\leq n}{|a_{ij}|} \\] <p>(if \\(\\exists i, s.t. s_i=0\\), then the system has no unique root. So we assume \\(\\forall i, s_i&gt;0\\))</p> <p>For each procedure of executing \\(E_k \\leftarrow E_k - m_{k,i}E_i\\) for \\(k=i+1, \\cdots, n\\), where \\(m_{k, i} = a_{ki}/{a_{ii}}\\). let </p> \\[ p = \\arg \\max_{i\\leq k \\leq n}{\\frac{|a_{ki}|}{s_k}} \\] <p>perform \\((E_i)\\leftrightarrow(E_p)\\)</p> <p>Incorporate the interchange of both rows and columns.</p>"},{"location":"courses/Numerical_Analysis/MC/#time-cost","title":"Time Cost","text":"<p>As we all know the time expense for Gaussion elimination is</p> \\[ O(n^3) \\]"},{"location":"courses/Numerical_Analysis/MC/#lu-lu-matrix-factorization","title":"LU\u5206\u89e3 | LU Matrix Factorization","text":"<p>The idea is encouraged by Gaussion Elimination. See that a matrix \\(A\\) can be transformed into an upper-trianglar matrix \\(U\\) by primary row operations:</p> \\[ \\begin{equation}  U = M_{n-1}\\cdots M_2M_1A  \\label{eq: LU} \\end{equation}  \\] <p>where \\(M_k (k=1,2,\\cdots n-1)\\) denotes a series of row operations. There are two perspetives.</p> Version 1Version 2 <p>\\(M_k (k=1, \\cdots, n-1)\\) can be interpreted that the \\(k+1\\) row has to make its column \\(1\\) to \\(k\\) to be \\(0\\). That is,</p> \\[ E_{k+1} \\leftarrow E_{k+1} - \\sum_{j=1}^{k} m_{k+1, j}E_j \\] <p>\\(M_k (k=1,\\cdots, n-1)\\) can be defined in another way as </p> \\[ E_j \\leftarrow E_j - \\sum\\limits_{k=j}^{n}m_{j,k}E_k \\quad \\text{for } j=k+1, \\cdots n \\] <p>which is also a lower-triangular matrix. To be proved by readers. </p> <p>And we can see \\(M_k\\) formed through the above two interpretations are the same.</p> <p>If we denote \\(L_k = M_k^{-1}\\), then apply \\(L = L_1L_2\\cdots L_{n-1}\\) left to both sides of the equation \\(\\ref{eq: LU}\\), then</p> \\[ LU = L_1L_2\\cdots L_{n-1} \\cdot M_{n-1}\\cdots M_2M_1A = A \\] <p>We know that matrix \\(L_k\\) and \\(M_k\\) are lower-triangular matrix(explaned by definition, to be proved by readers), so the product of matrix L is alao a lower-triangular matrix.</p> <p>So with the triangular matrix, it can be much quicker to solve the solution. See that</p> \\[  \\begin{align*} A\\vec{x} &amp;= \\vec{b} \\\\ LU\\vec{x} &amp;= \\vec{b} \\end{align*}  \\] <p>First solve \\(L \\vec{y} = \\vec{b}\\), then solve \\(U \\vec{x} = \\vec{y}\\).</p>"},{"location":"courses/Numerical_Analysis/MC/#time-cost_1","title":"time cost","text":"<p>Eliminate \\(0.5n^2\\) elements, it needs time \\(O(0.5n^3)\\).</p> <p>Solving \\(y\\) and \\(x\\), it needs time \\(O(2n^2)\\).</p>"},{"location":"courses/Numerical_Analysis/MC/#iterative-techniques-in-matrix-algebra","title":"\u77e9\u9635\u4ee3\u6570\u7684\u8fed\u4ee3\u6cd5 | Iterative Techniques in Matrix Algebra","text":"<p>This section, we introduce the iterative thoughts from Fixed-Point Iteration\uff08\u4e0d\u52a8\u70b9\u6cd5\uff09 to solve a linear system.</p> <p>We aim to find a iterative equation like equation \\(\\pmb{x}^{k} = f(\\pmb{x}^{k-1})\\). To be more specific, a linear iterative equation like</p> \\[ \\pmb{x}^{k} = T \\pmb{x}^{k-1} + \\pmb{c} \\] <p>and its corresponding convergent relation is</p> \\[ \\pmb{x}= T \\pmb{x} + \\pmb{c} \\]"},{"location":"courses/Numerical_Analysis/MC/#preliminary-knowledge-norm-of-vectors-and-matrixes","title":"\u8303\u6570 | Preliminary knowledge: Norm of Vectors and Matrixes","text":"<p>Definition of Norm of vectors</p> <p>A vector norm on \\(\\mathbb{R}^n\\) is a function, denoted as \\(\\Vert \\cdot \\Vert\\), mapping from \\(\\mathbb{R}^n\\) into \\(\\mathbb{R}\\) with the following properties for all \\(x, y \\in\\mathbb{R}^n\\) and \\(\\alpha \\in \\mathbb{C}\\).</p> <ul> <li>\u6b63\u6027 | positive</li> </ul> \\[ \\Vert \\vec{x} \\Vert\\geq 0 \\] <ul> <li>\u5b9a\u6027 | definite</li> </ul> \\[ \\Vert \\vec{x}\\Vert = 0 \\Leftrightarrow \\vec{x}=\\vec{0} \\] <ul> <li>\u9f50\u6027 | homogeneous</li> </ul> \\[ \\Vert \\alpha\\vec{x} \\Vert = |\\alpha|\\Vert \\vec{x} \\Vert \\] <ul> <li>\u4e09\u89d2\u4e0d\u7b49\u5f0f | triangle inequality</li> </ul> \\[ \\Vert \\vec{x}+\\vec{y} \\Vert \\leq \\Vert \\vec{x} \\Vert+\\Vert \\vec{y} \\Vert \\] <p>We usually use \\(p\\) norm</p> \\[ \\Vert \\vec{x} \\Vert_p = \\left(\\sum_{i=1}^n|x_i|^p\\right)^{1/p} \\] <p>with its common forms:</p> \\[ \\Vert \\vec{x} \\Vert_1 = \\sum_{i=1}^{n}|x_i|, \\quad \\Vert \\vec{x} \\Vert_2 = \\sqrt{\\sum_{i=1}^{n}|x_i|^2}, \\quad \\Vert \\vec{x} \\Vert_\\infty = \\max_{1\\leq i\\leq n}|x_i| \\] <p>Definition of Norm of Matrixes</p> <p>A matrix norm on the set of all matrices \\(R \\in \\mathbb{R}^{n\\times n}\\) is a real-valued function, denoted as \\(\\Vert \\cdot \\Vert\\), defined on this set, satisfying for all \\(A, B \\in \\mathbb{R}^{n\\times n}\\) and all \\(\\alpha \\in \\mathbb{C}\\):</p> <ul> <li>\u6b63\u6027 | positive</li> </ul> \\[ \\Vert \\mathbfit{A} \\Vert\\geq 0 \\] <ul> <li>\u5b9a\u6027 | definite</li> </ul> \\[ \\Vert \\mathbfit{A} \\Vert = 0 \\Leftrightarrow \\mathbfit{A}=\\mathbfit{0} \\] <ul> <li>\u9f50\u6027 | homogeneous</li> </ul> \\[ \\Vert \\alpha\\mathbfit{A} \\Vert = |\\alpha|\\Vert \\mathbfit{A} \\Vert \\] <ul> <li>\u4e09\u89d2\u4e0d\u7b49\u5f0f | triangle inequality</li> </ul> \\[ \\Vert \\mathbfit{A}+\\mathbfit{B} \\Vert \\leq \\Vert \\mathbfit{A} \\Vert+\\Vert \\mathbfit{B} \\Vert \\] <ul> <li>\u4e00\u81f4\u6027 | consistent</li> </ul> \\[ \\Vert \\mathbfit{A}\\mathbfit{B} \\Vert\\leq \\Vert \\mathbfit{A} \\Vert \\cdot \\Vert \\mathbfit{B} \\Vert \\] <p>Usually we use Natural Norm:</p> \\[ \\Vert \\mathbfit{A} \\Vert = \\max_{\\vec{x}\\neq 0}\\frac{\\Vert \\mathbfit{A}\\vec{x} \\Vert_p}{\\Vert \\vec{x} \\Vert_p} = \\max_{\\Vert \\vec{x} \\Vert_p =1}\\Vert \\mathbfit{A}\\vec{x} \\Vert \\] <p>with its common forms:</p> \\[ \\begin{align*} \\Vert \\mathbfit{A} \\Vert_1 &amp;= \\max_{1\\leq i\\leq n}\\sum_{j=1}^{n}|a_{ij}| \\quad\\text{the maximum of row summation}\\\\ \\Vert \\mathbfit{A} \\Vert_2 &amp;= \\sqrt{\\max \\rho(A^T A)} \\quad\\text{the maximum of spectrum radius}\\\\ \\Vert \\mathbfit{A} \\Vert_\\infty &amp;= \\max_{1\\leq j\\leq n}\\sum_{i=1}^{n}|a_{ij}| \\quad\\text{the maximum of column summation}\\\\ \\end{align*} \\] <p>for \\(p=2\\) norm of matrix, we have special expression for special matrix:</p> <p>Sepecial Expression of Norm 2 of matrix</p> <p>If matrix \\(A\\) is symetrical, then</p> \\[ \\Vert \\mathbfit{A} \\Vert_2 = \\sqrt{\\max \\rho(A)}. \\] <p>If matrix \\(A\\) is orthogonal(only rotate), then</p> \\[ \\Vert \\mathbfit{A} \\Vert_2 = 1. \\]"},{"location":"courses/Numerical_Analysis/MC/#jacobi-jacobis-method","title":"Jacobi\u65b9\u6cd5 | Jacobi's Method  <p>Here we denote \\(L\\) and \\(U\\) to be the lower-triangular and upper-triangular matrix of matrix \\(A\\) without its diagonal elements respectively. (different from \\(LU\\) factorization!) And then we denote \\(D\\) to be the diagonal elements of the matrix of \\(A\\). That is, </p> \\[ A = D - L -U \\] <p>Then</p> \\[  \\begin{align*} A\\pmb{x} &amp;= \\pmb{b} \\\\ (D-L-U)\\pmb{x} &amp;= \\pmb{b} \\\\ D\\pmb{x} &amp;= (L+U)\\pmb{x} + \\pmb{b} \\\\ \\pmb{x} &amp;= D^{-1}(L+U)\\pmb{x} + D^{-1}\\pmb{b}  \\end{align*}  \\] <p>which gives matrix form of the Jacobi iterative technique</p> \\[ \\pmb{x}^{k} = D^{-1}(L+U)\\pmb{x}^{k-1} + D^{-1}\\pmb{b} \\]","text":""},{"location":"courses/Numerical_Analysis/MC/#gauss-seidel-the-gauss-seidel-method","title":"Gauss-Seidel\u65b9\u6cd5 | The Gauss-Seidel Method <p>This method sees that a little slowness in Jacobi's Method. That is, for each itearion period(\\(\\pmb{x}^{k} \\leftarrow \\pmb{x}^{k-1}\\)), it makes use of the generated \\(\\pmb{x}^{k}_{i}\\) in the \\(i\\)th row of \\(\\pmb{x}^{k}\\) and use it to update the coressponding element in \\(\\pmb{x}^{k-1}\\).</p> <p>In matrix form, we have</p> \\[ D\\pmb{x}^{k} = U\\pmb{x}^{k-1} + L\\pmb{x}^{k}+ \\pmb{b} \\] <p>(to be proved by readers)</p> <p>then</p> \\[ \\pmb{x}^{k} = (D-L)^{-1}U\\pmb{x}^{k-1} + (D-L)^{-1}\\pmb{b} \\]","text":""},{"location":"courses/Numerical_Analysis/MC/#approximating-eigenvalues","title":"\u7279\u5f81\u503c\u903c\u8fd1 | Approximating Eigenvalues","text":""},{"location":"courses/Numerical_Analysis/MC/#the-power-method","title":"\u5e42\u6cd5 | The Power Method <p>Assume that \\(A\\) has eigenvalues \\(|\\lambda_1|&gt;|\\lambda_2|\\geq |\\lambda_3|\\geq \\cdots \\geq |\\lambda_n|\\geq 0\\), we can use the following method to make the largest \\(lambda_1\\) stand out.</p>  <p>\u5e42\u6cd5 | The Power Method</p> <p>Initialize randomly \\(\\vec{x}\\), which can be represented by \\(n\\) linearly irrelevant eigenvectors \\(\\vec{v}_1, \\vec{v}_2, \\cdots, \\vec{v}_n\\), with parameters \\(\\beta_1, \\beta_2, \\cdots, \\beta_n\\), such that</p> \\[ \\vec{x} = \\sum_{i=1}^{n}\\beta_i\\vec{v_i} \\] <p>multiply both sides by \\(A\\), according to \\(A\\vec{v_i}=\\lambda_i \\vec{v_i}\\), we get</p> \\[ A\\vec{x} = \\sum_{i=1}^{n}\\beta_i \\lambda_i \\vec{v_i} \\] <p>repeat this process for \\(n\\) times we get </p> \\[ A^n\\vec{x} = \\sum_{i=1}^{n}\\beta_i \\lambda_i^n \\vec{v_i} = \\lambda_1^n\\sum_{i=1}^{n}\\beta_i (\\frac{\\lambda_i}{\\lambda_1})^n \\vec{v_i}\\rightarrow \\lambda_1^n \\beta_1 \\vec{v_1} \\quad (n\\rightarrow \\infty) \\] <p>That is, we can neglect eigenvalues that are smaller than \\(\\lambda_1\\) through multiplying \\(A\\) and \"extract\" the biggest one.</p>  <p>To avoid divengence caused by \\(\\lambda_1&gt;0\\), we need to normalize \\(\\vec{x}^{k} = A\\vec{x}^{k-1}\\) each step after multiplying. Usually we choose \\(\\Vert\\  \\Vert_\\infty\\).</p> <p>To get the \\(\\lambda_1\\) out, we can use </p> \\[ \\frac{\\Vert\\vec{x}^{k}\\Vert}{\\Vert\\vec{x}^{k-1}\\Vert} \\approx \\lambda_1 \\quad (n\\rightarrow \\infty) \\] <p>to get \\(\\lambda_1\\).</p> <p>The next question is, naively, how about the speed of converging? Luckily, the question is easy to answer:</p> <p>rely on ratio \\(\\left|\\frac{\\lambda_2}{\\lambda_1}\\right|\\).</p>","text":""},{"location":"courses/Numerical_Analysis/MC/#inverse-power-method","title":"\u53cd\u5e42\u6cd5 | Inverse Power Method <p>This is a trick from the Power method. It comes from a question: what if we want to calculate the smallest eigenvalue of a matrix \\(A\\)?</p> <p>The answer is, by taking use of metrix inverse.</p>  <p>\u53cd\u5e42\u6cd5 | Inverse Power Method</p> <p>Matrix \\(A\\) has eigenvalues \\(|\\lambda_1| &lt; |\\lambda_2| \\leq \\cdots \\leq |\\lambda_n|\\), then matrix \\(A^{-1}\\) has eigenvalues </p> \\[ \\left|\\frac{1}{\\lambda_1}\\right| &gt; \\left|\\frac{1}{\\lambda_2}\\right| \\geq \\cdots \\geq \\left|\\frac{1}{\\lambda_n}\\right| \\] <p>Then use the power method to get \\(\\left|\\frac{1}{\\lambda_1}\\right|\\) out.</p>  <p>Actually, the above method is more often being used in situation where we have known an eigenvalue \\(\\lambda_1\\) (not neecssarily the largest or smallest) of \\(A\\) is close to a constant \\(q\\). That is, we can formulate matrix </p> \\[ (A-qI) \\] <p>which has eigenvalues </p> \\[ |\\lambda_1 - q| &lt; |\\lambda_2 - q| \\leq \\cdots \\leq \\left|\\lambda_n - q\\right| \\] <p>Then we can use the above method to get \\(\\lambda_1\\) out.</p>","text":""},{"location":"courses/Numerical_Analysis/NA/","title":"Index","text":""},{"location":"courses/Numerical_Analysis/NA/#numerical-approximation","title":"Numerical Approximation","text":"<p>Reference</p> <p>\u6570\u503c\u903c\u8fd1, \u848b\u5c14\u96c4 \u8d75\u98ce\u5149 \u82cf\u4ef0\u5cf0</p>"},{"location":"courses/Numerical_Analysis/NA/#solutions-of-equations-in-one-variables","title":"\u4e00\u5143\u51fd\u6570\u65b9\u7a0b\u6c42\u89e3 | Solutions of Equations in One Variables","text":""},{"location":"courses/Numerical_Analysis/NA/#interpolation-and-polynomial-approximation","title":"\u51fd\u6570\u63d2\u503c\u548c\u591a\u9879\u5f0f\u903c\u8fd1 | Interpolation and Polynomial Approximation","text":""},{"location":"courses/Numerical_Analysis/NA/#approximation-theory","title":"\u6700\u4f73\u903c\u8fd1\u7406\u8bba | Approximation Theory","text":""},{"location":"courses/Numerical_Analysis/NA/#numerical-differentiation","title":"\u6570\u503c\u5fae\u5206 | Numerical Differentiation","text":"<ul> <li>Three Point Midpoint Formula</li> </ul> \\[ f'(x_0) = \\frac{1}{2h}[f(x_0+h)-f(x_0-h)] -\\frac{h^2}{6}f'''(\\xi) \\] <p>Use Lagrange Polynomial to approximate a function. Then the \\(n\\)th derivative of Lagrange Poly approximate the \\(n\\)th derivative of the original function.</p>"},{"location":"courses/Numerical_Analysis/NA/#numerical-integration","title":"\u6570\u503c\u79ef\u5206 | Numerical Integration","text":""},{"location":"courses/Numerical_Analysis/NA/Appro/","title":"Approximation Theory","text":""},{"location":"courses/Numerical_Analysis/NA/Appro/#best-square-approximation","title":"\u6700\u4f73\u5e73\u65b9\u903c\u8fd1 | Best Square Approximation","text":""},{"location":"courses/Numerical_Analysis/NA/Appro/#lead-in","title":"\u5f15\u5165 | Lead-in","text":"<p>We call a linear space with norm if there exists a function \\(\\|\\cdot\\|\\) that satisfies 3 properties(check here).</p> <p>So here we introduce</p> \\[ \\Delta(x,Y)=\\inf_{y\\in Y}\\|x-y\\| \\] <p>to be the best square approximation of element \\(x\\), where \\(x\\in X\\) is a linear space with norm 2 and \\(Y\\) is a subspace of \\(X\\).</p> <p>We can also introduce norm with the definition of inner product. In Euclid space, \\((\\cdot,\\cdot)\\) is defined as </p> \\[ (x,y)=x^Ty,\\quad x,y\\in \\mathbb{R}^n \\] <p>It is easy to see that \\(\\|x\\|_2=\\sqrt{(x,x)}\\). So we can give a more specific problem of inner product space. Assume \\(\\varphi_i\\), \\((i=1,2\\cdots,n)\\) are \\(n\\) linearly irrelevant elements in inner product space \\(X\\), choose \\(f\\in X\\), then subset </p> \\[ \\varPhi_n=\\text{span}(\\varphi_1,\\varphi_2,\\cdots,\\varphi_n) \\] <p>has a best approximation of \\(f\\), which is defined as </p> \\[ \\Delta(f,\\varPhi_n)=\\min_{\\varphi\\in \\varPhi_n}\\|f-\\varphi\\|_2 \\] <p>we call the \\(\\varphi\\) that enables the above equation to be the best square appromation element.</p>"},{"location":"courses/Numerical_Analysis/NA/Appro/#properties-of-best-square-appromation-element","title":"\u6700\u4f73\u5e73\u65b9\u903c\u8fd1\u5143 | Properties of Best Square Appromation Element","text":"<p>Sufficient and Necessary Condition for best square approximation element</p> <p>Assume \\(X\\) is an inner product space, \\(f\\in X\\), \\(\\varphi^*\\in \\varPhi_0\\) is the best square approximation element, if and only if</p> \\[ (f-\\varphi^*,\\varphi_i)=0, \\quad, i=1,2\\cdots,n. \\] Proof <p>\"\\(\\Leftarrow\\)\".</p> <p>\"\\(\\Rightarrow\\)\". </p> <p>The above theorem gives a general method to solve the best square approximation element. That is, we define</p> \\[ G=\\left[\\begin{array}{cccc} (\\varphi_1,\\varphi_1) &amp; (\\varphi_1,\\varphi_2) &amp; \\cdots &amp; (\\varphi_1,\\varphi_n)\\\\ (\\varphi_2,\\varphi_1) &amp; (\\varphi_2,\\varphi_2) &amp; \\cdots &amp; (\\varphi_2,\\varphi_n)\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ (\\varphi_n,\\varphi_1) &amp; (\\varphi_n,\\varphi_2) &amp; \\cdots &amp; (\\varphi_n,\\varphi_n)\\\\ \\end{array}\\right], \\quad \\pmb{\\alpha}^*=\\left[\\begin{array}{c} \\alpha_1^*\\\\ \\alpha_2^*\\\\ \\vdots\\\\ \\alpha_n^* \\end{array}\\right], \\quad \\pmb{\\beta}=\\left[\\begin{array}{c} (\\varphi_1,f)\\\\ (\\varphi_2,f)\\\\ \\vdots\\\\ (\\varphi_n,f) \\end{array}\\right] \\] <p>then we can solve \\(G\\pmb{\\alpha}^*=\\pmb{\\beta}\\) for the best square approximation element</p> \\[ \\varphi = \\sum_{i=1}^n\\alpha^*_i \\varphi_i \\] <p>The above theorem also tells us, if \\(\\{\\varphi_i\\}_{i=1}^n\\) are group of linearly irrelevant elements, then \\(G\\) has rank \\(n\\) and the parameters \\(\\{\\alpha_i\\}_{i=1}^n\\) is unique. And it is easy to give the error estimation form</p> \\[ \\begin{align*} \\|f-\\varphi^*\\|_2^2&amp;=(f-\\varphi^*,f-\\varphi^*)\\\\ &amp;=(f-\\varphi^*, f) - (f-\\varphi^*, \\varphi^*) \\\\ &amp;=(f-\\varphi^*, f) = (f,f) - (\\varphi^*,f)\\\\ &amp;=\\|f\\|_2^2-\\sum_{i=1}^n\\alpha^*_i (\\varphi_i, f) \\end{align*} \\] <p>The geometrical meaning is also clear, if we define \\(\\Delta=f-\\varphi^*\\) and then</p> \\[ \\begin{align*} \\|f-\\varphi^*\\|_2^2&amp;=(\\Delta+\\varphi^*,\\Delta+\\varphi^*)\\\\ &amp;=(\\Delta, \\Delta) +2 (\\Delta, \\varphi^*)+(\\varphi^*,\\varphi^*) \\\\ &amp;=(\\Delta, \\Delta) +2 (f-\\varphi^*, \\varphi^*)+(\\varphi^*,\\varphi^*) \\\\ &amp;= (\\Delta, \\Delta) - (\\varphi^*,\\varphi^*) \\quad \\text{(by property of BSPE)}\\\\ \\end{align*} \\] <p>which means \\(\\varphi^*\\) is the orthogonal projection of \\(f\\) on \\(\\varPhi_n\\).</p> <p>But actually it is not so easy to solve the above linear system, so we consider another way in specific situation.</p>"},{"location":"courses/Numerical_Analysis/NA/Appro/#best-square-approximation-on-l_rho2ab","title":"\u6709\u9650\u51fd\u6570\u7a7a\u95f4\u4e0a\u7684\u6700\u4f73\u5e73\u65b9\u903c\u8fd1 | Best Square Approximation on \\(L_\\rho^2[a,b]\\)","text":"<p>Weight Function</p> <p>Assume \\(\\rho(x)\\) is Lebesgue integrable on \\([a,b]\\) and is \\(0\\) on at most only one set of measure zero, then we call \\(\\rho(x)\\) the weight function.</p> <p>If \\(\\rho(x)\\) is the weight function on \\([a,b]\\) then we denote all the measurable function \\(\\rho(x)f^2(x)\\) that is Lebesgue integrable on \\([a,b]\\), by \\(L^2_\\rho[a,b]\\), which is a linear space. The inner product is defined by</p> \\[ (f,g)=\\int_{a}^b\\rho(x)f(x)g(x)dx, \\quad f,g\\in L^2_\\rho[a,b] \\] <p>which satisfies the 4 properties of inner product definition. So we can introduce natural norm </p> \\[ \\|f\\|_2=\\left[\\int_a^b\\rho(x)f^2(x)dx\\right] \\] <p>then \\(L^2_\\rho[a,b]\\) is a linear space with norm, so we can consider the best approximation problem.</p> <p>Because \\(L^2_\\rho[a,b]\\) is a special inner product space \\(X\\),  so the former discussion about the best square approximation element can be directly applied here. That is, if \\(\\{\\varphi_i\\}_{i=1}^n\\) is a group of linearly irrelevant function in \\(L^2_\\rho[a,b]\\), then the subset </p> \\[ \\varPhi_n=\\text{span}\\{\\varphi_1,\\varphi_2, \\cdots, \\varphi_n\\} \\] <p>is call the generalized polynomial space, whose element is called generalized polynomial, or polynomial for short. The best square approximation of \\(f\\) on \\(\\varPhi_n\\) is defined by</p> \\[ \\Delta(f,\\varPhi_n)=\\min_{\\varphi\\in \\Phi}\\|f-\\varphi\\|_2 \\] <p>and so does the best square approximation element \\(\\varphi^*\\) is called the best square approximation polynomial(BSAP).</p> ps <p>There is another way to get the BSAP.</p> <p>If we choose </p> \\[ E = (P-y,P-y)=\\|P-y\\|^2 \\] <p>we can easily solve the approximation polynomial \\(\\sum\\limits_{i=0}^\\infty a_ix^i\\) by letting \\(\\frac{\\partial E}{\\partial a_k}=0\\), and get</p> \\[ \\sum_{j=0}^n(\\varphi_k, \\varphi_j)a_j=(\\varphi_k,f),\\quad k=0,1,\\cdots,n \\]"},{"location":"courses/Numerical_Analysis/NA/Appro/#orthogonal-polynomials","title":"\u6b63\u4ea4\u591a\u9879\u5f0f | Orthogonal Polynomials","text":"<p>If we limit the base function of the subspace, we can simplify the matrix \\(G\\) and simplify the solving process. Actually, we are narrawing down the condition number of \\(G\\) by using orthogonal polynomials. Readers can use exactly the same statement from abstrat inner product space and its corresponding statement of orthogonal elements.</p> <p>Orthogonal Polynomials</p> <p>If \\(\\{\\varphi_i\\}_{i=1}^n \\subset L^2_\\rho[a,b]\\) satisfy</p> \\[ (\\varphi_i,\\varphi_j)=\\int_a^b\\rho(x)\\varphi_i(x)\\varphi_j(x)dx=\\begin{cases} 0, \\quad &amp;i=j,\\\\ \\sigma, \\quad &amp; i\\neq j. \\end{cases} \\] <p>where \\(\\sigma\\) is a non-zero number, then we call \\(\\{\\varphi_i\\}_{i=1}^n\\) are orthogonal on \\([a,b]\\) according to weight \\(\\rho(x)\\). Furthermore, if we limit </p> \\[ \\int_a^b\\rho(x)\\varphi_i^2(x)dx=1 \\] <p>then we call \\(\\{\\varphi_i\\}_{i=1}^n\\) are normalized orthogonal system.</p> <p>So from above definition we can get BSAP more easily with orthogonal functions \\(\\{\\varphi_i\\}_{i=1}^n\\)</p> \\[ \\varphi^*=\\sum_{i=1}^n\\frac{(\\varphi_i,f)}{(\\varphi_i,\\varphi_i)}\\varphi_i(x) \\] <p>and its corresponding error</p> \\[ \\|f-\\varphi^*\\|^2_2=\\|f\\|^2_2-\\sum_{i=0}^n\\frac{(\\varphi_i,f)^2}{(\\varphi_i,\\varphi_i)} \\] <p>Q1. Use \\(y=2^{ax+b}\\) to approximate the following 3 points.</p> \\(x_i\\) 0 1 4 \\(f(x_i)\\) 1 2 8 \\(w\\) 1 1 1"},{"location":"courses/Numerical_Analysis/NA/Appro/#discrete-least-squares-approximation","title":"\u79bb\u6563\u6700\u5c0f\u4e8c\u4e58\u6cd5 | Discrete Least Squares Approximation","text":"<p>This problem can be discribed as the best square approximation on Euclid space. That is, if \\(y\\in \\mathbb{R}^n\\), \\(\\{\\pmb{x}_i\\}_{i=1}^m\\subset \\mathbb{R}^n\\) is a group of linearly irrelevant vectors, then </p> \\[ V=\\text{span}\\{\\pmb{x}_1,\\pmb{x}_2,\\cdots,\\pmb{x}_m\\} \\] <p>is a subspace of \\(\\mathbb{R}^n\\). </p> <p>So we can consider the best square approximation problem</p> \\[ \\Delta(\\pmb{y},V)=\\min_{\\pmb{x}\\in V}\\|\\pmb{y}-\\pmb{x}\\|_2 \\] <p>where \\(\\|\\cdot\\|_2\\) is the Euclid norm, i.e. \\(\\|\\pmb{x}\\|_2=\\pmb{x}^T\\pmb{x}\\), \\(x\\in \\mathbb{R}^n\\)</p> <p>That is, we have to solve </p> \\[ \\left[\\begin{array}{cccc} (\\pmb{x}_1,\\pmb{x}_1) &amp; (\\pmb{x}_1,\\pmb{x}_2) &amp; \\cdots &amp; (\\pmb{x}_1,\\pmb{x}_n)\\\\ (\\pmb{x}_2,\\pmb{x}_1) &amp; (\\pmb{x}_2,\\pmb{x}_2) &amp; \\cdots &amp; (\\pmb{x}_2,\\pmb{x}_n)\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ (\\pmb{x}_n,\\pmb{x}_1) &amp; (\\pmb{x}_n,\\pmb{x}_2) &amp; \\cdots &amp; (\\pmb{x}_n,\\pmb{x}_n)\\\\ \\end{array}\\right] \\left[\\begin{array}{c} \\alpha_1^*\\\\ \\alpha_2^*\\\\ \\vdots\\\\ \\alpha_n^* \\end{array}\\right] = \\left[\\begin{array}{c} (\\pmb{x}_1,\\pmb{y})\\\\ (\\pmb{x}_2,\\pmb{y})\\\\ \\vdots\\\\ (\\pmb{x}_n,\\pmb{y}) \\end{array}\\right] \\] <p>If we denote \\(\\pmb{x}_i=(x_{1i},x_{2i},\\cdots,x_{ni})^T\\) and </p> \\[ A=[\\pmb{x}_1,\\pmb{x}_2,\\cdots,\\pmb{x}_m]=(x_{ij})_{n\\times m}=\\left[\\begin{array}{cccc} x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1m}\\\\ x_{21} &amp; x_{22} &amp; \\cdots &amp; x_{2m}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{n1} &amp; x_{n2} &amp; \\cdots &amp; x_{nm}\\\\ \\end{array}\\right] \\] <p>then it is not so hard to detect that the above equation can be rewritten as</p> \\[ A^TA\\pmb{\\alpha^*}=A^T\\pmb{y} \\] <p>Cause \\(A^TA\\) has full rank, we can get \\(\\pmb{\\alpha^*} =(A^TA)^{-1}A^T\\pmb{y}\\).</p>"},{"location":"courses/Numerical_Analysis/NA/Appro/#best-uniform-approximation","title":"\u6700\u4f73\u4e00\u81f4\u903c\u8fd1 | Best Uniform Approximation","text":""},{"location":"courses/Numerical_Analysis/NA/Appro/#lead-in_1","title":"\u5f15\u5165 | Lead-in","text":"<p>All the continuous function defined on \\([a,b]\\) compose a infinite-dimensional linear space, denoted by \\(C_{[a,b]}\\). To simplify our discription, we introduce norm \\(\\|\\cdot\\|_\\infty\\) to denote Chebyshev Norm</p> \\[ \\|\\cdot\\|_\\infty = \\max_{[a,b]}|f(x)|, \\quad \\forall f\\in C_{[a,b]}. \\] <p>With the following theorem explored by Weierstrass, we can find an algebraic polynomial to sufficiently approximate function \\(f\\) \\(\\in C_{[a,b]}\\).</p> <p>Weierstrass First Approximation Theorem</p> <p>For all given function \\(f(x)\\in C_{[a,b]}\\), \\(\\forall \\varepsilon&gt;0\\), \\(\\exists p(x)\\) which is an algebraic function, s.t.</p> \\[ \\|f(x)-p(x)\\|&lt; \\varepsilon \\] Hints <p>Prove it by Bernstein Polynomials.</p> <p>But what we care more about is whether we can find a polynomial of degree no more than \\(n\\) to approximate function \\(f(x)\\)?</p> <p>The answer to the above question results in Chebyshev Approximation.</p> <p>Consider set</p> \\[ P_n(x)=\\text{span}\\{1,x,\\cdots, x^n\\} \\] <p>It is not hard to see that \\(P_n\\) is a subspace of \\(C_{[a,b]}\\) with \\(n+1\\) dimension. And the goal is to get</p> \\[ \\Delta(f,P_n)=\\min_{p\\in P_n}\\|f(x)-p(x)\\|_\\infty \\] <p>which is called the best uniform approximation of \\(f(x)\\).</p>"},{"location":"courses/Numerical_Analysis/NA/Appro/#characteristics-of-best-uniform-approximation","title":"\u6700\u4f73\u4e00\u81f4\u903c\u8fd1\u7684\u7279\u5f81 | Characteristics of Best Uniform Approximation","text":"<p>We can represent the best uniform approximation with the introduction of deviation points.</p> <p>Deviation Point Set</p> <p>we define</p> \\[ \\begin{cases} E^+(f)=\\{x\\in [a,b]: f(x)=\\|f\\|_\\infty\\} \\\\ E^-(f)=\\{x\\in [a,b]: f(x)=-\\|f\\|_\\infty\\} \\end{cases} \\] <p>to be positive and negative deviation point set, and define \\(E(f)=E^+(f)\\cup E^-(f)\\) to be deviation point set.</p> <p>Recall that \\(f(x)\\in C_{[a,b]}\\), so \\(E^+(f)\\) and \\(E^-(f)\\) are both bounded closed set. </p> <p>Alternating Point Set</p> <p>If set \\(\\{x_1,x_2,\\cdots,x_k\\}\\) \\(\\subset E(f)\\) satisfies</p> \\[ \\begin{cases} a\\leq x_1&lt;x_2&lt;\\cdots &lt;x_k\\leq b  \\\\ f(x_j)=-f(x_{j+1}), \\quad j=1,2\\cdots,k-1 \\end{cases} \\] <p>then we call the above set Alternating Point Set. We call them them maximal alternating point set, if there does not exist an alternating point set of size larger than \\(k\\).</p> <p>We can construct maximal alternating point set by the following way.</p> <p>Construction of Maximal Alternating Point Set</p> <p>Let \\(S_1=\\inf E^+(f)\\cup E^-(f)\\), construct \\(x_k\\) recursively</p> \\[ x_k=\\inf S_k \\] <p>where</p> \\[ S_{k+1}=\\begin{cases} E^-(f)\\cap [x_k,b],\\quad x_k\\in E^+(f)\\\\ E^+(f)\\cap [x_k,b],\\quad x_k\\in E^-(f) \\end{cases} ,\\quad k=1,2 \\cdots \\] <p>We assure \\(x_k\\in S_k\\) when we let \\(x_k=\\inf S_k\\) because \\(S_k\\) is bounded closed set. The recursion assures \\(x_{k+1}\\) is the minimal deviation point with opposite sign to \\(x_k\\), which of course assures \\(\\{x_i\\}\\) is monotonically increasing. We can prove that, if \\(f\\neq 0\\), the above constructed set is maximal and finite(to be proved by readers).</p> <p>Property of best uniform approximation on \\(C_{[a,b]}\\)</p> <p>Assume \\(f\\in C_{[a,b]}\\) and \\(f\\notin P_n\\). If \\(p(x)\\) is the best uniform approximation polynomial of \\(f(x)\\) on \\([a,b]\\), then \\(f-p\\) has at least \\(n+2\\) alternating points.</p> Proof <p>By contradiction.</p> <p>Vall\u00e9e-Poisson Theorem</p> <p>Assume \\(f\\in C_{[a,b]}\\), if there exists polynomial \\(p\\in P_n\\), such that \\(f-p\\) has at least \\(n+2\\) points \\(x_1,x_2,\\cdots,x_{n+2} \\in [a,b]\\) alternated with positive and negative, then </p> \\[ \\Delta(f,P_n)\\geq \\mu=\\min_{1\\leq i\\leq n+2}|f(x_i)-g(x_i)| \\] Proof <p>By contradiction.</p> <p></p> <p>Chebyshev Theorem</p> <p>For all \\(f\\in C_{[a,b]}\\), \\(f\\notin P_n\\), \\(p\\in P_n\\) is the best uniform approximation of \\(f\\) if and only if, \\(f-p\\) has at least \\(n+2\\) alternating points on \\([a,b]\\).</p> Proof <p>By Sequence Theorem.</p> <p>Uniqueness of best uniform approximation</p> <p>If \\(f\\in C_{[a,b]}\\), then there exists a unique polynomial \\(p\\in P_n\\) that is the best uniform approximation of \\(f\\).</p> HintsProof <p>By prove two best uniform approximation polynomials \\(p_1,p_2\\) are the same.</p> <p>Prove Uniqueness.</p> <p>Assume \\(p_1,p_2\\in P_n\\) are both the best uniform approximation polynomials of \\(f\\),i.e. </p> \\[   \\|f-p_1\\|_\\infty = \\|f-p_2\\|_\\infty = \\min_{p\\in P_n}\\|f-p\\|_\\infty = \\Delta(f,P_n) \\] <p>then define \\(p_0=(p_1+p_2)/2\\), then</p> \\[ \\|f-p_0\\|_\\infty\\leq \\frac{1}{2}(\\|f-p_1\\|_\\infty+ \\|f-p_2\\|_\\infty) =\\Delta(f,P_n) \\] <p>which means \\(p_0\\) is also the best uniform approximation polynomial of \\(f\\). By Chebyshev Theorem, there exists \\(n+2\\) alternating points \\(x_0,x_1,\\cdots,x_{n+1}\\).</p> <p>Note that </p> \\[ \\Delta(f,P_n)=|f(x_k)-p_0(x_k)|\\leq \\frac{1}{2}(|f(x_k)-p_1(x_k)|+|f(x_k)-p_2(x_k)|)\\leq \\Delta(f,P_n),\\quad k=0,1,\\cdots,n+1 \\] <p>which means \"=\" must holds, i.e.</p> \\[ |f(x_k)-p_1(x_k)| = |f(x_k)-p_2(x_k)| = \\Delta(f,P_n),\\quad k=0,1,\\cdots,n+1 \\] <p>and </p> \\[ f(x_k)-p_1(x_k) = f(x_k)-p_2(x_k),\\quad k=0,1,\\cdots,n+1 \\quad \\text{(by Triangular inequation)} \\] <p>which means \\(p_1(x_k)=p_2(x_k)\\), that is, \\(p_1-p_2\\) has \\(n+2\\) roots, so \\(p_1=p_2\\).</p>"},{"location":"courses/Numerical_Analysis/NA/Appro/#chebyshev-first-class-chebyshev-polynomials","title":"\u7b2c\u4e00\u7c7bChebyshev\u591a\u9879\u5f0f | First Class Chebyshev Polynomials","text":"<p>Coonsider \\(n+1\\) extreme points of \\(cosn\\theta\\) on \\([0,\\pi]\\). If \\(p(x)\\) is the polynomial of best uniform  approximation of \\(f\\) on \\([a,b]\\), then \\(f-p\\) has at least \\(n+2\\) alternating points.</p> <p>We define \\(x = \\cos\\theta\\) and </p> \\[ \\begin{align*} T_n(x)&amp;=\\cos (n\\cos^{-1}x)\\\\ &amp;=\\cos(n \\theta)\\\\ &amp;=\\sum\\limits_{k=1}^n a_k(\\cos\\theta)^i\\\\ &amp;=\\sum\\limits_{k=1}^n a_k x^i \\end{align*} \\] <p>is a polynomial of degree \\(n\\), i.e. \\(T_n(x)\\in P_n\\). It is easy to see the resursive definition</p> \\[ \\begin{cases} T_0(x)=1, T_1(x)=x,\\\\  \\displaystyle T_{n+1}(x)=2xT_n(x)-T_{n-1}(x),\\quad n=1,2,\\cdots,n-1 \\end{cases} \\] <p>By definition, we have properties for first class Chebyshev Polynomials.</p> <p>Properties of First Class Chebyshev Polynomials</p> <p>(1) Prove the recursive form of definition.</p> <p>(2) the coefficient of item with the highest degree of \\(T_n(x)\\) is \\(1/2^{n-1}\\).</p> <p>(3) \\(|T_n(x)|\\leq 1\\), \\(\\forall |x|\\leq 1\\).</p> <p>(4) \\(T_n(x)\\) has \\(n\\) different real roots </p> \\[ \\cos\\left[\\frac{(2k-1)\\pi}{2n}\\right],\\quad k=1,2,\\cdots,n \\] <p>(5) \\(\\{\\cos(k\\pi/n)\\), \\(k=0,1,\\cdots,n\\}\\) are a maximal alternating point set of \\(T_n(x)\\) on \\([-1,1]\\).</p> <p>(6) \\(T_n(x)=(-1)^nT_n(-x)\\).</p> <p>(7)</p> \\[ \\int_{-1}^1\\frac{T_m(x)T_n(x)}{\\sqrt{1-x^2}}dx=\\begin{cases}\\pi, \\quad &amp;m=n=0;\\\\ \\pi/2, \\quad &amp;m=n\\neq 0;\\\\ 0,\\quad &amp;m\\neq n.\\end{cases} \\] HintsProof <p>(1) by combination property of triangular functions.</p> <p>(7) by the orthogonal property of triangular functions.</p> <p>With the above property, we can see the following theorem.</p> <p>\\(T_n(x)\\) is the is the best uniform approximation of 0</p> <p>\\(T_n(x)\\) is the best uniform approximation of function \\(f\\equiv 0\\), that is, \\(\\forall\\) Monicpolynomial \\(p\\in P_n\\)</p> \\[ \\|p\\|\\geq |T_n|/2^{n-1} = 2^{1-n} \\] <p>Use \\(n\\) Polynomial \\(P_n(x)\\) to approximate function \\(f\\) on region \\([-1,1]\\), its remainder </p> \\[ \\begin{align*} |P_n(x)-f(x)|=|R_n(x)|&amp;=\\left|\\frac{f^{(n+1)}(\\xi)}{(n+1)!}\\prod_{i=0}^n(x-x_i)\\right|\\\\ &amp;\\leq \\max\\limits_{x\\in [-1,1]}\\left|f^{(n+1)}(x)\\right|\\frac{1}{(n+1)!} \\left| \\prod_{i=0}^n(x-x_i)\\right|\\\\ &amp;\\leq \\max\\limits_{x\\in [-1,1]}\\left|f^{(n+1)}(x)\\right|\\frac{1}{(n+1)!}\\left| \\frac{T_{n+1}(x)}{2^{n}} \\right|\\\\ &amp;\\leq \\frac{1}{(n+1)!} \\frac{1}{2^{n}}\\max\\limits_{x\\in [-1,1]}\\left|f^{(n+1)}(x)\\right|  \\end{align*} \\] <ul> <li>Minimizing Approximation Error on Arbitrary Intervals</li> </ul> <p>The technique for choosing points to minimize the interpolating error is extended to a general closed interval \\([a, b]\\) by using the change of variables</p> \\[ \\tilde{x} = \\frac{1}{2}[(b-a)x+a+b] \\] <p>Q. Find the best approximating polynomial of  \\(f (x) = e^x\\) on \\([0, 1]\\) such that the absolute error is no larger than \\(0.5\\times 10^4\\).</p> Answer <p>\\(a=0\\), \\(b=1\\), so </p> \\[ x=\\frac{a+b}{2}+\\frac{b-a}{2}t = \\frac{1}{2}(t+1), \\quad t\\in [-1,1] \\] <p>So the actual function to be approximated is </p> \\[ g(t) = f\\left[\\frac{1}{2}(t+1)\\right] = e^{\\frac{1}{2}(t+1)}, \\quad t\\in [-1,1] \\] \\[ \\max\\limits_{x\\in [-1,1]}\\left|g^{(n+1)}(x)\\right| = \\max\\limits_{x\\in [-1,1]}\\left|\\frac{1}{2^{n+1}}e^{\\frac{1}{2}(t+1)}\\right|=\\frac{e}{2^{n+1}} \\] <p>So</p> \\[ \\begin{align*} |P_n(x)-f(x)|&amp;\\leq \\frac{1}{(n+1)!} \\frac{1}{2^{n}}\\max\\limits_{x\\in [-1,1]}\\left|g^{(n+1)}(x)\\right| \\\\ &amp;= \\frac{1}{(n+1)!} \\frac{1}{2^{n}}\\frac{e}{2^{n+1}} \\end{align*} \\]"},{"location":"courses/Numerical_Analysis/NA/Appro/#economization-of-power-series","title":"\u5e42\u7ea7\u6570\u7684\u964d\u7ef4 | Economization of Power series","text":"<p>This part is also called Reducing the Degree of Approximating Polynomials.</p> <p>Consider approximating an arbitrary \\(n\\)th-degree polynomial</p> \\[ P_n(x) = a_nx^n+a_{n-1}x^{n-1}+\\cdots+a_1x+a_0, \\quad x\\in [\u22121, 1] \\] <p>with a polynomial of degree at most \\(n \u2212 1\\).</p> <p>To let \\(\\max\\limits_{x\\in [-1,1]}|P_n(x)-P_{n-1}(x)|\\) to be mininal, equals to let it be</p> \\[ a_n\\tilde{T}_n(x) = a_n T_n(x)/2^{n-1} \\]"},{"location":"courses/Numerical_Analysis/NA/Appro/#appendix-orthogonal-polynomials-on-l2_rhoab","title":"\u9644\u5f55: \u6709\u9650\u533a\u95f4\u4e0a\u7684\u6b63\u4ea4\u591a\u9879\u5f0f | Appendix: Orthogonal polynomials on \\(L^2_\\rho[a,b]\\)","text":""},{"location":"courses/Numerical_Analysis/NA/Appro/#properties","title":"\u6027\u8d28 | Properties","text":"<p>Properties</p> <p>If \\(\\omega_0(x), \\omega_1(x),\\cdots\\) are orthogonal polynomials on space \\(L^2_\\rho[a,b]\\) by orthogonalizing power series, then is must follow</p> <p>(i) \\(\\omega_n(x)\\) is a \\(n\\)th algebraic polynomial.</p> <p>(ii) \\(\\forall p \\in P_k\\), \\(k\\leq n\\), \\(p\\) can be represented as </p> \\[ p=\\sum_{i=0}^na_i\\omega_i(x) \\] <p>(iii) \\(\\omega_n(x)\\) is orthogonal to all polynomials whose degree is less than \\(n\\), that is, </p> \\[ \\int_{a}^b\\rho(x)\\omega_n(x)p_{n-1}(x)dx=0 \\]"},{"location":"courses/Numerical_Analysis/NA/Appro/#construction-of-monic-orthogonal-polynomials","title":"\u9996\u4e00\u6b63\u4ea4\u591a\u9879\u5f0f | Construction of Monic Orthogonal Polynomials","text":"<p>Construction of Monic Orthogonal Polynomials</p> <p>Assume \\(\\{\\overline{\\omega}_i(x)\\}_{i=0}^\\infty\\) are Monic Orthogonal Polynomials, then they satisfy the following recurrence relation</p> \\[ \\overline{\\omega}_{n+1}(x)=(x-B_n)\\overline{\\omega}_n(x)-C_n\\overline{\\omega}_{n-1}, \\quad n=1,2,\\cdots \\] <p>where </p> \\[ \\begin{align*} B_n &amp;= \\frac{(x\\overline{\\omega}_{n},\\overline{\\omega}_{n})}{(\\overline{\\omega}_{n},\\overline{\\omega}_{n})} \\\\ C_n&amp;=\\frac{( \\overline{\\omega}_{n}, \\overline{\\omega}_{n})}{(\\overline{\\omega}_{n-1},\\overline{\\omega}_{n-1})} \\end{align*} \\] HintsProof <p>By using the property of orthogonal polynomials.</p> <p>To simplify the notation, we temporarily use \\(\\omega_n(x)\\) to replace \\(\\overline{\\omega}_n(x)\\).</p> <p>We focus on \\(x\\omega_n(x)\\), which is a \\(n+1\\)th polynomial, so it can be represented by \\(\\omega_0,\\omega_1,\\cdots,\\omega_n\\), i.e.</p> \\[ \\begin{equation} x\\omega_n(x)=\\omega_{n+1}(x)+\\sum_{i=0}^nc_i\\omega_i(x) \\label{eq1} \\end{equation} \\] <p>where \\(c_i\\) are parameters to be determined.</p> <p>Now we notice that \\((\\omega_n,\\omega_s)=0\\), \\(s\\leq n-1\\), so we first employ inner product on both sides of \\(\\ref{eq1}\\) with \\(\\omega_s\\) (\\(s=0,1,\\cdots,n-2\\))</p> \\[ \\begin{equation} (x\\omega_n, \\omega_s)=(\\omega_{n+1},\\omega_s)+\\sum_{i=1}^nc_i(\\omega_i,\\omega_s)\\label{eq2} \\end{equation} \\] <p>Because we have an exact meaning of inner product, that is, integral form, so we have \\((x\\omega_n, \\omega_s)=(\\omega_n, x\\omega_s)\\), then</p> <p>for \\(s=0,1,\\cdots,n-2\\), we get </p> \\[ (x\\omega_n, \\omega_s)=0,\\quad (\\omega_{n+1},\\omega_s)=0 \\] <p>and </p> \\[ (\\omega_i,\\omega_s)=0,\\quad i\\geq s+1 \\text{ or }i\\leq s-1 \\] <p>So equation \\(\\ref{eq2}\\) becomes </p> \\[ 0=0+c_s(\\omega_s,\\omega_s) \\] <p>which means \\(c_s=0\\), \\(s=0,1,\\cdots n-2\\). Then we rewrite equation \\(\\ref{eq1}\\)</p> \\[ \\begin{equation} x\\omega_n(x)=\\omega_{n+1}(x)+c_n\\omega_n(x)+c_{n-1}\\omega_{n-1}(x) \\label{eq3} \\end{equation} \\] <p>In a similar way, we try employing inner product on both sides of the above equation \\(\\ref{eq3}\\) with \\(\\omega_{n-1}\\)</p> \\[ (x\\omega_n, \\omega_{n-1})=0+c_{n-1}(\\omega_{n-1},\\omega_{n-1}) \\] <p>which gives \\(c_{n-1}=(x\\omega_n, \\omega_{n-1})/(\\omega_{n-1},\\omega_{n-1})\\). Notice </p> \\[ (x\\omega_n, \\omega_{n-1})=(\\omega_n, x\\omega_{n-1})=(\\omega_n, \\omega_{n}) \\text{(by representing } x\\omega_{n-1} \\text{ again)} \\] <p>so \\(c_{n-1}=(\\omega_n, \\omega_{n})/(\\omega_{n-1},\\omega_{n-1})\\).</p> <p>In a similar way, employing inner product on both sides of the above equation \\(\\ref{eq3}\\) with \\(\\omega_{n}\\)</p> \\[ (x\\omega_n, \\omega_{n})=0+c_{n}(\\omega_{n},\\omega_{n}) \\] <p>which gives \\(c_{n}=(x\\omega_n, \\omega_{n})/(\\omega_{n},\\omega_{n})\\).</p> <p>Rewrite equation \\(\\ref{eq3}\\) and we prove the theorem.</p>"},{"location":"courses/Numerical_Analysis/NA/Appro/#roots-of-orthogonal-polynomials","title":"\u96f6\u70b9\u5206\u5e03 | Roots of Orthogonal Polynomials","text":"<p>Roots of Orthogonal Polynomials</p> <p>\\(n\\)th Orthogonal Polynomial \\(\\omega_n(x)\\) has \\(n\\) distinct roots on \\([a,b]\\).</p> Hints <p>By contradiction. First show that \\(\\omega_n(x)\\) must have root, and then show it is not multiple root. Finally show the number of roots must be equal to \\(n\\). All the proof can be done by making use of properties.</p>"},{"location":"courses/Numerical_Analysis/NA/Appro/#common-orthogonal-polynomials","title":"\u5e38\u89c1\u7684\u6b63\u4ea4\u591a\u9879\u5f0f | Common Orthogonal Polynomials","text":"<ul> <li> <p>Legendre Polynomial (on \\(L^2[-1,1]\\))</p> </li> <li> <p>First class Chebyshev Polynomial (on \\(L^2_\\rho[-1,1]\\) with \\(\\rho=1/\\sqrt{1-x^2}\\))</p> </li> <li> <p>Second class Chebyshev Polynomial (on \\(L^2_\\rho[-1,1]\\) with \\(\\rho=\\sqrt{1-x^2}\\))</p> </li> <li> <p>Laguerre Polynomial (on \\(L^2(0,\\infty)\\) with \\(\\rho=e^{-x}\\))</p> </li> <li> <p>Hermite Polynomial (on \\(L^2(\\infty,\\infty)\\) with \\(\\rho=e^{x^2}\\))</p> </li> </ul>"},{"location":"courses/Numerical_Analysis/NA/IntPo_Poly/","title":"Interpolation & Polynomial Approximation","text":""},{"location":"courses/Numerical_Analysis/NA/IntPo_Poly/#interpolating-polynomial","title":"\u63d2\u503c\u591a\u9879\u5f0f | Interpolating Polynomial","text":""},{"location":"courses/Numerical_Analysis/NA/IntPo_Poly/#lagrange-lagrange-interpolating-polynomial","title":"Lagrange\u63d2\u503c\u591a\u9879\u5f0f | Lagrange Interpolating Polynomial","text":"<p>Inspired by \u52a0\u6743\u5e73\u5747.</p> <p>There exists and only exists a \\(n\\)th Lagrange interpolating polynomial (\u62c9\u683c\u6717\u65e5\u57fa\u51fd\u6570) </p> \\[  L_n(x) = \\sum_{i=0}^{n}l_i(x)y_i = \\begin{bmatrix} l_0(x) &amp; l_1(x) &amp;l_2(x) &amp; \\cdots &amp; l_n(x)  \\end{bmatrix} \\begin{bmatrix} y_0 \\\\ y_1 \\\\ \\vdots \\\\ y_n  \\end{bmatrix} = \\Phi_n(x)\\vec{y}  \\] <p>such that for each pair of given points \\((x_i, y_i)\\), \\(i = 0, 1,2,\\cdots n\\), we have \\(y_i = L_n(x_i)\\).</p> <p>Here we can consider \\(l_i(x)\\) as a base of a linear space \\(\\mathcal{P}_n(x)\\), and it can be displayed by natural base \\(1, x, x^2, \\cdots x^n\\). To be more specific,</p> \\[ l_i(x) = \\prod_{j=0 \\atop j \\neq i }^{n}\\frac{(x - x_i)}{(x_i-x_j)} \\quad i=0,1,\\cdots n  \\] <p>readers can prove the above \\(n+1\\) polynomials are linearly irrelevant.</p> <p>Another form of \\(l_i(x)\\)</p> <p>If we denote \\(\\omega(x)=\\prod\\limits_{k=0}^{n}(x-x_k)\\), so the numerator of \\(l_i(x)\\) is </p> \\[ \\frac{\\omega(x)}{x-x_i} \\] <p>and its demunerator is </p> \\[ \\omega'(x)|_{x=x_i}=\\sum_{j=0}^{n}\\prod_{k=0 \\atop k\\neq j}^{n}(x-x_k)|_{x=x_i}=\\prod_{k=0\\atop k\\neq i}^{n}(x_i-x_k) \\] <p>so </p> \\[ l_i(x)=\\frac{\\omega(x)}{(x-x_i)\\omega'(x_i)} \\] <p>which satisfies </p> \\[ l_i(x_j)=\\begin{cases}1,\\quad &amp;j=i,\\\\ 0,\\quad &amp;j\\neq i.\\end{cases} \\] <p></p> <p>Q1. Calculate the Lagrange polynomial that interpolates the following 3 points.</p> \\(x_i\\) 1 2 4 \\(f(x_i)\\) 8 1 5 Answer \\[ \\begin{align*} p_2(x) &amp;= \\frac{(x-2)(x-4)}{(1-2)(1-4)}\\times 8 + \\frac{(x-1)(x-4)}{(2-1)(2-4)}\\times 1 + \\frac{(x-1)(x-2)}{(4-1)(4-2)}\\times 5\\\\ &amp;=\\frac{8}{3}(x^2-6x+8)-\\frac{1}{2}(x^2-5x+4)+\\frac{5}{6}(x^2-3x+2)\\\\ &amp;=3x^2-16x+21 \\end{align*} \\] <p>In fact, if we assume \\(P_n(x) = \\sum\\limits_{i=0}^{n}a_ix^i\\)(natural base), and to get the parameters \\(\\{a_i\\}\\) such that \\(P_n(x_i) = y_i\\), we have to solve the following linear system</p> \\[ \\begin{bmatrix} 1 &amp; x_0 &amp; x_0^2 &amp;\\cdots &amp; x_0^n \\\\  1 &amp; x_1 &amp; x_1^2 &amp;\\cdots &amp; x_1^n \\\\  \\vdots &amp; \\vdots &amp; \\vdots &amp; &amp;\\vdots \\\\ 1 &amp; x_n &amp; x_n^2 &amp;\\cdots &amp; x_n^n  \\end{bmatrix} \\begin{bmatrix}  a_0 \\\\ a_1 \\\\ \\vdots \\\\ a_n  \\end{bmatrix}=  \\begin{bmatrix} y_0 \\\\ y_1 \\\\ \\vdots \\\\ y_n  \\end{bmatrix}  \\] <p>which is a little tedious.</p>"},{"location":"courses/Numerical_Analysis/NA/IntPo_Poly/#neville-nevilles-method","title":"Neville\u65b9\u6cd5 | Neville's Method","text":"<p>There is another way to express polynomial, which is also a weighted average.</p> <p>An interpolation polynomial for a set of points \\(A =\\{x_0, x_1, \\cdots x_n \\}\\) can be expressed by two polynomials that interpolate \\(A\\) \\ \\(\\{ x_i\\}\\) and \\(A\\) \\ \\(\\{ x_j\\}\\). That is,</p> \\[ \\begin{align*} P_{0,1,\\cdots,n}(x) &amp;= \\frac{(x-x_i)P_{0,1,\\cdots,i-1,i+1,\\cdots,n}(x)-(x-x_j)P_{0,1,\\cdots,j-1,j+1,\\cdots,n}(x)}{(x_j-x_i)}\\\\ &amp;=\\frac{1}{(x_j-x_i)}\\left|\\begin{array}{cc} (x-x_i) &amp; P_{0,1,\\cdots,j-1,j+1,\\cdots,n}(x) \\\\ (x-x_j) &amp; P_{0,1,\\cdots,i-1,i+1,\\cdots,n}(x) \\end{array}\\right| \\end{align*} \\] <p>where \\(P_{0,1,\\cdots,i-1,i+1,\\cdots,n}(x)\\) and \\(P_{0,1,\\cdots,j-1,j+1,\\cdots,n}(x)\\) denotes the polynomial that interpolates \\(A\\) \\ \\(\\{ x_i\\}\\) and \\(A\\) \\ \\(\\{ x_j\\}\\) respectively.</p> <p>For the first item, we have</p> \\[ \\begin{align*} P_{0,1}(x) &amp;= \\frac{(x-x_0)\\times f(x_1)-(x-x_1)\\times f(x_0)}{(x_1-x_0)}\\\\ &amp;=\\frac{1}{(x_1-x_0)}\\left|\\begin{array}{cc} (x-x_0) &amp; f(x_0) \\\\ (x-x_1) &amp; f(x_1) \\end{array}\\right| \\end{align*} \\] <p>Q2. Get the polynomial \\(p_3(x)\\) that interpolates the following 4 points and estimate \\(f(5)\\) by calculate \\(p_3(5)\\).</p> \\(x_i\\) 2 4 6 8 \\(f(x_i)\\) -8 0 8 64 Answer <p>It is a little tedious if we write the form of the polynomial and then substitute \\(5\\) in. It is more suitable to list a table.</p> <p> \\(x\\) \\(f(x)\\) \\(p_1(5)\\) \\(p_2(5)\\) \\(p_3(5)\\) 2 -8 4 0 \\(\\frac{1}{(4-2)}\\left|\\begin{array}{cc}     (5-2) &amp; -8 \\\\     (5-4) &amp; 0     \\end{array}\\right|=4\\) 6 8 \\(\\frac{1}{(6-4)}\\left|\\begin{array}{cc}     (5-4) &amp; 0 \\\\     (5-6) &amp; 8     \\end{array}\\right|=4\\) \\(\\frac{1}{(6-2)}\\left|\\begin{array}{cc}     (5-2) &amp; 4 \\\\     (5-6) &amp; 4     \\end{array}\\right|=4\\) 8 64 \\(\\frac{1}{(8-6)}\\left|\\begin{array}{cc}     (5-6) &amp; -8 \\\\     (5-8) &amp; 64     \\end{array}\\right|=-20\\) \\(\\frac{1}{(8-4)}\\left|\\begin{array}{cc}     (5-4) &amp; 4 \\\\     (5-8) &amp; -20     \\end{array}\\right|=-2\\) \\(\\frac{1}{(8-2)}\\left|\\begin{array}{cc}     (5-2) &amp; 4 \\\\     (5-8) &amp; -2     \\end{array}\\right|=1\\) </p>"},{"location":"courses/Numerical_Analysis/NA/IntPo_Poly/#newton-newtons-divided-difference-formula","title":"Newton\u5dee\u5546\u8868\u8fbe\u5f0f | Newton's Divided Difference Formula","text":"<p>If we rewrite the \\(n\\)th Lagrange polynomial \\(P_n(x)\\) into another form:</p> \\[ P_n(x) = a_0+a_1(x-x_0)+a_2(x-x_0)(x-x_1)+\\cdots+a_n(x-x_0)\\cdots(x-x_n) \\] <p>By letting \\(x =x_0, x_1,\\cdots, x_n\\), we get</p> \\[ \\begin{align*} P_n(x_0) = &amp; a_0\\\\ P_n(x_1) = &amp; a_0 + a_1(x_1-x_0)\\\\ P_n(x_2) = &amp; a_0+a_1(x_2-x_0)+a_2(x_2-x_0)(x_2-x_1)\\\\ &amp;\\vdots\\\\ P_n(x_n) = &amp; a_0+a_1(x_n-x_0)+a_2(x_n-x_0)(x_n-x_1)+\\\\ &amp;\\cdots+a_n(x_n-x_0)\\cdots(x_n-x_{n-1}) \\end{align*} \\] <p>Then we can define:</p> \\[ \\begin{align*} f_n[x_0] &amp;\\overset{\\Delta}{=} f(x_0) = a_0\\\\ f_n[x_0, x_1] &amp;\\overset{\\Delta}{=} \\frac{f(x_1) - f(x_0)}{x_1-x_0} = a_1\\\\ f_n[x_0, x_1, x_2] &amp;\\overset{\\Delta}{=} \\frac{f[x_1,x_2]-f[x_0,x_1]}{x_2-x_0} \\\\&amp;= \\frac{\\frac{f(x_2)-f(x_1)}{x_2-x_1}-\\frac{f(x_1)-f(x_0)}{x_1-x_0} }{x_2-x_0} \\\\&amp;= \\frac{\\frac{f(x_2)-f(x_0)-(f(x_1)-f(x_0))}{x_2-x_1}-a_1}{x_2-x_0} \\\\ &amp;= \\frac{\\frac{f(x_2)-f(x_0)}{x_2-x_1} - \\frac{a_1(x_1-x_0)}{x_2-x_1}-\\frac{a_1(x_2-x_1)}{x_2-x_1}}{x_2-x_0} \\\\ &amp;= \\frac{\\frac{f(x_2)-f(x_0)}{x_2-x_1} - \\frac{a_1(x_2-x_0)}{x_2-x_1}}{x_2-x_0}\\\\ &amp;= \\frac{f(x_2)-a_0 - a_1(x_2-x_0)}{(x_2-x_0)(x_2-x_1)}=a_2\\\\ &amp;\\vdots\\\\ f[x_1,x_2,\\cdots, x_n] &amp;\\overset{\\Delta}{=} \\frac{f[x_1,x_2,\\cdots,x_n] - f[x_0,x_1,\\cdots,x_{n-1}]}{x_n-x_0} = a_n \\end{align*} \\] <p>We can prove the above definition \\(f[x_0,x_1\\cdots,x_n]\\), which is called divided difference, to be equal to \\(a_n\\) by induction.</p> <p>We can also show that \\(a_1\\) is the coefficient of the highest item of polynomial of degree \\(1\\) that interpolats \\(x_0,x_1\\), \\(a_2\\) is the coefficient of the highest item of polynomial of degree \\(2\\) that interpolates \\(x_0,x_1,x_2\\) ...</p> <p>This iterative method is quite useful in determining the parameters of \\(n\\)th Lagrange polynomial.</p> \\[ P_n(x) = f[x_0] + \\sum_{i=1}^{n}\\left(f[x_0,x_1,\\cdots,x_i]\\prod_{j=0}^{i-1}(x-x_j)\\right) \\] <p>The following relation gives a slightly quicker way to compute \\(f[x_0,x_1,\\cdots,x_n]\\):</p> <p>\u8ba1\u7b97\u5dee\u5546 | Calculating divided difference</p> \\[ f[x_0,x_1,\\cdots,x_n] = \\sum_{k=0}^{n}\\frac{f(x_k)}{\\omega'_{n+1}(x_k)} \\] <p>where </p> \\[ \\omega_{n+1}(x) = \\prod_{i=0}^{n}(x-x_i),\\quad \\omega'_{n+1}(x_j) = \\prod_{i=0 \\atop i \\neq j}^{n}(x_j-x_i) \\] Proof <p>We know from Lagrange polynomial of degree \\(n\\)</p> \\[ P_n(x) = \\sum_{i=0}^{n}f(x_i)\\frac{\\omega(x)}{(x-x_i)\\omega'(x_i)} \\] <p>where </p> \\[ \\omega(x) = \\prod_{j=0}^n(x-x_j), \\quad \\omega'(x_i) = \\prod_{j=0\\atop j\\neq i}^n(x_i-x_j) \\] <p>So the divided difference equals to the coefficient of the highest item of \\(P_n(x)\\), and we are done.</p> <p>Now solve Question 1 in the above part of the article.</p> Answer <p> \\(x_i\\) \\(f(x_i)\\) \\(f[x_i,x_j]\\) \\(f[x_i,x_j,x_k]\\) 1  2  4 8  1  5 -7  2 3 </p> <p>So the interpolating polynomial is </p> \\[ P_2(x) = 8-7(x-1)+ 3(x-1)(x-2) \\] <p>Some properties are the followings.</p> <p>\u5dee\u5546\u4e0e\u5fae\u5206\u5747\u503c\u7684\u5173\u7cfb | Relationship of DD &amp; Mean Value Differentials</p> <p>Suppose that \\(f \\in C^n[a, b]\\) and \\(x_0, x_1, \\cdots, x_n\\) are distinct numbers in \\([a, b]\\). Then a number \\(\\xi\\) exists in \\((a, b)\\) with</p> \\[ f[x_0,x_1\\cdots,x_n] = \\frac{f^{(n)}(\\xi)}{n!} \\] <p>Specifically, we denote \\(a=\\min\\{x_i:i=0,1\\cdots,n\\}\\) and \\(b=\\max\\{x_i:i=0,1\\cdots,n\\}\\)</p> <p>It is like </p> \\[ f[x_0,x_1] = \\frac{f(x_1)-f(x_0)}{x_1-x_0} = f'(\\xi) \\] <p>but add \\(n!\\) to the denominator.</p> <p>The collary can be quite simple.</p> <p>Collary of the above</p> <p>If \\(f(x)\\) is a polynomial of degree \\(k\\), \\(k&lt;n\\), so the \\(n\\)th divided difference is \\(0\\).</p>"},{"location":"courses/Numerical_Analysis/NA/IntPo_Poly/#error-analysis-of-interpolation","title":"\u63d2\u503c\u8bef\u5dee\u5206\u6790 | Error Analysis of Interpolation","text":"<p>The following theorem gives the error bound.</p> <p>\u62c9\u683c\u6717\u65e5\u57fa\u51fd\u6570\u7684\u4f59\u9879 | The remainder of Lagrange interpolating polynomial</p> <p>Suppose \\(x_0, x_1, \\cdots , x_n\\) are distinct numbers in the interval \\([a, b]\\) and \\(f \\in C^{n+1{[a, b]\\). Then, for each \\(x \\in [a, b]\\), a number \\(\\xi(x)\\) (generally unknown) between \\(x_0, x_1, \\cdots , x_n\\), and hence in \\((a, b)\\), exists with</p> \\[ f(x) = P(x) + \\frac{f^{n+1}(\\xi(x))}{(n+1)!}\\prod_{i=0}^{n}(x - x_i) \\] <p>where \\(P(x)\\) is the Lagrange interpolating polynomial.</p> <p>Prove it.</p> HintsProof <ul> <li>Using a function </li> </ul> \\[ g(t) = f(t) - P(t) - [f(t)-g(t)]\\prod_{i=0}^{n}\\frac{(t-x_i)}{x-x_i} \\] <p>which is \\(C^{n+1}[a, b]\\). Note that \\(f(x_k)=0\\) and \\(f(x)=0\\) for \\(x \\in [a, b]\\). We can see that \\(n+1+1=+\\) zero points here. </p> <ul> <li>Using generalized Rolle's Theorem: There exists \\(\\xi(x) \\in [a, b]\\) such that \\(g^{n+1}(\\xi(x))=0\\)</li> </ul> <p>And make \\(n+1\\) derivatives to \\(g(t)\\)(corresponding to \\(t\\)), and we can get the result. </p> <p>Or by using the relationship between divided difference and mean value differentials.</p> <p>Compared to the remainder of Taylor's extension</p> \\[ R_n(x) = \\frac{f^{n+1}(\\xi(x))}{(n+1)!}(x-x_0)^{n+1} \\] <p>Because \\(\\xi(x)\\) is usually unknown, so we often use a number \\(x' \\in  [a, b]\\) such that </p> \\[ |f^{n+1}(\\xi(x))|\\leq |f^{n+1}(x')| \\]"},{"location":"courses/Numerical_Analysis/NA/IntPo_Poly/#interpolation-of-equidistant-points","title":"\u7b49\u8ddd\u63d2\u503c | Interpolation of Equidistant Points","text":"<p>In actual calculation in computer, we use difference to replace divided difference when we interpolate points with equal distance.</p> <p>Definition of Difference on Equidistant Points</p> <p>Difference of first order is defined by</p> \\[ \\Delta f(x_i)=f(x_{i+1})-f(x_i) \\] <p>Difference of second order is defined by</p> \\[ \\begin{align*} \\Delta^2 f(x_i)&amp;=\\Delta f(x_{i+1})-\\Delta f(x_i)\\\\ &amp;=f(x_{i+2})-2f(x_{i+1})+f(x_i) \\end{align*} \\] <p>By recursion, we define Difference of \\(n\\)th order</p> \\[ \\Delta^n f(x_i)=\\Delta^{n-1}f(x_{i+1})-\\Delta^{n-1}f(x_i) \\] <p>By induction, it is easy to see that</p> \\[ \\begin{align*} \\Delta^{n}f(x_i) = &amp;f(x_{i+n})-nf(x_{i+n-1})+C^1_{n}f(x_{i+n-2})\\\\ +&amp;\\cdots+(-1)^{n-1}nf(x_{i+1})+(-1)^nf(x_i)\\\\ =&amp;\\sum_{k=0}^n(-1)^k C_n^kf(x_{i+k}). \\end{align*} \\] <p>We usually use the recursive method for computing.</p> <p>In situation where points are equally distant, we have a relationship between divided difference and difference.</p> <p>relationship between divided difference and difference</p> <p>Under equidistant points, we have </p> \\[ f[x_0,x_1,\\cdots,x_n]=\\frac{\\Delta^{n}f(x_0)}{h^n n!} \\] <p>where \\(h\\) is the distance between two adjacent points.</p> Proof <p>By induction.</p>"},{"location":"courses/Numerical_Analysis/NA/IntPo_Poly/#hermite-hermite-polynomials","title":"Hermite\u591a\u9879\u5f0f | Hermite Polynomials","text":"<p>We want to consider the smoothness of interpolating polynomials, so we need to consider its derivatives, especially the first order derivative. </p> <p>Definition of Osculating Polynomial</p> <p>The osculating polynomial approximating \\(f\\) is the polynomial \\(P(x)\\) of least degree such that</p> \\[ \\frac{d^kP(x_i)}{dx^k} = \\frac{d^kf(x_i)}{dx^k},\\quad, \\forall i = 0,1,\\cdots, n, \\forall k = 0,1,\\cdots, m_i \\] <p>Where \\(n\\) is the total number of sampling points and \\(m_i\\) is the degree of smoothness at point \\(x_i\\).</p> <p>If \\(m_i=1\\) for all \\(i=0,1,\\cdots, n\\), then the above polynomial is called Hermite Polynomial.</p> <p>Composition of Hermite Polynomial</p> <p>If \\(f \\in C^1[a, b]\\) and \\(x_0, \\cdots , x_n \\in [a, b]\\) are distinct, the unique polynomial of least degree agreeing with \\(f\\) and \\(f'\\) at \\(x_0,\\cdots , x_n\\) is the Hermite polynomial of degree at most \\(2n + 1\\) given by</p> \\[ H_{2n+1}(x) = \\sum_{j=0}^{n}f(x_j)H_{n,j}(x) + \\sum_{j=0}^{n}f'(x_j)\\hat{H}_{n,j}(x) \\] <p>where, for \\(L_{n, j}(x)\\) denoting the \\(j\\)th Lagrange coefficient polynomial of degree \\(n\\), we have</p> \\[ H_{n,j}(x) = [1-2(x-x_j)L'_{n,j}(x_j)]L^2_{n,j}(x),\\quad \\hat{H}_{n,j}(x) = (x-x_j)L^2_{n,j}(x) \\] <p>The composition is usually a little tedious.</p>"},{"location":"courses/Numerical_Analysis/NA/IntPo_Poly/#cubic-spline-interpolation","title":"\u6837\u6761\u63d2\u503c | Cubic Spline Interpolation","text":"<p>We change \\(\\frac{d^kP(x_i)}{dx^k} = \\frac{d^kf(x_i)}{dx^k}\\) into equations between adjacent curves. Because we often do not know the derivatives of the point.</p> <p>Define \\(s(x)\\) piece-wisely with \\(s_i(x)\\in [x_i,x_{i+1}]\\)</p> \\[ s_i(x)=a_i+b_ix+c_ix^2+d_ix^3, \\quad i=0,1,\\cdots,n-1 \\] <p>which has 4 parameters to be determined. So the overall number of parameters to be determined is \\(4n\\). The condition they have to satisfy</p> \\[ s_i(x_i)=f(x_i), s_i(x_{i+1})=f(x_{i+1}), i=0,1,\\cdots,n-1 \\quad \\text{[$2n$ equations]} \\] \\[ s'_i(x_{i})=s'_{i-1}(x_{i}),i=1,2,\\cdots, n-1\\quad \\text{[$n-1$ equations]} \\] \\[ s''_i(x_{i})=s''_{i-1}(x_{i}),i=1,2,\\cdots, n-1\\quad \\text{[$n-1$ equations]} \\] <p>So the above necessary condition gives \\(4n-2\\) equations while we have \\(4n\\) parameters to be determined. So we need to add another two equations for computing. There are many ways while the followings are usually seen.</p> <p>Three possible dealings</p> <p>(i) provide the derivatives of \\(2\\) endpoints, i.e. \\(s'(a)=f'(a)\\), \\(s'(b)=f'(b)\\), which is called D1 cubic spline or Clamped Cubic Spline.</p> <p>(ii) provide the second derivatives of \\(2\\) endpoints, i.e. \\(s''(a)=f''(a)\\), \\(s''(b)=f''(b)\\), which is called D1 cubic spline. Specially, if \\(s''(a)=s''(b)=0\\), it is called Natural Spline.</p> <p>(iii) take \\(s(a)=f(a)\\) or \\(s(b)=f(b)\\) out and provide extra \\(3\\) equations \\(s(a)=s(b)\\), \\(s'(a)=s'(b)\\), \\(s''(a)=s''(b)\\), which is called periodical cubic spline.</p> <p>The solution for solving parameters by given condition.</p> <p>Calculation of parameters</p> <p>Consider \\(s(x)\\) on \\([x_i,x_{i+1}]\\), if we denote \\(s''(x_i)=M_i\\), then the overall equation becomes</p> \\[ \\mu_i M_{i-1}+2M_i+\\lambda_i M_{i+1} = 6f[x_{i-1},x_i,x_{i+1}], \\quad i=1,2,\\cdots,n-1 \\] <p>where </p> \\[ \\mu_i=\\frac{x_i-x_{i-1}}{x_{i+1}-x_{i-1}}, \\lambda_i=\\frac{x_{i+1}-x_i}{x_{i+1}-x_{i-1}} \\] <p>for \\(i=0\\) and \\(i=n\\) two situation, we deduce different equations for different condition.</p> <p>(i) D1 cubic spline. we have</p> \\[ \\begin{cases} 2M_0+M_1=6f[x_0,x_0,x_1]\\\\ M_{n-2}+2M_{n-1}=6f[x_{n-2},x_{n-1}] \\end{cases} \\] <p> Q3. Interpolate the following points with natural cubic spline. \\(x_i\\) 0 1 2 \\(f(x_i)\\) 1 2 6 Answer <p>Define \\(s_0(x)=a_0+b_0x+c_0x^2+d_0x^3\\) and \\(s_1(x)=a_1+b_1(x-1)+c_1(x-1)^2+d_1(x-1)^3\\), then </p> \\[ \\begin{cases} b_0+2c_0+3d_0=b_1 \\quad &amp;\\text{derivative}\\\\ 2c_0+6d_0=2c_1\\quad &amp;\\text{second derivative}\\\\ 2c_0=0 \\quad &amp;s''(a)=0\\\\ 2c_1+6d_1=0 \\quad &amp; s''(b)=0 \\end{cases} \\] <p>with condition that satisfies passing data points</p> \\[ \\begin{cases} a_0=1\\\\  a_0+b_0+c_0+d_0=2\\\\ a_1=2\\\\ a_1+b_1+c_1+d_1=6 \\end{cases} \\] <p>So represent all the parameters with \\(d_1\\) or \\(d_0\\), we get</p> \\[ \\begin{cases} a_0=1\\\\ b_0=\\frac{1}{4}\\\\ c_0=0\\\\ d_0=\\frac{3}{4} \\end{cases} ,\\quad  \\begin{cases} a_1=2\\\\ b_1=\\frac{5}{2}\\\\ c_1=\\frac{9}{4}\\\\ d_1=-\\frac{3}{4}\\\\ \\end{cases} \\]"},{"location":"courses/Numerical_Analysis/NA/NI/","title":"Numerical Integration","text":"<p>Generally speaking, we encounter an integration problem </p> \\[ I(f) =\\int_a^b\\rho(x)f(x)dx \\] <p>which cannot be solved by indefinite integral, so we have to define a numerical formula to approximate the integration. Typecal quadrature formula (\u6c42\u79ef\u516c\u5f0f) can be written as</p> \\[ I_n(f)=\\sum_{k=1}^nA_kf(x_k) \\] <p>where \\(x_k\\) are nodes on \\([a,b]\\) and \\(A_k\\) are coefficients, whose value are only related with \\([a,b]\\), \\(\\rho(x)\\) and the sampling points, rather than the formula of \\(f\\) itself.</p> <p>Naively, we get error</p> \\[ E_n(f)= I(f)-I_n(f) \\] <p>which is obviously hard to get because we do not know the exact integration value. So we transfer to a concept called  Algebraic Precision.</p>"},{"location":"courses/Numerical_Analysis/NA/NI/#algebraic-precision","title":"\u4ee3\u6570\u7cbe\u5ea6 | Algebraic Precision","text":"<p>The degree of accuracy, or precision, of a quadrature formula is the largest positive integer \\(m\\) such that the formula is exact for \\(x^k\\) , for each \\(k = 0, 1,\\cdots, m\\). That is, we call a formula has \\(m\\) algebraic precision if there exists \\(m\\in \\mathbb{N}^+\\), s.t.</p> \\[ E_n(x^k)=0, k=0,1,\\cdots,m,\\quad E_n(x^{m+1})\\neq 0. \\] <p>It is easy to see that for all polynomial \\(p(x)\\) of degree no more than \\(m\\), we have \\(E_n(p)=0\\).</p> <p>If we find a simpler function \\(p(x)\\) that approximates \\(f(x)\\) on \\([a,b]\\), and \\(I(p)\\) is easy to get, then we can use \\(I(p)\\) to approximate \\(I(f)\\). That is, if </p> \\[ \\|f-p\\|=\\max_{x\\in [a,b]}|f(x)-p(x)| &lt;\\varepsilon \\] <p>then </p> \\[ |I(f)-I(p)|=\\left|\\int_a^b\\rho(x)[f(x)-p(x)]dx\\right| &lt;\\varepsilon (b-a) \\] <p>which is also small enough.</p>"},{"location":"courses/Numerical_Analysis/NA/NI/#newton-cotes-newton-cotes-formulas","title":"Newton-Cotes \u516c\u5f0f | Newton-Cotes Formulas","text":"<p>The simplest function is polynomials. So we try to find a polynomial \\(p(x)\\) to approximate \\(f(x)\\) and then use \\(I(p)\\) to approximate \\(I(f)\\). The following formulas are really natural if readers have been familiar with Interpolating Polynomial.</p> <p>Given \\(n+1\\) points \\(x_0&lt;x_1&lt;\\cdots&lt;x_n\\), we have a Lagrange Polynomial</p> \\[ p_n(x)=\\sum_{i=0}^n\\prod_{j=0\\atop j\\neq i}^n\\frac{(x-x_j)}{x_i-x_j}f(x_i) \\] <p>use the above polynomial as an approximation for integration. That is, </p> \\[ \\begin{equation} \\begin{cases} \\displaystyle I_{n+1}(p)=I(p_n)=\\sum\\limits_{i=0}^nA^n_if(x_i)\\\\ \\displaystyle A_i^n=\\int_a^b\\prod\\limits_{j=0\\atop j\\neq i}^n \\frac{(x-x_j)}{(x_i-x_j)}dx, i=0,1,\\cdots, n. \\end{cases}\\label{quadrature} \\end{equation} \\]"},{"location":"courses/Numerical_Analysis/NA/NI/#deduction","title":"\u63a8\u5bfc | Deduction","text":"<p>In equation \\(\\ref{quadrature}\\), if we let \\(\\rho(x)\\equiv1\\) and choose equidistant points:</p> \\[ x_k=a+kh, \\quad h=\\frac{b-a}{n},\\quad  k=0,1,\\cdots,n \\] <p>then the corresponding quadrature formula is called Newton-Cotes Formula. In fact, if we let \\(x=a+th\\), \\(t\\in [0,n]\\), then we have</p> \\[ \\begin{align*} A_i^n&amp;=\\int_0^n\\prod_{j=0\\atop j\\neq i}^n \\frac{(th-jh)}{(ih-jh)}d(a+th)\\\\ &amp;=h\\int_0^n\\prod_{j=0\\atop j\\neq i}^n\\frac{(t-j)}{(i-j)}dt \\end{align*} \\] <p>common rule of quadrature formula</p> <p>(i) Trapezoidal Rule</p> <p>If we let \\(n=1\\) and get Trapezoidal Rule of quadrature</p> \\[ I_2(f)=I(p_1)=\\frac{b-a}{2}[f(a)+f(b)] \\] <p>\\(f\\) is approximated by a linear expression.</p> <p>(ii) Simpson\u2019s Rule(Commonly used)</p> <p>If we let \\(n=2\\) and get Simpson\u2019s Rule of quadrature</p> \\[ I_3(f)=I(p_2)=\\frac{b-a}{6}\\left[f(a)+4f\\left(\\frac{a+b}{2}\\right)+f(b)\\right] \\] <p>\\(f\\) is approximated by a parabola expression.</p> <p>(iii) Cotes' Rule</p> <p>If we let \\(n=4\\) and get Cotes' rule of quadrature</p> \\[ I_5(f)=I(p_4)=\\frac{b-a}{90}\\left[7f(x_0)+32f(x_1)+12f(x_2)+32f(x_3)+7f(x_4)\\right] \\] <p>where \\(x_i=a+(b-a)/4\\cdot i\\), \\((i=0,1,2,3,4)\\).</p>"},{"location":"courses/Numerical_Analysis/NA/NI/#error-analysis","title":"\u8bef\u5dee\u5206\u6790 | Error analysis","text":""},{"location":"courses/Numerical_Analysis/NA/NI/#composite-numerical-integration","title":"\u6df7\u5408\u79ef\u5206 | Composite Numerical Integration","text":""},{"location":"courses/Numerical_Analysis/NA/NI/#romberg-romberg-integration","title":"Romberg \u79ef\u5206 | Romberg Integration","text":"<ul> <li>Richardson's Extrapolation</li> </ul> \\[ \\begin{equation} T_0(h) - I=\\alpha_1 h + \\alpha_2 h^2 +\\cdots \\label{eq1} \\end{equation} \\] <p>Let</p> \\[ \\begin{equation} T_0(h/2)-I=\\alpha_1 h/2 + \\alpha_2 (h/2)^2 + \\cdots \\label{eq2} \\end{equation} \\] <p>multiply \\(2\\) to both sides of equation \\(\\ref{eq2}\\) and Subtract equation \\(\\ref{eq1}\\), get</p> \\[ \\frac{2T_0(h/2)-T_0(h)}{2-1}-I=-\\frac{1}{2}\\alpha_2 h^2 +\\cdots \\] <p>So </p> \\[ \\begin{align*} T_1(h) &amp;= \\frac{2T_0(h/2)-T_0(h)}{2-1} = I + \\beta_1 h^2+\\beta_2 h^3+\\cdots \\\\ T_2(h) &amp;= \\frac{2^2T_1(h/2)-T_1(h)}{2^2-1} = I + \\gamma_1 h^3 +\\cdots \\\\ &amp;\\vdots\\\\ T_n(h) &amp;= \\frac{2^nT_{n-1}(h)-T_{n-1}(h)}{2^n-1} = I + \\xi_1 h^{n+1} +\\cdots  \\end{align*} \\]"},{"location":"courses/Numerical_Analysis/NA/NI/#adaptive-quadrature-methods","title":"\u81ea\u9002\u5e94\u6c42\u79ef\u65b9\u6cd5 | Adaptive Quadrature Methods","text":""},{"location":"courses/Numerical_Analysis/NA/NI/#gaussian-gaussian-quadrature","title":"Gaussian \u6c42\u79ef\u516c\u5f0f | Gaussian Quadrature","text":""},{"location":"courses/Numerical_Analysis/NA/Solve_Equ/","title":"Solutions of Equations in One Variables","text":""},{"location":"courses/Numerical_Analysis/NA/Solve_Equ/#basic-ideas-for-solving-equation","title":"\u57fa\u672c\u60f3\u6cd5 | Basic ideas for Solving Equation","text":"<p>To find the solution of an equation, we hope to have an iteration method which takes good advantage of Computer resources like</p> \\[ \\begin{equation}  \\pmb{x}^{k} = f(\\pmb{x}^{k-1}) \\label{eq: iterative eq}  \\end{equation} \\] <p>for \\(k = 1, 2, \\cdots n\\). We use \\(\\pmb{x}\\) instead of \\(x\\) because the above iteration method also applies to solving linear system.</p> <p>Hopefully, if the above equation converges, that is, for \\(k \\rightarrow \\infty\\), \\(\\pmb{x}^{k-1}\\rightarrow \\pmb{x}^*\\), \\(\\pmb{x}^{k}\\rightarrow \\pmb{x}^*\\), and the equation becomes</p> \\[ \\pmb{x}^* = f(\\pmb{x}^*) \\] <p>where \\(\\pmb{x^*}\\) is the sulution of the equation to be solved.</p> <p>If the above thought gets right, then we can consider the converging speed of the iterative process, which makes great sense in practical applications. That is, in a given definition of distence,</p> \\[ \\frac{\\|\\pmb{x}^{k+1} - \\pmb{x}^*\\|}{\\|\\pmb{x}^{k}-\\pmb{x}^*\\|^\\alpha}  \\] <p>to be small as much as possible for each \\(k\\).</p>"},{"location":"courses/Numerical_Analysis/NA/Solve_Equ/#the-bisection-method","title":"\u4e8c\u5206\u6cd5 | the Bisection Method","text":"<p>This method is quite intuitive. By choosing two end points \\(a, b\\), we get another point (Mid-point here)</p> \\[ p = a+\\frac{b-a}{2} \\] <p>Then update \\(a, b\\) by evaluating whether \\(f(p)&gt;0\\) or not to narrow down the interval.</p> <p>What is interesting is the stopping procedure. Readers can see the following question if interested.</p> <p>When we calculate the new point \\(p\\) based on \\(a, b\\), we need to judge whether \\(p\\) is an appropriate answer. Apart from \\(f(p)=0\\), which condition do you think is the best?</p> <ol> <li>\\((b-a)/{|\\min{(a, b)}|}&lt;\\epsilon\\)</li> <li>\\(|p-p_{prev}|=(b-a)/2 &lt; \\epsilon\\)</li> <li>\\(f(p)&lt;\\epsilon\\)</li> </ol> Choose an answerAnwser <ul> <li>1</li> <li>2</li> <li>3</li> </ul> <p>Choose 1, which is close to relative error, currently the best.</p> <p>2: consider \\(\\{p_n\\}=\\sum\\limits_{i=1}^{n}\\frac{1}{k}\\).</p> <p>3: easy to see.</p> <p>The converging speed can discribed as the following:</p> \\[ |x_n- x^*| &lt; \\frac{(b-a)}{2^{n}} \\]"},{"location":"courses/Numerical_Analysis/NA/Solve_Equ/#Fixed-Point-Iteration","title":"\u4e0d\u52a8\u70b9\u6cd5 | Fixed-Point Iteration","text":"<p>As we said previously in Basic ideas for solving equation, we hope to find an iterative relation such that the converging point \\(x^*\\) is exactly what we want, which in this case, means that </p> \\[ f(x^*) = 0 \\] <p>So intuitively, we ask: Whether can we derive a relation from </p> \\[ \\begin{equation} f(x) = 0 \\label{zero-equation} \\end{equation} \\] <p>to </p> \\[ x = g(x) \\] <p>for iterative method?</p> <p>The answer is, of course, YES!</p> <p>One of the easist way to transform is adding \\(x\\) to both sides of equation \\(\\ref{zero-equation}\\), but in most cases this does not work. Because we rely on \\(f(x)\\) ifself for the convergence! </p> <p>Thus, it is necessary to find the condition for \\(x = g(x)\\) to converge. The following theorem <p></p> gives a Sufficient condition.</p> <p>\u4e0d\u52a8\u70b9\u5b58\u5728\u5b9a\u7406 | Fixed-Point Theorom</p> <p>Let \\(g \\in C[a, b]\\) be such that \\(g(x) \\in [a, b]\\), for all \\(x\\) in \\([a, b]\\). Suppose, in addition, that \\(g'\\) exists on \\((a, b)\\) and that a constant \\(0 &lt; k &lt; 1\\) exists with</p> \\[ |g'(x)|\\leq k \\quad \\forall x \\in (a, b) \\] <p>Then for any initial number \\(p_0 \\in [a, b]\\), the sequence \\(\\{p_n\\}_{n=0}^{\\infty}\\) defined by </p> \\[ p_n = g( p_{n\u22121}) \\quad n \\geq 1 \\] <p>converges to the unique fixed point \\(p \\in [a, b]\\).</p> <p>Proving it is easily.</p> HintsProof <ul> <li>using the differential mean value theorem.</li> </ul> <p>\\(\\forall n \\geq 1, \\exists \\zeta_n \\in (p_{n-1}, p) \\subset (a, b)\\), we have</p> \\[ |p_n-p| = |g(p_{n-1}) - g(p)| = g'(\\zeta_n)|p_{n-1}-p|\\leq k|p_{n-1}-p| \\] <p>by induction, we have</p> \\[ |p_n-p|\\leq k^{n}|p_0-p| \\] <p>Let \\(n \\rightarrow \\infty\\), \\(|p_n-p| \\rightarrow 0\\), that is, \\(p_n\\) converges to \\(p\\). </p> <p>What we use in the proof will benefit us in identifying the speed of converging process.</p>"},{"location":"courses/Numerical_Analysis/NA/Solve_Equ/#newtons-method","title":"\u725b\u987f\u6cd5 | Newton's Method","text":"<p>This method is also a fixed-point method. There are two perspectives to get the inspirations.</p> HintsVersion 1Version 2 <ul> <li>version1: shrink the derivative of the iterative function \\(g(x)\\).</li> <li>version2: using Taylor's expansion.</li> </ul> <p>We can know that given a random function \\(f(x)\\), it may not be convergent to some point \\(x^*\\) for \\(x^{k} = f(x^{k-1}) + x^{k-1}\\) in a given interval. So the queation is, can we formulate a function \\(g(x)\\) such that \\(x^{k} = g(x^{k-1})\\) is convergent?</p> <p>The answer is, again, YES!</p> <p>The following content tells us we can formulate \\(g(x)= x - f(x)/(f'(x))\\) such that \\(g'(x) &lt; 1\\) in a given interval.</p> <p>Readers can easily see that </p> \\[ \\begin{align*} g'(x) &amp;= 1 - \\frac{f'^2(x)-f''(x)f(x)}{f'^2(x)}\\\\ &amp;=\\frac{f''(x)f(x)}{f'^2(x)} \\end{align*} \\] <p>if we add some constrictions, it can be easy to make \\(g'(x)&lt;1\\).</p> <p>Here we make use of the Taylor's expansion(or the derivatives) of the goal function. </p> <p>Suppose that \\(f \\in C^2[a, b]\\), Let \\(x_0 \\in [a, b]\\) be an approximation to \\(x^*\\) such that \\(f(x^*) \\neq 0\\) and \\(|x_0-x^*|\\) is \"small\". Consider the first Taylor polynomial for \\(f(x)\\) expanded at \\(x_0\\):</p> \\[  f(x) = f(x_0) + (x-x_0)f'(x_0) + \\frac{(x-x_0)^2}{2}f''(\\zeta). \\] <p>If we let \\(x = x^*\\), and according to \\(f(x^*)=0\\), we get </p> \\[ 0 = f(x_0) + (x^*-x_0)f'(x_0) + \\frac{(x^*-x_0)^2}{2}f''(\\zeta) \\] <p>neglecting the square item, we get </p> \\[ 0 \\approx f(x_0) + (x^*-x_0)f'(x_0) \\] <p>to represent \\(x^*\\), we get</p> \\[ x^* = x_0 - \\frac{f(x_0)}{f'(x_0)} \\] <p>Then we can define the iterative relation as</p> \\[  x_n = x_{n-1} - \\frac{f(x_{n-1})}{f'(x_{n-1})}  \\] <p>The following statement guarrantees the convergence of the above iterative method.</p> <p>\u725b\u987f\u6cd5\u6536\u655b\u6761\u4ef6 | conditions for convergence of Newton's method</p> <p>Let \\(f \\in C^2[a, b]\\). If \\(p \\in (a, b)\\) is such that \\(f (p) = 0\\) and \\(f'( p) \\neq 0\\), then there exists a \\(\\delta &gt; 0\\) such that Newton\u2019s method generates a sequence \\(\\{p_n\\}_{n=1}^{\\infty}\\) converging to \\(p\\) for any initial approximation \\(p_0 \\in [p \u2212 \\delta, p + \\delta]\\).</p> <p>Prove it.</p> HintsProof <ul> <li>make use of the condition \\(f(p) = 0\\) and \\(f'(p)\\neq 0\\)</li> </ul> <p>for \\(x \\in (a, b)\\), we aim to find a narrower interval \\((x^*-\\delta, x^*+\\delta)\\) to have \\(g(x)\\) map into itself. That is, </p> \\[ g(x)\\leq k, \\forall k\\in (0,1) \\] <p>Firstly, \\(f'(p)\\neq 0\\) implies that \\(\\exists \\delta_1 &gt;0\\) such that \\(f'(x)\\neq 0, \\forall x \\in [x^* - \\delta, x^*+\\delta]\\subset [a, b]\\).</p> <p>THus, we have</p> \\[ g'(x) = \\frac{f(x)f''(x)}{(f'^2(x))} \\] <p>capable of dividing non-zero numbers.</p> <p>Since \\(f\\in C^2[a,b]\\), we have \\(g' \\in C^1[x^*-\\delta_1, x^*+\\delta_1]\\) for the exact solution \\(x^*\\), we have \\(f(x^*)=0\\), so </p> \\[ g'(x^*) = 0 \\] <p>which implies that \\(\\exists 0&lt;\\delta &lt; \\delta_1\\), such that </p> \\[ g'(x)\\leq k, \\forall k \\in [x^*-delta, x^*+\\delta] \\] <p>By differential Mean Value Theorem, for \\(x \\in [x^*-delta, x^*+\\delta], \\exists \\zeta \\in [x, x^*]\\) such that </p> \\[ |g(x)-g(x^*)|=g'(\\zeta)|x - x^*|\\leq k|x-x^*|&lt;|x-x^*| \\] <p>which means that \\(g\\) maps into itself. By Fixed-Point Theorom, the sequence defined by Newton's method converges. </p> <ul> <li>Secant Method It may not be easy to find derivarive of function \\(f\\), so we can use difference instead. That is, we have to store two adjacent points for calculating differnce</li> </ul> \\[ f'(x^{k}) \\approx \\frac{f(x^{k}) - f(x^{k-1})}{x^{k}-x^{k-1}} \\] <p>generate \\(p_{k+1}\\) using the above approximation and iterate.</p>"},{"location":"courses/Numerical_Analysis/NA/Solve_Equ/#order-of-convergence","title":"\u6536\u655b\u9636\u6570 | Order of Convergence","text":"<p>So how to identify the speed of convergence? The following definition gives a glimpse.</p> <p>Suppose \\(\\{p_n\\}_{n=1}^{\\infty}\\) is a sequence that converges to \\(p\\), with \\(p_n \\neq p (\\forall n)\\). If positive constants \\(\\lambda\\) and \\(\\alpha\\) exist with</p> \\[ \\lim_{n\\rightarrow \\infty}{\\frac{|p_{n+1}-p|}{|p_n-p|^{\\alpha}}}=\\lambda \\] <p>then \\(\\{p_n\\}_{n=0}^{\\infty}\\) converges to \\(p\\) of order \\(\\alpha\\), with asymptotic error constant \\(\\lambda\\).</p> <ul> <li>(i) If \\(\\alpha=1 (\\lambda&lt;1)\\), the sequence is linearly convergent.</li> <li>(ii) If \\(\\alpha=2\\), the sequence is quadratically convergent.</li> </ul> <p>The following theorem gives a sufficient condition for linear convergence.</p> <p>\u7ebf\u6027\u6536\u655b\u7684\u5145\u5206\u6761\u4ef6 | sufficient condition of linear convergence</p> <p>Let \\(g \\in C[a, b]\\) be such that \\(g(x) \\in [a, b], \\forall x \\in [a, b]\\). Suppose, in addition, that \\(g\\) is continuous on \\((a, b)\\) and a positive constant \\(k &lt; 1\\) exists with</p> \\[ |g'(x)|\\leq k \\quad \\forall x \\in (a, b) \\] <p>If \\(g'(p) \\neq 0\\), then for any number \\(p_0=p\\) in \\([a, b]\\), the sequence </p> \\[ p_n=g(p_{n-1}) \\quad \\forall n \\geq 1 \\] <p>converges only linearly to the unique fixed point \\(p\\) in \\([a, b]\\).</p> <p>Prove it.</p> Hints <p>Prove that linear convergence represents \\(\\exists alpha=1, lambda\\), such that \\(\\lim\\limits_{n\\leftarrow \\infty}\\frac{|p_{n+1}-p^*|}{|p_{n}-p^*|} = \\lambda\\).</p> <ul> <li>multiple roots We see that the speed of convergence is limited by multiple roots.</li> </ul> <p>Here we have modified Newton's Method:</p> \\[ g(x)= x - \\frac{f(x)f'(x)}{f'^2(x)-f(x)f''(x)} \\]"},{"location":"courses/Numerical_Analysis/NA/Solve_Equ/#accelerating-convergence","title":"\u52a0\u901f\u6536\u655b | Accelerating convergence","text":"<ul> <li>Aitken's \\(\\Delta^2\\) Method</li> </ul> <p>Suppose \\(\\{p_n\\}_{n=0}^{\\infty}\\) is a linearly convergent sequence with limit \\(p\\). To motivate the construction of a sequence \\(\\{\\hat{p}_n\\}_{n=1}^{\\infty}\\) that converges more rapidly to \\(p\\) than does \\(\\{p_n\\}_{n=0}^{\\infty}\\), let us first assume that the signs of \\(p_n-p\\), \\(p_{n+1}-p\\) and \\(p_{n+2}-p\\) agree and that \\(n\\) is sufficiently large that </p> \\[ \\frac{p_{n+1}-p}{p_n-p} \\approx \\frac{p_{n+2}-p}{p_{n+1}-p} \\] <p>Then solving for \\(p\\) gives</p> \\[ p \\approx \\frac{p_{n+2}p_n-p_{n+1}^2}{p_{n+2}-2p_{n+1}+p_n} \\] <p>And to get \\(p_n\\) out gives</p> \\[ \\begin{align*} p &amp;\\approx p_n - \\frac{(p_{n+1}-p_n)^2}{p_{n+2} - 2p_{n+1} + p_n}\\\\ \\Rightarrow \\hat{p}_n &amp;= p_n - \\frac{(\\Delta p_n)^2}{\\Delta p_{n+1}-\\Delta p_{n}} \\quad \\text{(denote $\\Delta p_n = p_{n+1} - p_n$)}\\\\ &amp;= p_n - \\frac{(\\Delta p_n)^2}{\\Delta^2 P_{n}}  \\end{align*}  \\] <ul> <li>Steffensen's Method</li> </ul> <p>The following thought is based on that the generated sequence \\(\\hat{p}\\) is a better approximation to true \\(p^*\\). We make use of the constructed sequence \\(\\{\\hat{p}_n\\}\\) to update the original sequence \\(\\{p_n\\}\\). That is, after generating a new \\(\\hat{p}\\), we can update \\(p_0 \\leftarrow p\\).</p>"},{"location":"courses/Ordinary_Differential_Equation/","title":"Ordinary Differential Equation","text":"<p>Reference</p> <ul> <li>\u300a\u5e38\u5fae\u5206\u65b9\u7a0b\u300b \u67f3\u5f6c</li> <li>\u300a\u5e38\u5fae\u5206\u65b9\u7a0b\u300b \u65b9\u9053\u5143</li> </ul>"},{"location":"courses/Ordinary_Differential_Equation/#elementary-integration-method","title":"\u521d\u7b49\u79ef\u5206\u6cd5 | Elementary Integration Method","text":""},{"location":"courses/Ordinary_Differential_Equation/#system-of-linear-differential-equations-lodes","title":"\u7ebf\u6027\u5fae\u5206\u65b9\u7a0b\u7ec4 | System of Linear Differential Equations (LODEs)","text":""},{"location":"courses/Ordinary_Differential_Equation/#general-theory-of-ode","title":"\u5fae\u5206\u65b9\u7a0b\u7684\u4e00\u822c\u7406\u8bba | General Theory of ODE","text":""},{"location":"courses/Ordinary_Differential_Equation/#existence-and-uniqueness-theorem","title":"\u5b58\u5728\u552f\u4e00\u6027\u5b9a\u7406 | Existence and Uniqueness Theorem","text":""},{"location":"courses/Ordinary_Differential_Equation/#contraction-mapping-method","title":"\u538b\u7f29\u6620\u5c04\u6cd5 | Contraction Mapping Method","text":""},{"location":"courses/Ordinary_Differential_Equation/#method-of-power-series","title":"\u5e42\u7ea7\u6570\u89e3\u6cd5 | Method of Power Series","text":""},{"location":"courses/Ordinary_Differential_Equation/#continuous-dependence-of-solution-on-initial-value","title":"\u89e3\u5bf9\u521d\u503c\u7684\u8fde\u7eed\u4f9d\u8d56\u6027 | Continuous Dependence of Solution on Initial Value","text":""},{"location":"courses/Ordinary_Differential_Equation/#stability-theory-of-ode","title":"\u5fae\u5206\u65b9\u7a0b\u7684\u7a33\u5b9a\u6027\u7406\u8bba | Stability Theory of ODE","text":""},{"location":"courses/Ordinary_Differential_Equation/EIM/","title":"Elementary Integration Method","text":""},{"location":"courses/Ordinary_Differential_Equation/EIM/#elementary-integration-method","title":"\u521d\u7b49\u79ef\u5206\u6cd5 | Elementary Integration Method","text":"<p>This chapter gives primary method of solving special differential functions, which plays a great role in future study.</p>"},{"location":"courses/Ordinary_Differential_Equation/EIM/#exact-equation","title":"\u6070\u5f53\u65b9\u7a0b | Exact Equation","text":"<p>We focus on the symmetrical form</p> \\[ \\begin{equation} M(x,y)dx + N(x,y)dy = 0 \\label{eq-exact} \\end{equation} \\] <p>This can bring us great convenience for digging into one-order ODE because it can gives us both the relation \\(y=f(x)\\) or \\(x=g(y)\\).</p> <p>\u5168\u5fae\u5206\u65b9\u7a0b\u3001\u6070\u5f53\u65b9\u7a0b\u7684\u5b9a\u4e49 | Definition of Exact Equation</p> <p>If there exists a \\(\\mathit{\\varphi}(x, y) \\in C^{1}(D)\\) such that </p> \\[ d\\mathit{\\varphi}(x, y) = M(x,y)dx + N(x,y)dy \\] <p>then equation \\(\\ref{eq-exact}\\) is called Exact Equation.</p> <p>There are some questions to answer:</p> <ul> <li>How to judge an equation to be exact Equation?</li> <li>If so, how to find original function \\(\\varphi(x, y)\\)?</li> <li>If not, how to transform it into one exact Equation?</li> </ul> <p>In this pattern, we answer the first two equation and leave the third one after learning LFODE.</p> <p>\u65b9\u7a0b\u662f\u6070\u5f53\u7684\u5145\u8981\u6761\u4ef6 | Necessary and Sufficient Condition for exact Equation</p> <p>Assume \\(D\\) is a simply connected region, and \\(M(x, y)\\), \\(N(x, y) \\in C(D)\\) with \\(\\frac{\\partial M}{\\partial y}\\) and \\(\\frac{\\partial N}{\\partial x} \\in C^{1}(D)\\). Then equation \\(\\ref{eq-exact}\\) is exact Equation if and only if</p> \\[ \\frac{\\partial M}{\\partial y} = \\frac{\\partial N}{\\partial x} \\] <p>Prove it.</p> Hints <p>\\(\\Rightarrow\\) is easy, by using second-order mixed partial derivatives of \\(\\mathit{\\varphi}\\).</p> <p>\\(\\Leftarrow\\). Using Green Formula/Theorem. </p> \\[ \\begin{align*} &amp;\\frac{\\partial P}{\\partial y} = \\frac{\\partial Q}{\\partial x} \\\\ \\Leftrightarrow \\ &amp;\\int_{\\gamma}P(x, y)dx+Q(x,y)dy = 0 \\quad \\forall\\text{closed loop } \\gamma\\\\ \\Leftrightarrow \\ &amp;\\int_{\\gamma}P(x, y)dx+Q(x,y)dy = C \\quad \\forall\\text{curve } \\gamma \\text{ connecting } (x_0, y_0), (x, y) \\\\ \\Leftrightarrow \\ &amp;\\exists \\mathit{\\varphi}(x,y) \\in C^{1}(D) \\text{ s.t } d\\mathit{\\varphi}(x, y) = P(x, y)dx+Q(x,y)dy \\end{align*} \\]"},{"location":"courses/Ordinary_Differential_Equation/EIM/#integral-factor","title":"\u79ef\u5206\u56e0\u5b50 | Integral Factor","text":"<p>This part we hope to find \\(\\mu(x, y)\\) so when we multiply it to both sides of equation \\(\\ref{eq-exact}\\)</p> \\[ \\mu(x,y)M(x,y)dx + \\mu(x,y)N(x,y)dy = 0  \\] <p>there exists \\(\\mathit{\\varphi}(x, y)\\) such that</p> \\[ d\\mathit{\\varphi}(x,y) = \\mu(x,y)M(x,y)dx + \\mu(x,y)N(x,y)dy  \\] <p>Naively, if \\(\\mathit{\\varphi}(x,y)\\in C^2\\), then </p> \\[ \\frac{\\partial (\\mu M)}{\\partial y} = \\frac{\\partial^2 \\mathit{\\varphi}}{\\partial x\\partial y}=\\frac{\\partial (\\mu N)}{\\partial x}  \\] <p>Theoretically speaking, we have to solve a PDE</p> \\[ \\begin{equation} M(x,y)\\frac{\\partial \\mu}{\\partial y} - N(x,y)\\frac{\\partial \\mu}{\\partial x} = \\left(\\frac{\\partial N}{\\partial x}-\\frac{\\partial M}{\\partial y}\\right)\\mu(x,y) \\label{eq-pde} \\end{equation} \\] <p>However, actually, it is very hard to solve the above PDE. So we focus on some special case like \\(\\mu(x,y)=\\mu(x)\\), \\(\\mu(y)\\), \\(\\mu(x+y)\\), \\(\\mu(xy)\\).</p> <p>Now we have the following theorem to judge whether we can get the above form of integral factors.</p> <p>\u65b9\u7a0b\u6709\u7279\u6b8a\u7c7b\u578b\u7684\u79ef\u5206\u56e0\u5b50\u7684\u5145\u8981\u6761\u4ef6 | Necessary and Sufficient Condition of special integral factor of ODE</p> <p>Equation \\(\\ref{eq-pde}\\) has solution \\(\\mu(x)\\) depending only on \\(x\\), if and only if</p> \\[ \\frac{\\frac{\\partial N}{\\partial x}-\\frac{\\partial M}{\\partial y}}{M} \\overset{\\Delta}{=} G(x) \\] <p>only depends only on \\(x\\). Then </p> \\[ \\mu(x) = e^{\\int_{x_0}^{x}G(t)dt} \\] <p>More generally, equation \\(\\ref{eq-pde}\\) has solution \\(\\mu(\\varphi(x,y))\\), if and only if</p> \\[ \\frac{\\frac{\\partial N}{\\partial x}-\\frac{\\partial M}{\\partial y}}{N\\frac{\\partial \\varphi}{\\partial x}-M\\frac{\\partial \\varphi}{\\partial y}} \\overset{\\Delta}{=} f(\\varphi(x,y)) \\]"},{"location":"courses/Ordinary_Differential_Equation/EIM/#variable-separation-equation","title":"\u53d8\u91cf\u5206\u79bb\u65b9\u7a0b | Variable Separation Equation","text":"<p>This chapter we discuss how to solve equation when it is not Exact Equation. The basic idea is, through transformation, we can convert an equation into an exact Equation.</p> <p>\u53d8\u91cf\u5206\u79bb\u65b9\u7a0b\u7684\u5b9a\u4e49 | Definition of Variable Separation Equation</p> <p>If there exists \\(M_1(x), M_2(y), N_1(x), N_2(y) \\in C^1(D)\\) such that </p> \\[ M(x, y) =M_1(x)M_2(y), N(x, y) = N_1(x), N_2(y) \\] <p>then we call equation \\(\\ref{eq-exact}\\) Variable Separation Equation.</p> <p>For this type of equation, we can multiply both sides </p> \\[ \\begin{equation} \\frac{1}{M_2(y)N_1(x)} \\label{eq-sep-factor} \\end{equation} \\] <p>equation \\(\\ref{eq-exact}\\) becomes</p> \\[ \\frac{M_1(x)}{N_1(x)}dx + \\frac{M_2(y)}{N_2(y)}dy = 0 \\] <p>This is an exact equation, and \\(\\ref{eq-sep-factor}\\) is called an Integral Factor of the equation.</p> <p>we can get its integral</p> \\[ \\int_{x_0}^{x}\\frac{M_1(t)}{N_1(t)}dt + \\int_{y_0}^{y}\\frac{M_2(s)}{N_2(s)}ds = c \\] <p>which is easily seen a solution of the original equation.</p> <p>And don't forget that if there exists \\(a_i (i= 1,2,\\cdots, m)\\) such that \\(N_1(a_i) = 0\\), or exists \\(b_j (j=1,2,\\cdots, n)\\) such that \\(M_2(b_j) = 0\\), then of course \\(x = a_i, y = b_j\\) are also solutions of the original solution.</p> <ul> <li>\u9f50\u6b21\u65b9\u7a0b | Homogeneous Equation</li> </ul> <p>The following equation can also be transferred into Variable Separation Equation.</p> <p>\u9f50\u6b21\u65b9\u7a0b\u7684\u5b9a\u4e49 | Definition of Homogeneous Equation</p> <p>We call \\(f(x, y)\\) Homogeneous Function of degree \\(n\\) if</p> \\[ f(tx, ty) = t^n f(x, y) \\] <p>and call equation \\(\\ref{eq-exact}\\) Homogeneous equation if \\(M(x, y), N(x, y)\\) are Homogeneous Function.</p> <p>When we let \\(y = u x\\), then \\(dy = xdu + udx\\), substitute in the equation and get</p> \\[ \\begin{align*} M(x, ux)dx+N(x,ux)(xdu+udx)&amp;=0 \\\\ \\Leftrightarrow [M(x, ux)+N(x,ux)u]dx+N(x,ux)xdu&amp;=0  \\end{align*} \\] <p>extract \\(x\\) out by definition of Homogeneous Equation:</p> \\[ x^n[M(1, u)+N(1,u)u]dx+x^{n+1}N(1,u)du=0  \\] <p>If \\(x^{n+1}[M(1, u)+N(1,u)u]\\neq 0\\), then we divide both sides by this and get</p> \\[ \\frac{1}{x}dx + \\frac{N(1,u)}{M(1,u)+uN(1,u)}du=0 \\] <p>which is also Variable Separation Equation.</p>"},{"location":"courses/Ordinary_Differential_Equation/EIM/#linear-first-order-differential-equation","title":"\u4e00\u9636\u7ebf\u6027\u5fae\u5206\u65b9\u7a0b | Linear First-Order Differential Equation","text":"<p>Now we focus on a really important expression of ODE: Linear First-Order Differential Equation(LFODE):</p> \\[ \\begin{equation} \\frac{dy}{dx} + p(x)y = q(x) \\label{eq-LFODE} \\end{equation} \\] <ul> <li>Homogeneous LFODE(H-LFODE)</li> </ul> <p>We let \\(q(x)\\equiv 0\\) in \\(\\ref{eq-LFODE}\\), we get</p> \\[ \\begin{equation} \\frac{dy}{dx} + p(x)y = 0 \\label{eq-H-LFODE} \\end{equation} \\] <p>rewrite it into symmetrical form:</p> \\[ p(x)ydx + dy = 0  \\] <p>which is Variable Separation Equation.</p> <p>So when \\(y\\neq 0\\), multiply both sides \\(1/y\\) and integrate</p> \\[ \\ln{|y|} + \\int_{x_0}^{x}p(t)dt = C \\] <p>get \\(y\\) out of form \\(x\\):</p> \\[ \\begin{align} y &amp;= \\pm e^{C}\\cdot e^{-\\int_{x_0}^{x}p(t)dt} \\nonumber \\\\ &amp;= C_1\\cdot e^{-\\int_{x_0}^{x}p(t)dt}  \\end{align} \\] <p>where \\(C_1=\\pm e^{C} \\neq 0\\), but we can include trivial solution \\(y \\equiv 0\\) by letting \\(C_1 = 0\\).</p> <ul> <li>Non-Homogeneous linear First-Order Differential Equation</li> </ul> <p>we have two ways to get the answer.</p> Version 1Version 2 <p>Making use of Integral Factors.</p> <p>To begin with, we convert equation \\(\\ref{eq-LFODE}\\) into symmetrical form:</p> \\[ \\begin{equation} (p(x)y-q(x))dx+dy=0 \\label{eq-SYM-LFODE} \\end{equation}  \\] <p>Multiply \\(e^{\\int_{x_0}^{x}p(t)dt}\\) to both sides of equation \\(\\ref{eq-SYM-LFODE}\\):</p> \\[ d\\left(\\ e^{\\int_{x_0}^{x}p(t)dt} y \\right) - e^{\\int_{x_0}^{x}p(t)dt} q(x)dx = 0 \\] <p>That is,</p> \\[ d\\left(\\ e^{\\int_{x_0}^{x}p(t)dt} y- \\int_{x_0}^{x}e^{\\int_{x_0}^{s}p(t)dt} q(x)ds \\right)  = 0 \\] <p>integrate and get</p> \\[ e^{\\int_{x_0}^{x}p(t)dt} y - \\int_{x_0}^{s}e^{\\int_{x_0}^{x}p(t)dt} q(s)ds + C = 0 \\] <p>extract \\(y\\) out and get:</p> \\[ y = e^{-\\int_{x_0}^{x}p(t)dt} \\left( C + \\int_{x_0}^{s}e^{\\int_{x_0}^{x}p(t)dt} q(s)ds \\right) \\] <p>Through Variation of Constants.</p> <p>We make a brave treatment: assume one special solution to equation \\(\\ref{eq-LFODE}\\) is </p> \\[ y = u\\cdot e^{-\\int_{x_0}^{x}p(t)dt} \\] <p>where \\(u\\) is a new variable.</p> <p>Subsititute in equation \\(\\ref{eq-LFODE}\\) and get</p> \\[ \\left(p(x) u e^{-\\int_{x_0}^{x}p(t)dt} -q(x) \\right) dx - u p(x) e^{-\\int_{x_0}^{x}p(t)dt} dx + e^{-\\int_{x_0}^{x}p(t)dt} du = 0 \\] <p>That is</p> \\[ -q(x) dx + e^{-\\int_{x_0}^{x}p(t)dt} du = 0 \\] <p>multiply \\(e^{\\int_{x_0}^{x}p(t)dt}\\) to both sides and  integrate </p> \\[ u =  \\int_{x_0}^{x}e^{\\int_{x_0}^{s}p(t)dt}q(s)ds \\] <p>So the special solution is</p> \\[ y =  e^{-\\int_{x_0}^{x}p(t)dt}\\cdot \\int_{x_0}^{x}e^{\\int_{x_0}^{s}p(t)dt}q(s)ds \\]"},{"location":"courses/Ordinary_Differential_Equation/EIM/#first-order-implicit-differential-equation","title":"\u4e00\u9636\u9690\u5f0f\u5fae\u5206\u65b9\u7a0b | First-order Implicit Differential Equation","text":"<p>Now we focus on equation</p> \\[ \\begin{equation} F(x,y, y') = 0 \\label{eq-para} \\end{equation} \\] <p>where \\(y'\\) cannot be explicitly solved out.</p> <ul> <li>\u53c2\u6570\u6cd5 | parametric method</li> </ul> <p>If we let \\(p = y'\\), then equation</p> \\[ \\begin{equation} F(x,y,p) = 0 \\label{eq-para-p} \\end{equation} \\] <p>represents a curved surface in 3-D space.</p> <p>And we can juggle equation \\(\\ref{eq-para-p}\\) and \\(dy=pdx\\) to get a curve in the space.</p>"},{"location":"courses/Ordinary_Differential_Equation/LODEs/","title":"System of Linear Differential Equations","text":""},{"location":"courses/Ordinary_Differential_Equation/LODEs/#system-of-linear-differential-equations-lodes","title":"\u7ebf\u6027\u5fae\u5206\u65b9\u7a0b\u7ec4 | System of Linear Differential Equations (LODEs)","text":"<p>This chapter we focus on </p> \\[ \\begin{equation} \\frac{d \\symbfit{X}(t)}{dt} = \\symbfit{A}(t)\\symbfit{X}(t)+\\symbfit{B}(t) \\label{eq-LODEs} \\end{equation} \\] <p>with initial condition</p> \\[ \\begin{equation} \\symbfit{X}(t_0)=\\symbfit{X}_0 \\label{eq-initial-LODEs} \\end{equation} \\] <p>where \\(t_0\\in I=(a,b)\\), \\(\\symbfit{X}_0 = (x_1^0,x_n^0,\\cdots, x_n^0)^T\\) is a given constant vector.</p>"},{"location":"courses/Ordinary_Differential_Equation/LODEs/#existence-and-uniqueness-of-lodes","title":"\u7ebf\u6027\u5fae\u5206\u65b9\u7a0b\u7ec4\u89e3\u7684\u5b58\u5728\u552f\u4e00\u6027 | Existence and Uniqueness of LODEs","text":"<p>This is quite similar to Picard Theorem in chapter Existence and Uniqueness Theorem, but it is still useful to give a special form of Picard Sequence for LODEs, which is also an approximation to solving it.</p> <p>\u7ebf\u6027\u5fae\u5206\u65b9\u7a0b\u7ec4\u89e3\u7684\u5b58\u5728\u552f\u4e00\u6027\u5b9a\u7406 | Theorem of Existence and Uniqueness of LODEs</p> <p>LODEs \\(\\ref{eq-LODEs}\\) with initial condition \\(\\ref{eq-initial-LODEs}\\) has only one solution on interval \\(I\\).</p> <p>Prove it.</p> Hints <p>We have to measure the distance in matrix. Now we need to give a definition of norm of vectors and matrixes(to see more details in Norm of vectors and matrixes) in Numerical Analysis.</p> \\[ \\Vert\\mathbfit{X} \\Vert = \\sum_{i=1}^{n}|x_i|, \\quad \\Vert\\mathbfit{A} \\Vert = \\sum_{i=1}^{n}\\sum_{j=1}^{n}|{a_{ij}}| \\] <p>It is easy to see that ...</p> <ul> <li>convert LODEs into its equivalent integral equations.</li> </ul> \\[ \\mathbfit{X}(t) = \\mathbfit{X}_0 + \\int_{t_0}^{t}\\left[ \\mathbfit{A}(s)\\mathbfit{X}(s)+\\mathbfit{B}(s)\\right] ds \\] <ul> <li>formulate Picard Sequence.</li> </ul> <p>Define:</p> \\[ \\begin{align} \\mathbfit{X}_0(t) &amp;= \\mathbfit{X}_0 \\nonumber\\\\ \\mathbfit{X}_1(t) &amp;= \\mathbfit{X}_0 + \\int_{t_0}^{t}\\left[ \\mathbfit{A}(s)\\mathbfit{X}_0(s)+\\mathbfit{B}(s)\\right] ds \\nonumber\\\\ \\mathbfit{X}_2(t) &amp;= \\mathbfit{X}_0 + \\int_{t_0}^{t}\\left[ \\mathbfit{A}(s)\\mathbfit{X}_1(s)+\\mathbfit{B}(s)\\right] ds \\nonumber\\\\ &amp;\\vdots \\nonumber\\\\ \\mathbfit{X}_n(t) &amp;= \\mathbfit{X}_0 + \\int_{t_0}^{t}\\left[ \\mathbfit{A}(s)\\mathbfit{X}_{n-1}(s)+\\mathbfit{B}(s)\\right] ds \\label{eq-LODEs-integral}\\\\ \\end{align} \\] <p>Consider similarly and we can say the above sequence is well-defined.</p> <ul> <li>Prove Picard Sequence convergent.</li> </ul> <p>denote</p> \\[ C = \\sup_{s\\in J}\\Vert\\mathbfit{A}(s)\\Vert, \\quad D = C\\Vert\\mathbfit{X}(s)\\Vert + \\sup_{s\\in J}\\Vert\\mathbfit{B}(s)\\Vert \\] <p>we can get </p> \\[ \\begin{align*} \\Vert \\mathbfit{X}_1(t) - \\mathbfit{X}_0(t) \\Vert &amp;\\leq D |t-t_0|\\\\ \\Vert\\mathbfit{X}_2(t) - \\mathbfit{X}_1(t) \\Vert &amp;\\leq \\int_{t_0}^{t} \\Vert \\mathbfit{A}(s) \\Vert \\Vert \\mathbfit{X}_{1}(s)- \\mathbfit{X}_{0}(s) \\Vert ds \\\\ &amp;\\leq \\int_{t_0}^{t} C D |s-t_0| ds = \\frac{D}{C} \\frac{(C|t-t_0|)^2}{2}\\\\ &amp;\\vdots\\\\ \\Vert\\mathbfit{X}_n(t) - \\mathbfit{X}_{n-1}(t) \\Vert &amp;\\leq \\frac{D}{C} \\frac{(C|t-t_0|)^{n}}{(n)!} \\end{align*} \\] <p>which shows the Picard Sequence converges.</p> <ul> <li>Prove the convergent function is solution of LODEs \\(\\ref{eq-LODEs}\\).</li> </ul> <p>If we denote \\(\\mathbfit{X}(t) = \\lim_{n\\rightarrow \\infty}\\mathbfit{X}_n(t)\\) and let \\(n\\rightarrow \\infty\\) on both sides of integral equation \\(\\ref{eq-LODEs-integral}\\), we get </p> \\[ \\mathbfit{X}(t) = \\mathbfit{X}_0 + \\int_{t_0}^{t}\\left[ \\mathbfit{A}(s)\\mathbfit{X}(s)+\\mathbfit{B}(s)\\right] ds \\] <p>which is a solution.</p> <ul> <li>prove uniqueness.</li> </ul> <p>Similar to proof in Picard Theorem.</p>"},{"location":"courses/Ordinary_Differential_Equation/LODEs/#boundary-problem-of-second-order-lode","title":"\u4e8c\u9636\u65b9\u7a0b\u8fb9\u503c\u95ee\u9898 | Boundary Problem of Second-Order LODE","text":"<p>This pattern we focus on LODE</p> \\[ \\begin{equation} y''+p(x)'+q(x)y = f(x) \\label{eq: BP-SecondOrder} \\end{equation} \\] <p>with Boundary Condition \\(y(a)=\\alpha, y(b)=\\beta\\), where \\(p(x), q(x) \\in C^1[a, b]\\)</p> <ul> <li>H-LODE</li> </ul> <p>\u5171\u8f6d\u70b9 | Conjugate Point</p> <p>If homogeneous LODE(H-LODE)</p> \\[ \\begin{equation} y''+p(x)'+q(x)y = 0 \\label{eq: BP-SO-H} \\end{equation} \\] <p>with boundary condition \\(y(a)=0, y(b)=0\\), has non-zero solution, then \\(\\{a, b\\}\\) is called the Conjugate Point of the H-LODE.</p> <p>We usually take use of the following method to check if the boundary point will induce indefinite solutions or no solutions.</p> <p>\u5171\u8f6d\u70b9\u7684\u5145\u8981\u6761\u4ef6 | Necessary and Sufficient Condition for Conjugate Point</p> <p>\\(\\{a, b\\}\\) is the Conjugate Point of H-LODE, if and only if \\(\\forall y_1, y_2\\) of the solution of H-LODE, which are linear irrelevant, satisfies</p> \\[ \\left| \\begin{array}{cc} y_1(a)&amp; y_2(a)\\\\ y_1(b)&amp; y_2(b) \\end{array} \\right| =0 \\] Hints <p>substitute the boundary condition and we get two linear equation system for parameters \\(c_1, c_2\\). And the above is the determinant of the system.</p> <p>\u9f50\u6b21\u65b9\u7a0b\u5b58\u5728\u552f\u4e00\u89e3\u7684\u5145\u8981\u6761\u4ef6 | Necessary and Sufficient Condition for existing only one solution for H-LODE</p> <p>H-LODE \\(\\ref{eq: BP-SO-H}\\) with boundary point \\(y(a)=\\alpha, y(b)=\\beta\\) has only one solution, if and only if \\(\\{a, b\\}\\) is not the conjugate point of the H-LODE.</p> Hints <p>Focus on the determinant of the linear irelevantly solutions of \\(y_1, y_2\\).</p> <ul> <li>Non-H-LODE</li> </ul> <p>We partition the solution of LODE \\(\\ref{eq: BP-SecondOrder}\\) into three parts.</p> <p>\u7ebf\u6027\u975e\u9f50\u6b21\u65b9\u7a0b\u5b58\u5728\u552f\u4e00\u89e3\u7684\u5145\u5206\u6761\u4ef6 | Sufficient Condition for existing only one solution for Non-H-LODE</p> <p>If \\(\\{a, b\\}\\) is not the conjugate point of H-LODE \\(\\ref{eq: BP-SO-H}\\), then Non-H-LODE \\(\\ref{eq: BP-SecondOrder}\\) has only one solution.</p> <p>There are two ways to prove it.</p> Version 1Version 2 <p>Assume \\(y_1(x)\\) is a solution of H-LODE </p> \\[ \\begin{equation} y'' + p(x)y' +q(x)y = 0, \\quad y(a) = 0, \\quad y(b) = 1 \\label{eq: BP-1} \\end{equation} \\] <p>and \\(y_2(x)\\) is a solution of H-LODE</p> \\[ \\begin{equation} y'' + p(x)y' +q(x)y = 0, \\quad y(a) = 1, \\quad y(b) = 0 \\label{eq: BP-2} \\end{equation} \\] <p>and \\(y_3(x)\\) is a solution of Non-H-LODE</p> \\[ \\begin{equation} y'' + p(x)y' +q(x)y = f(x), \\quad y(a) = 0, \\quad y(b) = 0 \\label{eq: BP-3} \\end{equation} \\] <p>and the solution of LODE \\(\\ref{eq: BP-SecondOrder}\\) can be represented as</p> \\[ y = \\alpha y_1 + \\beta y_2 + y_3 \\] <p>where \\(y_1, y_2\\) are linearly irrelevant because of  existence and uniqueness theorem.</p> <p>We can get \\(y_3\\) through Variation of Constant. That is, let \\(y_3 = u_1(x) y_1(x)+u_2(x)y_2(x)\\), then </p> \\[ u_1 = \\int \\frac{-y_2 f}{W}dt, \\quad u_2 = \\int \\frac{y_1 f}{W}dt \\] <p>Substitute the boundary condition \\(\\ref{eq: BP-1}, \\ref{eq: BP-2}, \\ref{eq: BP-3}\\) we get </p> \\[ u_2(a)=0, \\quad u_1(b)=0 \\] <p>Based on this, we can transform the \\(u_1(x), u_2(x)\\) to definite integral whose upper limit of integral is variable</p> \\[ u_1 = \\int_{x}^{b} \\frac{y_2 f}{W}dt, \\quad u_2 = \\int_{x}^{a} \\frac{y_1 f}{W}dt \\] <p>So we can write particular solution </p> \\[ y_3 = y_1(x) \\int_{x}^{b} \\frac{y_2(t) f(t)}{W(t)}dt  + y_2(x)\\int_{x}^{a} \\frac{y_1(x) f(x)}{W(x)}dt \\] <p>If we define Green Function as</p> \\[ G(x,t)= \\begin{cases} \\displaystyle \\frac{y_2(x)y_1(t)}{W(t)}, \\quad a\\leq t\\leq x  \\\\ \\displaystyle \\frac{y_1(x)y_2(t)}{W(t)}, \\quad x\\leq t\\leq b  \\end{cases} \\] <p>then </p> \\[ y_3(x) = \\int_{a}^{b}G(x, t)f(t)dt \\] <p>According to the textbook.</p>"},{"location":"courses/Ordinary_Differential_Equation/LODEs/#s-l-sturm-liouville-boundary-problem","title":"S-L \u8fb9\u503c\u95ee\u9898 | Sturm-Liouville Boundary Problem","text":"<p>Solve for PDE</p> \\[ \\begin{cases} \\displaystyle u_{tt} = a^2 u_{xx} , \\quad 0\\leq x\\leq L, t\\geq 0\\\\ \\displaystyle u|_{x=0} = u|_{t=0} = 0 \\end{cases} \\] <p>Assume we can seperate the variables \\(x, t\\). That is, let \\(u = X(x)T(t)\\) and substitute, we get </p> \\[ \\begin{align*} X(x)T''(t)&amp;=a^2X''(x)T(t)\\\\ \\Rightarrow \\frac{T''(t)}{a^2T(t)} = \\frac{X''(x)}{X(x)} &amp;\\overset{\\Delta}{=}constant =-\\lambda \\end{align*} \\] <p>Thus, we have to find \\(T(t), X(x)\\) such that </p> \\[ T''(t)+a^2\\lambda T(t)= 0 \\] \\[ \\begin{cases} X''(x) + \\lambda X(x)= 0 \\\\ X(0)=X(L)=0 \\end{cases} \\]"},{"location":"courses/Ordinary_Differential_Equation/Lyapunov/","title":"Stability Theory of ODE","text":"<p>This chapter we focus on </p> \\[ \\begin{equation} \\dot{\\mathbfit{x}} = \\mathbfit{f}(t, \\mathbfit{x})\\label{eq1} \\end{equation} \\] <p>We consider continuous dependence of the solutions on initial condition on a larger interval, i.e. \\((\\beta,\\infty)\\), for we have show the dependency holds on finite intervals. However, this is not always as expected. So we have to introduce some basic ideas.</p>"},{"location":"courses/Ordinary_Differential_Equation/Lyapunov/#lyapunov-lyapunov-stability","title":"Lyapunov \u7a33\u5b9a\u6027 | Lyapunov Stability","text":"<p>We assume that \\(\\pmb{x}=\\pmb{\\varphi}(t)\\) is a special solution of the ODE \\(\\ref{eq1}\\). </p> <p>Lyapunov Stability</p> <p>(i) Lyapunov Stable</p> <p>If \\(\\forall \\varepsilon&gt;0\\), \\(\\forall t_0&gt;\\beta\\), \\(\\exists \\delta&gt;0\\), \\(\\forall \\pmb{x}_0\\), s.t. \\(\\|\\pmb{x}_0-\\pmb{\\varphi}(t_0)\\|&lt;\\delta\\), ODE \\(\\ref{eq1}\\) with initial value \\(\\pmb{x}(t_0)=\\pmb{x}_0\\) has a solution \\(\\pmb{x}(t;t_0,\\pmb{x}_0)\\) which exists on \\([t_0,+\\infty)\\) and satisfies</p> \\[ \\|\\pmb{x}(t;t_0,\\pmb{x}_0)-\\varphi(t)\\|&lt;\\varepsilon, \\quad \\forall t\\in [t_0,+\\infty) \\] <p>then we call the special solution \\(\\pmb{x}=\\pmb{\\varphi}(t)\\) is Lyapunov stable, or stable for short.</p> <p>(ii) Lyapunov Unstable</p> <p>If \\(\\exists \\varepsilon_0&gt;0\\), \\(\\exists t_0&gt;\\beta\\), \\(\\forall \\delta&gt;0\\), \\(\\exists \\pmb{x}_0\\), s.t. \\(\\|\\pmb{x}_0-\\pmb{\\varphi}(t_0)\\|&lt;\\delta\\), ODE \\(\\ref{eq1}\\) with initial value \\(\\pmb{x}(t_0)=\\pmb{x}_0\\) has a solution \\(\\pmb{x}(t;t_0,\\pmb{x}_0)\\) which exists on \\([t_0, \\alpha)\\)(\\(\\alpha&lt;+\\infty\\)) or there exists \\(t_1&gt;t_0\\), and it satisfies </p> \\[ \\|\\pmb{x}(t_1;t_0,\\pmb{x}_0)-\\varphi(t)\\|\\geq\\varepsilon_0 \\] <p>then the special solution is Lyapunov Unstable.</p> <p>(iii) Attractive</p> <p>If \\(\\exists t_0&gt;\\beta\\), \\(\\exists \\zeta&gt;0\\), \\(\\forall \\pmb{x}_0\\), s.t. \\(\\|\\pmb{x}_0-\\pmb{\\varphi}(t_0)\\|&lt;\\zeta\\), solution of ODE \\(\\ref{eq1}\\) with initial value \\(\\pmb{x}(t_0)=\\pmb{x}_0\\) satisfies</p> \\[ \\lim_{t\\rightarrow +\\infty}\\|\\pmb{x}(t;t_0,\\pmb{x}_0)-\\pmb{\\varphi}(t)\\|=0 \\] <p>then the special solution is Attractive.</p> <p>(iv) Asymptotic stable</p> <p>If a special solution is Lyapunov stable and attractive, then we call it Asymptotic stable.</p> <p>To simplify the analysis, we choose to make a translation. That is, if we let </p> \\[ \\pmb{y}=\\pmb{x}-\\pmb{\\varphi}(t), \\pmb{F}(t,\\pmb{y})=\\pmb{f}(t,\\pmb{y}+\\pmb{\\varphi}(t))-\\pmb{f}(t,\\pmb{\\varphi}(t)) \\] <p>then we only need to discuss </p> \\[ \\begin{align*} \\frac{d\\pmb{y}}{dt}&amp;=\\frac{d\\pmb{x}}{dt}-\\frac{d\\pmb{\\varphi}(t)}{dt}\\\\ &amp;=\\pmb{f}(t,\\pmb{x})-\\pmb{f}(t,\\pmb{\\varphi}(t))\\\\ &amp;=\\pmb{f}(t,\\pmb{y}+\\pmb{\\varphi}(t))-\\pmb{f}(t,\\pmb{\\varphi}(t))\\\\ &amp;=\\pmb{F}(t,\\pmb{y}) \\end{align*} \\] <p>with special solution \\(\\pmb{y}=0\\), i.e. \\(\\pmb{f}(t,\\pmb{0})=\\pmb{0}\\). We call solution \\(\\pmb{y}\\equiv 0\\) zero solution.</p>"},{"location":"courses/Ordinary_Differential_Equation/Lyapunov/#linear-approximate-method","title":"\u7ebf\u6027\u8fd1\u4f3c\u5224\u522b\u6cd5 | Linear Approximate Method","text":"<p>If we represent ODE \\(\\ref{eq1}\\) as </p> \\[ \\begin{equation} \\frac{d\\pmb{x}}{dt}=\\pmb{A}(t)\\pmb{x}+\\pmb{R}(t,\\pmb{x})\\label{eq3} \\end{equation} \\] <p>where </p> \\[ \\pmb{A}(t)=\\frac{\\partial \\pmb{f}}{\\partial \\pmb{x}}\\Bigg|_{\\pmb{x}=\\pmb{0}} \\] <p>and \\(\\pmb{R}(t,\\pmb{x})\\) is the summation of higher order items, i.e.</p> \\[ \\lim_{\\|\\pmb{x}\\|\\rightarrow 0}\\frac{\\|\\pmb{R}(t,\\pmb{x})\\|}{\\|\\pmb{x}\\|}=0. \\]"},{"location":"courses/Ordinary_Differential_Equation/Lyapunov/#lyapunov-stability-of-lodes","title":"Lyapunov Stability of LODEs","text":"<p>Firstly, we discuss the Lyapunov stability of LODEs</p> \\[ \\begin{equation} \\frac{d\\pmb{x}}{dt}=\\pmb{A}(t)\\pmb{x}. \\label{eq2} \\end{equation} \\] <p>Theorem of Lyapunov Stability of LODEs</p> <p>Assume \\(\\pmb{\\varPhi}(t)\\) is a basic solution matrix of LODEs \\(\\ref{eq2}\\), then its zero solution is </p> <p>(i) stable iff \\(\\forall t_0&gt;\\beta\\), \\(\\exists K(t_0)&gt;0\\), s.t.</p> \\[ \\|\\pmb{\\varPhi}(t)\\| \\leq K,\\quad, t\\geq t_0 \\] <p>(ii) asymptotic stable iff </p> \\[ \\lim_{t\\rightarrow +\\infty}\\|\\pmb{\\varPhi}(t)\\|=0 \\] <p>For time-varying system, it is hard to solve a basic solution matrix. But for time-invariant system, we can not only solve the basic solution matrix, but also deduce a better result.</p> <p>Theorem for Lyapunov Stability of time-invariant System</p> <p>Consider const-coeffient system</p> \\[ \\frac{d\\pmb{x}}{dt}=\\pmb{A}\\pmb{x} \\] <p>its zero solution is </p> <p>(i) asymptotic stable iff all the eigenbalues of \\(\\pmb{A}\\) are negative.</p> <p>(ii) stable iff all the eigenvalues of \\(\\pmb{A}\\) are not positive, and for eigenvalue whose real part is zero, its corresponding Jordan block is one roder (or can be diagonalized).</p> <p>(iii) unstable iff there exists one positive eigenvalue or for eigenvalue whose real part is zero, its corresponding Jordan block is one roder(or cannot be diagonalized).</p>"},{"location":"courses/Ordinary_Differential_Equation/Lyapunov/#lyapunov-stability-of-non-linear-odes","title":"Lyapunov Stability of Non-linear ODEs","text":"<p>For non-linear system, we can have some results that is similar to the above. To simplify the problem, we let the linear part \\(\\pmb{A}(t)\\equiv \\pmb{A}\\) in ODE \\(\\ref{eq3}\\) be a constant matrix. Then, we can have the following Theorem</p> <p>Theorem for non-linear system using Linear Approximatie Method</p> <p>(i) If all the eigenvalues of \\(\\pmb{A}\\) are negative, then the zero solution of ODE \\(\\ref{eq3}\\) is asymptotic stable.</p> <p>(ii) If there exists an eigenvalue of \\(\\pmb{A}\\) which is positive, then the zero solution of ODE \\(\\ref{eq3}\\) is unstable.</p> HintsProof <p>Use Picard theorem and extension theorem to get a solution on \\([0,t*)\\), then show that solution can tend to \\(0\\) by using Gronwall inequation.</p>"},{"location":"courses/Ordinary_Differential_Equation/Lyapunov/#second-method-of-lyapunov","title":"\u7b2c\u4e8c\u6cd5 | Second Method of Lyapunov","text":"<p>The first method does not carry on because it makes use of power series. But his second method did gain ground, which employs an energy function which characterizes the solution. So this method is also called Direct Method.</p> <p>Here, we only consider autonomous ODEs, that is,</p> \\[ \\begin{equation} \\frac{d\\pmb{x}}{dt}=\\pmb{f}(\\pmb{x})\\label{eq4} \\end{equation} \\] <p>with the right side of ODE not containing variable \\(t\\).</p> <p>The basic idea can be shown in the following diagram.</p> \\[ \\begin{align*} \\|\\pmb{x}_0\\|&amp;\\ll 1\\\\ &amp;\\Downarrow \\quad \\text{(by continuity of $V$)} \\\\ V(\\pmb{x}_0)&amp;\\ll 1\\\\  &amp;\\Downarrow \\quad \\text{(by $\\frac{d V}{dt}&lt; 0$)}\\\\ V[\\pmb{x}(t)]&amp;\\leq V[\\pmb{x}(0)]\\ll 1\\\\ &amp;\\Downarrow \\quad \\text{(by monotony)}\\\\ \\|\\pmb{x}(t)\\|&amp;\\leq\\|\\pmb{x}(0)\\|\\ll 1 \\end{align*} \\] <p>Definition of Definite Sign Function</p> <p>Assume \\(h&gt;0\\), function \\(V(\\pmb{x})\\in C^1(\\|\\pmb{x}\\|\\leq h)\\), if \\(V(\\pmb{0})=0\\) and </p> <p>(i) \\(V(\\pmb{x})&gt;0\\)(or \\(&lt;0\\)) on \\(0&lt;\\|\\pmb{x}\\|\\leq h\\), then we call \\(V(\\pmb{x})\\) is a definite positive(or negative) function.</p> <p>(ii) \\(V(\\pmb{x})\\geq0\\)(or \\(\\leq0\\)) on \\(0&lt;\\|\\pmb{x}\\|\\leq h\\), then we call \\(V(\\pmb{x})\\) is a constant positive(or negative) function.</p> <p>Denifite positive function has a property.</p> <p>Property of Definite Positive Function</p> <p>\\(\\forall \\varepsilon&gt;0\\), \\(\\exists \\delta&gt;0\\), \\(\\forall \\pmb{x}\\), s.t. \\(V(\\pmb{x})&lt;\\delta\\), then \\(\\|\\pmb{x}\\|&lt;\\varepsilon\\). </p> HintsProof <p>Pay attention to the positive definite function.</p> <p>\\(\\forall \\varepsilon&gt;0\\), let </p> \\[ \\delta=\\min_{\\varepsilon\\leq\\|\\pmb{x}\\|\\leq h}V(\\pmb{x}) \\] <p>then </p> \\[ \\|\\pmb{x}\\|\\geq \\delta \\Rightarrow V(\\pmb{x})\\geq \\min_{\\varepsilon\\leq\\|\\pmb{x}\\|\\leq h}V(\\pmb{x}) = \\delta. \\] <p>So</p> \\[ V(\\pmb{x})&lt;\\delta\\Rightarrow \\|\\pmb{x}\\|&lt;\\varepsilon \\] <p></p> <p>Theorem of Stability test</p> <p>In ODE \\(\\ref{eq4}\\), assume \\(h&gt;0\\). If there exists a definite positive function \\(V(\\pmb{x})\\) on \\(\\|\\pmb{x}\\|\\leq h\\), whose total derivative </p> \\[ \\frac{dV}{dt}=\\nabla^{\\pmb{x}}V(\\pmb{x})\\cdot \\frac{d\\pmb{x}}{dt}=\\sum_{i=1}^n\\frac{\\partial V}{\\partial x_i}\\frac{d x_i}{dt}=\\sum_{i=1}^n\\frac{\\partial V}{\\partial x_i}f_i \\] <p>(i) is a constant negative function, then the zero solution to ODE \\(\\ref{eq4}\\) is Lyapunov stable.</p> <p>(ii) is a definite negative function, then the zero solution to ODE \\(\\ref{eq4}\\) is Lyapunov asymptotic stable.</p> <p>(iii) is a definite positive function, then the zero solution to ODE \\(\\ref{eq4}\\) is Lyapunov unstable.</p> <p>For (i) in Theorem of Stability Test, if we find another condition, then the system can also be asymptotic stable, and this is exactly the following theorem.</p> <p>LaSalle's invariance principle</p> <p>If ODE \\(\\ref{eq4}\\) satisfies </p> <p>For (iii) in Theorem of Stability Test, we can have a weaker theorem.</p> <p>Theorem for Lyapunov Unstable</p> <p>For ODE \\(\\ref{eq4}\\), if there exists a open region \\(\\mathcal{N}\\subset B_h(\\pmb{0})\\), satisfies</p> <p>(i) \\(V(\\pmb{x})\\in C^1(\\mathcal{N})\\).</p> <p>(ii) \\(\\pmb{0}\\in \\partial \\mathcal{N}\\), \\(\\exists \\delta&gt;0\\), \\(\\forall \\pmb{x}\\in \\partial \\mathcal{N}\\cap B_\\delta(\\pmb{0})\\), such that</p> \\[ V(\\pmb{x})=0 \\] <p>(iii) \\(\\forall \\pmb{x}\\in \\mathcal{N}\\cap B_\\delta(\\pmb{0})\\), \\(V(\\pmb{x})\\) and \\(\\frac{dV}{dt}\\) are both definite positive function.</p> <p>For a specific problem, we can choose first quadrant of the plane </p> \\[ \\mathcal{N}=\\{(x,y): x&gt;0,y&gt;0\\} \\] <p>with a typical \\(V(\\pmb{x})=xy\\), then test if \\(\\frac{dV}{dt}\\) is definite positive on \\(\\mathcal{N}\\).</p> <p>Example1. Determine the Lyapunov stability of the following system.</p> \\[ \\begin{cases} x'=x^5+\\sin{x^5}+2y+6y^5\\\\ y'=-e^x+8x^3+e^{y^3} \\end{cases} \\] Answer <p>use Hamilton system approximation.</p> <p>Example2. Determine the Lyapunov stability of the following system.</p> \\[ \\begin{cases} x'=-3x^3y^4\\\\ y'=x^4y^3 \\end{cases} \\] Answer <p>The above one cannot use Hamilton system approximation for it only has one item. But we can divide one by two, and get</p> \\[ \\frac{dy}{dx}=-\\frac{x}{3y} \\] <p>which is a homogeneous equation. So the solution is \\(x^2+3y^2=C\\). We can choose Lyapunov function $V=0.5x<sup>2+1.5y</sup>3, then \\(dV/dt\\equiv 0\\), which is       Lyapunov stable but not asymptotic stable.</p>"},{"location":"courses/Ordinary_Differential_Equation/Qualitative_Theory/","title":"Qualitative Theory of ODE","text":""},{"location":"courses/Ordinary_Differential_Equation/Qualitative_Theory/#singular-point-analysis","title":"Singular Point Analysis","text":""},{"location":"courses/Ordinary_Differential_Equation/Qualitative_Theory/#linear-system-of-two-dimension","title":"Linear System of Two dimension","text":"<p>Consider </p> \\[ \\begin{cases} x'=ax+by\\\\ y'=cx+dy \\end{cases}, \\quad \\hat{\\pmb{x}}=\\pmb{Q}x \\] <p>with det\\((\\pmb{Q})\\neq 0\\).</p> <p>On linear curve, we have \\(y(t)=kx(t)\\), so \\(y'(t)=kx'(t)\\), and </p> \\[ \\frac{y'}{x'}\\Bigg|_{y=kx}=k \\] <p>Solve for \\(k\\).</p> <p>Determine the direction of curves by checking the vector field on one point.</p> <p>For \\(0&lt;\\lambda_1&lt;\\lambda_2\\) or \\(\\lambda_1&lt;\\lambda_2&lt;0\\), we can substitute \\((1,0)\\) into \\(y'\\) to see the direction.</p> <p>For \\(\\lambda_1&lt;0&lt;\\lambda_2\\), we can inspect a point \\((0,1)\\) to see the direction.</p> <p>For \\(\\lambda_1=\\lambda_2\\), first we check if there is indefinite solution of \\(k\\). If so, then ... we can inspect a point on linear curve.</p> <p>For \\(\\lambda=\\alpha\\pm i\\beta\\), we can also substitute \\((1,0)\\) into \\(y'\\) to see the direction.</p> <p>Example. Draw the craft.</p> \\[ \\begin{cases} x'=4x+\\sqrt{3}y\\\\ y'=2x+5y \\end{cases} \\] Answer <ul> <li> <p>check the type of \\(\\lambda_1\\), \\(\\lambda_2\\).</p> </li> <li> <p>determine the linear curve.</p> </li> <li> <p>determine the type of direction.</p> </li> </ul>"},{"location":"courses/Ordinary_Differential_Equation/General_Theory/","title":"General Theory of ODE","text":""},{"location":"courses/Ordinary_Differential_Equation/General_Theory/#existence-and-uniqueness-theorem","title":"\u5b58\u5728\u552f\u4e00\u6027\u5b9a\u7406 | Existence and Uniqueness Theorem","text":""},{"location":"courses/Ordinary_Differential_Equation/General_Theory/#proof-using-contraction-mapping","title":"\u538b\u7f29\u6620\u5c04\u6cd5 | Proof using Contraction Mapping","text":""},{"location":"courses/Ordinary_Differential_Equation/General_Theory/#extension-of-solution","title":"\u89e3\u7684\u5ef6\u62d3 | Extension of Solution","text":""},{"location":"courses/Ordinary_Differential_Equation/General_Theory/#method-of-power-series","title":"\u5e42\u7ea7\u6570\u89e3\u6cd5 | Method of Power Series","text":""},{"location":"courses/Ordinary_Differential_Equation/General_Theory/#continuous-dependence-of-solution-on-initial-value","title":"\u89e3\u5bf9\u521d\u503c\u7684\u8fde\u7eed\u4f9d\u8d56\u6027 | Continuous Dependence of Solution on Initial Value","text":""},{"location":"courses/Ordinary_Differential_Equation/General_Theory/Continuous_Dependence/","title":"Continuous Dependence of Solution on Initial Value","text":"<p>Here we focus on </p> \\[ \\begin{equation} \\dot{\\mathbfit{x}} = \\mathbfit{f}(t, \\mathbfit{x}),\\quad  \\mathbfit{x}(t_0) = \\mathbfit{x}_0 \\label{eq-vector-cauchy} \\end{equation} \\] <p>where \\(\\mathbfit{x}\\in \\mathbb{R}^n\\) and \\(\\mathbfit{f}: \\mathbb{R}^{n+1} \\mapsto \\mathbb{R}^n \\in C(D)\\) is a vector function(or vector field). To simplify the problem, we assume \\(\\pmb{f} \\in C^r(D)\\), where \\(r\\geq 1\\).</p>"},{"location":"courses/Ordinary_Differential_Equation/General_Theory/Contraction_Mapping_Method/","title":"Contraction Mapping Method","text":"<p>Reference</p> <p>Ordinary Differential Equation, Vladimir Igorevich Arnold</p> <p>\u300a\u5e38\u5fae\u5206\u65b9\u7a0b\u300b \u041b.\u0421.\u5e9e\u7279\u91cc\u4e9a\u91d1</p>"},{"location":"courses/Ordinary_Differential_Equation/General_Theory/Contraction_Mapping_Method/#proof-using-contraction-mapping","title":"\u538b\u7f29\u6620\u5c04\u6cd5 | Proof using Contraction Mapping","text":"<p>This part we want to prove Picard Theorem from another perspective. And this method is also universal on multi-dimensional Cauchy Problem. So to simplify the notation, we change the problem equivalently to </p> \\[ \\begin{equation} \\dot{\\mathbfit{x}} = \\mathbfit{f}(t, \\mathbfit{x}),\\quad  \\mathbfit{x}(t_0) = \\mathbfit{x}_0 \\label{eq-vector-cauchy} \\end{equation} \\] <p>where \\(\\mathbfit{x}\\in \\mathbb{R}^n\\) and \\(\\mathbfit{f}: \\mathbb{R}^{n+1} \\mapsto \\mathbb{R}^n \\in C(D)\\) is a vector function(or vector field). To simplify the problem, we assume \\(\\pmb{f} \\in C^r(D)\\), where \\(r\\geq 1\\).</p> <p>Assume we give a Euvlid Structure in region \\(D\\in \\mathbb{R}^{n+1}\\). For all \\((t_0,\\pmb{x}_0)\\in D\\), we consider a cylinder(\u67f1\u4f53) with sufficiently small parameters \\(a\\) and \\(b\\)</p> \\[ \\Gamma= \\{(t, \\pmb{x}): |t-t_0|\\leq a, \\|\\pmb{x}-\\pmb{x}_0\\|\\leq b\\} \\] <p>which is still a subset of \\(D\\).</p> <p>Then the Picard Theorem becomes:</p> <p>Picard Theorem</p> <p>Assume \\(\\pmb{f}\\) of problem \\(\\ref{eq-vector-cauchy}\\) is continous and differentiable(or Lipschitz condition) on region \\(\\Gamma\\), then for all given \\(\\pmb{x}\\) which is sufficiently close to \\(\\pmb{x}_0\\), there exsits a neighberhood of \\(t_0\\), such that there exists a unique solution \\(\\pmb{\\varphi}(t)\\).</p> Hints <ul> <li>Using the fixed point theorem (existence and uniqueness).</li> </ul>"},{"location":"courses/Ordinary_Differential_Equation/General_Theory/Contraction_Mapping_Method/#group-basis","title":"\u7fa4\u8bba\u57fa\u7840 | Group Basis","text":"<p>Firstly, let us introduce some basic ideas about groups.</p> <p>\u7fa4\u7684\u5b9a\u4e49 | Definition of Group</p> <p>Firstly, we have to define Law of Composition.</p> <p>Assume \\(S\\) is a set. A Law of Composition is a map </p> \\[ S\\times S \\mapsto S. \\] <p>\\(S\\times S\\) deontes the product set, whose elements are pairs \\(a\\), \\(b\\) of elements of \\(S\\).</p> <p>A group is a set \\(G\\) together with a law of composition(Here we use sign \\(\\cdot\\) (multiplicative notation), and sign \\(+\\) (additive notation) also can be applied) that has the following properties:</p> <ul> <li>The law of composition is associative.</li> </ul> \\[ (ab)c=a(bc), \\quad \\forall a,b,c\\in G \\] <ul> <li>\\(G\\) contains an identity element \\(1\\) (\\(0\\) in sign \\(+\\)) such that </li> </ul> \\[ 1a=a \\text{ and } a1=a,\\quad \\forall a\\in G \\] <ul> <li>Every element \\(a\\) of \\(G\\) has an inverse, i.e. an element \\(b\\) such that </li> </ul> \\[ ab=1 \\text{ and } ba=1 \\] <p>which is denoted by \\(a^{-1}\\) (\\(-a\\) in additive notation).</p> <p>\u7fa4\u7684\u6027\u8d28 | Properties of Group</p> <ul> <li>Cancellation Law</li> </ul> <p>Let \\(a\\), \\(b\\), \\(c\\) be elements of a group \\(G\\) whose law of composition is written multiplicatively. If </p> \\[ ab=ac \\text{ or } ba = ca \\Rightarrow b=c. \\] <p>If </p> \\[ ab=a \\text{ or } ba =a \\Rightarrow b=1. \\] Proof <p>Multiply both sides of the above equation on the left by \\(a^{-1}\\).</p> <p>Example.</p> <p>\\(n\\times n\\) general liear group is the group of all invertible \\(n\\times n\\) matrices, denoted by</p> \\[ GL_{n} =\\{n\\times n \\text{ invertible matrices } A\\}. \\] <p>where the law of composition is matrix multiplication.</p> <p>\u540c\u6001 | Homomorphism</p> <p>Let \\(G\\) and \\(G'\\) be groups written with multiplicative notation. A Homomorphism \\(\\varphi:G\\mapsto G'\\) is a map from \\(G\\) to \\(G'\\) such that </p> \\[ \\varphi(ab) = \\varphi(a)\\varphi(b), \\quad \\forall a,b\\in G \\] <p>If we let \\(\\varphi\\) to be a bijection, then we call it Isomorphism(\u540c\u6784).</p> <p>Example.</p> <p>the determinent function det: \\(GL_n(\\mathbb{R})\\mapsto \\mathbb{R}^\\times\\)</p> <p>\u540c\u6001\u7684\u6027\u8d28 | Properties of Homomorphism</p> <p>Let \\(\\varphi: G\\mapsto G'\\) be a group homomorphism.</p> <p>(i) If \\(a_1,a_2,\\cdots, a_k\\) are elements of \\(G\\), then </p> \\[ \\varphi(a_1 a_2\\cdots a_k)=\\varphi(a_1)\\varphi(a_2)\\cdots\\varphi(a_n) \\] <p>(ii) \\(\\varphi\\) maps the identity to identity, i.e. </p> \\[ \\varphi(1_G)=1_{G'} \\] <p>(iii) \\(\\varphi\\) maps inverses to inverses, i.e.</p> \\[ \\varphi(a^{-1}) = \\varphi(a)^{-1} \\] Proof <p>(i) by induction(Strong induction).</p> <p>(ii) using \\(1\\cdot 1=1\\) and \\(\\varphi(1)\\varphi(1)=\\varphi(1\\cdot 1)=\\varphi(1)\\), cancel both sides to obtain and get \\(\\varphi(1)=1_{G'}\\).</p> <p>(iii) similarly, \\(a^{-1}a=1\\) and \\(\\varphi(a^{-1})\\varphi(a)=\\varphi(a^{-1}a)=\\varphi(1_G)=1_{G'}\\), and we are done.</p>"},{"location":"courses/Ordinary_Differential_Equation/General_Theory/Contraction_Mapping_Method/#phase-space","title":"\u76f8\u7a7a\u95f4 | Phase Space","text":"<p>Then we have to introduce some comception about phase.</p> <p>\u5355\u53c2\u6570\u53d8\u6362\u7fa4 | One-Parameter Group of Transformation</p> <p>\\(M\\) is a set. A family of mapping \\(\\{g^t\\}_{t\\in \\mathbb{R}}\\) which maps set \\(M\\) into itself is called One-Parameter Group of Transformation of set \\(M\\), if </p> \\[ g^{t+s} = g^tg^s, \\quad \\forall t,s\\in \\mathbb{R} \\] <p>and \\(g^0\\) is identity mapping(\u6052\u7b49\u6620\u5c04).</p> <p>\u76f8\u6d41\u3001\u76f8\u7a7a\u95f4\u3001\u8fd0\u52a8\u3001\u76f8\u66f2\u7ebf\u7684\u5b9a\u4e49 | Definition of Phase Flow</p> <p>A couple composed of set \\(M\\) and its one-parameter group of transformation \\(\\{g^t\\}\\), denoted as \\((M, \\{g^t\\})\\), is called Phase Flow. And here \\(M\\) is called the phase space of the phase flow. The element of \\(M\\) is called Phase Point.</p> <p>Consider a mapping </p> \\[ \\begin{equation} \\varphi: \\mathbb{R}\\mapsto M, \\quad \\varphi(t) = g^tx,\\quad x\\in M \\label{map-motion} \\end{equation} \\] <p>which maps a real straight line into phase space. Then it is called a motion of phase point \\(x\\) under the action of phase flow. </p> <p>The image of \\(\\mathbb{R}\\) under mapping \\(\\varphi\\) is called the phase curve of phase flow \\((M,\\{g^t\\})\\).</p> <ul> <li>Fixed Point</li> </ul> <p>If the phase curve of a phase point \\(x\\in M\\) is itself, i.e.</p> \\[ g^tx=x,\\quad \\forall t\\in \\mathbb{R} \\] <p>then \\(x\\) is called the fixed point of the phase flow \\((M, \\{g^t\\})\\).</p> <p>In fact one-parameter group of transformation is exchangeable(\\(g^tg^s=g^{t+s}=g^{s+t}=g^sg^t\\)). And it is also a bijection. This is easy to prove. </p> <p>Firstly we prove it is surjection. \\(\\forall x\\in M\\), \\(\\exists g^{-t}x \\in M\\), such that \\(g^t(g^{-t}x)=x\\). Then we prove it is a injective mapping. \\(g^tx=g^ty\\), then \\(x=g^0x=g^{-t}g^tx=g^{-t}g^ty=g^0y=y\\).</p> <p>With the above property we can easily see the following theorem.</p> <p>\u76f8\u7a7a\u95f4\u4e2d\u7684\u70b9\u4ec5\u6709\u4e00\u6761\u76f8\u66f2\u7ebf</p> <p>For all \\(x\\in M\\), there only exists one phase curve.</p> Proof <p>Because \\(g^t\\) is a bijection.</p> <p>Now we introduce two important conceptions.</p> <p>\u6620\u5c04\u7684\u56fe\u5f62 | Graph of a mapping</p> <p>The graph of a mapping \\(f : A\\mapsto B\\) is a subset of the direct product(\u76f4\u79ef) \\(A\\times B\\):</p> \\[ \\{(a,f(a))| a\\in A\\} \\] <p>\u6269\u5f20\u76f8\u7a7a\u95f4\u3001\u79ef\u5206\u66f2\u7ebf | Expanded Phase Space, Integral Curve</p> <p>The Expanded Phase Space of phase flow \\((M, \\{g^t\\})\\) is the direct product \\(\\mathbb{R} \\times M\\).</p> <p>The Integral Curve of phase flow \\((M, \\{g^t\\})\\) is the graph of the motion \\(\\ref{map-motion}\\).</p> <p>Now we have to make use of DIfferential in Euclid Space.</p> <p>\u53ef\u5fae\u51fd\u6570\u3001\u53ef\u5fae\u6620\u5c04\u3001\u5fae\u5206\u540c\u80da | Differentiable Function, Differentiable Mapping, Diffeomorphism</p> <p>Assume \\(U \\subset\\mathbb{R}^n, V \\subset\\mathbb{R}^m\\). Then</p> <p>A Differentiable Function is a function \\(f: U\\mapsto \\mathbb{R}\\) which is \\(r\\) times differentiable.</p> <p>A Differentiable Mapping is a mapping \\(f:U\\mapsto V\\) defined by </p> \\[ y_i = f_i(x_1,x_2,\\cdots,x_n), \\quad i=1,2\\cdots, m \\] <p>where \\(f_i: U\\mapsto \\mathbb{R}\\) is a Differentiable function. If \\(y_i: V\\mapsto \\mathbb{R}\\) is a coordinate of \\(\\pmb{y}\\in \\mathbb{R}^m\\), then \\(y_i\\circ f: U\\mapsto \\mathbb{R}\\) is a Differentiable function in \\(U\\).</p> <p>A Diffeomorphism is a bijection \\(f:U\\mapsto V\\), such that \\(f\\) and \\(f^{-1}\\) are both Differentiable mappings.</p> <p>\u4e0e\u5750\u6807\u8f74\u6709\u5173\u7684\u76f8\u901f\u5ea6\u3001\u5411\u91cf\u573a | Phase Speed, Vector Field</p> <p>The Phase Speed \\(\\pmb{v}(x)\\) of phase flow \\(g^t\\) at point \\(x\\in M\\) is </p> \\[ \\pmb{v}(x) = \\frac{d}{dt}\\Bigg|_{t=0}g^tx \\] <p>And at time \\(\\tau\\), we have phase speed </p> \\[ \\pmb{v}(g^\\tau x) = \\frac{d}{dt}\\Bigg|_{t=\\tau}g^tx. \\] <p>Now we let \\(M\\) to be a region in Euclid Space \\(\\mathbb{R}^n\\) with coordinates \\(x_1,x_2,\\cdots,x_n\\). And if \\(x_i:M\\mapsto \\mathbb{R}\\) is the coordinate of \\(\\pmb{x}\\in M\\), then the vector \\(\\pmb{v}(x)\\) is defined by \\(v_i: M\\mapsto \\mathbb{R}, i=1,2\\cdots, n\\):</p> \\[ v_i(\\pmb{x}) = \\frac{d}{dt}\\Bigg|_{t=0}x_i(g^t\\pmb{x}) \\] <p>So we define a Vector Field \\(\\pmb{v}\\) on \\(M\\) when neglecting \\(t\\).</p> <p>\u4e0e\u5750\u6807\u8f74\u65e0\u5173\u7684\u76f8\u901f\u5ea6\u3001\u5411\u91cf\u573a | Phase Speed, Vector Field</p> <p>Firstly, we have to define that a coordinate \\(\\{y_i\\}_{i=1}^{n}:U\\mapsto \\mathbb{R}\\) is admissive, if mapping</p> \\[ y:U\\mapsto \\mathbb{R}^n,\\quad y(\\pmb{x}) = y_1(\\pmb{x})\\pmb{e}_1 + y_2(\\pmb{x})\\pmb{e}_2 + \\cdots +y_n(\\pmb{x})\\pmb{e}_n \\] <p>is a diffeomorphism.</p> <p>So we have a proposition:</p> <p>Two curves \\(\\varphi_1\\), \\(\\varphi_2\\) that pass \\(\\pmb{x}\\in U\\) are tangent with each other if and only if two curves \\(y\\circ \\varphi_1\\), \\(y\\circ\\varphi_2\\) that pass \\(y(\\pmb{x})\\in V\\) are tangent with each other.</p> <p><p> </p></p> <p>So the velocity vector of curve \\(\\varphi:I\\mapsto U\\) that pass \\(\\pmb{x}\\in U\\) is </p> \\[ \\pmb{v}=\\dot{\\varphi}(0), \\quad \\pmb{v}=\\frac{d\\varphi}{dt}\\Bigg|_{t=0}. \\] <p>\u5207\u5411\u91cf\u3001\u5207\u7a7a\u95f4 | Tangent Vector, Tangent Space</p> <p>Assume \\(U\\in \\mathbb{R}^n\\) with coordinates \\(x_1,x_2,\\cdots, n\\) and map \\(\\varphi:I\\mapsto U\\) maps an interval on \\(\\mathbb{R}\\) to \\(U\\) such that \\(\\varphi(0)=\\pmb{x}\\in U\\). and also its velocity is determined by </p> \\[ v_i = \\frac{d}{dt}\\Bigg|_{t=0}(x_i\\circ \\varphi), i=1,2,\\cdots, n. \\] <p>Two curves \\(\\varphi_1, \\varphi_2:I\\mapsto U\\) pass the same point \\(\\pmb{x}\\) are tangent with each other if \\(t\\rightarrow 0\\), \\(\\rho(\\varphi_1, \\varphi_2)\\rightarrow 0\\).</p> <p><p> </p></p> <p>A set composed by all tangent vectors of the curves passing \\(\\pmb{x}\\) is a linear space with dimension \\(n\\), which is called Tangent Space, denoted by \\(TU_x\\).</p> <p>Now we want to give a space without coordinates.</p> <p>\u6620\u5c04\u7684\u5bfc\u6570 | Derivative of Mapping</p> <p>Assume that \\(f:U\\mapsto V\\) is a differentiable mapping from \\(\\pmb{x}=(x_1,x_2,\\cdots,x_n)\\) to \\(\\pmb{y}=(y_1,y_2,\\cdots,y_m)\\). Let \\(\\pmb{y}=f(\\pmb{x})\\in V\\).</p> <p>The Derivative of mapping \\(f\\) at \\(\\pmb{x}\\) is a mapping from tangent space at \\(\\pmb{x}\\in U\\) to tangent space at \\(\\pmb{y}\\in V\\)</p> \\[ f_*|_{\\pmb{x}}: TU_{\\pmb{x}} \\mapsto TV|_{f(\\pmb{x})} \\] <p>This mapping maps velocity vector of curve \\(\\varphi\\) at \\(\\pmb{x}\\) into velocity vector of \\(f\\circ \\varphi\\) at \\(f(\\pmb{x})\\), i.e.</p> \\[ f_*|_{\\pmb{x}}\\left(\\frac{d\\varphi}{dt}\\Bigg|_{t=0} \\right) = \\frac{d}{dt}\\Bigg|_{t=0}(f\\circ \\varphi). \\] <p>which defines a linear mapping from \\(TU_{\\pmb{x}}\\) to \\(TV_{f(\\pmb{x})}\\).</p>"},{"location":"courses/Ordinary_Differential_Equation/General_Theory/Contraction_Mapping_Method/#contraction-mapping-in-metric-space","title":"\u5ea6\u91cf\u7a7a\u95f4\u4e0b\u7684\u538b\u7f29\u6620\u5c04 | Contraction Mapping in Metric Space","text":"<p>Then, let us recall the definition of metric space(\u5ea6\u91cf\u7a7a\u95f4).</p> <p>\u5ea6\u91cf\u7a7a\u95f4\u7684\u5b9a\u4e49 | Definition of Metric Space</p> <p>Assume \\(M\\) is a set. If there is a function \\(\\rho: M\\times M\\mapsto \\mathbb{R}\\) defined on it, satisfies that, \\(\\forall x,y,z \\in M\\)</p> <ul> <li>Positive</li> </ul> \\[ \\rho(x,y)&gt;0,\\quad x\\neq y \\] <ul> <li>Definite</li> </ul> \\[ \\rho(x, y)=0 \\Leftrightarrow x=y \\] <ul> <li>Symmetry</li> </ul> \\[ \\rho(x,y) = \\rho(y,x) \\] <ul> <li>Triangle inequality</li> </ul> \\[ \\rho(x,z) \\leq \\rho(x,y) +\\rho(y,z) \\] <p>then \\(M\\) is called metric space with metrc \\(\\rho\\).</p> <p>Now we can rewrite the Lipschitz Condition with terms of metric space.</p> <p>Lipschitz \u6761\u4ef6 | Lipschitz Condition</p> <p>Assume \\(A: M_1 \\mapsto M_2\\) is a mapping from a metric space \\(M_1\\) (with metric \\(\\rho_1\\)) to another matric space \\(M_2\\) (with metric \\(\\rho_1\\)). If there exists a constant \\(L&gt;0\\), s.t.</p> \\[ \\rho_2(Ax,Ay)\\leq L\\rho_1(x,y),\\quad \\forall x,y \\in M_1 \\] <p>then we say mapping \\(A\\) satisfies Lipschitz Condition.</p> <p>In Proof 1 we will show how to use Lipschitz condition to shrink the region of \\((t, \\pmb{x})\\)(Expanded Phase Space) such that the contraction mapping maps the metric space into itself.</p> <p>Usually, a mapping that maps a space \\(M\\) into itself would be of great use for us, so here comes the following definition.</p> <p>\u538b\u7f29\u6620\u5c04\u7684\u5b9a\u4e49 | Definition of Contraction mapping</p> <p>Assume \\(A: M \\mapsto M\\) is a mapping from complete metric space with metric \\(\\rho\\) to itself. If there exists a constant \\(k\\), \\(0&lt;k&lt;1\\), s.t. </p> \\[ \\rho(Ax,Ay)\\leq k\\rho(x, y),\\quad \\forall x, y\\in M. \\] <p>then \\(A\\) is called a Contraction mapping.</p> <p>If \\(Ax =x \\in M\\), we call \\(x\\) is a fixed point of mapping \\(A\\). Then the following theorem is quite useful in our future proof. </p> <p>Banach\u4e0d\u52a8\u70b9\u5b9a\u7406 | Banach Fixed-Point Theorem</p> <p>(This also known as contraction mapping theorem.)</p> <p>Assume \\(A: M\\mapsto M\\) is a contraction mapping, then \\(A\\) has a unique fixed-point.</p> HintsProof <p>We prove by showing that \\(A^nx\\) is a Cauchy Sequence and its limit \\(X\\) falls in \\(M\\). Then use the property of limits to show it is fixed-point of \\(A\\). Then prove the uniqueness by using property of metric \\(\\rho\\).</p> <p>Let \\(d = \\rho(x,Ax)\\), then </p> \\[ \\rho(A^nx,A^{n+1}x)\\leq k^n\\rho(x,Ax) = k^nd \\] <p>Sequence \\(A^nx\\), \\(n=1,2,\\cdots\\) is convergent. So its limit exists and let \\(X \\overset{\\Delta}{=} \\lim\\limits_{n\\rightarrow \\infty}A^nx\\).</p> <p>So </p> \\[ AX = A \\lim\\limits_{n\\rightarrow \\infty}A^nx = \\lim\\limits_{n\\rightarrow \\infty}A^{n+1}x = X \\] <p>which means \\(X\\) is a fixed-point of \\(A\\). Then we prove the uniqueness of fixed point. Assume there are two fixed points of \\(A\\), denoted as \\(X\\), \\(Y\\). See that</p> \\[ \\rho(X, Y)  =\\rho(AX, AY) \\leq k\\rho(X,Y) \\] <p>Because \\(0&lt;k&lt;1\\), so \\(\\rho(X,Y)=0\\) \\(\\Rightarrow\\) \\(X=Y\\).</p> <p>Readers can compare this theorem to Fixed-Point Theorem in Numerical Analysis. </p> <p>Note that we have an abstract set \\(M\\) with abstract metric \\(\\rho\\). In the next two parts, we will let \\(M\\) to be a set of mappings and \\(\\rho\\) to be defined based on natural norm of vectors. </p>"},{"location":"courses/Ordinary_Differential_Equation/General_Theory/Contraction_Mapping_Method/#Proof-1","title":"\u8bc1\u660e1 | Proof 1","text":"<p>Now we consider Picard mapping \\(A\\) is a mapping from a mapping to another mapping</p> \\[ A: (\\pmb{\\varphi}: t\\mapsto \\pmb{x}) \\mapsto (A\\pmb{\\varphi}: t\\mapsto \\pmb{x}) \\] <p>defined by</p> \\[ (A\\pmb{\\varphi})(t) = \\pmb{x}_0 + \\int_{t_0}^{t}\\pmb{f}(\\tau, \\pmb{\\varphi}(\\tau))d\\tau \\] <p>Geometrically speaking, tangent line of each point on \\(A\\pmb{\\varphi}\\), i.e. \\((A\\pmb{\\varphi})'(t)\\), equals to the vector field determined by \\(\\pmb{\\varphi}(t)\\), i.e. \\(f(t, \\pmb{\\varphi}(t))\\). This can help us find a smaller region for \\(A\\pmb{\\varphi}\\) to be well-defined.</p> <p>And note that \\(\\pmb{\\varphi}\\) is a solution of Cauchy Problem \\(\\ref{eq-vector-cauchy}\\) if and only if \\(\\pmb{\\varphi} = A \\pmb{\\varphi}\\). So we have to prove \\(A\\) is a contraction mapping in a complete metric space.</p> <ul> <li>\u5b9a\u4e49\u57df\\((t, \\pmb{x})\\) | Some Constants \\(a\\), \\(b\\), \\(M\\)</li> </ul> <p>Let </p> \\[ M=\\max_{\\pmb{x}\\in \\Gamma}\\|\\pmb{f}\\|, \\quad L= \\max_{\\pmb{x}\\in \\Gamma}\\|\\pmb{f}_*\\| \\text{(or by Lipschitz condition)} \\] <p>which can be obtained because \\(\\Gamma\\) is a compact set. Firstly, we consider a Cylinder</p> \\[ \\Gamma = \\{(t,\\pmb{x}): |t-t_0|\\leq a, \\|\\pmb{x}-\\pmb{x}_0\\|\\leq b\\} \\] <p><p> </p></p> <p>Now consider a Cone(\u9525\u4f53) with opening \\(M\\) and sufficient small height \\(a\\) at point \\((t_0,\\pmb{x}_0)\\)</p> \\[ K_0 = \\{(t,\\pmb{x}): |t-t_0|\\leq a, \\|\\pmb{x}-\\pmb{x}_0\\|\\leq M|t-t_0|\\} \\] <p>Which means our mapping \\(\\pmb{\\varphi}\\) after contraction mapping \\(A\\) is still well-defined. See details in the following proof.</p> Proof \\[ \\begin{align*} \\|A\\pmb{\\varphi}(t) - \\pmb{x}_0\\| &amp;=\\left\\|\\int_{t_0}^t\\pmb{f}(\\tau,\\pmb{\\varphi}(\\tau))d\\tau\\right\\| \\\\ &amp;\\leq \\left|\\int_{t_0}^t\\|\\pmb{f}(\\tau,\\pmb{\\varphi}(\\tau))\\|d\\tau\\right|\\\\ &amp;\\leq M |t-t_0| \\end{align*} \\] <p>So as long as \\(a&lt;b/M\\), \\(\\|A\\pmb{\\varphi}(t)-\\pmb{x}_0\\|\\leq b\\).</p> <ul> <li>Prove \\(\\pmb{\\varphi}\\) compose a complete metric space.</li> </ul> <p>We define a Space \\(M\\) composed of mappings \\(\\pmb{\\varphi}: \\mathbb{R}\\mapsto \\mathbb{R}^n\\) defined by \\(\\pmb{\\varphi}(t)=\\pmb{x}\\).</p> <p>Define a metric \\(\\rho\\) to be</p> \\[ \\rho(\\pmb{\\varphi}_1, \\pmb{\\varphi}_2) = \\|\\pmb{\\varphi}_1-\\pmb{\\varphi}_2\\| = \\max_{|t-t_0|\\leq a}|\\pmb{\\varphi}_1(t)-\\pmb{\\varphi}_2(t)| \\] <p>Then \\(\\{M, \\rho\\}\\) is a complete metric space.</p> <ul> <li>Prove \\(A\\) is a contraction mapping.</li> </ul> Proof <p>That is, \\(\\forall \\pmb{\\varphi}_1, \\pmb{\\varphi}_2 \\in M\\)</p> \\[ \\begin{align*} \\|A\\pmb{\\varphi}_1- A\\pmb{\\varphi}_2\\| &amp;= \\left\\|\\int_{t_0}^t[\\pmb{f}(\\tau, \\pmb{\\varphi}_1(\\tau))-\\pmb{f}(\\tau, \\pmb{\\varphi}_2(\\tau))]d\\tau\\right\\|\\\\ &amp;\\leq \\left|\\int_{t_0}^t\\|\\pmb{f}(\\tau, \\pmb{\\varphi}_1(\\tau))-\\pmb{f}(\\tau, \\pmb{\\varphi}_2(\\tau))\\|d\\tau \\right|\\\\ &amp;\\leq L \\int_{t_0}^t \\|\\pmb{\\varphi}_1(\\tau) - \\pmb{\\varphi}_2(\\tau)\\|d\\tau \\\\ &amp;\\leq L \\|\\pmb{\\varphi}_1 - \\pmb{\\varphi}_2\\| \\int_{t_0}^t d\\tau = L |t-t_0| \\|\\pmb{\\varphi}_1 - \\pmb{\\varphi}_2\\|  \\end{align*} \\] <p>So as long as \\(a&lt;1/L\\), \\(A\\) is a contraction mapping.</p> <p>By Banach Fixed Point Theorem, there exists only one soluion of ODE.</p>"},{"location":"courses/Ordinary_Differential_Equation/General_Theory/Contraction_Mapping_Method/#2-proof-2","title":"\u8bc1\u660e2 | Proof 2","text":"<p>In this part, we consider a different mapping.</p> <ul> <li>\u5b9a\u4e49\u57df\\((t, \\pmb{x})\\)</li> </ul> <p>In order to let all the possible cone to be in the cylinder, we have to shrink the region to make the region is well defined. That is, there exsits sufficiently small parameters \\(b'&lt;b\\) such that \\(\\|\\pmb{x}-\\pmb{x}_0\\|&lt;b'\\). So now we get a smaller region (a smaller cylinder)</p> \\[ \\Gamma' = \\{(t, \\pmb{x}): |t-t_0|&lt;a', \\|\\pmb{x}-\\pmb{x}_0\\|&lt;b'\\} \\] <p><p> </p></p> <p>We also have to prove that \\(A\\) maps \\(M\\) into itself. That is, \\(\\|\\varphi(t)-\\pmb{x}_0\\|\\leq b'\\). Here we have to make \\(a'\\) to satisfy some condition.</p> <p>Consider all the possible continuous mapping \\(\\pmb{h}\\) which maps the above cylinder into \\(\\mathbb{R}^n\\), which is a solution of original ODE problem. Assume \\(M\\) is a set composed of these mappings with additional condition</p> \\[ \\|\\pmb{h}(t, \\pmb{x})\\| \\leq C|t-t_0| \\] <p>Specially, we let \\(\\pmb{h}(t_0,\\pmb{x})=0\\). We introduce a metric </p> \\[ \\rho(\\pmb{h}_1,\\pmb{h}_2)=\\|\\pmb{h}_1-\\pmb{h}_2\\|=\\max_{(t,\\pmb{x})\\in \\Gamma' }\\|\\pmb{h}_1(t,\\pmb{x})-\\pmb{h}_2(t,\\pmb{x})\\| \\] <p> </p> <p>Note: Space \\(M\\) is dependent on \\(a'\\), \\(b'\\) and \\(C\\).</p> <p>Now we really have to consider a mapping </p> \\[ A: M\\mapsto M \\] <p>defined by</p> \\[ (A\\pmb{h})(t, \\pmb{x}) = \\int_{t_0}^t\\pmb{f}(\\tau, \\pmb{x}+\\pmb{h}(\\tau, \\pmb{x}))d\\tau \\] <p>\\(A\\) \u662f\u538b\u7f29\u6620\u5c04 | \\(A\\) is a contraction mapping</p> <p>\\(A\\) is a contraction mapping from \\(M\\) to \\(M\\), if \\(a'\\) is sufficiently small.</p> <p>Prove it. The following proof are set to find how small \\(a'\\) shoulc be.</p> HintsProof <p>Firstly, prove \\(A\\) maps \\(M\\) to itself. Then Prove if is a constraction mapping.</p> <ul> <li>\\(A\\) maps \\(M\\) to itself.</li> </ul> \\[ \\begin{align*} \\|(A\\pmb{h}(t, \\pmb{x}))\\| &amp;\\leq \\|\\int_{t_0}^t\\pmb{f}(\\tau, \\pmb{x}+\\pmb{h}(\\tau,\\pmb{x}))d\\tau\\| \\\\ &amp;\\leq |\\int_{t_0}^tCd\\tau| \\leq C|t-t_0| \\end{align*} \\] <p>so \\(AM\\subset M\\).</p> <ul> <li>\\(A\\) is a constraction mapping.</li> </ul> <p>Estimate \\(A\\pmb{h}_1- A\\pmb{h}_2\\):</p> \\[ (A\\pmb{h}_1-A\\pmb{h}_2)(t,\\pmb{x}) = \\int_{t_0}^t[\\pmb{f}(\\tau, \\pmb{x}+\\pmb{h}_1(\\tau,\\pmb{x}))-\\pmb{f}(\\tau, \\pmb{x}+\\pmb{h}_2(\\tau,\\pmb{x}))]d\\tau \\] <p>Denote \\(\\pmb{f}_i(\\tau) = \\pmb{f}(\\tau, \\pmb{x}+\\pmb{h}_i(\\tau,\\pmb{x}))\\), \\(i=1,2\\), so</p> \\[ \\begin{align*} \\|\\pmb{f}_1(\\tau)- \\pmb{f}_2(\\tau)\\| &amp;\\leq L\\|\\pmb{h}_1(\\tau)-\\pmb{h}_2(\\tau)\\| \\quad\\text{(using Lipschitz condition)}\\\\  &amp;\\leq L\\|\\pmb{h}_1-\\pmb{h}_2\\| = L\\rho(\\pmb{h}_1, \\pmb{h}_2) \\quad \\text{(by definition of metric)} \\end{align*} \\] <p>And </p> \\[ \\|(A\\pmb{h}_1-A\\pmb{h}_2)(t,\\pmb{x})\\| \\leq L a'\\rho(\\pmb{h}_1, \\pmb{h}_2) \\] <p>So if \\(La'&lt;1\\), then \\(A\\) is a contraction mapping.</p>"},{"location":"courses/Ordinary_Differential_Equation/General_Theory/Contraction_Mapping_Method/#lipschitz-appendix-from-continuity-differentiability-to-lipschitz-condition","title":"\u9644\u5f55\uff1a\u8fde\u7eed\u53ef\u5fae\u5230Lipschitz\u8fde\u7eed | Appendix: from Continuity &amp; Differentiability to Lipschitz Condition","text":"<p>we will choose natural metric </p> \\[ \\rho(\\pmb{x}, \\pmb{y}) = \\|\\pmb{x}-\\pmb{y}\\| = \\sqrt{(\\pmb{x}-\\pmb{y},\\pmb{x}-\\pmb{y})} \\] <p>so space \\(\\mathbb{R}^n\\) with the above metric is a complete metric space.</p> <p>So in this case, we have a parallel theorem for smoothness and Lipschitz condition in \\(\\mathbb{R}\\).</p> <p>\u8fde\u7eed\u53ef\u5fae\u6620\u5c04\u6ee1\u8db3Lipschitz\u6761\u4ef6 | Continuously Differentiable mapping satisfies Lipschitz condition</p> <p>Assume \\(V\\subset U \\subset\\mathbb{R}^m\\) is a convex and contract set, and continuously differentiable mapping \\(\\pmb{f}\\) which maps \\(U\\) to \\(\\mathbb{R}^n\\) satisfies Lipschitz condition, and the constant </p> \\[ L = \\sup_{\\pmb{x}\\in V} \\|\\pmb{f}_{*\\pmb{x}}\\|  \\] <p>where \\(\\pmb{f}_*|_{\\pmb{x}}=\\pmb{f}_{*\\pmb{x}}:T\\mathbb{R}^m_{\\pmb{x}}\\mapsto T\\mathbb{R}^n_{\\pmb{x}}\\)</p> <p><p> </p></p> <p>Prove it.</p> Hints <p>Let \\(\\pmb{z}(t)=\\pmb{x}+t(\\pmb{y}-\\pmb{x})\\), \\(0\\leq t\\leq 1\\). Then</p> \\[ \\begin{align*} \\pmb{f}(\\pmb{y})-\\pmb{f}(\\pmb{x})&amp;=\\int_{0}^1\\frac{d}{dt}\\pmb{f}(\\pmb{z}(\\tau))d\\tau\\\\ &amp;=\\int_0^1\\pmb{f}_{*\\pmb{z}(\\tau)}[\\pmb{\\dot{z}}(\\tau)]d\\tau \\\\ &amp;=\\int_0^1 \\pmb{f}_{*\\pmb{z}(\\tau)} (\\pmb{y}-\\pmb{x})d\\tau \\end{align*}\\\\ \\] <p>where \\(\\pmb{y}-\\pmb{x}\\) is a constant, so</p> \\[ \\begin{align*} \\left\\| \\pmb{f}(\\pmb{y})-\\pmb{f}(\\pmb{x}) \\right\\| &amp;=\\left\\|\\int_0^1 \\pmb{f}_{*\\pmb{z}(\\tau)} (\\pmb{y}-\\pmb{x})(\\tau)d\\tau\\right\\|\\\\ &amp;\\leq \\int_0^1 \\|\\pmb{f}_{*\\pmb{x}}\\| \\|\\pmb{y}-\\pmb{x}\\|d\\tau\\\\ &amp;\\leq L\\|\\pmb{y}-\\pmb{x}\\| \\end{align*} \\] <p>and we are done.</p>"},{"location":"courses/Ordinary_Differential_Equation/General_Theory/Existence_Uniqueness_Theorem/","title":"Existence & Uniqueness Theorem","text":""},{"location":"courses/Ordinary_Differential_Equation/General_Theory/Existence_Uniqueness_Theorem/#Existence-and-Uniqueness-Theorem","title":"\u5b58\u5728\u552f\u4e00\u6027\u5b9a\u7406 | Existence and Uniqueness Theorem","text":"<p>This chapter we focus on ODE with initial value problem</p> \\[ \\begin{equation} \\frac{dy}{dx} = f(x, y), \\quad y(x_0) = y_0. \\label{eq-cauchy} \\end{equation} \\] <p>The above problem is also called Cauchy Problem, for Cauchy firstly prove that the solution to the problem \\(\\ref{eq-cauchy}\\) exists uniquely when \\(f(x, y)\\) has continuous partial derivative to \\(y\\), that is, \\(\\frac{\\partial f}{\\partial y} \\in C(G)\\).</p>"},{"location":"courses/Ordinary_Differential_Equation/General_Theory/Existence_Uniqueness_Theorem/#preliminary-knowledge","title":"\u9884\u5907\u77e5\u8bc6 | Preliminary knowledge","text":"<p>The following inequation is really useful in estimating solution of ODE by offering the upper bound.</p> <p>Gronwall \u4e0d\u7b49\u5f0f | Gronwall Inequation</p> <p>Assume \\(\\alpha(x), u(x) \\in C[a, b]\\) are non-negative functions and \\(C, K\\) are non-negative constants. If </p> \\[ u(x) \\leq C + \\int_{a}^{b} \\left[ \\alpha(s) u(s)+K\\right] ds \\] <p>then </p> \\[ \\begin{align*} u(x) &amp;\\leq \\left[ C + \\int_{a}^{x}Ke^{-\\int_{a}^{s}\\alpha(t)dt}ds\\right]e^{\\int_{a}^{x}\\alpha(s)ds}\\\\ &amp;\\leq \\left[ C + K(x-a)\\right]e^{\\int_{a}^{x}\\alpha(s)ds} \\end{align*} \\] <p>Prove it.</p> Hints <p>Convert it into Ordinary Differential Inequation. Then solve it like what you do in solving ODE.</p> <p>\u4e00\u81f4\u6709\u754c\u7684\u5b9a\u4e49 | Definition of Uniform Bound</p> <p>Assume \\(\\Lambda\\) is an infinite set, set \\(I\\)(or interval \\([x, b]\\)) \\(\\subset \\mathbb{R}\\). A family of function \\(\\{f_\\lambda\\}_{\\lambda\\in\\Lambda}\\) defined on \\(I\\) is Uniformly Bounded, if \\(\\exists M&gt;0\\), s.t.</p> \\[ |f_\\lambda(x)|\\leq M, \\quad \\forall \\lambda\\in \\Lambda, \\forall x \\in I \\] <p>The above definition means that functions in \\(\\{f_\\lambda\\}_{\\lambda\\in\\Lambda}\\) are all bounded by some constant \\(M\\).</p> <p>\u7b49\u5ea6\u8fde\u7eed\u7684\u5b9a\u4e49 | Definition of Equicontinuous</p> <p>Assume \\(\\Lambda\\) is an infinite set, set \\(I\\)(or interval \\([x, b]\\)) \\(\\subset \\mathbb{R}\\). A family of function \\(\\{f_\\lambda\\}_{\\lambda\\in\\Lambda}\\) defined on \\(I\\) is Equicontinuous, if \\(\\forall \\varepsilon&gt;0, \\exists \\delta&gt;0\\), s.t. \\(\\forall x_1, x_2\\in I\\) and \\(|x_1-x_2|&lt;\\delta\\)</p> \\[ |f_\\lambda(x_1)-f_\\lambda(x_2)|&lt; \\varepsilon \\] <p>The above definition means that functions in \\(\\{f_\\lambda\\}_{\\lambda\\in\\Lambda}\\) are continuous equally. Usually \\(\\delta&gt;0\\) depends on \\(f_\\lambda\\), but here we mean the degree of continuouity of these functions is similar.</p> <p>Example. Function sequence \\(\\{f_n(x)\\}\\) satisfying</p> \\[ f_n(x) = (-1)^{n} + x^n \\] <p>is uniformly bounded and equicontinuous on region \\(\\{x: |x|\\leq 1/2\\}\\), is uniformly bounded but not equicontinuous on region \\(\\{x: |x|\\leq 1\\}\\), and is either not uniformly bounded and equicontinuous on region \\(\\{x: |x|\\leq 2\\}\\).</p> <p>The condition of the following theorem can be narrowed down to denumerable sets \\(\\Lambda\\) and interval \\(R \\subset \\mathbb{R}\\).</p> <p></p> <p>\u5f15\u74061: \u4e00\u81f4\u6709\u754c\u7684\u51fd\u6570\u5217\u6709\u6536\u655b\u51fd\u6570\u5b50\u5217(\u70b9\u6001) | Lemma 1: Uniformly Bounded Function Sequence has convergent Function Subsequence</p> <p>Assume set \\(\\Lambda\\) is denumerable. If a family of function \\(\\{f_\\lambda\\}_{\\lambda\\in\\Lambda}\\) defined on \\(I \\subset \\mathbb{R}\\) is uniformly bounded, then \\(\\forall A = \\{x_m\\}_{m=1}^{\\infty} \\subset I\\), \\(\\exists \\{f_{\\lambda_k}\\}_{k=1}^\\infty\\), a function subsequence of \\(\\{f_\\lambda\\}_{\\lambda\\in\\Lambda}\\), such that </p> \\[ \\{f_{\\lambda_k}(x)\\}, \\forall x \\in A \\] <p>is convergent.</p> <p>Prove it.</p> HintsProof <p>Using diagonal methods. </p> <p>Note that \\(\\{f_\\lambda(x_1)\\}_{\\lambda \\in \\Lambda}\\) is obviously bounded. So by Weierstrass Balzano Theorem, we get a convergent subsequence </p> \\[ f_{11}(x_1), f_{12}(x_1), \\cdots, f_{1n}(x_1) \\cdots \\] <p>which converges to \\(y_1\\).</p> <p>Now consider substitute \\(x_1\\) by \\(x_2\\), which becomes</p> \\[ f_{11}(x_2), f_{12}(x_2), \\cdots, f_{1n}(x_2) \\cdots \\] <p>According to the condition \"Uniformly Bounded\", the above sequence is also bounded, so we can find another convergent subsequence from it</p> \\[ f_{21}(x_2), f_{22}(x_2), \\cdots, f_{2n}(x_2) \\cdots \\] <p>which converges to \\(y_2\\).</p> <p>So we can say function subsequence of \\(\\{f_\\lambda(x_1)\\}_{\\lambda \\in \\Lambda}\\)</p> \\[ f_{21}(x), f_{22}(x), \\cdots, f_{2n}(x) \\cdots \\] <p>is convergent on \\(x\\in \\{x_1,x_2\\}\\). That is, \\(\\lim\\limits_{n\\rightarrow \\infty}f_{2n}(x_1) = y_1\\), \\(\\lim\\limits_{n\\rightarrow \\infty}f_{2n}(x_2) = y_2\\).</p> <p>Continue the above procedure, we can get another function subsequnce</p> \\[ f_{31}(x), f_{32}(x), \\cdots, f_{3n}(x) \\cdots \\] <p>which is convergent on \\(x \\in \\{x_1,x_2,x_3\\}\\). That is, \\(\\lim\\limits_{n\\rightarrow \\infty}f_{3n}(x_1) = y_1\\), \\(\\lim\\limits_{n\\rightarrow \\infty}f_{3n}(x_2) = y_2\\), \\(\\lim\\limits_{n\\rightarrow \\infty}f_{3n}(x_3) = y_3\\).</p> <p>Continue, it is not hard to imagine we get a function subsequence \\(\\{f_{nn}(x)\\}\\) which converges to \\(y(x)\\) on \\(x \\in \\{x_1,x_2,\\cdots, x_n\\}\\), with \\(y(x)\\) defined as </p> \\[ y(x_m) = y_m,\\quad  m=1,2,\\cdots \\] <p>So to express the subsequence more specificly, we can define \\(\\tilde{f}_n(x) = f_{nn}(x)\\), and get a subsequence \\(\\{\\tilde{f}_n(x)\\}_{n=1}^{\\infty}\\) such that </p> \\[ \\lim_{n\\rightarrow \\infty}\\tilde{f}_n(x) = y(x), \\forall x\\in I. \\] <p>And we are done.</p> <p>The above theorem offers that we can find a function sequence convergent on denumerable set \\(A\\), which is pointwise convergence.</p> <p>Now if we let the above function subsequence \\(\\{f_{\\lambda_k}(x)\\}\\) to be Equicontinuous, then it can be uniformly convergent on the whole interval \\(I\\). This is exactly the following theorem(in which \\(I\\) is a general region, not just an interval). Note that the above denumerable set can be said as \"dense set\".</p> <p></p> <p>\u5f15\u74062: \u70b9\u6001\u6536\u655b\u3001\u7b49\u5ea6\u8fde\u7eed\u7684\u51fd\u6570\u5217\u5728\u7d27\u96c6\u4e0a\u4e00\u81f4\u6536\u655b | Lemma 2: Equicontinuous Function Sequence is Uniformly Convergent given Pointwise Convergence on Denumerable Sets</p> <p>Assume \\(\\{f_n\\}_{n=1}^\\infty\\) defined on compact set \\(I \\subset \\mathbb{R}\\) is Equicontinuous, and there exists a dense subset \\(R \\subset I\\), such that \\(\\{f_n\\}_{n=1}^\\infty\\) is convergent on \\(R\\), then \\(\\{f_n\\}_{n=1}^\\infty\\) is uniformly convergent on \\(I\\). And denote</p> \\[ f_n(x) \\rightrightarrows f(x), \\quad n\\rightarrow \\infty \\] <p>where the convergent function \\(f(x)\\) is continuous on \\(I\\). </p> <p>Prove it.</p> HintsProofIf \\(I=[a,b]\\) <p>Using Equicontinuity and compact characteristic of set \\(I\\).</p> <p>We hope to prove that \\(\\forall \\varepsilon, \\exists N&gt;0\\), s.t. \\(\\forall n&gt;m&gt;N\\), \\(\\forall x \\in I\\), we have </p> \\[ |f_n(x)-f_m(x)| &lt; \\varepsilon \\] <ul> <li>use the Equicontinuity. </li> </ul> <p>That is, \\(\\forall \\varepsilon\\), \\(\\exists \\delta&gt;0\\), \\(\\forall x_1,x_2\\in I\\) and \\(|x_1-x_2|&lt;\\delta\\), \\(\\forall n\\), we have</p> \\[ |f(x_1)-f(x_2)| &lt; \\frac{\\varepsilon}{9} \\] <ul> <li>use the characteristic of compact set.</li> </ul> <p>Note that there exists open coverings of finite number for compact set \\(I\\). That is, we have \\(x_j\\in I\\), \\(j=1,2,\\cdots, k_0\\), and \\(\\{O_{\\delta'}(x_j)\\}_{j=1}^{k_0}\\) s.t. </p> \\[ I \\subset \\bigcup_{j=1}^{k_0}O_{\\delta'}(x_j) \\] <p>To let it help us prove, we can let \\(\\delta'&lt;\\delta\\) and we can still have a valid corresponding \\(k_0\\) which is finite.</p> <p>Because \\(R\\) is a dense set on \\(I\\), so \\(\\forall O(x_j)\\), \\(\\exists y_j\\in R\\) such that \\(y_j \\in O(x_j)\\).</p> <ul> <li>use the hypothesis of pointwise convergence.</li> </ul> <p>Because \\(\\{f_n\\}_{n=1}^{\\infty}\\) is convergent on dense set \\(R\\), we can have \\(\\forall \\varepsilon&gt;0\\)(choose the same one as the above one), \\(\\exists N&gt;0\\), \\(\\forall n&gt;m&gt;N\\), \\(\\forall y_j \\in R\\), we have</p> \\[ |f_n(y_j)-f_m(y_j)|&lt;\\frac{\\varepsilon}{9} \\] <p>That is, as long as \\(n&gt;m&gt;N\\), we have</p> \\[ \\begin{align} |f_n(x_j)-f_m(x_j)| &amp;\\leq |f_n(x_j)-f_n(y_j)| + |f_n(y_j)-f_m(y_j)| + |f_m(y_j)-f_m(x_j)| \\nonumber\\\\ &amp;&lt; |f_n(x_j)-f_n(y_j)| + \\frac{\\varepsilon}{9} + |f_m(y_j)-f_m(x_j)|\\quad \\text{by pointwise-convergence}\\nonumber\\\\ &amp;&lt;\\frac{\\varepsilon}{3} \\quad \\text{the 1st and 3rd hold for equicontinuity} \\label{bound-open-covering} \\end{align} \\] <p>Note that \\(\\forall x \\in I\\), \\(\\exists j\\in {1,2,\\cdots, k_0}\\), such that \\(x\\in O_{\\delta'}(x_j)\\), so</p> \\[ \\begin{align*} |f_n(x)-f_m(x)| &amp;\\leq |f_n(x)-f_n(x_j)| + |f_n(x_j)-f_m(x_j)| + |f_m(x_j)-f_m(x)|\\\\ &amp;&lt; |f_n(x)-f_n(x_j)| + \\frac{\\varepsilon}{3} + |f_m(x_j)-f_m(x)|\\quad \\text{by the above condition }\\ref{bound-open-covering} \\\\ &amp;&lt;\\varepsilon \\quad \\text{the 1st and 3rd hold for equicontinuity} \\end{align*} \\] <p>In this case, we do not have to find a open covering of \\([a,b]\\) but just partition the closed interval \\([a,b]\\) into \\(k_0\\) distinct closed intervals whose length are less then \\(\\delta\\). Denote these intervals as \\(I_j\\), \\(j=1,2\\cdots, k_0\\).</p> <p>Then \\(\\forall x \\in [a, b]\\), \\(\\exists j\\in {1,2,\\cdots, k_0}\\) such that \\(x\\in I_j\\). Because \\(R\\) is dense on \\([a,b]\\), then we are able to find an element \\(x_j \\in I_j\\), which means </p> \\[ |x-x_j|&lt; \\text{length of }I_q &lt;\\delta \\] <p>So </p> \\[ \\begin{align*} |f_n(x)-f_m(x)| &amp;\\leq |f_n(x)-f_n(x_j)| + |f_n(x_j)-f_m(x_j)| + |f_m(x_j)-f_m(x)|\\\\ &amp;&lt; |f_n(x)-f_n(x_j)| + \\frac{\\varepsilon}{9} + |f_m(x_j)-f_m(x)|\\quad \\text{by pointwise-convergence} \\\\ &amp;&lt;\\frac{\\varepsilon}{3}&lt;\\varepsilon \\quad \\text{the 1st and 3rd hold for equicontinuity} \\end{align*} \\] <p>By utilizing the above two theorems, we can prove the following important theorem.</p> <p>Ascoli-Arzel\u00e0 \u5b9a\u7406 | Ascoli-Arzel\u00e0 Theorem</p> <p>Assume \\(\\Lambda\\) is denumerable. If a family of sequence \\(\\{f_\\lambda\\}_{\\lambda\\in\\Lambda}\\) is uniformly bounded and equicontinuous on closed interval \\([a, b]\\), then there exists a function subsequence \\(\\{f_{\\lambda_k}\\}_{k=1}^{\\infty}\\) of \\(\\{f_\\lambda\\}_{\\lambda\\in\\Lambda}\\) which is uniformly convergent on \\([a, b]\\).</p> <p>Prove it.</p> HintsProof <p>Making use of the above two lemmas.</p> <p>Firstly, we have to find a dense set \\(R\\). Naively, we can choose rational numbers on \\([a,b]\\). That is,</p> \\[ R \\overset{\\Delta}{=}\\mathbb{Q}\\cap [a,b]. \\] <p>Then by lemma 1, using its uniformly bounded,  we can find a function subsequence of \\(\\{f_\\lambda(x)\\}_{\\lambda\\in\\Lambda}\\), denoted as \\(\\{f_{n}\\}_{n=1}^{\\infty}\\), which is convergent on \\(R\\). Finally, by lemma 2, using its equicontinous, the subsequence is uniformly convergent on \\([a,b]\\).</p> <p>And by property of uniformly convergent function sequence, we can see that the convergent funtion is continuous on \\(I\\).</p>"},{"location":"courses/Ordinary_Differential_Equation/General_Theory/Existence_Uniqueness_Theorem/#Picard-Theorem","title":"Picard\u5b58\u5728\u552f\u4e00\u6027\u5b9a\u7406 | Picard Theorem of Existence and Uniqueness","text":"<p>Picard uses the following condition to prove his theorem.</p> <p>Lipschitz\u6761\u4ef6\u7684\u5b9a\u4e49 | Definition of Lipschitz Condition</p> <p>Function \\(f(x, y)\\) defined at region \\(G\\), satisfies Lipschitz condition with respect to \\(y\\), if \\(\\exists L\\) s.t. \\(\\forall (x, y_1), (x, y_2) \\in G\\)</p> \\[ |f(x, y_1)-f(x,y_2)|\\leq L|y_1-y_2| \\] <p>Also, Picard focuses on a typical rectangular region</p> \\[ \\begin{equation} R = \\{(x,y): |x-x_0| \\leq a, |y-y_0|\\leq b\\} \\label{region} \\end{equation} \\] <p>to give his iterative method.</p> <p>Picard\u5b9a\u7406 | Picard Theorem</p> <p>Assume \\(f(x, y) \\in C(G)\\) satisfies Lipschitz condition with respect to \\(y\\), then Cauchy Problem has unique solution on interval \\([x-\\alpha, x+\\alpha]\\), where</p> \\[ \\alpha = \\min \\left\\{a, \\frac{b}{M} \\right\\}, \\quad M = \\max_{(x, y) \\in R}\\left\\{\\left|f(x, y)\\right|\\right\\} \\] <p>Prove it.</p> HintsClassical Proof <p>There typically 4 steps. </p> <p>Firstly, Convert the differential problem into an equivalent integral problem. Then, formulate the so-called Picard sequence and prove it convergent. Furthermore, we have to prove that Picard sequence converges to the solution of integral equation. Finally, we prove the uniqueness by Gronwall Inequation.</p> <ul> <li>Convert the differential problem into an integral problem. That is, solving equation \\(\\ref{eq-cauchy}\\) equals to solving integral equation</li> </ul> \\[ \\begin{align} y(x) &amp;= y(0) + \\int_{x_0}^{x} f(s, y(s))ds \\nonumber\\\\ &amp;=y_0+ \\int_{x_0}^{x} f(s, y(s))ds \\label{eq-integral} \\end{align} \\] <p>readers can prove its equivalence(by proving solution of one side is also solution of the other).</p> <ul> <li>Formulate Picard sequence.</li> </ul> <p>Define:</p> \\[ \\begin{align*} y_0(x) &amp;= y_0\\\\ y_1(x) &amp;= y_0 + \\int_{x_0}^{x} f(s, y_0(s))ds\\\\ y_2(x) &amp;= y_0 + \\int_{x_0}^{x} f(s, y_1(s))ds\\\\ &amp;\\vdots\\\\ y_n(x) &amp;= y_0 + \\int_{x_0}^{x} f(s, y_{n-1}(s))ds\\\\ \\end{align*} \\] <p>We can say the above function sequence \\(\\{y_n(x)\\}_{n=1}^\\infty\\) is well-defined because of the following condition it satisfies:</p> \\[ |y_n(x)-y(x)| \\leq b \\quad \\&amp; \\quad y_n(x)\\in C(R) \\] <p>The above conition enables \\(f(x, y_{n-1}(x))\\) still falls on \\(R\\) and can be integrated(readers can prove that above two condition by induction).</p> <ul> <li>Prove Picard Sequence convergent.</li> </ul> <p>This is the most magic part. The following deduction may be the inspiration:</p> \\[ \\begin{align*} |y_1(x)-y_0(x)| &amp;= \\left| \\int_{x_0}^{x} f(s, y_0)ds \\right| \\leq M\\left| x - x_0\\right| \\\\ |y_2(x)-y_1(x)| &amp;= \\left| \\int_{x_0}^{x}\\left[ f(s, y_1(s)) - f(s, y_0(s))\\right]ds \\right| \\\\  &amp;\\leq \\int_{x_0}^{x} \\left| f(s, y_1(s)) - f(s, y_0(s)) \\right| ds  \\\\  &amp;\\leq L \\int_{x_0}^{x} \\left|y_1(s) - y_0(s)\\right| ds\\quad \\text{(Using Lipschitz Condition)}\\\\ &amp;\\leq LM \\int_{x_0}^{x}\\left|s - x_0\\right|ds =\\frac{LM}{2}|x-x_0|^2 \\quad \\text{(Using the first item)} \\end{align*} \\] <p>So we can allege that</p> \\[ |y_n(x)-y_{n-1}(x)| \\leq \\frac{ML^{n-1}}{n!}|x-x_0|^n \\] <p>and prove it by induction(to be proved by readers).</p> <p>With the above condition, we can use Weierstrass test to prove Picard sequence converges. To be specific, we can see that the Picard Sequence is controlled by a Series of constant terms, which satisfies Cauchy Convergence Theorem. That is, \\(\\forall \\varepsilon&gt;0, \\exists N&gt;0, \\forall n&gt;m&gt;N\\), s.t.</p> \\[ \\begin{align*} |y_n(x)-y_{m}(x)| &amp;\\leq \\sum_{k=m+1}^{n}\\frac{ML^{k-1}}{k!}|x-x_0|^k \\\\ &amp;= \\frac{M}{L}\\sum_{k=m+1}^{n}\\frac{L^{k}}{k!}|x-x_0|^k \\\\ &amp;\\leq \\frac{M}{L}\\sum_{k=m+1}^{n}\\frac{(L\\alpha)^{k}}{k!} \\end{align*} \\] <p>And we notice that Series of constant terms</p> \\[ \\sum_{n=1}^\\infty \\frac{(L\\alpha)^{n}}{n!} \\] <p>converges(to be specific, converges to \\(e^{L\\alpha}\\)), so Picard Sequence also converges.</p> <ul> <li>Prove Picard Sequence converges to solution of equation \\(\\ref{eq-cauchy}\\).</li> </ul> <p>This part is quite easy, to be done by readers.</p> <ul> <li>Prove uniqueness.</li> </ul> <p>Follow the traditional logic: contradiction.</p> <p>Assume there are \\(\\varphi_1(x), \\varphi_2(x)\\) two distinct solutions to equation \\(\\ref{eq-cauchy}\\), then subtract one from the other:</p> \\[ \\begin{align} |\\varphi_1(x) - \\varphi_2(x)| &amp;=\\left| \\int_{x_0}^{x} \\left[ f(s, \\varphi_1(s)) - f(s, \\varphi_2(s))\\right]ds \\right| \\nonumber\\\\ &amp;\\leq L \\int_{x_0}^{x} \\left|\\varphi_1(s) - \\varphi_2(s)\\right|ds  \\quad \\text{(Using Lipschitz Condition)} \\label{eq-same} \\end{align} \\] <p>And by using Gronwall inequation, we can get </p> \\[ |\\varphi_1(x) - \\varphi_2(x)| \\leq 0 \\] <p>So \\(\\varphi_1(x) = \\varphi_2(x)\\), that is, there exists only one solution for equation \\(\\ref{eq-cauchy}\\).</p> <p>In another way, if we don't want to use Gronwall Inequation, we can still say that the two solutions are the same. Assume that \\(\\varphi_1(x), \\varphi_2(x)\\) have a common region \\(J = [x_0-d,x_0+d]\\), where \\(0&lt;d\\leq \\alpha\\). Then \\(|\\varphi_1(x)-\\varphi_2(x)|\\) is continuous and bounded on region \\(J\\) and denote</p> \\[ K = \\max_{x\\in J}\\{|\\varphi_1(x)-\\varphi_2(x)|\\} \\] <p>So the right side of inequation \\(\\ref{eq-same}\\) can be bounded:</p> \\[ \\begin{align} |\\varphi_1(x) - \\varphi_2(x)| &amp;\\leq L \\int_{x_0}^{x} \\left|\\varphi_1(s) - \\varphi_2(s)\\right|ds  \\nonumber\\\\ &amp;\\leq LK |x-x_0| \\label{eq-same1} \\end{align} \\] <p>Substitute inequation \\(\\ref{eq-same1}\\) again into the right side of inequation \\(\\ref{eq-same}\\) and get</p> \\[ \\begin{align*} |\\varphi_1(x) - \\varphi_2(x)| \\leq KL^2 \\frac{|x-x_0|^2}{2} \\end{align*} \\] <p>repeat the above method iteratively and we can prove the following by induction:</p> \\[ |\\varphi_1(x) - \\varphi_2(x)| \\leq K \\frac{(L|x-x_0|)^n}{n!} \\] <p>Let \\(n \\rightarrow \\infty\\), we get \\(|\\varphi_1(x) - \\varphi_2(x)| \\rightarrow 0\\).</p>"},{"location":"courses/Ordinary_Differential_Equation/General_Theory/Existence_Uniqueness_Theorem/#osgood-osgood-condition","title":"Osgood \u6761\u4ef6 | Osgood Condition","text":"<p>We consider a condition which is slightly weaker than Lipschitz condition but still can guarrantee the convergence of Picard Sequence and its uniqueness.</p> <p>Osgood \u6761\u4ef6 | Osgood Condition</p> <p>Assume \\(D\\) is a region on \\(\\mathbb{R}^2\\), and function \\(f(x,y)\\in C(D)\\). If \\(\\forall (x,y_1), (x,y_2)\\in D\\), s.t.</p> \\[ |f(x,y_1)-f(x,y_2)|\\leq F(|y_1-y_2|) \\] <p>where \\(F(r)&gt;0 \\in C(\\mathbb{R})\\) satisfies </p> \\[ \\int_{0}^{\\varepsilon}\\frac{1}{F(r)}dr = +\\infty, \\quad \\forall \\varepsilon &gt;0 \\] <p>then we say that \\(f(x,y)\\) satisfies Osgood condition with respect to \\(y\\).</p> <p>Obviously, if \\(f(x,y)\\) satisfies Lipschitz condition, it satisfies Osgood condition. In fact, we can choose \\(F(r) = Lr\\).</p> <p>Osgood \u5b9a\u7406 | Osgood Theorem</p> <p>If  \\(f(x,y) \\in C(D)\\) satisfies Osgood condition with respect to \\(y\\), then \\(\\forall (x_0,y_0)\\in D\\), th solution to Cauchy problem \\(\\ref{eq-cauchy}\\) exists uniquely.</p> <p>Prove it.</p> HintsProof <p>The existence is guarranteed by Peano Theorem in the following part. You only need to prove the uniqueness, which is proved by contradiction, which is similar to prove the uniqueness of \\(f(x, y)\\) that decrease monotonically with respect to \\(y\\).</p> <p>Assume we have two distinct solution \\(y_1(x)\\), \\(y_2(x)\\), then there exists \\(x_2\\) such that \\(y_1(x_2)\\neq y_2(x_2)\\). Let \\(y_1(x) &gt; y_2(x)\\). </p> <p>Then by feature of guarantee code(\u4fdd\u53f7\u6027), there must exist a region such that \\(y_1(x) &gt; y_2(x)\\), so let \\(x_1 = \\max\\{x\\in [x_0,x_2] : y_1(x) = y_2(x)\\}\\).</p> <p><p> </p></p> <p>So we have </p> \\[ y_1(x) &gt; y_2(x), \\quad \\forall x\\in (x_1,x_2] \\] <p>define \\(r(x) = y_1(x) - y_2(x) \\in (0, m]\\), where m is determined by \\(m = \\max\\limits_{x\\in (x_1,x_2]}\\{y_1(x) - y_2(x)\\}\\)</p> <p>So by condition of the proposition, we have</p> \\[ |r'(x)| = |y_1'(x) - y_2'(x)| = |f(x,y_1(x))- f(x,y_2(x))| \\leq F(|y_1(x) - y_2(x)|) \\] <p>because \\(y_1(x) &gt; y_2(x)\\), so we take \"\\(\\vert \\cdot \\vert\\)\" out and get</p> \\[ r'(x) = y_1'(x) - y_2'(x) = f(x,y_1(x))- f(x,y_2(x)) \\leq F(y_1(x) - y_2(x)) \\] <p>divide both sides \\(F(y_1(x) - y_2(x))\\) and integrate on \\((x_1, x_2]\\), which is an improper integral on the left</p> \\[ \\int_{x_1}^{x_2}\\frac{r'(x)}{F(r(x))}dx\\leq \\int_{x_1}^{x_2}dx = x_2-x_1  \\] <p>so the left item can be \\(+\\infty\\) by condition while the right item is less than \\(+\\infty\\), i.e.</p> \\[ +\\infty=\\int_{0}^{r(x_2)}\\frac{1}{F(r)}dr=\\int_{r(x_1)}^{r(x_2)}\\frac{1}{F(r)}dr\\leq x_2-x_1 &lt;+\\infty \\] <p>which contradicts!</p>"},{"location":"courses/Ordinary_Differential_Equation/General_Theory/Existence_Uniqueness_Theorem/#Peano-Theorem","title":"Peano\u5b9a\u7406 | Peano Theorem","text":"<p>When \\(f(x,y)\\) does not satisfy Lipschitz Condition with respect to \\(y\\), we cannot guarantee the existence and uniqueness of the solution to Cauchy Problem \\(\\ref{eq-cauchy}\\). However, when \\(f(x,y)\\) is continuous, Peano proved that Cauchy Problem \\(\\ref{eq-cauchy}\\) has solution.</p> <p>Peano\u5b9a\u7406 | Peano Theorem</p> <p>Assume \\(f(x, y)\\) is continous in \\(R(\\ref{region})\\), then Cauchy Problem \\(\\ref{eq-cauchy}\\) has at least one solution in interval \\([x_0-\\alpha, x_0+\\alpha]\\), where</p> \\[ \\alpha = \\min \\left\\{a, \\frac{b}{M} \\right\\}, \\quad M = \\max_{(x, y) \\in R}\\left\\{\\left|f(x, y)\\right|\\right\\} \\] <p>Prove it.</p> HintsProof <p>There are several methosd to prove Peano Theorem, like Euler's Arc method, Tonelli(\u6258\u5185\u5229) Sequence method, fixed-point method in functional analysis, while Euler's Arc method is thought to be the dawm to calculating ODE numerically, so we will prove it this way. </p> <p>The idea is to construct approximation solution which converges to the real solution. In fact, Picard Sequence is also an approximation solution.</p> <ul> <li>Cauchy Problem \\(\\ref{eq-cauchy}\\) can be converted equivalently to integral equation \\(\\ref{eq-integral}\\).</li> </ul> <p>We only discuss the existence of solution of right side(\\([x_0, x_0+\\alpha]\\)). We can make similar treatment to the left side.</p> <ul> <li>Formulate Euler Polygons/Polygonal Arc(\u6b27\u62c9\u6298\u7ebf).</li> </ul> <p>The idea is simple: go ahead step by step as small as possible, employing the slope of this sampling points.</p> <p>Firstly, we partition the region \\([x_0,x_0+\\alpha]\\) into \\(n\\) parts(usually of equal length), that is</p> \\[ x_0 &lt; x_1 &lt; x_2 &lt; \\cdots &lt; x_n = x_0 + \\alpha \\] <p>and then define curves</p> \\[ \\begin{align*} l_1: y &amp;= y_0 + f(x_0, y_0)(x-x_0), \\quad x_0\\leq x\\leq x_1 \\\\ l_2: y &amp;= y_1 + f(x_1, y_1)(x- x_1), \\quad x_1 \\leq x\\leq x_2\\\\ &amp;\\vdots \\\\ l_n: y &amp;= y_{n-1} + f(x_{n-1}, y_{n-1})(x - x_{n-1}), \\quad x_{n-1} \\leq x \\leq x_n \\end{align*} \\] <p><p> </p> we can formulate a polygonal arc on \\([x_0, x_0+\\alpha]\\):</p> \\[ E_n = \\bigcup_{s=1}^{n}l_s \\] <p>and its function expresstion is </p> \\[ \\begin{equation} y_n(x) = y_0 + \\sum_{i=0}^{j-1}f(x_i, y_i)(x_{i+1} - x_{i}) + f(x_j, y_j)(x - x_j) \\label{eq-euler} \\end{equation} \\] <p>where \\(\\forall x \\in (x_0, x_0 +\\alpha], \\exists j\\) s.t. </p> \\[ x_j &lt; x\\leq x_{j+1} \\] <p>Here closed interval \\([x_0, x_0+\\alpha]\\) is of great importance, for it can guarantee the condition for Ascoli-Arzel\u00e0 Theorem to be true.</p> <ul> <li>\u4e00\u81f4\u6709\u754c | Uniform Bound</li> </ul> <p>By definition, it is easy to prove that </p> \\[ |y_n(x)-y_0| \\leq b, \\quad \\forall n, \\forall x \\in [x_0,x_0+\\alpha] \\] <ul> <li>\u7b49\u5ea6\u8fde\u7eed | Equicontinuous</li> </ul> <p>By definition, it is easy to see that \\(\\forall \\varepsilon&gt;0\\), \\(\\exists \\delta = \\varepsilon/M\\), s.t. \\(\\forall x_1, x_2 \\in [x_0,x_0+\\alpha]\\) and \\(|x_1-x_2|&lt;\\delta\\), we have</p> \\[ |y_n(x_1)-y_n(x_2)|\\leq \\max_{(x,y)\\in R}|f(x,y)| |x_1-x_2| &lt; M \\cdot \\frac{\\varepsilon}{M} = \\varepsilon \\] <ul> <li>Using Ascoli-Arzel\u00e0 Theorem</li> </ul> <p>So the function sequence \\(\\{y_n(x)\\}\\) has a uniformly convergent subsequence \\(\\{y_{n_j}(x)\\}\\).</p> <ul> <li>Prove Euler's Arc converges to the solution of ODE.</li> </ul> <p>This means that we have to consider the error of \\(y_{n_j}(x)\\) and \\(y(x)\\). Firstly, we rewrite the formula of Euler's Arc \\(\\ref{eq-euler}\\).</p> <p>Note that</p> \\[ \\begin{align*} f(x_i,y_i)(x_{i+1}-x_i)&amp;=\\int_{x_i}^{x_{i+1}}f(x_i,y_i)dx\\\\ &amp;=\\int_{x_i}^{x_{i+1}}f(x,y_n(x)) + f(x_i,y_i) - f(x,y_n(x))dx\\\\ \\end{align*} \\] <p>Define</p> \\[ d_n(i) \\overset{\\Delta}{=} \\int_{x_i}^{x_{i+1}}[f(x_i,y_i) - f(x,y_n(x))]dx \\] <p>then </p> \\[ \\begin{equation} f(x_i,y_i)(x_{i+1}-x_i) = \\int_{x_i}^{x_{i+1}}f(x,y_n(x))dx + d_n(i) \\label{eq-front} \\end{equation} \\] <p>Similarly, we have \\(x_j&lt;x\\leq x_{j+1}\\)</p> \\[ \\begin{align*} f(x_j,y_j)(x-x_j)&amp;=\\int_{x_j}^{x_{j+1}}f(x_j,y_j)dx\\\\ &amp;=\\int_{x_j}^{x_{j+1}}f(x,y_n(x)) + f(x_j,y_j) - f(x,y_n(x))dx\\\\ \\end{align*} \\] <p>Define</p> \\[ d^*_n(x) \\overset{\\Delta}{=} \\int_{x_j}^{x}[f(x_j,y_j) - f(x,y_n(x))]dx  \\] <p>then </p> \\[ \\begin{equation} f(x_j,y_j)(x-x_j) = \\int_{x_j}^{x_{j+1}}f(x,y_n(x))dx +d_n^*(x) \\label{eq-back} \\end{equation} \\] <p>So with the above two transformation \\(\\ref{eq-front}\\), \\(\\ref{eq-back}\\), the formula of Euler's Arc \\(\\ref{eq-euler}\\)(summation of linear expresstions) becomes an integral-like expression:</p> \\[ \\begin{align*} y_n(x) &amp;= y_0 + \\sum_{i=0}^{j-1}f(x_k, y_k)(x_{i+1} - x_{i}) + f(x_j, y_j)(x - x_j)\\\\ &amp; = y_0 + \\int_{x_0}^{x}f(x,y_n(x))dx + \\delta_n(x) \\end{align*} \\] <p>where </p> \\[ \\begin{equation} \\delta_n(x) = \\sum_{i=0}^{j-1}d_n(i) + d^*_n(x) \\label{eq-residue} \\end{equation} \\] <p>we know that for each \\(y_n\\), we can have variables \\(x, y\\) bounded by</p> \\[ |x-x_j|\\leq \\frac{\\alpha}{n}, \\quad |y-y_j|\\leq M \\frac{\\alpha}{n} \\] <p>So \\(\\forall \\varepsilon &gt;0\\), \\(\\exists N(\\varepsilon) = \\frac{\\alpha}{\\varepsilon}\\), s.t. \\(n&gt;N\\), </p> \\[ |x-x_j|\\leq \\frac{\\alpha}{N} = \\varepsilon, \\quad |y-y_j|\\leq M \\frac{\\alpha}{N} = M\\varepsilon \\] <p>which means the region of each integral interval can be as small as possible so long as \\(n\\rightarrow \\infty\\).</p> <p>So in this case, for each item in residue \\(\\ref{eq-residue}\\), we can bound it into small value corresponding to \\(\\varepsilon\\) by making use of the continuity of \\(f(x,y)\\), which means \\(\\forall\\varepsilon&gt;0\\), \\(\\exists \\delta&gt;0\\), s.t. \\(\\forall (x,y) \\in O[(x_j,y_j),\\delta]\\), </p> \\[ |f(x,y)-f(x_j,y_j)| &lt;\\varepsilon \\] <p>The above one is easy to accomplish because we can let \\(n\\) be large enough, so all \\(x \\in [x_j,x_{j+1}]\\) with its \\(y_n(x)\\) can fall in \\(O[(x_j,y_j),\\delta]\\).</p> <p>So the whole residue \\(\\ref{eq-residue}\\) can be bounded:</p> \\[ \\delta_n(x) \\leq \\varepsilon \\frac{\\alpha}{n} + j \\varepsilon \\frac{\\alpha}{n} &lt; \\alpha \\varepsilon \\] <p>So we can say \\(y_{n_j}(x)\\) defined as</p> \\[ y_{n_j}(x) = y_0 + \\int_{x_0}^{x}f(s, y_{n_j}(s))ds + \\delta_{n_j}(x), \\quad x \\in [x_0,x_0+\\alpha] \\] <p>where \\(\\delta_{n_j}(x)\\) satisfies</p> \\[ \\delta_{n_j}(x)\\rightrightarrows 0 \\] <p>Which also means, if we denote </p> \\[ \\varphi(x) \\overset{\\Delta}{=} \\lim_{n_j\\rightarrow \\infty}y_{n_j}(x) \\] <p>and let \\(n_j \\rightarrow \\infty\\), we get</p> \\[ \\varphi(x) = y_0 + \\int_{x_0}^{x}f(s, \\varphi(s))ds \\] <p>which means function subsequence \\(\\{y_{n_j}(x)\\}\\) converges to the solution of integral form \\(\\ref{eq-integral}\\) of ODE \\(\\ref{eq-cauchy}\\).</p>"},{"location":"courses/Ordinary_Differential_Equation/General_Theory/Existence_Uniqueness_Theorem/#comments-on-previous","title":"Comments on Previous","text":"<p>Here we list the difference between Picard Theorem and Peano Theorem.</p> Theorem Picard Theorem Peano Theorem Condition \\(f\\in C(R)\\), Lipschitz Condition with respect to \\(y\\) only \\(f\\in C(R)\\) Sequence the whole Picard Sequence \\(\\{y_n(x)\\}\\) converges subsequence of Euler's Arc \\(\\{\\varphi_n(x)\\}\\) converges Solution exists uniquely exists only, might have a lot of solutions <ul> <li>Note 1: Uniqueness can help bound Euler's Arc. See the following theorem.</li> </ul> <p>\u6709\u552f\u4e00\u89e3\u7684\u67ef\u897f\u95ee\u9898\uff0c\u5176\u6b27\u62c9\u6298\u7ebf\u5168\u5e8f\u5217\u6536\u655b | Euler's Arc converges given uniqueness</p> <p>If Cauchy problem \\(\\ref{eq-cauchy}\\) has unique solution, then the whole sequence of Euler's Arc \\(\\{\\varphi_n(x)\\}\\) converges.</p> <p>Prove it.</p> HintsProof <p>Use contradiction.</p> <p>Assume that the conclusion does not hold, then by definition of negative proposition, we have</p> \\[ \\exists \\varepsilon_0&gt;0, \\exists \\overline{x} \\in I, \\forall N_0&gt;0, \\exists n_0,m_0&gt;N_0, |\\varphi_{n_0}(\\overline{x})-\\varphi_{m_0}(\\overline{x})|\\geq\\varepsilon_0 \\] <p>which is the negative proposition of the Cauchy Convergence form</p> \\[ \\forall \\varepsilon&gt;o, \\forall x\\in I, \\exists N&gt;0, \\forall n&gt;m&gt;N, |\\varphi_{n}(x)-\\varphi_{m}(x)|&lt;\\varepsilon. \\] <p>In order to help us prove, we can take two sequence from the above negative proposition. That is, let \\(N_0=j\\), \\(j=1,2,\\cdots\\), we can take the corresponding \\(n_j\\), \\(m_j\\) out to compose a subsequence of Euler's arc \\(\\{\\varphi_n(x)\\}\\), i.e.</p> \\[ \\begin{equation} \\exists \\varepsilon_0&gt;0, \\exists \\overline{x} \\in I, \\forall j=1,2,\\cdots, \\exists n_j,m_j&gt;j, |\\varphi_{n_j}(\\overline{x})-\\varphi_{m_j}(\\overline{x})|\\geq\\varepsilon_0 \\label{con-non-convergent} \\end{equation} \\] <p>See that \\(\\{\\varphi_{n_j}(x)\\}\\) is still uniformly bounded and equicontinous on \\(I\\), so there exists its subsequence \\(\\{\\tilde{\\varphi}_{n_j}(x)\\}\\) such that</p> \\[ \\{\\tilde{\\varphi}_{n_j}(x)\\} \\rightrightarrows \\varphi_1(x), \\quad \\forall x\\in I \\] <p>and also \\(\\varphi_1(x)\\) is still the solution of Cauchy problem \\(\\ref{eq-cauchy}\\). Similarly, we can have  </p> \\[ \\{\\tilde{\\varphi}_{m_j}(x)\\} \\rightrightarrows \\varphi_2(x), \\quad \\forall x\\in I \\] <p>where \\(\\{\\tilde{\\varphi}_{m_j}(x)\\}\\) is a subsequence of \\(\\{\\varphi_{m_j}(x)\\}\\). According to the condition of the theorem, we have </p> \\[ \\varphi_1(x) = \\varphi_2(x), \\quad \\forall x \\in I \\] <p>while in \\(\\ref{con-non-convergent}\\) we also take the subsequence of \\(\\{\\varphi_{n_j}(x)\\}\\) and \\(\\{\\varphi_{m_j}(x)\\}\\)</p> \\[ |\\tilde{\\varphi}_{n_j}(\\overline{x})-\\tilde{\\varphi}_{m_j}(\\overline{x})|\\geq\\varepsilon_0 \\] <p>which means \\(\\varphi_1(x)\\) and \\(\\varphi_2(x)\\) must have distance on some \\(\\overline{x}\\), which contradicts with \\(\\varphi_1(x) = \\varphi_2(x)\\) on all \\(x\\in I\\)!</p>"},{"location":"courses/Ordinary_Differential_Equation/General_Theory/Extension_of_Solution/","title":"Extension of Solution","text":""},{"location":"courses/Ordinary_Differential_Equation/General_Theory/Extension_of_Solution/#extension-of-solution","title":"\u89e3\u7684\u5ef6\u62d3 | Extension of Solution","text":"<p>For Cauchy problem</p> \\[ \\begin{equation} \\frac{dy}{dx} = f(x, y), \\quad y(x_0) = y_0. \\label{eq-cauchy} \\end{equation} \\] <p>it is clear that we are satisfied with the result of intervals in the previous chapter, especially when we have to shrink the interval of solution using Contraction Mapping Method.</p> <p>So a naive idea is, can we use the Peano/Picard theorem repeatedly to extend the interval of parameter \\(t\\)? If so, in what cases can we extend it and to where can we extend?</p> <p>Here comes the following theorem.</p> <p>\u89e3\u7684\u5ef6\u62d3\u5b9a\u7406 | Theorem of extension of solution</p> <p>Assume \\(f(x,y)\\in C(G)\\), where \\(G\\) is an open set(region). Then the solution of Cauchy problem \\(\\ref{eq-cauchy}\\) can extend to its boundary. </p> HintsProof <p>The proof is equivalent to prove that, for each closed set \\(G_1\\subset G\\) and \\((x_0,y_0)\\in G_1\\), the solution \\(\\Gamma\\) can extend to \\(G\\) \\ \\(G_1\\).</p> <p>We only consider positive extension, i.e. \\(x\\geq x_0\\).</p> <p>Consider closed region \\(G_1\\subset G\\) that has point \\((x_0,y_0)\\) with the solution denoted by \\(\\varphi(x)\\) that satisfies initial condition \\(y(x_0)=y_0\\). </p> <ul> <li>Use the property of Open Set.</li> </ul> <p>Because \\(G\\) is an open set, the distance between \\(G\\) and \\(G_1\\) can not be zero, that is, \\(\\exists \\delta_0&gt;0\\)., s.t.</p> \\[ \\{(x,y): |x-a|&lt;\\delta_0, |y-b|&lt;\\delta_0, (a,b)\\in \\partial G_1\\} \\subset G \\] <p>Here \\(\\delta_0\\) is a constaint condition that guarantees the extended interval of solution will not exceed the boundary of \\(G_1\\).</p> <ul> <li>Using \\(\\delta_0\\) to generate extended intervals with length \\(\\delta_0'\\)</li> </ul> <p>For any closed region \\(G_1\\subset G\\), Denote </p> \\[ M=\\max_{(x,y)\\in G_1}|f(x_y)|+1 &lt; \\infty, \\quad R_{\\delta_0}(x',y')=\\{(x,y): |x-x'|\\leq \\delta_0, |y-y'|\\leq \\delta_0\\} \\] <p>where \\((x',y')\\in G_1\\).</p> <p>For Cauchy problem with initial condition \\(y(x_0)=y_0\\) in region \\(R_{\\delta_0} (x_0,y_0)\\), according to Peano Theorem, we can have a solution \\(\\varphi_0(x)\\) on interval \\([x_0-\\delta',x_0+\\delta']\\), where \\(\\delta'=\\min{\\delta_0, \\delta_0/M}\\). If there exists point on the curve \\((x,\\varphi_0(x)) \\in G\\)\\\\(G_1\\), then we prove it.</p> <p>If not, then the furthest point on the right \\((x_0+\\delta', \\varphi_0(x_0+\\delta'))\\) must be in \\(G_1\\). So consider cauchy problem with initial condition \\(y(x_0+\\delta') = \\varphi_0(x_0+\\delta')\\) in region \\(R_{\\delta_0} (x_0+\\delta',\\varphi_0(x_0+\\delta'))\\), according to Peano Theorem, we can get another solution \\(\\varphi_1(x)\\) on interval \\(x_0,x_0+2\\delta'\\).</p> <p>Repeat the above procedure, we can say the interval can be extended to \\([x_0, x_0+n\\delta']\\). If we denote the distance between \\((x_0,y_0)\\) and \\(\\partial G_1\\) as \\(D_1\\), distance between \\((x_0,y_0)\\) and \\(\\partial G\\) as \\(D\\), then choose \\(n\\) such that \\(n\\delta'&gt;D_1\\) and in the same time \\(n\\delta'&lt;D\\).</p> <p>And we are done.    </p>"},{"location":"courses/Ordinary_Differential_Equation/General_Theory/Power_Series/","title":"Method of Power Series","text":"<p>Lots of ODE cannot be sovled using elementary integration method, so we have to give up solution of finite form and try to find solution of infinite form, like series.</p> <p>To have a more global understanding, we focus on</p> \\[ \\begin{equation} \\frac{d \\pmb{y}}{dx}=\\pmb{f}(x,\\pmb{y}), \\quad \\pmb{y}(x_0)=\\pmb{y}_0 \\label{eq-cauchy} \\end{equation} \\] <p>where \\(\\pmb{f}(x,\\pmb{y})\\) is analytic on region \\(R\\subset \\mathbb{R}\\times \\mathbb{R}^n\\), i.e.</p> \\[ \\pmb{f}(x,\\pmb{y})=\\sum_{i,j_1,j_2,\\cdots,j_n=0}^\\infty \\pmb{a}_{i{j_1}{j_2}\\cdots{j_n}}(x-x_0)^i(y_1-y_{10})^{j_1}\\cdots(y_n-y_{n0})^{j_n} \\] <p>where \\(\\pmb{a}_{i{j_1}{j_2}\\cdots{j_n}}\\in \\mathbb{R}^n\\).</p> <p>To simplify the notation, we denote \\(\\pmb{j}=(j_1,j_2,\\cdots,j_n)\\), and </p> \\[ (\\pmb{y}-\\pmb{y}_0)^{\\pmb{j}}=(y_1-y_{10})^{j_1}\\cdots(y_n-y_{n0})^{j_n} \\] <p>denote </p> \\[ \\sum_{i=0,\\pmb{j}=0}^\\infty=\\sum_{i,j_1,j_2,\\cdots,j_n=0}^\\infty \\] <p>Since \\(\\pmb{f}(x,\\pmb{y})\\) is analytic with respect to \\(\\pmb{y}\\), so by Picard Theorem, there exists only one solution. Now the question is, to prove that the solution is analytic, i.e. \\(\\exists \\delta&gt;0\\), s.t. \\(x\\in O_\\delta(x_0)\\)</p> \\[ \\pmb{y}(x)=\\sum_{k=0}^{\\infty}\\pmb{c}_k(x-x_0)^k \\] <p>where \\(\\pmb{c}_k=(c_{n1},c_{n2},\\cdots,c_{nk}) \\in \\mathbb{R}^n\\).</p>"},{"location":"courses/Ordinary_Differential_Equation/General_Theory/Power_Series/#excellent-series","title":"Excellent Series","text":"<p>To prove that the solution is analytic, we have to use a method of proof, that is, using Excellent Series.</p> <p>Definition of Excellent Series</p> <p>Assume there are two power series</p> \\[ \\begin{equation} \\sum_{i=0.\\pmb{j}=0}^\\infty a_{i\\pmb{j}}(x-x_0)^i(\\pmb{y}-\\pmb{y}_0)^{\\pmb{j}} \\label{series1} \\end{equation} \\] <p>and</p> \\[ \\begin{equation} \\sum_{i=0.\\pmb{j}=0}^\\infty A_{i\\pmb{j}}(x-x_0)^i(\\pmb{y}-\\pmb{y}_0)^{\\pmb{j}}. \\label{series2} \\end{equation} \\] <p>If \\(A_{i\\pmb{j}}&gt;0\\) and they satisfies</p> \\[ |a_{i\\pmb{j}}|&lt;A_{i\\pmb{j}}, \\quad \\forall i,\\pmb{j} \\] <p>Then we call series \\(\\ref{series2}\\) is an excellent series of series \\(\\ref{series1}\\). If series \\(\\ref{series2}\\) converges on closed region</p> \\[ \\{(x,y): |x-x_0|\\leq \\alpha,|\\pmb{y}-\\pmb{y}_0|\\leq\\beta\\} \\] <p>then we call its summing function \\(\\pmb{f}(x,\\pmb{y})\\) is an Excellent function of series \\(\\ref{series1}\\).(note that in this case, series \\(\\ref{series1}\\) also converges)</p> <p></p> <p>\u5f15\u74061: \u89e3\u6790\u51fd\u6570\u6709\u5b9a\u4e49\u57df\u6536\u7f29\u7684\u4f18\u51fd\u6570 | Lemma 1: A analytic function has an excellent function within smaller region</p> <p>If \\(f(x,\\pmb{y})\\) is analytic on region</p> \\[ R: \\{(x,y): |x-x_0|&lt;\\alpha, |\\pmb{y}-\\pmb{y}_0|&lt;\\beta\\} \\] <p>then \\(\\exists M&gt;0\\), s.t.</p> \\[ F(x,\\pmb{y})=\\frac{M}{\\left(1-\\frac{x-x_0}{a}\\right)\\left(1-\\frac{y_1-y_{10}}{b}\\right)\\cdots\\left(1-\\frac{y_n-y_{n0}}{b}\\right)} \\] <p>is an Excellent function of \\(f(x,\\pmb{y})\\) on a smaller region </p> \\[ R_0:\\{(x,y): |x-x_0|&lt;a,|\\pmb{y}-\\pmb{y}_0|&lt;b\\}. \\] HintsProof <p>Use Abel Second Theorem. Note that we can not guarrantee the convergence on boundaries, so we choose open intervals.</p> <p>We can represent \\(f(x,\\pmb{y})\\) in terms of power series</p> \\[ f(x,\\pmb{y})=\\sum_{i=0.\\pmb{j}=0}^\\infty a_{i\\pmb{j}}(x-x_0)^i(\\pmb{y}-\\pmb{y}_0)^{\\pmb{j}} \\] <p>in region \\(R\\). Then by Abel Second Theorem, \\(\\exists a\\in(0,\\alpha), b\\in(0,\\beta)\\) s.t.</p> \\[ \\sum_{i=0.\\pmb{j}=0}^\\infty a_{i\\pmb{j}}a^i b^{j_1+j_2+\\cdots+j_n} \\] <p>is convergent, so each item of the above series can be bounded by a number \\(M&gt;0\\):</p> \\[ |a_{i\\pmb{j}}|a^i b^{j_1+j_2+\\cdots+j_n}\\leq M \\Rightarrow |a_{i\\pmb{j}}|\\leq \\frac{M}{a^i b^{j_1+j_2+\\cdots+j_n}} \\] <p>Now, the following thing is a little tricky. Define </p> \\[ A_{i\\pmb{j}}=\\frac{M}{a^i b^{j_1+j_2+\\cdots+j_n}} \\] <p>Consider a power of series </p> \\[ \\begin{equation} \\sum_{i=0.\\pmb{j}=0}^\\infty A_{i\\pmb{j}}(x-x_0)^i(\\pmb{y}-\\pmb{y}_0)^{\\pmb{j}} \\label{series-exce} \\end{equation} \\] <p>which is convergent because it can add up to:</p> \\[ F(x,\\pmb{y})=\\frac{M}{\\left(1-\\frac{x-x_0}{a}\\right)\\left(1-\\frac{y_1-y_{10}}{b}\\right)\\cdots\\left(1-\\frac{y_n-y_{n0}}{b}\\right)} \\] <p>with its range of definition \\(R_0\\). By definition, it is an excellent function of \\(f(x,\\pmb{y})\\).</p> <p>With the definition of Excellent function, we have to use it to formulate an excellent series of the original series. This is the following theorem.</p> <p></p> <p>\u5f15\u74062: \u7528\u4e0a\u8ff0\u4f18\u51fd\u6570\u5efa\u7acb\u7684\u5fae\u5206\u65b9\u7a0b\u6709\u89e3\u6790\u89e3 | Lemma2: ODE combined with The above Excellent Function has a solution that can be represented by Power Series</p> <p>Cauchy problem </p> \\[ \\frac{d \\pmb{y}}{dx}=\\pmb{F}(x,\\pmb{y}), \\quad \\pmb{y}(x_0)=\\pmb{y}_0 \\label{eq-cauchy-prime} \\] <p>has a analytic solution \\(\\pmb{y}=\\pmb{y}(x)\\) on region \\(O_\\rho(x_0)\\), where \\(F_i(x,\\pmb{y}) = F(x,\\pmb{y})\\) that is same all over \\(i\\) is given from the above lemma and </p> \\[ \\rho=a\\{1-e^{b/[(n+1)aM]}\\}. \\] HintsProof <p>use elementary integration method.</p> <p>We let \\(u=y_i\\), \\(i=1,2,\\cdots,n\\) and we only need to solve the equation </p> \\[ \\frac{d u}{dx}=F(x,u), \\quad u(x_0)=u_0 \\] <p>where </p> \\[ F(x,u) = \\frac{M}{\\left(1-\\frac{x-x_0}{a}\\right)\\left(1-\\frac{u-u_{0}}{b}\\right)^n} \\] <p>(Here readers can see that \\(u-u_0=y_i-y_{i0}\\).)</p> <p>The above ODE is a variable separation equation. So change the form and integrate on \\([x_0,x]\\)</p> \\[ \\frac{-b}{n+1}\\left(1-\\frac{u-u_0}{b}\\right)^{n+1} +\\frac{b}{n+1}= -aM\\ln\\left(1-\\frac{x-x_0}{a}\\right) \\] <p>get \\(u\\) out:</p> \\[ u = u_0 + b- b\\left[\\frac{aM(n+1)}{b} \\ln\\left(1-\\frac{x-x_0}{a}\\right) + 1\\right]^{\\frac{1}{n+1}} \\] <p>That is,</p> \\[ y_i(x) = y_{i0} + b- b\\left[\\frac{aM(n+1)}{b} \\ln\\left(1-\\frac{x-x_0}{a}\\right) + 1\\right]^{\\frac{1}{n+1}}, \\quad \\forall i=1,2\\cdots,n \\] <p>We want to use this form to get a power series. See that \\(\\ln{\\left(1-\\frac{x-x_0}{a}\\right)}\\) can be represented by power series of \\((x-x_0)\\) once \\(|x-x_0|&lt;a\\). And also \\((1+s)^{\\frac{1}{n+1}}\\) can be represented by power series of \\(s\\) when \\(|s|&lt;1\\). So by combine the above two, we know \\(y_i(x)\\) can be represented by \\((x-x_0)\\) once \\(|x-x_0|&lt;a\\). To be more specific, We have to let the radius of converence to satisfy</p> \\[ \\begin{cases} \\displaystyle 1-\\frac{\\rho}{a} \\geq 0 \\\\ \\displaystyle \\frac{aM(n+1)}{b}\\ln\\left(1-\\frac{\\rho}{a}\\right)\\leq 1  \\end{cases} \\Rightarrow \\begin{cases} \\rho\\leq a \\\\ \\rho\\leq a\\{1-e^{b/[(n+1)aM]}\\} \\end{cases} \\] <p>choose</p> \\[ \\rho=a\\{1-e^{b/[(n+1)aM]}\\} \\] <p>So solution of the above ODE</p> \\[ \\pmb{y}(x)=(y_1(x),y_2(x),\\cdots, y_n(x)) \\] <p>can be represented by power series of \\((x-x_0)\\) when \\(|x-x_0|&lt;\\rho\\).</p>"},{"location":"courses/Ordinary_Differential_Equation/General_Theory/Power_Series/#proof","title":"\u8bc1\u660e | Proof","text":"<p>Cauchy \u5b9a\u7406 | Cauchy Theorem</p> <p>Assume \\(\\pmb{f}(x,\\pmb{y})=[f_1(x,\\pmb{y}), f_2(x,\\pmb{y}),\\cdots, f_n(x,\\pmb{y})]\\) is an analytic function on region \\(R\\). So problem \\(\\ref{eq-cauchy}\\) has a unique analytic solution \\(\\pmb{y} = \\pmb{y}(x)\\) on \\(O_\\rho(x)\\), where \\(\\rho\\) is given in Lemma 2.</p> HintsProof <ul> <li> <p>Represent solution with power series. Show that it is unique.</p> </li> <li> <p>Use an excellent series to prove the above power series convergent.</p> </li> </ul> <ul> <li>Represent \\(f_k(x,\\pmb{y})\\) with power series</li> </ul> \\[ \\begin{equation} f_k(x,\\pmb{y})=\\sum_{i=0,\\pmb{j}=0}^\\infty a_{i\\pmb{j}}^k (x-x_0)^i(\\pmb{y}-\\pmb{y}_0)^{\\pmb{j}} \\label{eq-f} \\end{equation} \\] <p>And represent solution with power series</p> \\[ \\begin{equation} y_k(x) = y_{k0} + \\sum_{i=1}^\\infty c_i^k(x-x_0)^i,\\quad k=1,2,\\cdots,n \\label{eq-y} \\end{equation} \\] <p>substitute \\(\\ref{eq-f}\\) and \\(\\ref{eq-y}\\) into ODE</p> \\[ \\frac{d y_k}{dx} = f_k(x,\\pmb{y}),\\quad k=1,2\\cdots,n \\] <p>and get</p> \\[ \\begin{align*} \\sum_{i=0}^\\infty (i +1) c_{i+1}^k(x-x_0)^i = \\sum_{i,j_1,j_2,\\cdots,j_n=0}^\\infty \\Bigg\\{ &amp; a^k_{i{j_1}{j_2}\\cdots{j_n}}(x-x_0)^i \\times \\\\  &amp;\\left[\\sum_{i=1}^\\infty c_i^1(x-x_0)^i\\right]^{j_1} \\times \\\\ &amp;\\left[\\sum_{i=1}^\\infty c_i^2(x-x_0)^i \\right]^{j_2} \\times \\\\ &amp;\\cdots \\\\ &amp; \\left[\\sum_{i=1}^\\infty c_i^n(x-x_0)^i \\right]^{j_n}\\Bigg\\},\\quad k=1,2\\cdots,n. \\end{align*} \\] <p>Denote \\(X = x-x_0\\), and we have</p> \\[ \\sum_{i=0}^\\infty (i +1) c_{i+1}^kX^i = \\sum_{i,j_1,j_2,\\cdots,j_n=0}^\\infty a^k_{i{j_1}{j_2}\\cdots{j_n}}X^i \\left(\\sum_{i=1}^\\infty c_i^1X^i\\right)^{j_1} \\left(\\sum_{i=1}^\\infty c_i^2X^i\\right)^{j_2} \\cdots \\left(\\sum_{i=1}^\\infty c_i^nX^i\\right)^{j_n} \\] <p>and get \\(c_i^k\\) out in terms of \\(a^k_{i\\pmb{j}}\\)</p> \\[ \\begin{align*} c_1^k &amp;= a^k_{00\\cdots 0}\\\\ c_2^k &amp;= \\frac{1}{2!} (a^k_{10\\cdots 0}+a^k_{010\\cdots 0}c^1_1 + a^k_{0010\\cdots 0}c^2_1+\\cdots a^k_{00\\cdots01}c^n_1) \\\\ &amp;= \\frac{1}{2!} (a^k_{10\\cdots 0} + a^k_{010\\cdots 0}a^1_{00\\cdots 0} + a^k_{0010\\cdots 0}a^2_{00\\cdots 0}+\\cdots a^k_{00\\cdots01}a^n_{00\\cdots 0}) \\end{align*} \\] <p>Generally, we have </p> \\[ c^k_m=P_m^k(a^l_{00\\cdots 0}, a^l_{01\\cdots 0},\\cdots,a^l_{i{j_1}\\cdots{j_n}}) \\] <p>where \\(i+j_1+j_2+\\cdots+j_n\\leq m-1\\), \\(1\\leq l\\leq n\\). Thus, \\(P_m^k\\) is a polynomial represented by \\(a^l_{00\\cdots 0}\\), \\(a^l_{01\\cdots 0}\\), \\(\\cdots\\), \\(a^l_{i{j_1}\\cdots{j_n}}\\) with positve operator \"+\". Theoretically, we can represent the solution by definite power series.</p> <p>We leave the proof of this part as an additional work in Appendix at the end of the doc.</p> <ul> <li>Prove the above series converges.</li> </ul> <p>Here we formulate another ODE and use Excellent function to bound the above power series.</p> <p>Since \\(f_k(x,\\pmb{y})\\) is analytic on region \\(R\\), by Lemma 1, there exists an excellent function of \\(f_k(x,\\pmb{y})\\) on a smaller region \\(R_0\\):</p> \\[ F_k(x,\\pmb{y}) = \\frac{M}{\\left(1-\\frac{x-x_0}{a}\\right)\\left(1-\\frac{y_1-y_{10}}{b}\\right)\\cdots\\left(1-\\frac{y_n-y_{n0}}{b}\\right)}, \\quad k=1,2\\cdots n \\] <p>If we represent both of them in terms of power series</p> \\[ \\begin{align*} f_k(x,\\pmb{y})&amp;=\\sum_{i=0.\\pmb{j}=0}^\\infty a_{i\\pmb{j}}(x-x_0)^i(\\pmb{y}-\\pmb{y}_0)^{\\pmb{j}} \\\\ F_k(x,\\pmb{y})&amp;=\\sum_{i=0.\\pmb{j}=0}^\\infty A_{i\\pmb{j}}(x-x_0)^i(\\pmb{y}-\\pmb{y}_0)^{\\pmb{j}} \\end{align*} \\] <p>then we have a relation \\(|a_{i\\pmb{j}}|&lt;A_{i\\pmb{j}}\\), which matters in the following proof.</p> <p>Now consider an ODE</p> \\[ \\frac{d y_k}{dx} = F_k(x,\\pmb{y}),\\quad y_k(x_0)=y_k,\\quad  k=1,2,\\cdots,n,\\quad   \\] <p>by Lemma 2, the above ODE has an analytic solution \\(\\pmb{y}=\\pmb{y}(x)\\), represented by power series</p> \\[ y_k(x) = y_{k0} + \\sum_{i=1}^\\infty C_i^k(x-x_0)^i,\\quad k=1,2,\\cdots,n \\] <p>Similarly, we have </p> \\[ \\begin{align*} C^k_m&amp;=P_m^k(A^l_{00\\cdots 0}, A^l_{01\\cdots 0},\\cdots,A^l_{i{j_1}\\cdots{j_n}})\\\\ &amp;=P_m^k(|A^l_{00\\cdots 0}|, |A^l_{01\\cdots 0}|,\\cdots,|A^l_{i{j_1}\\cdots{j_n}}|)\\\\ &amp;\\geq P_m^k(|a^l_{00\\cdots 0}|, |a^l_{01\\cdots 0}|,\\cdots,|a^l_{i{j_1}\\cdots{j_n}}|)\\\\ &amp;\\geq |c_m^k| \\end{align*} \\] <p>So power series \\(\\sum\\limits_{i=1}^\\infty C_i^k(x-x_0)^i\\) is en excellent series of \\(\\sum\\limits_{i=1}^\\infty c_i^k(x-x_0)^i\\). Since the former is convergent by Lemma 2, so the latter also converges.</p>"},{"location":"courses/Ordinary_Differential_Equation/General_Theory/Power_Series/#Appendix","title":"\u9644\u5f55 | Appendix: Relation between \\(c\\) and \\(a\\)","text":"<p> Theorem. Prove  \\[ c^k_m=P_m^k(a^l_{00\\cdots 0}, a^l_{01\\cdots 0},\\cdots,a^l_{i{j_1}\\cdots{j_n}}) \\] <p>where \\(i+j_1+j_2+\\cdots+j_n\\leq m-1\\), \\(1\\leq l\\leq n\\). Thus, \\(P_m^k\\) is a polynomial represented by \\(a^l_{00\\cdots 0}\\), \\(a^l_{01\\cdots 0}\\), \\(\\cdots\\), \\(a^l_{i{j_1}\\cdots{j_n}}\\) with positve operator \"+\".  </p> HintsProof <p>Use induction.</p>"},{"location":"courses/Sensing%26Detection/Midterm/","title":"Midterm Exam","text":""},{"location":"courses/Sensing%26Detection/Midterm/#_1","title":"\u7b80\u7b54\u9898","text":""},{"location":"courses/Sensing%26Detection/Midterm/#_2","title":"\u201c\u5dee\u52a8\u201d\u548c\u201c\u53c2\u6bd4\u201d\u8bbe\u8ba1\u65b9\u6cd5","text":"\u7b80\u8981\u6bd4\u8f83\u4e00\u4e0b\u201c\u5dee\u52a8\u5f0f\u201d\u548c\u201c\u53c2\u6bd4\u5f0f\u201d\u4e24\u79cd\u68c0\u6d4b\u4eea\u8868\u8bbe\u8ba1\u65b9\u6cd5\u7684\u5f02\u540c\u70b9\u3002  <p>\u5f02\uff1a</p> <ul> <li>\u5dee\u52a8\u5f0f\u662f\u91c7\u7528\u4e24\u4e2a\u8f6c\u6362\u5143\u4ef6\u540c\u65f6\u611f\u53d7\u654f\u611f\u5143\u4ef6\u7684\u8f93\u51fa\u91cf\uff0c\u5e76\u628a\u5b83\u8f6c\u6362\u6210\u4e24\u4e2a\u6027\u8d28\u76f8\u540c\uff0c\u4f46\u6cbf\u76f8\u53cd\u65b9\u5411\u53d8\u5316\u7684\u7269\u7406\u91cf\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u4f7f\u5f97\u6709\u6548\u8f93\u51fa\u4fe1\u53f7\u63d0\u9ad8\u4e00\u500d\uff0c\u4fe1\u566a\u6bd4\u5f97\u5230\u6539\u5584\uff0c\u975e\u7ebf\u6027\u8bef\u5dee\u51cf\u5c0f\uff1b\u6613\u4e8e\u5b9e\u73b0\u521d\u59cb\u72b6\u6001\uff08\u201c\u96f6\u201d\u8f93\u5165\uff09\u7684\u96f6\u8f93\u51fa\uff0c\u80fd\u6d88\u9664\u90e8\u5206\u73af\u5883\u56e0\u7d20\u7684\u5f71\u54cd\u3002</li> <li>\u53c2\u6bd4\u5f0f\u662f\u91c7\u7528\u4e24\u4e2a\u6027\u80fd\u5b8c\u5168\u76f8\u540c\u7684\u68c0\u6d4b\u5143\u4ef6\uff0c\u4ed6\u4eec\u540c\u65f6\u611f\u53d7\u73af\u5883\u6761\u4ef6\u91cf\uff0c\u4f46\u53ea\u6709\u4e00\u4e2a\u611f\u53d7\u88ab\u6d4b\u91cf\u3002\u5176\u4f5c\u7528\u662f\u5c06\u540c\u65f6\u4f5c\u7528\u5728\u4e24\u4e2a\u68c0\u6d4b\u5143\u4ef6\u4e0a\u7684\u73af\u5883\u6761\u4ef6\u91cf\u7684\u5e72\u6270\u4fe1\u606f\u9664\u53bb\uff0c\u5bf9\u88ab\u6d4b\u91cf\u4fe1\u606f\u8fdb\u884c\u653e\u5927\u3002\u53c2\u6bd4\u53ef\u4ee5\u8f83\u597d\u5730\u6d88\u9664\u5e72\u6270\u6765\u6e90\u660e\u786e\u5730\u73af\u5883\u6761\u4ef6\u91cf\u7684\u5f71\u54cd\u3002</li> </ul> <p>\u540c\uff1a</p> <ul> <li>\u90fd\u80fd\u4e00\u5b9a\u7a0b\u5ea6\u514b\u670d\u73af\u5883\u5e72\u6270\u3002</li> </ul>"},{"location":"courses/Sensing%26Detection/Midterm/#_3","title":"\u538b\u963b\u5f0f\u3001\u538b\u7535\u5f0f\u3001\u538b\u78c1\u5f0f","text":"\u4ece\u5e94\u7528\u89d2\u5ea6\u8ba8\u8bba\u5e76\u5206\u6790\u538b\u963b\u5f0f\u3001\u538b\u7535\u5f0f\u548c\u538b\u78c1\u5f0f\u68c0\u6d4b\u5143\u4ef6\u5404\u6709\u4ec0\u4e48\u7279\u70b9\uff1f  <ul> <li>\u538b\u963b\u5f0f</li> </ul> <p>\u6d4b\u91cf\u8303\u56f4\u5bbd\u3001\u51c6\u786e\u5ea6\u9ad8\uff0c\u54cd\u5e94\u901f\u5ea6\u5feb\uff0c\u9002\u5408\u9759\u6001\u548c\u52a8\u6001\u6d4b\u91cf\uff0c\u4f7f\u7528\u5bff\u547d\u957f\uff0c\u6027\u80fd\u7a33\u5b9a\uff0c\u4ef7\u683c\u4fbf\u5b9c\uff0c\u53ef\u4ee5\u5728\u9ad8\u5f3a\u5ea6\u6076\u52a3\u73af\u5883\u4e0b\u5de5\u4f5c\u3002</p> <p>\u4f46\u6709\u8f93\u51fa\u4fe1\u53f7\u5fae\u5f31\uff0c\u6297\u5e72\u6270\u80fd\u529b\u5dee\uff0c\u6613\u53d7\u6e29\u5ea6\u7b49\u73af\u5883\u56e0\u7d20\u7684\u5f71\u54cd\uff0c\u5927\u5e94\u53d8\u72b6\u6001\u4e0b\u6709\u8f83\u5927\u7684\u975e\u7ebf\u6027\u3002</p> <p>\u5e94\u7528\uff1a\u88ab\u7c98\u8d34\u5728\u5404\u79cd\u5f39\u6027\u5143\u4ef6\u4e0a\uff0c\u4ee5\u611f\u53d7\u538b\u529b\u53d8\u5316\u3002</p> <ul> <li>\u538b\u7535\u5f0f</li> </ul> <p>\u5177\u6709\u9891\u5e26\u5bbd\u3001\u7075\u654f\u5ea6\u9ad8\u3001\u7ed3\u6784\u7b80\u5355\u3001\u5de5\u4f5c\u53ef\u9760\u3001\u91cd\u91cf\u8f7b\u7b49\u4f18\u70b9\u3002</p> <p>\u4f46\u53ea\u9002\u5408\u52a8\u6001\u6d4b\u91cf\u3002</p> <p>\u5e94\u7528\uff1a\u53ef\u4ee5\u5b9e\u73b0\u529b\u3001\u538b\u529b\u3001\u52a0\u901f\u5ea6\u548c\u626d\u77e9\u7b49\u7269\u7406\u91cf\u7684\u6d4b\u91cf\u3002</p> <ul> <li>\u538b\u78c1\u5f0f</li> </ul> <p>\u8f93\u51fa\u529f\u7387\u5927\uff0c\u6297\u5e72\u6270\u80fd\u529b\u53ca\u8fc7\u8f7d\u80fd\u529b\u5f3a\uff0c\u4fbf\u4e8e\u5236\u9020\uff0c\u7ecf\u6d4e\u5b9e\u7528\uff0c\u5e76\u80fd\u5728\u6076\u52a3\u7684\u6761\u4ef6\u4e0b\u957f\u671f\u4f7f\u7528\u3002</p> <p>\u4f46\u6d4b\u91cf\u7cbe\u5ea6\u4e0d\u9ad8\uff0c\u53cd\u5e94\u901f\u5ea6\u8f83\u6162\u3002</p> <p>\u5e94\u7528\uff1a\u4e3b\u8981\u5e94\u7528\u4e8e\u6d4b\u529b\u3001\u79f0\u91cd\u3001\u6e29\u5ea6\u6d4b\u91cf\u53ca\u76c8\u5229\u65e0\u635f\u68c0\u6d4b\u7b49\u65b9\u9762\u3002</p>"},{"location":"courses/Sensing%26Detection/Midterm/#_4","title":"\u51cf\u5c11\u968f\u673a\u8bef\u5dee\u3001\u975e\u7ebf\u6027\u8865\u507f","text":"\u68c0\u6d4b\u4eea\u8868\u5e38\u7528\u7684\u51cf\u5c11\u968f\u673a\u8bef\u5dee\u548c\u8fdb\u884c\u975e\u7ebf\u6027\u8865\u507f\u7684\u65b9\u6cd5\u4e3b\u8981\u6709\u54ea\u4e9b\uff1f  <ul> <li>\u51cf\u5c11\u968f\u673a\u8bef\u5dee\u7684\u65b9\u6cd5\u3002</li> </ul> <p>\u63d0\u9ad8\u68c0\u6d4b\u7cfb\u7edf\u51c6\u786e\u5ea6\uff0c\u5bf9\u6d4b\u91cf\u7ed3\u679c\u8fdb\u884c\u7edf\u8ba1\u5904\u7406\uff0c\u6291\u5236\u566a\u58f0\u5e72\u6270\u3002</p> <ul> <li>\u8fdb\u884c\u975e\u7ebf\u6027\u8865\u507f\u3002</li> </ul> <p>\u76f4\u63a5\u4e32\u8054\u6cd5\uff0c\u975e\u7ebf\u6027\u8d1f\u53cd\u9988\u6cd5\uff0c\u8f6f\u4ef6\u7ebf\u6027\u5316\u6cd5</p>"},{"location":"courses/Sensing%26Detection/Midterm/#_5","title":"\u5f00\u73af\u3001\u95ed\u73af\u7ed3\u6784\u4eea\u8868","text":"\u6bd4\u8f83\u5206\u6790\u4e00\u4e0b\u5f00\u73af\u7ed3\u6784\u548c\u95ed\u73af\u7ed3\u6784\u4eea\u8868\u5404\u81ea\u7684\u7279\u70b9\u3002(\u53cb\u60c5\u63d0\u9192:\u7b2c\u4e00\u7ae0\u548c\u7b2c\u4e09\u7ae0\u7684\u76f8\u5173\u5185\u5bb9\u8981\u4e00\u8d77\u8003\u8651\uff0c\u5e76\u4ece\u81ea\u52a8\u63a7\u5236\u539f\u7406\u7684\u89d2\u5ea6\u6765\u8fdb\u884c\u5206\u6790\u3002)  <ul> <li>\u5f00\u73af\u7ed3\u6784\u4eea\u8868</li> </ul> <p>\u7531\u82e5\u5e72\u4e2a\u73af\u8282\u4e32\u8054\u7ec4\u6210\uff0c\u4eea\u8868\u7684\u4fe1\u606f\u548c\u53d8\u6362\u53ea\u6cbf\u4e00\u4e2a\u65b9\u5411\u4f20\u9012\u3002\u5176\u603b\u7684\u4f20\u9012\u51fd\u6570\u4e3a\u5404\u73af\u8282\u4f20\u9012\u51fd\u6570\u4e4b\u79ef\uff0c\u6574\u53f0\u4eea\u8868\u7684\u76f8\u5bf9\u8bef\u5dee\u4e3a\u5404\u4e2a\u73af\u8282\u7684\u76f8\u5bf9\u8bef\u5dee\u4e4b\u548c\u3002</p> <p>\u603b\u4f53\u6765\u8bf4\u7ed3\u6784\u7b80\u5355\u3002\u5f53\u7ec4\u6210\u4eea\u8868\u7684\u73af\u8282\u8f83\u591a\u65f6\uff0c\u51c6\u786e\u5ea6\u8f83\u4f4e\u3002</p> <ul> <li>\u95ed\u73af\u7ed3\u6784\u4eea\u8868</li> </ul> <p>\u7531\u6b63\u5411\u901a\u9053\u548c\u53cd\u9988\u901a\u9053\u7ec4\u6210\u3002\u5bf9\u4e8e\u4e00\u9636\u73af\u8282\\(G = \\frac{k}{1+Ts}\\)\uff0c\u82e5\u6709\u53cd\u9988\u589e\u76ca\\(\\beta\\)\uff0c\u5176\u653e\u5927\u500d\u6570\u548c\u65f6\u95f4\u5e38\u6570\u90fd\u662f\u5f00\u73af\u7ed3\u6784\u4eea\u8868\u7684\\(1/(1+k\\beta)\\)\u3002\u82e5\\(k\\beta \\rightarrow \\infty\\)\uff0c\u5219\\(G' = K_0/\\beta\\)\uff0c\u5373\u4eea\u8868\u7279\u6027\u4e3b\u8981\u53d6\u51b3\u4e8e\u53cd\u9988\u901a\u9053\u7279\u6027\u3002</p> <p>\u603b\u4f53\u7ed3\u6784\u4f1a\u590d\u6742\u4e00\u4e9b\uff0c\u7a33\u5b9a\u6027\u4f1a\u8f83\u5dee\u3002\u4f46\u662f\u53cd\u5e94\u901f\u5ea6\u5feb\uff0c\u7ebf\u6027\u597d\uff0c\u51c6\u786e\u5ea6\u9ad8\u3002</p>"},{"location":"courses/Sensing%26Detection/Midterm/#_6","title":"\u6807\u51c6\u5dee\u4e0e\u7cfb\u7edf\u3001\u7c97\u5927\u8bef\u5dee","text":"\u8bf7\u5217\u51fa\u6d4b\u91cf\u4fe1\u53f7\u5747\u503c\u548c\u6807\u51c6\u5dee\u7684\u5b9a\u4e49\u516c\u5f0f\uff0c\u5e76\u7b80\u8981\u8bba\u8ff0\u4e00\u4e0b\u8be5\u4e24\u4e2a\u91cd\u8981\u7edf\u8ba1\u91cf\u5728\u7cfb\u7edf\u8bef\u5dee\u548c\u7c97\u5927\u8bef\u5dee\u5224\u522b\u4e2d\u7684\u4f5c\u7528\u3002  <ul> <li>\u5747\u503c</li> </ul> \\[ \\overline{x}=\\sum\\limits_{i=1}^{n}x_i \\] <ul> <li>\u6807\u51c6\u5dee</li> </ul> <p>\u5355\u6b21\u6d4b\u91cf\u503c\u7684\u6807\u51c6\u5dee(\u8d1d\u585e\u5c14Bessel\u516c\u5f0f)</p> \\[ \\sigma_B=s(x) = \\sqrt{\\frac{\\sum\\limits_{i=1}^{n}(x_i - \\overline{x})^2}{n-1}} \\] <p>\u7b97\u672f\u5e73\u5747\u503c\\(\\overline{x}\\)\u7684\u6807\u51c6\u5dee</p> \\[ s(\\overline{x}) = \\frac{s(x)}{\\sqrt{n}} \\] <p>\u53ef\u4ee5\u7528\u6807\u51c6\u5dee\u5224\u636e\uff0c\u770b\u4e24\u4e2a\u4f30\u8ba1\u6807\u51c6\u5dee\u662f\u5426\u6e10\u8fdb\u76f8\u7b49\u3002\u82e5\u5b58\u5728\u7cfb\u7edf\u8bef\u5dee\uff0c\u5219\u4e24\u8005\u76f8\u5dee\u5f88\u5927\u3002</p> <p>\\(\\sigma\\)\u6cd5\u53ef\u4ee5\u5254\u9664\u7c97\u5927\u8bef\u5dee\u3002\u5f53\u6d4b\u91cf\u503c\\(x_i\\)\u6ee1\u8db3\u62c9\u4f0a\u8fbe\u6cd5\u6216\u683c\u62c9\u5e03\u65af\u6cd5\u7684\u6761\u4ef6\u65f6\uff0c\u53ef\u4ee5\u8ba4\u5b9a\u4e3a\u7c97\u5927\u8bef\u5dee\uff08\u5982\\(3\\sigma\\)\uff09\u3002</p>"},{"location":"courses/Sensing%26Detection/Midterm/#_7","title":"\u6700\u5927\u7edd\u5bf9\u8bef\u5dee\u3001\u6807\u51c6\u5dee\u7684\u4f30\u8ba1","text":"\u73b0\u6709\u4e00\u53f0\u6e29\u5ea6\u4f20\u611f\u5668\uff0c\u91cf\u7a0b\u4e3a0~100\u00b0C\uff0c\u5176\u51c6\u786e\u5ea6(\u7cbe\u5ea6)\u7b49\u7ea7\u4e3a1.0\u3002\u8bf7\u95ee\u8be5\u4f20\u611f\u5668\u5728\u91cf\u7a0b\u8303\u56f4\u5185\u53ef\u80fd\u51fa\u73b0\u7684\u6700\u5927\u7edd\u5bf9\u8bef\u5dee\u7684\u5408\u7406\u4f30\u8ba1\u503c\u548c\u53ef\u80fd\u51fa\u73b0\u7684\u6700\u5927\u6807\u51c6\u5dee\u7684\u5408\u7406\u4f30\u8ba1\u503c\u5206\u522b\u4e3a\u591a\u5c11?\u4e3a\u4ec0\u4e48?(\u53cb\u60c5\u63d0\u9192:\u4ed4\u7ec6\u770b\u4e00\u4e0b\u6559\u6750P13\u9875)  <ul> <li>\u6700\u5927\u7edd\u5bf9\u8bef\u5dee</li> </ul> \\[ e = 100 \\times 1\\% = 1 \u00b0C \\] <ul> <li>\u6700\u5927\u6807\u51c6\u5dee</li> </ul> \\[ 3\\sigma = 1 \u00b0C\\Rightarrow \\sigma = 1/3\u00b0C \\]"},{"location":"courses/Sensing%26Detection/Midterm/#_8","title":"\u8ba1\u7b97\u9898","text":"<p>\u4e3a\u4e86\u4fdd\u8bc1\u6d4b\u91cf\u51c6\u786e\u5ea6\uff0c\u5728\u538b\u529b\u68c0\u6d4b\u8868\u9009\u578b\u65f6\uff0c\u4e00\u822c\u8981\u6c42\u6700\u5927\u5de5\u4f5c\u538b\u529b\u4e0d\u5e94\u8d85\u8fc7\u4eea\u8868\u6ee1\u91cf\u7a0b\u7684\\(3/4\\)\uff0c\u6700\u5c0f\u5de5\u4f5c\u538b\u529b\u4e0d\u5e94\u4f4e\u4e8e\u6ee1\u91cf\u7a0b\u7684\\(1/3\\)\u3002\u76ee\u524d\u6211\u56fd\u51fa\u5382\u7684\u538b\u529b(\u5305\u62ec\u5dee\u538b)\u68c0\u6d4b\u4eea\u8868\u6709\u7edf\u4e00\u7684\u91cf\u7a0b\u7cfb\u5217\uff0c\u5b83\u4eec\u662f\\(1\\)\u3001\\(1.6\\)\u3001\\(2.5\\)\u3001\\(4.0\\)\u3001\\(6.0kPa\\) \u4ee5\u53ca\u5b83\u4eec\u7684\\(10^n\\)\u500d\u6570(\\(n\\)\u4e3a\u6574\u6570)\u3002</p> <p>\u67d0\u538b\u529b\u5bb9\u5668\u6b63\u5e38\u5de5\u4f5c\u65f6\u538b\u529b\u8303\u56f4\u4e3a\\(1.0~1.5MPa\\),\u8981\u6c42\u6d4b\u91cf\u8bef\u5dee\u4e0d\u5927\u4e8e\u88ab\u6d4b\u538b\u529b\u76845%,\u8bd5\u786e\u5b9a\u8be5\u8868\u7684\u91cf\u7a0b\u548c\u51c6\u786e\u5ea6\u7b49\u7ea7\u3002</p> <ul> <li>\u89e3\uff1a</li> </ul> <p>\u8bbe\u91cf\u7a0b\u4e3a\\(A (MPa)\\)\uff0c\u5219\u6ee1\u8db3</p> \\[ \\begin{cases} \\frac{1}{3}A \\leq 1.0 \\\\ 1.5 \\leq \\frac{3}{4}A \\end{cases} \\] <p>\u89e3\u5f97\\(2.0\\leq A \\leq 3.0\\)\uff0c\u6545\u53d6\\(A=2.5 MPa\\)</p> <p>\u7531\u9898\u610f\uff0c\u4eea\u8868\u57fa\u672c\u8bef\u5dee\\(e \\leq 5\\% P\\)\uff0c\u6545\u5fc5\u987b\u8981</p> \\[ e \\leq 5\\% \\min{P} = 0.05\\times 1.0 = 0.05MPa \\] <p>\u6240\u4ee5\u4eea\u8868\u6ee1\u91cf\u7a0b\u57fa\u672c\u8bef\u5dee</p> \\[ e' \\leq \\frac{e}{A} = 2\\% \\] <p>\u6545\u9009\u62e9\u51c6\u786e\u5ea6\\(1.5\\)\u3002</p>"},{"location":"courses/cpp/cpp/","title":"C Plus Plus","text":""},{"location":"courses/cpp/cpp/#c","title":"C++","text":""},{"location":"courses/cpp/cpp/#basic-ideas","title":"Basic Ideas","text":"<ul> <li>object-oriented | \u7269\u4ef6\u5bfc\u5411-\u9762\u5411\u5bf9\u8c61</li> </ul> <p>An object, or entity(either visible or invisible), is a variable in programming languages, made up of two primary components:</p> <ul> <li>Attibutes, or Data, representing the object's properties and status</li> <li>Services, or Operations, refered to as functions in programming.</li> </ul> <p>C++ Focuses on things instead of operations.</p>"},{"location":"courses/cpp/cpp/#key-words","title":"Key words","text":"<ul> <li>interface</li> <li>communications</li> <li>protection</li> <li>the hidden implementation</li> <li>encapsulation</li> </ul> First Program of C++<pre><code># include &lt;iostream&gt;\nusing namespace std;\nint main(){\n  cout&lt;&lt;\u201cHello,World!I am\u201d\n}\n</code></pre>"},{"location":"courses/cpp/cpp/#oop-characteristics","title":"OOP characteristics","text":"<ul> <li>Everything is an object.</li> <li>A program is a bunch of objects telling each other what to do/(not how to do) by sending messages</li> <li>Each object has its own memory made up of other objects.</li> <li>Every object has a type.</li> <li>All objects of a particular type can receive the same messages. (Using the method to distinguish between different types or classes)</li> </ul>"},{"location":"courses/cpp/cpp/#header-files","title":"Header Files","text":"<p>To prevent defining repeatedly:</p> <p>x.h<pre><code># pragma once //\u53ea\u5305\u542b\u8fd9\u4e2a\u5934\u6587\u4ef6\u4e00\u6b21\n</code></pre> or the same as C language:</p> log.h<pre><code># ifndef _LOG_H\n# define _LOG_H\n//...//\n# endif\n</code></pre>"},{"location":"courses/cpp/cpp/#declaration","title":"declaration","text":"<ul> <li>external variables</li> <li>function prototypes</li> <li>class/struct declarations</li> </ul>"},{"location":"courses/cpp/cpp/#resolver","title":":: resolver","text":"::<pre><code>&lt;Class Name&gt;::&lt;function name&gt;//not free\n::&lt;function name&gt;\n\nvoid S::f(){\n  ::f();//would be recursive otherwise\n  ::a++;//select the global a\n  a\u2014;//select the partial a\n}\n</code></pre>"},{"location":"courses/cpp/cpp/#string","title":"String","text":"C++<pre><code>#include&lt;string&gt;\nString age, name; \ncin &gt;&gt; age &gt;&gt; name;\ncout &lt;&lt; name; \n</code></pre> <ul> <li>A class type, not a primitive type.</li> <li>Initially <code>name</code> is all zero. No matter it is static or global.</li> <li>No <code>\\0</code> at the end of the string.</li> </ul>"},{"location":"courses/cpp/cpp/#reference","title":"Reference","text":"<p>Reference make use of  the idead of a pointer, but is used as a normal variable rather than pointer. It is widely used in passing variables into a function.</p> C++<pre><code>char c;\nchar &amp; r = c;// a reference to c;\n</code></pre> <p>Some notes on reference:</p> <ul> <li>cannot be <code>NULL</code>.</li> <li>cannot calculate.</li> <li>No reference to reference</li> </ul> C++<pre><code>int &amp;* r; // No pointer to reference \nint *&amp; r; // We have reference to pointer\n</code></pre>"},{"location":"courses/cpp/cpp/#classstruct","title":"Class(Struct)","text":"<p>Intuitively, we put a function into a <code>struct</code> and it bacome <code>class</code>.(we can use functional point in C) act like a type.</p> <ul> <li>Definition We all know the principle of designing:</li> </ul> <p>separated .h &amp;.cpp are used to define one class</p> <ul> <li> <p>Header file(.h): class declaration&amp;prototype</p> </li> <li> <p>Source file(.cpp): all the bodies of functions</p> </li> </ul> <p>Hidden parameter: <code>this</code>, which is a pointer to the variable.</p> Publicprivate C++<pre><code>struct point{\n  float x;\n  float y;\n  void init(int x, int y){\n    this-&gt;x=x;\n    this-&gt;y=y;\n  }\n  void print(){\n    cout &lt;&lt; x &lt;&lt; \", \" &lt;&lt; y &lt;&lt;endl;\n  }\n}\n</code></pre> C++<pre><code>class point{\nprivate:\n  float x;\n  float y; //the above data is protected.\n\npublic://the followings can be accessed from outside\n  void init(int x, int y){\n    this-&gt;x=x;\n    this-&gt;y=y;\n  }\n  void print(){\n    cout &lt;&lt; x &lt;&lt; \", \" &lt;&lt; y &lt;&lt;endl;\n  }\n}\n</code></pre>"},{"location":"courses/cpp/cpp/#object-an-instance-of-class","title":"Object | an instance of class","text":""},{"location":"courses/cpp/cpp/#ctor-constructor","title":"C\u2019tor (constructor)","text":"<ul> <li>Initialization List</li> </ul> <p>C++<pre><code>class A{\n  private:\n    int i;\n    int j= i;\n\n  public:\n    A():i(11){}\n}\n</code></pre> Initialization versus Assignment</p> <p>C++<pre><code>Stu::Stu(string s):name(s){} // better to use\nStu::Stu(string s){name=s;}\n</code></pre> Equivalent:</p> C++<pre><code>string place(\u201cHangzhou\u201d);\nstring place = \u201cHangzhou\u201d;\n\nint i = 6;\nint i(6); \n</code></pre> <p>A constructor function:</p> <p>C++<pre><code>point::point(int x, int y){\n  this-&gt;x=x;\n  this-&gt;y=y;\n}\n</code></pre> then use it:</p> <p>C++<pre><code>point a(1,2);\n</code></pre> And if the constructor function only takes in one parameter, like </p> <p>C++<pre><code>point::point(int dep){\n  this-&gt;x=this-&gt;y=dep;\n}\n</code></pre> then we can use it to initialize:</p> <p>C++<pre><code>point a(1);\npoint a=10;\n</code></pre> we can not initialize a point like we do in struct:</p> C++<pre><code>point a ={1,2}; // this can succeed only when the class does not have a constructor function and the parameter is public.\n</code></pre> <ul> <li>Default Constructor</li> </ul> <p>It is a function that can be called with no arguments input.</p> <p>If we don\u2019t give any constructor function, then the system can give one that does nothing.</p> <p>If we offer a constructor function that takes input of more than one parameter, we have no default constructor function.</p> <p>C++<pre><code>struct Y{\n  float y;\n  int i;\n  Y(int a);\n}\n</code></pre> then:</p> C++<pre><code>Y y1[] = {Y(1), Y(2)}; // right\nY y2[2] {Y(1)}; // wrong\nY y3[7]; //wrong\nY y4;  //wrong\n</code></pre>"},{"location":"courses/cpp/cpp/#dtordestructor","title":"D\u2019tor(destructor)","text":"<p>This function would be implemented before the memory is recycled.</p> <p>To design it, please add tilde <code>~</code> before the name of the class. The function have no input and output.</p> <p>It will delete local objects in a reverse manner.(caused by stack action)</p> C++<pre><code>class point{\nprivate:\n  float x;\n  float y;\n//the above data is protected.\n\npublic://can be accessed from outside\n  point(int dep); // reload\n  point(int x, int y);\n  ~point();\n  void print();\n}\n</code></pre>"},{"location":"courses/cpp/cpp/#stlstandard-template-library","title":"STL(standard template library)","text":"<p>All the following identifiers in library are in <code>std</code> namespace.</p> <p>This is also called Container(lowercase)</p>"},{"location":"courses/cpp/cpp/#sequential","title":"Sequential","text":"<ul> <li>vector(variable array)</li> </ul> <p>It is easy to use index to search and save memory</p> C++<pre><code>vector&lt;typeName&gt; vt(n_ele);\nvector&lt;int&gt; x;\nx.push_back(1);\n</code></pre> <p>reload p++ </p> <p>C++<pre><code>vector&lt;int&gt;::iterator p;\nfor(p=x.begin(); p&lt;x.end(); p++)\n  cout &lt;&lt; *p &lt;&lt; \u201c \u201c;\n</code></pre> or  C++<pre><code>for(auto k: x)\ncout &lt;&lt; k &lt;&lt; \u201c \u201c;\n</code></pre></p> <p><code>auto</code> means the compiler can identify the type of the variable itself.</p> <p>for \u4ec5\u7528\u4e8e\u904d\u5386\u5168\u90e8(range-based for loops)</p> <p>\u653e\u8fdb\u5bb9\u5668\u91cc\u7684\u5185\u5bb9\u662fclone</p> <p>\u76f4\u63a5\u4f7f\u7528\u4e0b\u6807\uff0c\u4e0d\u4f7f\u7528<code>push_back</code>/<code>pop</code> \u662f\u4e0d\u4f1a\u6539\u53d8size\u7684\u3002</p> C++<pre><code>x[999] = 9; // no error but invalid\n</code></pre> <ul> <li> <p>array</p> </li> <li> <p>fixed length, use stack.</p> </li> </ul> <p>C++<pre><code>array&lt;typeName, n_ele&gt; arr;\n</code></pre> <code>n_ele</code> should be constant</p> <ul> <li>list(double-linked-list) It can insert/delete items very quickly.</li> </ul> <p>C++<pre><code>list&lt;int&gt; L;\nlist&lt;int&gt;::iterator li;\nli = L.begin();\nL.erase();\n++li; //wrong! li has been removed.\n</code></pre> right:</p> C++<pre><code>li = L.erase(li)\nli now points to nest node\n</code></pre> <ul> <li>others</li> </ul> <p>Deque(double ended queue), forward_list, maps(HashMap)</p> C++<pre><code>map&lt;string, float&gt; price;\nprice[\u201csnapple\u201d] =0.75\n</code></pre>"},{"location":"courses/cpp/cpp/#using-class","title":"Using Class","text":""},{"location":"courses/cpp/cpp/#function-overloading","title":"Function overloading","text":"<p>For function with the same name, compiler will choose from different function according to different input/labels.</p> <p>Pay attention to primitive type input</p> C++<pre><code>void f(int i){}\nvoid f(double d){}\n\nint main() {\n  f(\u2018a\u2019); //despite smaller than int, it can be transformed\n  f(2); // ambiguous\n  f(2L); // ambiguous\n  f(3.2); // ok\n}\n</code></pre>"},{"location":"courses/cpp/cpp/#default-argument","title":"Default argument","text":"<p>we should give default argument from right to left:</p> C++<pre><code>int harpo(int n, int m, int j=5);\nint chico(int n, int m=6, int j); //illegal\n</code></pre> <p>default argument must be written in declaration, i.e. in \".h\" file. We can not write it in definition, but always in calling back.</p>"},{"location":"courses/cpp/cpp/#friend","title":"Friend","text":"<p>A declaration, which cannot append.</p> <p>Two classes can be a friend relationship when they cannot be \"is-a\"(public inheritance) or \"has-a\"(embedding), like Tv and remote.</p> C++<pre><code>class Tv\n{\n  friend class Remote; // a class Remote can access all the private member functions/variables of Tv.\n}\n</code></pre> <p>In fact, a class can use member functions of another class to achieve some usage, not by accessing directly its member variables. Only some functions must access these member variables.</p> C++<pre><code>class Tv\n{\n  friend void Remote::set_chan(Tv &amp;t, int c); // here only this function can affect private member variables of Tv.\n  // compiler need to first know Remote class.\n};\n</code></pre> <p>So </p> C++<pre><code>// this is right:\nclass Tv; //forward declaration\nclass Remote{...}; // Remote::set_chan declaration\nclass Tv{...};\n\n// this is wrong\nclass Remote; // forward declaration\nclass Tv{...}; // Tv declaration in which we have set_chan which has not been declared.\nclass Remote{...};\n</code></pre> <p>But if <code>Remote</code> has inline function which calls a function of <code>Tv</code>, the above right forward declaration does not work. So the solution becomes </p> C++<pre><code>class Tv;\nclass Remote{...}; // Tv0using methods declaration without definition.\nclass Tv{...};\n</code></pre>"},{"location":"courses/cpp/cpp/#inline-function","title":"inline function | \u5185\u8054","text":"<p>It can check the type, which is better than Macro(\u5b8f)!</p> <p>C++<pre><code>inline double square(double x);\ninline double square(double x){ return x*x;}\n</code></pre> is only a declaration instead of a definition, so the compiler must see the body of function!</p> <ul> <li>compiler must see body so it can compile!(not just write down as a declaration)</li> </ul> <p>Body of inline function must be put in \".h\" files so it can be used in another file!</p> <ul> <li>if you write a inline function in a \".cpp\" file, you mean the function should only be used locally.</li> </ul> <p>Function that can be used <code>inline</code>:</p> <ul> <li>small function</li> <li>frequently called function</li> </ul> <p>others that cannot be used <code>inline</code>:</p> <ul> <li>long function</li> <li>recursive function</li> </ul>"},{"location":"courses/cpp/cpp/#const","title":"Const","text":"<p>constants are variables -    (instant \u7acb\u5373\u6570)</p> C++<pre><code>const int a = 6; \n</code></pre> <ul> <li>literal -&gt; 6</li> <li>constant -&gt; a</li> </ul> <p>Distinguish:</p> C++<pre><code>String p1(\u201cFred\u201d);\nconst string * p = &amp;p1; //(1)\nstring const * p = &amp;p1; //(2)\nstring * const p = &amp;p1; //(3)\n</code></pre> HintsAnswer <p>Const only restrict one variable.</p> <p>(1)(2) are the same: <code>(*p)</code> can not change; That is, we cannot change <code>p1</code> through <code>(*p)</code>.</p> <p>(3) means the pointer <code>p</code> itself cannot change but p1 itself can still change.</p> C++<pre><code>int i; \nconst int ci = 3;\n\nint *ip = &amp;i; \nint *ip = &amp;ci; // illegal, that is, a changeable pointer now points a non-changeable variable, which is illegal in compiler.\n\nconst int * cip = &amp;i;\nconst int * cip = &amp;ci;\n</code></pre> <ul> <li>Passing &amp; returning by const value</li> </ul> <p>We do this in case that we change some value in a function. So we let compiler check.</p> <ul> <li>Const object </li> </ul> <p>in a function, we pass a pointer instead of a copy!</p> <p>(1) public</p> <p>(2) change value by inner function</p> C++<pre><code>int get_day(void) const;\nint get_day(void) const {return day;}\n\nconst A a; // must provide a an initial value!(or constructor) because later we cannot change it! Like below:\n\nconst int i=1; // we cannot do this before C11\n\npublic:\n  A(int k): i(k){}\n</code></pre> <ul> <li>constant i cannot change during execution, but need a value in initialization.</li> </ul> <p>C++<pre><code>class A {\nprivate:\n  int i=0;\npublic:\n  void f() {i=10;\n    cout &lt;&lt; \u201cA::f()\u201d&lt;&lt; end;\n  }\n  void f() const {\n    cout &lt;&lt; \u201cA::f() const\u201d&lt;&lt; end;\n  }\n}\n</code></pre> The above means:</p> C++<pre><code>public:\n  void f(A *this) {i=10;\n    cout &lt;&lt; \u201cA::f()\u201d&lt;&lt; end;\n  }\n  void f(const A *this) const {\n    cout &lt;&lt; \u201cA::f() const\u201d&lt;&lt; end;\n  }\n</code></pre> <p>So:</p> <p>C++<pre><code>const A a;\nA b;\na.f(); // &lt;&lt;\u201cA::f() const\u201d\nb.f(); // &lt;&lt;\u201cA::f()\u201d\n</code></pre> The above code is using overload, and there are two different f() that have been distinguished by \u201cconst\u201d.</p> <p>So a const object can only use function attached to \"const\" and cannot use function with no \"const\".</p>"},{"location":"courses/cpp/cpp/#static","title":"Static","text":"<p>on members which are </p> <ul> <li>Hidden</li> <li>Persistent</li> </ul> <p>static variable is actually global variable.</p> <p>static function can only access static variable!</p> <p>C++<pre><code>static int i; // can be accessed by all the objects of same class\n</code></pre> we must define the static variable (global variable) before main!</p> <p>C++<pre><code>int A::i;\n</code></pre> Note: without static!</p> <p>C++<pre><code>static void sf(){\n  i++;\n}\n</code></pre> we can call static function without creating an object! Just use class!</p> C++<pre><code>int main(){\n  A::sf();\n}\n</code></pre> <p>We can eliminate global variable because we can limit it in class, which can prevent arbitrary changes!</p>"},{"location":"courses/cpp/cpp/#overloaded-operator","title":"Overloaded Operator","text":"<p>operators of primitive class cannot be changed. C++<pre><code>Integer x(1), y(2);\nz = x + y // x.operator+(y)\nz = x + 3 // x.operator+(Interger(3))\nz = 3 + x // not allowed\n</code></pre></p> <p>Global operator. C++<pre><code>z = 3 + 7; // pass 10 to initialize z\n</code></pre></p> <p><code>= () [] -&gt; -&gt;*</code> must be members and all other binary operators(\u53cc\u76ee) should be non-members.</p> <p>Will the operator change the operation number\uff1f</p> <ul> <li>If not, use <code>const</code>.</li> </ul> Common Operators<pre><code>// + - * / % ^ &amp; | ~\nconst T operatorX(const T&amp;I, const T&amp;L)\n\n// ! &amp;&amp; || &lt; &lt;= == &gt;= &gt;\nbool operatorX(const T&amp;I, const T&amp;L)\n\n// []\nE&amp; T::operator[](int index);\n\n// prefix ++ -- e.g. ++a\nconst Integer&amp; Integer::operator++(){\n  *this +=1;\n  return *this; // reference for old one, without copying new\n}\n\n// postfix ++ -- e.g. a++\nconst Integer&amp; Integer::operator++(int){\n  Integer old(*this);\n  ++(*this);\n  return old;\n}\n\n// Relational operators\nbool Integer::operator==(const Integer &amp; rhs) const{\n  return i == rhs.i;\n}\n\nbool Integer::operator!=(const Integer &amp; rhs) const{\n  return !(*this == rhs);\n}\n\nbool Integer::operator&lt;(const Integer &amp; rhs) const{\n  return !(i &lt; rhs.i);\n}\n\nbool Integer::operator&gt;(const Integer &amp; rhs) const{\n  return rhs &lt; *this;\n}\n\nbool Integer::operator&lt;=(const Integer &amp; rhs) const{\n  return !(rhs &lt; *this);\n}\n\nbool Integer::operator&gt;=(const Integer &amp; rhs) const{\n  return !(*this &lt; rhs);\n}\n\n// operator [] must be member function, single argument\n</code></pre> <ul> <li>Extractor &amp; Inserter</li> </ul> Extractor &amp; Inserter<pre><code>// stream extractor cin &gt;&gt;: global function\noperator&gt;&gt;(istream &amp;is, T&amp; obj){\n  // ...\n  return is; // always this\n}\n\ncin &gt;&gt; a &gt;&gt; b // ((cin &gt;&gt; a ) &gt;&gt; b)\n\n// stream inserter cout &lt;&lt;: global function\noperator&lt;&lt;(ostream&amp; os, const T&amp; obj){\n  // ...\n  return os;\n}\n</code></pre> <ul> <li>Assignment <code>=</code></li> </ul> =<pre><code>// member function\n// usually before calling \"=\" the object being assigned already has had sth. \nT &amp; T::operator=(con T &amp; rhs){\n  // check for self assignment\n  if(*this != ths) // otherwise will cause error (you will delete allocated memory and new one according to the deleted memory)\n  {\n    // ...\n  }\n  return *this;\n}\n</code></pre>"},{"location":"courses/cpp/cpp/#inheritance","title":"Inheritance","text":"<p>Allow sharing of design for</p> <ul> <li>Member data</li> <li>Member functions</li> <li>Interfaces</li> </ul> <p>Advantages:</p> <ul> <li>extendable</li> <li>avoid code duplication</li> <li>code reuse </li> </ul> <p>B is a A:</p> <ul> <li>A: Base/super/parent class</li> <li>B: derived/sub/child class</li> </ul> employee.h<pre><code>class Employee\n{\npublic: \n  Employee(const string&amp; _name, const string &amp; _ssn): name(_name), ssn(_ssn){}\n\n  void print() const\n{\n  cout &lt;&lt; name &lt;&lt; endl;\n  cout &lt;&lt; ssn &lt;&lt; endl;\n}\n\n  void print(const string &amp; msg) const {\n  cout &lt;&lt; msg &lt;&lt; endl;\n  print(); // we rewrite the print function, then the child cannot access print from parent!\n// Name Hide!\n}\n\nprotected:\n// private: // can not access from child class\n  const string name;\n  const string ssn;\n}\n</code></pre> manager.h<pre><code>class Manager: public Employee\n{\npublic:\n  Manager(const string &amp; _name, const string &amp; _ssn, const string &amp; _title): Employee(_name, _ssn), title(_title) {}\n\n  const string &amp; getTitle() const\n{\n  return title;\n}\n\n  void print() const\n{\n  Employee::print();\n  cout &lt;&lt; title &lt;&lt; end;\n}\n\nprotected:\n  const string title;\n}\n</code></pre>"},{"location":"courses/cpp/cpp/#polymorphism","title":"Polymorphism","text":""},{"location":"courses/cpp/cpp/#up-casting","title":"up-casting | \u9020\u578b","text":"<ul> <li>cast: \u7c7b\u578b\u8f6c\u6362</li> </ul> C++<pre><code>int i = (int)3.62;\n</code></pre> <p>But up-casting is</p> <ul> <li>is the act of converting from a derived reference or pointer to a base class reference or pointer.</li> <li>take an object of a derived class as an object of the base one.</li> </ul> <p>(\u6539\u53d8\u4e86\u773c\u5149\uff0c\u4e0d\u6539\u53d8\u5185\u5bb9)</p> <p>encapsulation \u5c01\u88c5\uff1b\u5305\u88c5\uff1b capsulation \u5c01\u88c5\uff1b[\u9ad8\u5206\u5b50] \u5305\u56ca\u5316\u4f5c\u7528\uff1b bonding \u90a6\u5b9a</p>"},{"location":"courses/cpp/cpp/#binding","title":"Binding | \u7ed1\u5b9a","text":"<p>binding: which function to be called</p> <ul> <li>Static binding: call the function as the code(quick) - Non-virtual func</li> <li>Dynamic binding: call the function of the object(slow) - Virtual func</li> </ul> C++<pre><code>Manager Pete(\u201cPete\u201d,\u201d4\u201d, \u201cBakery\u201d);\nEmployee* ep = &amp;pete;\nEmployee &amp; er = pete;\n</code></pre>"},{"location":"courses/cpp/cpp/#override","title":"override | \u8986\u76d6","text":"C++<pre><code>class A\n{\nprotected:\n  int i;\npublic:\n  A() {i=10;}\n  virtual void f() {cout&lt;&lt; \u201cA::f()\u201d&lt;&lt; endl;}\n}\n\nclass B: public A\n{\npublic:\n  int i;\npublic: \n  B(){ i=20; cout&lt;&lt; \u201cB::i\u201d &lt;&lt; i &lt;&lt;endl;}\n  void f() {cout&lt;&lt; \u201cB::f()\u201d&lt;&lt; endl;}\n}\n\ncout &lt;&lt; sizeof(A) &lt;&lt;\u201c, \u201c &lt;&lt; sizeof(B) &lt;&lt; endl;\n// not just 4, 8 but a complex one!\n</code></pre> <ul> <li>polymorphic variable(* &amp;)</li> </ul> <p>static type  dynamic type</p>"},{"location":"courses/cpp/cpp/#virtual-function","title":"virtual function","text":"<p>a class which has a virtual function: vtable -&gt; table of the address of virtual functions(8 \u4f4dfor 64) (this is formed while compiling)</p> C++<pre><code>B b;\nA * p = &amp;b;\nlong long **vp (long long **)p;\n// vp is a pointer to *long long, that is, to a pointer of type long long.\n\nvoid (*pf)() = (void (*)())(**vp);\n// pf is a function pointer, which matches the definition of f()\n\npf(); // \u201cB::f()\u201d\n</code></pre> <p>Initialize: A will create A\u2019s vtable but B will than create B\u2019s vtable and change the point to the vtable.</p>"},{"location":"courses/cpp/cpp/#slice-off","title":"Slice off","text":"<ul> <li>will copy the content of child object to the parent object while ignoring the extra thins of the child.</li> </ul> <p>C++<pre><code>a = b;\np = &amp;a;\np-&gt;f(); // still execute A\u2019s func\n</code></pre> -   Never redefine an inherited non-virtual function. -   Never redefine an inherited default parameter value.</p> <p>Abstract base classes:</p> <ul> <li>has pure virtual functions</li> <li>Cannot be instantiated</li> </ul> <p>C++<pre><code>virtual void f() =0;\n</code></pre> - Multiple inheritance Day No</p>"},{"location":"courses/cpp/cpp/#dynamic-memory-allocation","title":"Dynamic memory allocation","text":""},{"location":"courses/cpp/cpp/#new-delete","title":"<code>new</code> &amp; <code>delete</code>","text":"<p><code>new</code> (operator) has 2 steps:</p> <ul> <li>allocate space</li> <li>call the constructor function</li> </ul> C++<pre><code>int *p1 = new int // malloc(sizeof(int)); + constructor\nint *p2 = new int [10] // continuous space allocated\n</code></pre> <p><code>delete</code> + pointer</p> <p>If delete a constructed type, it will implement <code>D\u2019tor</code> function</p> <p>C++<pre><code>p2 = new student [10];\ndelete p2; // remove the first one\ndelete[] p2; // remove whole 10 objects\n</code></pre> it is safe to delete a <code>NULL</code>.</p>"},{"location":"courses/cpp/cpp/#copy-constructor","title":"Copy constructor","text":"<p>has a unique signature</p> <p>C++<pre><code>T::T(const T&amp;)\n</code></pre> call by reference</p> <p>compiler (in-line) would do it automatically.</p> <p>But how?</p> <ul> <li>member-wise (versus bit-wise)</li> </ul> <p>if it has a class defined, it will iteratively call its copy function.</p> <p>It is neccesary to define a copy constructor when you have pointer member or what you don't want to be copied in your class.</p> C++<pre><code>A b(a);\nA c = a;\n\n// define function passing in value\nvoid f(A aa)\n{\n\n}\n\n// return value\nA f()\n{\n  A aa(19);\n  return aa;\n}\n</code></pre> <p>Assignment: can be done alot of time. Ctor can only be called once.</p>"},{"location":"courses/cpp/cpp/#template","title":"Template(\u5143\u4ee3\u7801)","text":"<p>Reuse source code. It generates code for compiling.</p> <ul> <li>Generic programming(\u6cdb\u578b, universal type)vs \u8303\u5f62 model shape. Use type as parameters in class or function definitions.</li> </ul> <p>Function Template | \u51fd\u6570\u6a21\u677f</p> C++<pre><code>void myswap(int &amp;x, int &amp;y) // only calls when passing in \"int\"\n{\n  int temp = x;\n  x = y;\n  y = temp;\n}\n\n// T is tyep parameter class\ntemplate &lt;class T&gt;\nvoid myswap(T &amp;x, T&amp;y)\n{\n  T temp = x;\n  x = y;\n  y = temp;\n}\n\nint main()\n{\n  int a=6,a=5;\n  double c=1,d=2;\n  myswap(a.b); // right\n  myswap(a,c); // error, parameters a,c must be the same type\n}\n\n// but if we do this\ntemplate &lt;class T1,class T2&gt;\nvoid myswap(T1 &amp;x, T2&amp;y)\n{\n  T1 temp = x;\n  x = y;\n  y = temp;\n}\n</code></pre> <p>compiler need to know the type <code>T</code> if we do not explicitly give varible of type.</p> C++<pre><code>template&lt;class T&gt;\nvoid f()\n{\n  T a;\n}\n\nint main()\n{\n  f&lt;double&gt;();\n}\n</code></pre> <p>Class Template | \u7c7b\u6a21\u677f</p> <p>A class declaration.</p> <p>All the functions in the template are function template.</p> C++<pre><code>template &lt;class T&gt;\nclass vector{\n  public:\n  vector(int s)size(s){}\n  T&amp; operator[](int s);\n};\n\n// definition\ntemplate &lt;class T&gt;\nT&amp; vector&lt;T&gt;::operator[](int s){\n  return 0;\n}\n\nTemplate nest:\n\n```c++\nvector&lt; vector&lt;double*&gt; &gt;\n</code></pre> <p>Template arguments can be constant expresstions, Non-type parameters with a default argument.</p> C++<pre><code>template&lt;class T, int bounds=100&gt;\nclass FixedVector{\npublic:\n  FixedVector();\n  T &amp; operator[](int);\nprivate:\n  T elements[bounds]; \n};\n\ntemplate&lt;class T, int bounds=100&gt;\nT &amp; FixedVector&lt;T, bounds&gt;::operator[](int i){\n  return elements[i];\n}\n\nint main()\n{\n  FixedVector&lt;int, 10&gt; v1;\n  FixedVector&lt;int&gt; v2; // use default parameter 100\n\n}\n</code></pre> <p>All the content of template should be put in <code>.h</code> file for they are only declarations. Remember we also have to put inline function and static member variables in <code>.h</code> file.</p>"},{"location":"courses/cpp/cpp/#exception","title":"Exception","text":"<ul> <li>Exception type</li> </ul> C++<pre><code>class VectorIndexError\n{\npublic:\n  VectorIndexError(int v):m_badValue(v){}\n  VectorIndexError({}\n  void diagnostic(){\n    cout&lt;&lt;\"index\"&lt;&lt; m_badValue &lt;&lt;\"out of range!\";\n  }\nprivate:\n  int m_badValue;\n}\n</code></pre> <p>C++<pre><code>throw &lt;&lt;something&gt;&gt;\n</code></pre> <code>throw</code> raises exception.</p> <ul> <li>Try blocks can select type of exceptions</li> </ul> C++<pre><code>try {...}\n  catch {...}\n</code></pre> C++<pre><code>try{\n  func();\n} catch(VIE v){ // take a single argument\n  cout&lt;&lt; \"8\\n\";\n} catch (...){ // others\n  cout&lt;&lt; \"7\\n\";\n}\n</code></pre> <p>it can re-raise exceptions</p> <ul> <li>Hierarchy of exception types</li> </ul> C++<pre><code>class MathErr{\n  ...\n  virtual void diagnostic();\n};\n\nclass OverflowErr : public MathErr {...}\nclass UnderflowErr : public MathErr {...}\n</code></pre> <p>a catch of parent Exception Type can catch its child type.</p> C++<pre><code>try{\n  throw VIEE(idx);\n}catch (VIE v){\n  cout&lt;&lt; \"7\\n\";\n}catch (...){\n  cout&lt;&lt; \"6\\n\";\n}\n// output 7\n\ntry{\n  throw VIEE(idx);\n}catch (VIEE v){\n  cout&lt;&lt; \"8\\n\";\n}catch (VIE v){\n  cout&lt;&lt; \"7\\n\";\n}catch (...){\n  cout&lt;&lt; \"6\\n\";\n}\n// output 8\n\ntry{\n  throw VIEE(idx);\n}catch (VIE v){\n  cout&lt;&lt; \"8\\n\";\n}catch (VIEE v){ // this expression is useless\n  cout&lt;&lt; \"7\\n\";\n}catch (...){\n  cout&lt;&lt; \"6\\n\";\n}\n// output 8\n</code></pre> <ul> <li> <p>Standard Library Exception</p> </li> <li> <p>Failure in C'tor</p> </li> </ul>"},{"location":"courses/mpm/","title":"Material Point Method","text":"<p>Reference</p> <p>The Material Point Method for Simulating Continuum Materials, Chenfanfu Jiang, SIGGRAPH 2016 Course Notes Version 1 (May 2016).</p>"},{"location":"courses/mpm/#preliminary","title":"Preliminary","text":""},{"location":"courses/mpm/#contimuum-motion","title":"Contimuum Motion","text":"<p>Here \\(d\\) denotes dimension of the space, usually \\(d=2,3\\).</p> <p>We denote \\(\\pmb{x}\\in \\Omega^t\\subset \\mathbb{R}^d\\) world (deformed) space(coordinates), current position. Physically, we focus on a fixed position in the space and measurethe velocity of whichever particla that is passing by the position.</p> <p>Then we denote \\(\\pmb{X}\\in \\Omega^0\\subset \\mathbb{R}^d\\) material (undeformed) space(coordinates), or initial position. Physically, we measure velocity from a fixed particle, which has its mass and occupies some volume since the beginning.</p> <p>Define </p> \\[ \\pmb{x}=\\phi(\\pmb{X},t) \\] <p>which is usually a bijection. This is associated with the assumption that no two different particles of material ever occupy the same space at the same time. So </p> \\[ \\forall \\pmb{x}\\in \\Omega^t, \\exists ! \\pmb{X}\\in \\Omega^0, \\text{ s.t. } \\phi(\\pmb{X},t)=\\pmb{x}. \\] <p></p> <p>Example</p> <p>For a given initial position \\(\\pmb{X}\\), we have some common function of \\(\\pmb{x}\\).</p> <p>(i) \\(\\pmb{x}=\\pmb{X}+tv(t)\\vec{N}\\)</p> <p>discrete form at time \\(n\\):</p> \\[ \\pmb{x}^{(n)}=\\pmb{x}^{(n-1)}+\\Delta t v^{(n)} \\vec{N}^{(n)} \\] <p>(ii) \\(\\pmb{x}=R\\pmb{X}+\\vec{b}\\)</p> <p>discrete form at time \\(n\\):</p> \\[ \\pmb{x}^{(n)}=R^{(n-1)}\\pmb{x}^{(n-1)}+\\vec{b}^{(n)} \\] <p>The velocity of a given material point \\(\\pmb{X}\\) at time \\(t\\) is a mapping \\(V(\\cdot,t):\\Omega^0\\rightarrow \\mathbb{R}^d\\) defined by</p> \\[ \\pmb{V}(\\pmb{X},t)\\overset{\\Delta}{=}\\frac{\\partial \\phi(\\pmb{X},t)}{\\partial t} \\] <p>and acceleration \\(A(\\cdot,t):\\Omega^0\\rightarrow \\mathbb{R}^d\\)</p> \\[ \\begin{align*} \\pmb{A}(\\pmb{X},t)&amp;\\overset{\\Delta}{=}\\frac{\\partial \\pmb{V}(\\pmb{X},t)}{\\partial t}\\\\ &amp;=\\frac{\\partial^2 \\phi(\\pmb{X},t)}{\\partial t^2} \\end{align*} \\]"},{"location":"courses/mpm/#deformation","title":"Deformation","text":"<p>For every small unit in a material, it has a deformation mapping :</p> \\[ \\pmb{F}(\\pmb{X},t)\\overset{\\Delta}{=}\\frac{\\partial \\pmb{x}}{\\partial \\pmb{X}} \\in \\mathbb{R}^{d\\times d} \\] <p>Note that \\(F\\) is also related with both \\(\\pmb{X}\\) and \\(t\\). And \\(J=\\det{(F)}\\) characterizes infinitesimal volume change.</p> <p>In Example we have \\(\\pmb{F}=I\\) (i) and \\(\\pmb{F}=R\\) (ii).</p> <p>The above is rigid transformation, and local.</p>"},{"location":"courses/mpm/#push-forward-and-push-back","title":"Push forward and Push back","text":"<p>Push forward of a function, is often referred to as Eulerian (a function of \\(\\pmb{x}\\)). That is, given \\(G: \\Omega^0\\rightarrow \\mathbb{R}\\), the push forward \\(g(\\cdot, t):\\Omega^t\\rightarrow \\mathbb{R}\\) is defined</p> \\[ g(\\pmb{x})=G(\\phi^{-1}(\\pmb{x}),t) \\] <p>Push back function is often referred to as Lagrangian (a function of \\(\\pmb{X}\\)). That is, given \\(g:\\Omega^t\\rightarrow \\mathbb{R}\\), the push back \\(G(\\cdot,t):\\Omega^0\\rightarrow \\mathbb{R}\\) is defined </p> \\[ G(\\pmb{X})=g(\\phi(\\pmb{x}), t) \\] <p>But we usually do not use push back but push forward more frequently since we can always have access to the grid velocity and acceleration based on the particles around it.</p> <p>So it is useful to define Eulerian counterparts. The velocity</p> \\[ \\pmb{v}(\\pmb{x},t)=\\pmb{V}(\\phi^{-1}(\\pmb{x},t),t) \\] <p>and the acceleration</p> \\[ \\pmb{a}(\\pmb{x},t)=\\pmb{A}(\\phi^{-1}(\\pmb{x},t),t) \\] <p>The following result is really important:</p> \\[ \\begin{align*} a_i(\\pmb{x},t)&amp;=A_i(\\phi^{-1}(\\pmb{x},t), t)\\\\ &amp;=\\frac{\\partial V_i}{\\partial t}(\\phi^{-1}(\\pmb{X}, t),t)\\\\ &amp;=\\frac{\\partial v_i}{\\partial t}(\\pmb{x}, t)+\\sum_{j=1}^d\\frac{\\partial v_i}{\\partial x_j}(\\pmb{x},t)v_j(\\pmb{x},t)\\\\ &amp;\\overset{\\Delta}{=}\\frac{D}{Dt}v_i(\\pmb{x},t) \\end{align*} \\] <p>So </p> \\[ \\pmb{a}=\\frac{D}{Dt}\\pmb{v} \\] <p>More generally, for a general Eulerian function \\(f(\\cdot, t):\\Omega^t\\rightarrow \\mathbb{R}\\), we can use this same notation to mean</p> \\[ \\frac{D}{Dt}f(\\pmb{x},t)=\\frac{\\partial f}{\\partial t}(\\pmb{x}, t)+\\sum_{j=1}^d\\frac{\\partial f}{\\partial x_j}(\\pmb{x},t)v_j(\\pmb{x},t) \\] <p>which is called material derivative, and the first item of derivative is called local rate of change. The above one is also the push forward of \\(\\partial F/\\partial t\\) where \\(F\\) is a Lagrangian function with \\(F(\\cdot,t):\\Omega^0\\rightarrow \\mathbb{R}\\).</p>"},{"location":"courses/mpm/#relationship-between-two-deformation-gradients","title":"Relationship between two Deformation Gradients","text":"<p>For deformation gradient, most of the time in the physics of a material, the Lagrangian view is the dominant one. There is however a useful evolution of the Eulerian (push forward) of \\(\\pmb{F}(\\cdot,t):\\Omega^0\\rightarrow \\mathbb{R}^{d\\times d}\\). If we denote \\(\\pmb{f}(\\cdot, t):\\Omega^t\\rightarrow \\mathbb{R}^{d\\times d}\\) be the push forward of \\(\\pmb{F}\\), then</p> \\[ \\frac{D}{Dt}\\pmb{f}(\\pmb{x},t) = \\frac{\\partial \\pmb{v}}{\\partial \\pmb{x}}\\pmb{f} \\] <p>or </p> \\[ \\dot{\\pmb{F}}=(\\nabla \\pmb{v})\\pmb{F} \\] <p>cause we have</p> \\[ \\begin{align*} \\frac{\\partial}{\\partial t}F_{ij}(\\pmb{X}, t)&amp;=\\frac{1}{\\partial t }\\frac{\\partial \\phi_i}{\\partial X_j}(\\pmb{X},t)\\\\ &amp;=\\frac{1}{\\partial X_j}\\frac{\\partial \\phi_i}{ \\partial t }(\\pmb{X},t)\\\\ &amp;=\\frac{\\partial V_i}{\\partial X_j}(\\pmb{X},t)\\\\ &amp;=\\frac{\\partial v_i}{\\partial X_j}(\\phi(\\pmb{X},t),t)\\\\ &amp;=\\sum_{k=1}^d\\frac{\\partial v_i}{\\partial x_k}(\\phi(\\pmb{x},t),t)\\cdot \\frac{\\partial \\phi_k}{\\partial X_j}(\\pmb{X},t)\\\\ &amp;=\\sum_{k=1}^d\\frac{\\partial v_i}{\\partial x_k}(\\phi(\\pmb{x},t),t)\\cdot F_{kj}(\\pmb{X},t) \\end{align*} \\] <p>The above equation will play an important role in deriving the discretized deformation gradient \\(F\\) update on each MPM particle.</p>"},{"location":"courses/mpm/#volume-and-area-change","title":"Volume and Area Change","text":"<ul> <li>Volume</li> </ul> <p>Consider \\(dV\\) being defined over the standard basis vectors \\(\\vec{e}_i\\), \\(i=1.2.3\\), with \\(dV=dL_1\\vec{e}_1\\cdot (dL_2\\vec{e}_2 \\times dL_3\\vec{e}_3)\\). If we denote \\(d\\pmb{L}_i=dL_i\\vec{e}\\), then </p> \\[ dV=dL_1dL_2dL_3, \\quad  \\] <p>cause we have \\(d\\pmb{l}_i=\\pmb{F}d\\pmb{L}_i\\) (like \\((\\pmb{x}_2-\\pmb{x}_1)=\\pmb{F}(\\pmb{X_2}-\\pmb{X}_1)\\)), so it can be shown that \\(dl_1dl_2dl_3=JdL_1dL_2dL_3\\) or </p> \\[ dv=JdV. \\] <p>Based on the above property we can have a great weapon in proof. Given function \\(G(\\pmb{X})\\) or \\(g(\\pmb{x},t)\\) we have </p> \\[ \\int_{B^t}g(\\pmb{x})d\\pmb{x}=\\int_{B^0}G(\\pmb{X})J(\\pmb{X},t)d\\pmb{X} \\] <p>Actually it is the variable substitution formula of multiple integral.</p> <ul> <li>Areas</li> </ul> <p>And similar analysis can be done for areas.</p> \\[ \\begin{align*} dv&amp;=JdV\\\\ \\pmb{n}ds\\cdot d\\pmb{l}&amp;=J\\pmb{N}dS\\cdot d\\pmb{L}\\\\ \\pmb{n}ds\\cdot \\pmb{F}d\\pmb{L}&amp;=J\\pmb{N}dS\\cdot d\\pmb{L}\\quad \\text{using $d\\pmb{l}=\\pmb{F}d\\pmb{L}$}\\\\ \\pmb{n}ds\\cdot \\pmb{F}&amp;=J\\pmb{N}dS \\end{align*} \\] <p>So </p> \\[ \\begin{equation} \\pmb{n}ds=J\\pmb{F}^{-T}\\pmb{N}dS.\\label{eq-area} \\end{equation} \\] <p>which is also called Nansom's formula.</p>"},{"location":"courses/mpm/#piola-kirchhoff-stress","title":"Piola-Kirchhoff Stress","text":"<p>Consider a vector element of surface in the material world(Lagrangian), \\(\\pmb{N}dS\\) and after deformation, the material particles making up this area now occupy the element defined by \\(\\pmb{n}ds\\), where \\(ds\\) is the area and \\(\\pmb{n}\\) is the normal vector in the current configuration(Eulerian).</p> <p>Then by definition of the Cauchy stress</p> \\[ d\\pmb{f}=\\pmb{\\sigma}\\pmb{n}ds \\] <p>The first Piola-Kirchhoff stress tensor \\(\\pmb{P}\\) (PK1 stress, Nominal Stress tensor) is defined by</p> \\[ d\\pmb{f}=\\pmb{P}\\pmb{N}dS \\] <p>which relates the force acting in the current configuration to the surface element in the reference configuration.</p> <p>So similarly with (Cauchy) traction vector was defined by</p> \\[ \\pmb{t}=\\frac{d\\pmb{f}}{ds} \\] <p>we can introduce a PK1 traction vector</p> \\[ \\pmb{T}=\\frac{d\\pmb{f}}{dS}, \\quad \\pmb{T}=\\pmb{P}\\pmb{N} \\] <p>which is a fictitious quatity.</p> <p>Note that \\(d\\pmb{f}=\\pmb{t}ds=\\pmb{T}dS\\), so \\(\\pmb{t}\\) and \\(\\pmb{T}\\) at the same area has the same direction but different magnitudes.</p> <p> </p> <ul> <li>Relation between the Cauchy and PK1 Stresses</li> </ul> <p>From the above definition,</p> \\[ \\begin{align*} \\pmb{P}\\pmb{N}dS&amp;=\\pmb{\\sigma}\\pmb{n}\\\\ \\pmb{P}\\pmb{N}dS&amp;=\\pmb{\\sigma}J\\pmb{F}^{-T}\\pmb{N}dS\\quad\\text{using equation $\\ref{eq-area}$}\\\\ \\pmb{P}&amp;=J\\pmb{\\sigma}\\pmb{F}^{-T}\\\\ \\pmb{\\sigma}&amp;=J^{-1}\\pmb{P}\\pmb{F}^T \\end{align*} \\]"},{"location":"courses/mpm/#hyperelasticity","title":"Hyperelasticity","text":"<p>Stress is related to strain(Deformation gradient \\(F\\)) through \"Constitutive Relationship\". </p> <p>For perfect hyperelastic materials, the relation is defined through the potential energy, which increases with non-rigid deformation from the initial state. That is, The elastic solids whose first Piola-Kirchoff stress \\(\\pmb{P}\\) can be derived from an strain energy density function \\(\\Psi(\\pmb{F})\\) (a scalar function) via</p> \\[ \\pmb{P}=\\frac{\\partial \\Psi}{\\partial \\pmb{F}} \\] <p>and Cauchy stress with respect to \\(\\Psi(\\pmb{F})\\)</p> \\[ \\sigma=\\frac{1}{J}\\pmb{P}\\pmb{F}^T=\\frac{1}{J}\\frac{\\partial \\Psi}{\\partial \\pmb{F}}\\pmb{F}^T \\]"},{"location":"courses/mpm/#energy-density-function","title":"Energy density function","text":"<ul> <li>Neo-Hookean</li> </ul> \\[ \\Psi(\\pmb{F})=\\frac{\\mu}{2}(\\text{tr}{(\\pmb{F}^T\\pmb{F})}-d)-\\mu\\log(J)+\\frac{\\lambda}{2}\\log^2(J) \\] <p>where \\(d\\) denoted dimension, \\(2\\) or \\(3\\) in practice and </p> \\[ \\mu=\\frac{E}{2(1+v)},\\quad \\lambda=\\frac{Ev}{(1+v)(1-2v)} \\] <p>in which \\(E\\) is Young's modulus and \\(v\\) is Poisson's ratio.</p> <ul> <li>Fixed corotated model</li> </ul> <p>This is derived from the Singular Value Decomposition (SVD). Assuming the polar SVD \\(\\pmb{F}=U\\Sigma V^T\\), then the energy for fixed corotated model is </p> \\[ \\Psi(\\pmb{F})=\\hat{\\Psi}(\\Sigma(\\pmb{F}))=\\mu\\sum_{i=1}^d(\\sigma_i-1)^2+\\frac{\\lambda}{2}(J-1)^2 \\] <p>where \\(J=\\prod\\limits_{i=1}^d\\sigma_i\\) and </p> \\[ \\pmb{P}(\\pmb{F})=\\frac{\\partial\\Psi}{\\partial \\pmb{F}}(\\pmb{F})=2\\mu(\\pmb{F}-\\pmb{R})+\\lambda(J-1)J\\pmb{F}^{-T}. \\]"},{"location":"courses/mpm/#governing-equations","title":"Governing Equations","text":"<p>Let </p> \\[ \\pmb{V}(\\pmb{X},t)=\\frac{\\partial \\phi(\\pmb{X},t)}{\\partial t}=\\frac{\\partial{\\pmb{x}}}{\\partial t} \\] <p>be the velocity defined over \\(X\\). Then from Lagrangian view of point, the equations are</p> \\[ \\begin{cases} \\displaystyle R(\\pmb{X},t)J(\\pmb{X},t)=R(\\pmb{X},0) \\quad &amp;\\text{Conservation of mass},\\\\ \\displaystyle R(\\pmb{X},t)\\frac{\\partial \\pmb{V}}{\\partial t}=\\nabla^{\\pmb{X}}\\cdot \\pmb{P}+R(\\pmb{X},0)g \\quad &amp;\\text{Conservation of momentum}. \\end{cases} \\] <p>where \\(R\\) is the Lagrangian mass density which is related to the more commonly used Eulerian mass density \\(\\rho\\). Note that mass conservation can also be written as </p> \\[ \\frac{\\partial }{\\partial t}[R(\\pmb{X},t)J(\\pmb{X},t)]=0 \\] <p>In Eulerian view, the governing equations are</p> \\[ \\begin{cases} \\displaystyle \\frac{D}{Dt}\\rho(\\pmb{x},t)+\\rho(\\pmb{x},t)\\nabla^{\\pmb{x}}\\cdot \\pmb{v}(\\pmb{x},t)=0,\\quad &amp;\\text{Conservation of mass},\\\\ \\displaystyle \\rho(\\pmb{x},t)\\frac{D\\pmb{v}}{Dt}=\\nabla^{\\pmb{x}}\\cdot \\sigma +\\rho(\\pmb{x},t)g, \\quad &amp;\\text{Conservation of momentum}.\\\\ \\end{cases} \\] <p>where </p> \\[ \\frac{D}{Dt}=\\frac{\\partial}{\\partial t}+\\pmb{v}\\cdot \\nabla^{\\pmb{x}} \\]"},{"location":"courses/mpm/#conservation-of-mass","title":"Conservation of Mass","text":"<p>To be more specific, we have two ways to get the Continuity Equation.</p> HintsFrom global viewFrom material view <p>By using multiple integral for density.</p> <p>Notice that </p> \\[ \\rho(\\pmb{x},t)=\\lim_{\\varepsilon\\rightarrow 0^+}\\frac{\\text{mass}(B_{\\varepsilon}^t)}{\\int_{B_{\\varepsilon}^t}d\\pmb{x}} \\] <p>We choose a fixed volume, in which the rate of increase of mass must equal the rate at which mass is flowing into the volume through its bounding surface.</p> <p>The rate of increase mass in a fixed volume \\(v\\) is</p> \\[ \\frac{\\partial m}{\\partial t}=\\frac{\\partial}{\\partial t}\\int_{v}\\rho(\\pmb{x},t)dv=\\int_v \\frac{\\rho(\\pmb{x},t)}{\\partial t}dv \\] <p>while the mass flux out through the surface is given by</p> \\[ \\int_s \\rho \\pmb{v}\\cdot \\pmb{n} ds \\] <p>here \\(s\\) can denote the overall outer surface of the material(we consider in and out).</p> <p>so combine the above two we get</p> \\[ \\int_v \\frac{\\rho(\\pmb{x},t)}{\\partial t}dv + \\int_s \\rho \\pmb{v}\\cdot \\pmb{n} ds=0 \\] <p>using divergence theorem, we get</p> \\[ \\begin{align*} \\int_v \\frac{\\rho(\\pmb{x},t)}{\\partial t}dv + \\int_v \\nabla^{\\pmb{x}}\\cdot(\\rho \\pmb{v}) dv&amp;=0\\\\ \\frac{\\rho(\\pmb{x},t)}{\\partial t}+ \\nabla^{\\pmb{x}}\\cdot(\\rho \\pmb{v}) &amp;=0\\\\ \\frac{\\partial \\rho(\\pmb{x},t)}{\\partial t}+ (\\nabla^{\\pmb{x}}\\cdot\\rho)\\pmb{v} +(\\nabla^{\\pmb{x}}\\cdot\\pmb{v})\\rho &amp;=0\\\\ \\frac{D\\rho}{Dt} +\\rho \\nabla^{\\pmb{x}}\\cdot(\\pmb{v})&amp;=0 \\end{align*} \\] <p>We check the fixed volume in terms of material point. Note \\(R(\\pmb{X},t)=\\rho(\\phi(\\pmb{X},t),t)\\), then we see the mass from initial time to time \\(t\\) must be the same:</p> \\[ \\begin{align*} \\text{mass}(v)&amp;=\\text{mass}V_0\\\\ \\int_{v}\\rho(\\pmb{x},t)d\\pmb{x}&amp;=\\int_{V}R(\\pmb{X},0)d\\pmb{X}\\\\ \\Rightarrow \\int_{V}R(\\pmb{X},t)J(\\pmb{X},t)d\\pmb{X}&amp;=\\int_{V}R(\\pmb{X},0)d\\pmb{X} \\end{align*} \\] <p>So </p> \\[ R(\\pmb{X},t)J(\\pmb{X},t)=R(\\pmb{X},0), \\quad \\forall \\pmb{X}\\in \\Omega^0, t\\geq 0 \\] <p>Note that \\(J(\\pmb{X},0)=1\\), so \\(R(\\pmb{X},t)J(\\pmb{X},t)=R(\\pmb{X},0)J(\\pmb{X},0)\\), that is, </p> \\[ \\frac{\\partial }{\\partial t}[R(\\pmb{X},t)J(\\pmb{X},t)]=0. \\]"},{"location":"courses/mpm/#conservation-of-momentum","title":"Conservation of Momentum","text":"<p>Also, we have two ways to consider.</p> HintsFrom glabal viewFrom material view <p>Use the similar logic in proof of mass conservation.</p> <p>This is also the spatial form.</p> <p>We know that</p> \\[ \\pmb{t}(\\pmb{x},\\pmb{n},t)=\\pmb{\\sigma}(\\pmb{x},t)\\pmb{n} \\] <p>We consider a fixed mass, so the space occupied by this matter may change over time.</p> <p>If \\(\\pmb{v}\\) denotes the Eulerian velocity, then the linear momentum of Euler can be denoted as</p> \\[ \\pmb{L}(t)=\\int_{B_{\\varepsilon}^t} \\rho\\pmb{v}d\\pmb{x}, \\] <p>where </p> <p>then by \\(\\pmb{f}^{ext}=\\frac{d(m\\pmb{v})}{dt}\\) formulated by Euler,</p> \\[ \\begin{align*} \\int_{\\partial B_{\\varepsilon}^t}\\pmb{\\sigma}\\pmb{n}ds+\\int_{B_{\\varepsilon}^t}\\pmb{f}^{ext}d\\pmb{x}&amp;=\\frac{d}{dt}\\int_{B_{\\varepsilon}^t}\\rho \\pmb{v}d\\pmb{x}\\\\ \\int_{B_{\\varepsilon}^t}\\nabla^{\\pmb{x}}\\cdot \\sigma+\\pmb{f}^{ext} d\\pmb{x}&amp;=\\frac{d}{dt}\\int_{B_{\\varepsilon}^0}R\\pmb{V}Jd\\pmb{X}\\\\ &amp;=\\int_{B_{\\varepsilon}^0}R\\pmb{A}Jd\\pmb{X}\\\\ &amp;=\\int_{B_{\\varepsilon}^t}\\rho\\pmb{a}d\\pmb{x} \\end{align*} \\] <p>Note \\(\\pmb{V}(\\pmb{X},t)=\\pmb{v}(\\phi(\\pmb{X},t),t)\\), so we have a fixed volume \\(V\\) with momentum </p> \\[ \\pmb{L}(t) = \\int_V R(\\pmb{X},t)V(\\pmb{X},t)dV \\] <p>So </p> \\[ \\begin{align*} \\frac{\\partial}{\\partial t}\\int_V R(\\pmb{X},t)V(\\pmb{X},t)dV &amp;=\\int_S\\pmb{T}dS +\\int_V \\pmb{F}dV\\\\ \\int_V R(\\pmb{X},t)\\frac{\\partial}{\\partial t}V(\\pmb{X},t)dV&amp;= \\int_S \\pmb{P}\\cdot\\pmb{N}dS+\\int_V\\pmb{F} dV\\\\ \\int_V R(\\pmb{X},t)\\frac{\\partial}{\\partial t}V(\\pmb{X},t)dV&amp;= \\int_V \\nabla^{\\pmb{X}}\\cdot \\pmb{P}+\\pmb{F} dV\\\\ \\Rightarrow R(\\pmb{X},t)\\frac{\\partial}{\\partial t}V(\\pmb{X},t) &amp;=\\nabla^{\\pmb{X}}\\cdot \\pmb{P}+\\pmb{F} \\end{align*} \\]"},{"location":"courses/mpm/#material-particles","title":"Material particles","text":"<p>Recall that the material point method is Lagrangian in the sense that we track actual particles of material. That is we keep track of mass (\\(m_p\\)), velocity (\\(v_p\\)) and position (\\(x_p\\)) for a collection of material particles \\(p\\).</p> <p>However, all stress based forces are computed on the Eulerian grid, so we have to transfer the material state to the Eulerian configuration to incorporate the effects of material forces. </p> <p>Then, we transfer these effects back to the material particles and move them in the normal Lagrangian way. The Lagrangian nature makes advection very trivial compared to pure Eulerian methods (such as grid-based fluid simulation).</p>"},{"location":"courses/mpm/#eulerian-interpolating-functions","title":"Eulerian Interpolating Functions","text":"<p>We can denote the interpolation function at grid node \\(\\pmb{i}=(i,j,k)\\) evaluated at a particle location \\(\\pmb{x}_p\\) with </p> \\[ N_{\\pmb{i}}(\\pmb{x}_p)=N\\left(\\frac{1}{h}(\\pmb{x}_p-\\pmb{x}_{\\pmb{i}})\\right)N\\left(\\frac{1}{h}(\\pmb{y}_p-\\pmb{y}_{\\pmb{i}})\\right)N\\left(\\frac{1}{h}(\\pmb{z}_p-\\pmb{z}_{\\pmb{i}})\\right) \\] <p>where \\(h\\) is the grid spacing. We can define diffenrent kernel \\(N:\\mathbb{R}\\rightarrow \\mathbb{R}\\).</p> <p>Common Kernel \\(N\\)</p> <ul> <li>cubic kernel. It is more expensive but provide wider coverage, thus less sensitive to numerical errors.</li> </ul> \\[ N(x)=\\begin{cases} \\displaystyle \\frac{1}{2}|x|^3-|x|^2+\\frac{2}{3}, \\quad &amp;0\\leq |x| &lt; 1\\\\ \\displaystyle \\frac{1}{6}(2-|x|)^3,\\quad &amp;1\\leq |x|&lt; 2\\\\ \\displaystyle 0,\\quad &amp;2\\leq|x| \\end{cases} \\] <ul> <li>quadratic kernel. It is more computational efficient and memory saving.</li> </ul> \\[ N(x)=\\begin{cases} \\displaystyle \\frac{3}{4}-|x|^2,\\quad &amp; \\displaystyle 0\\leq |x| &lt; \\frac{1}{2}\\\\ \\displaystyle \\frac{1}{2}\\left(\\frac{3}{2}-|x|\\right)^2,\\quad &amp;\\displaystyle \\frac{1}{2}\\leq |x|&lt;\\frac{3}{2}\\\\ \\displaystyle 0,\\quad &amp;\\displaystyle \\frac{3}{2}\\leq |x| \\end{cases} \\] <p>Then the gradient of function \\(N_{\\pmb{i}}(\\pmb{x}_p)\\) is </p> \\[ \\nabla N_{\\pmb{i}}(\\pmb{x}_p) =\\sum_{k=1}^d\\left[N'\\left(\\frac{1}{h}(x_k-x_{\\pmb{i}})\\right)\\prod_{j=1\\atop j\\neq k}^d N\\left(\\frac{1}{h}(x_{j}-x_{\\pmb{i}})\\right)\\right] \\] <p>where \\(x_k\\), \\(x_j\\) denote the component index of \\(\\pmb{x}_p\\), i.e. \\(x_k, x_j\\in\\{x_p,y_p,z_p\\}\\).</p>"},{"location":"courses/mpm/#eulerianlagrangian-mass","title":"Eulerian/Lagrangian Mass","text":"<p>Mass of the particle</p> \\[ m_p^n=\\int_{B_{\\Delta x,p}^{t^n}}\\rho(\\pmb{x},t^n)d\\pmb{x} \\] <p>and define the mass from Eulerian perspective</p> \\[ m_{\\pmb{i}}=\\sum_pm_p\\cdot N_{\\pmb{i}}(\\pmb{x}_p) \\] <p>Easy to see that </p> \\[ \\sum_{\\pmb{i}}m_{\\pmb{i}}=\\sum_p m_p \\] <p>since the weight function \\(N_\\pmb{i}(\\pmb{x}_p)\\) is normalized to \\(1\\).</p>"},{"location":"courses/mpm/#eulerian-momentum","title":"Eulerian Momentum","text":"<p>Similarly, we transfer particle monentum \\(m_p\\pmb{v}_p\\) to the grid</p> \\[ (m\\pmb{v})_{\\pmb{i}}=\\sum_p m_p \\pmb{v}_p N_{\\pmb{i}}(\\pmb{x}_p) \\] <p>Also easy to that </p> \\[ \\sum_{\\pmb{i}}(m\\pmb{v})_{\\pmb{i}}=\\sum_{p}m_p\\pmb{v}_p \\] <p>and the Eulerian velocity \\(\\pmb{v}_\\pmb{i}\\) is defined as </p> \\[ \\pmb{v}_{\\pmb{i}}=\\frac{(m\\pmb{v})_{\\pmb{i}}}{m_{\\pmb{i}}} \\]"},{"location":"courses/mpm/#eulerian-to-lagrangian-transfer","title":"Eulerian to Lagrangian Transfer","text":"<p>We do not need to transfer mass from the grid to the particles since Lagrangian particle mass never changes. But the Velocity is simply interpolated as</p> \\[ \\pmb{v}_p =\\sum_{\\pmb{i}}\\pmb{v}_{\\pmb{i}}N_{\\pmb{i}}(\\pmb{x}_p) \\] <p>Easy to see that</p> \\[ \\sum_p m_p \\pmb{v}_p=\\sum_{\\pmb{i}}m_i\\pmb{v}_{\\pmb{i}} \\]"},{"location":"courses/mpm/#discretization","title":"Discretization","text":""},{"location":"courses/mpm/#explicite-time-integration","title":"Explicite time Integration","text":""},{"location":"courses/mpm/#apic-transfers","title":"APIC Transfers","text":"<p>Transfer from particle to grid</p> \\[ \\begin{cases} m_i=\\sum_p w_{ip}m_p\\\\ m_i\\pmb{v}_i=\\sum_p w_{ip}m_p(\\pmb{v}_p+\\pmb{B}_p(\\pmb{D}_p)^{-1}(\\pmb{x}_i-\\pmb{x}_p)) \\end{cases} \\] <p>where \\(\\pmb{B}_p\\) is a matrix quatity stored at each particle(like mass, position and velocity), \\(\\pmb{D}_p\\) is given by </p> \\[ \\pmb{D}_p=\\sum_i w_{ip}(\\pmb{x}_i-\\pmb{x}_p)(\\pmb{x}_i-\\pmb{x}_p)^T \\] <p>which has a simple form \\(\\frac{1}{4}\\Delta x^2\\pmb{I}\\) for quadratic and \\(\\frac{1}{3}\\Delta x^2\\pmb{I}\\) for cubic interpolation stencils.</p> <p>Then from grid to particle</p> \\[ \\begin{cases} \\pmb{v}_p=\\sum_i w_{ip}\\pmb{v}_i\\\\ \\pmb{B}_p=\\sum_i w_{ip}\\pmb{v}_i (\\pmb{x}_i-\\pmb{x}_p)^T \\end{cases} \\]"},{"location":"courses/mpm/#deformation-gradient-update","title":"Deformation Gradient Update","text":"<p>Given \\(f_p\\), we can update the position and velocity of the grid</p> \\[ \\begin{cases} \\pmb{v}_i^{n+1}=\\pmb{v}_i^n+\\Delta t f_i(\\pmb{x}_i^n)/m_i\\\\ \\pmb{x}_{i}^{n+1}=\\pmb{x}_i^n+\\Delta t \\pmb{v}_i^{n+1} \\end{cases} \\] <p>Given a grid velocity field \\(\\pmb{v}_i^{n+1}\\), we can update \\(F\\) as</p> \\[ F_p^{n+1}=\\left(I+\\Delta t \\sum_i \\pmb{v}_i^{n+1}(\\nabla w_{ip}^n)^T\\right) F^n_p \\]"},{"location":"courses/mpm/#forces","title":"Forces","text":"<p>MPM Forces are defined on grid nodes. If we assume a deformation gradient based hyperelastic energy density, then the total elastic potential energy is then </p> \\[ e=\\sum_p V_p^0\\Psi_p(F_p) \\] <p>where \\(V_p^0\\) is the material space volume of particle.</p> <p>Nodal elastic force is the negative gradient of the total potential energy evaluated at nodal positions. So the MPM spatial discretization of the stress-based forces is given as </p> \\[ f_i(x_i^n)=-\\frac{\\partial e}{\\partial x_i}(x)=-\\sum_p V_p^0\\left(\\frac{\\partial \\Psi_p}{\\partial F}(F_p(x_i^n))\\right)(F^n_p)^T\\nabla w_{ip}^n \\] <p>which fully depends on the existing particle/grid weights and particle attributes.</p>"},{"location":"courses/mpm/preliminary/","title":"Preliminary","text":"<p>Reference</p> <p>Mechanics Lecture Notes Part III: Foundations of Continuum Mechanics, pa.kelly@auckland.ac.nz.</p> <p>Website</p>"},{"location":"courses/mpm/preliminary/#tensors","title":"Tensors","text":"<p>A tensor of order zero is simply another name for a scalar \\(\\alpha\\).</p> <p>A first-order tensor is simply another name for a vector \\(\\pmb{u}\\). </p> <p>We use uppercase bold-face Latin letters to denote second order tensor.</p> <p>A second-order tensor \\(\\pmb{T}\\) may be defined as an operator that acts on a vector \\(\\pmb{u}\\) generating another vector \\(\\pmb{v}\\), such that </p> \\[ \\pmb{T}(\\pmb{u})=\\pmb{v} \\] <p>which is a linear operator. </p> <ul> <li>dyad(tensor product)</li> </ul> <p>the tensor product of two vectors \\(\\pmb{u}\\) and \\(\\pmb{v}\\)</p> \\[ \\pmb{u}\\otimes \\pmb{v} \\] <p>is defined by</p> \\[ (\\pmb{u}\\otimes \\pmb{v} ) \\pmb{w} = \\pmb{u}(\\pmb{v}\\cdot \\pmb{w}) \\] <p>Properties</p> <p>(i)</p> \\[ (\\pmb{u}\\otimes \\pmb{v})(\\pmb{w}\\otimes \\pmb{x})=(\\pmb{v}\\cdot\\pmb{w})(\\pmb{u}\\otimes \\pmb{x}) \\] <p>cause</p> \\[ \\begin{align*} (\\pmb{u}\\otimes \\pmb{v})(\\pmb{w}\\otimes \\pmb{x})\\pmb{y}&amp;=(\\pmb{u}\\otimes \\pmb{v})(\\pmb{x}\\cdot\\pmb{y})\\pmb{w}\\\\ &amp;=(\\pmb{x}\\cdot\\pmb{y})(\\pmb{u}\\otimes \\pmb{v})\\pmb{w}\\\\ &amp;=(\\pmb{x}\\cdot\\pmb{y})(\\pmb{v}\\cdot\\pmb{w})\\pmb{u}\\\\ &amp;=(\\pmb{v}\\cdot\\pmb{w})(\\pmb{x}\\cdot\\pmb{y})\\pmb{u}\\\\ &amp;=(\\pmb{v}\\cdot\\pmb{w})(\\pmb{u}\\otimes \\pmb{x})\\pmb{y} \\end{align*} \\] <p>(ii)</p> \\[ \\pmb{u}(\\pmb{v}\\otimes \\pmb{w})=(\\pmb{u}\\cdot \\pmb{v})\\pmb{w} \\] <p>cause</p> \\[ \\begin{align*} (\\pmb{y}\\otimes \\pmb{u})(\\pmb{v}\\otimes \\pmb{w})&amp;=(\\pmb{u}\\cdot \\pmb{v})(\\pmb{y}\\otimes \\pmb{w})\\\\ &amp;=\\pmb{y} \\otimes [(\\pmb{u}\\cdot \\pmb{v})\\pmb{w}] \\end{align*} \\] <p>Some example</p> <ul> <li>Projection Tensor \\((\\pmb{e}\\otimes \\pmb{e})\\) </li> </ul> <p>So </p> \\[ (\\pmb{e}\\otimes \\pmb{e})\\pmb{u} = (\\pmb{e}\\cdot \\pmb{u})\\pmb{e} \\] <p>is the vector projection of \\(\\pmb{u}\\) on \\(\\pmb{e}\\), denoted by \\(\\pmb{P}\\).</p> <p>A dyadic is a linear combination of dyads (with scalar coefficients).</p> <p>In the following discussion, we can treat \\(\\pmb{T}\\) as a matrix.    </p>"},{"location":"courses/mpm/preliminary/#cartesian-tensors","title":"Cartesian Tensors","text":"<p>A second order tensor and the the vector it operates on can be described in terms of Cartesian components.</p> <p>Example</p> <ul> <li>Identity tensor/(or unit tensor).</li> </ul> \\[ \\pmb{I}=\\sum_{i=1}^d\\pmb{e}_i\\otimes \\pmb{e}_i \\] <p>cause it follows</p> \\[ \\begin{align*} \\pmb{I} \\pmb{u}&amp;=\\sum_{i=1}^d(\\pmb{e}_i\\otimes \\pmb{e}_i) \\pmb{u}\\\\ &amp;=\\sum_{i=1}^d(\\pmb{e}_i \\cdot \\pmb{u})\\pmb{e}_i \\\\ &amp;=\\sum_{i=1}^d u_i\\pmb{e}_i \\\\ &amp;=\\pmb{u} \\end{align*} \\] <p>Or identity tensor can be written as</p> \\[ \\pmb{I} = \\sum_{i,j=1}^d\\delta_{ij}(\\pmb{e}_i\\otimes \\pmb{e}_j) \\] <p>Second order tensor as a Dyadic</p> <p>Every second order tensor can always be written as a dyadic involving the Cartesian base vectors \\(\\pmb{e}_i\\), that is, if we denote </p> \\[ \\pmb{E}_i=\\pmb{T}\\left(\\pmb{e}_i\\right) \\] <p>then </p> \\[ \\pmb{T}= \\sum_{i=1}^d \\left(\\pmb{E}_i\\otimes \\pmb{e}_i\\right) \\] Proof \\[ \\begin{align*} \\pmb{b}&amp;=\\pmb{T}(\\pmb{a})\\\\ &amp;=\\pmb{T}\\left(\\sum_{i=1}^d a_i\\pmb{e}_i\\right)\\\\ &amp;=\\sum_{i=1}^d a_i \\pmb{T}(\\pmb{e}_i)\\\\ \\end{align*} \\] <p>Denote \\(\\pmb{T}(\\pmb{e}_i)\\) to be \\(\\pmb{E}_i\\), then</p> \\[ \\begin{align*} \\pmb{b}&amp;=\\sum_{i=1}^d a_i \\pmb{E}_i\\\\ &amp;=\\sum_{i=1}^d (\\pmb{a}\\cdot \\pmb{e}_i )\\pmb{E}_i\\\\ &amp;=\\sum_{i=1}^d (\\pmb{E}_i\\otimes \\pmb{e}_i) \\pmb{a} \\end{align*} \\] <p>So </p> \\[ \\pmb{T}= \\sum_{i=1}^d (\\pmb{E}_i\\otimes \\pmb{e}_i) \\] <p>If we write \\(\\pmb{E}_i\\) with base vectors like</p> \\[ \\pmb{E}_i=\\sum_{j=1}^d E_{ij}\\pmb{e}_j, \\quad i=1,\\cdots, d \\] <p>Then </p> \\[ \\begin{align*} (\\pmb{E}_i\\otimes \\pmb{e}_i)&amp;=\\left(\\sum_{j=1}^d E_{ij}\\pmb{e}_j\\right)\\otimes \\pmb{e}_i\\\\ &amp;=\\sum_{j=1}^d E_{ij} (\\pmb{e}_j\\otimes \\pmb{e}_i), \\quad i=1,\\cdots, d \\end{align*} \\] <p>Thus</p> \\[ \\pmb{T}=\\sum_{i=1}^d\\sum_{j=1}^d E_{ij} (\\pmb{e}_j\\otimes \\pmb{e}_i) \\] <p>Introduce 9 scalars \\(T_{ij}=E_{ji}\\), then </p> \\[ \\begin{align*} \\pmb{T}&amp;=\\sum_{i=1}^d\\sum_{j=1}^d T_{ji} (\\pmb{e}_j\\otimes \\pmb{e}_i)\\\\ &amp;=\\sum_{j=1}^d\\sum_{i=1}^d T_{ji} (\\pmb{e}_j\\otimes \\pmb{e}_i) \\quad \\text{switch summation turn}\\\\ &amp;=\\sum_{i=1}^d\\sum_{j=1}^d T_{ij} (\\pmb{e}_i\\otimes \\pmb{e}_j)\\quad \\text{switch $i$ and $j$} \\end{align*} \\] <p>We can see that 9 dyads \\(\\{\\pmb{e}_i\\otimes \\pmb{e}_j\\}_{i,j=1}^3\\) forms a basis for the space of second order tensors.</p> <p>Recall that </p> \\[ \\begin{align*} T_{ij}&amp;=E_{ji}\\\\ &amp;=\\pmb{E}_j \\cdot \\pmb{e}_i \\\\ &amp;=T(\\pmb{e}_j) \\cdot (\\pmb{e}_i)\\\\ &amp;=(\\pmb{e}_i) \\cdot T(\\pmb{e}_j)\\\\ \\end{align*} \\] <p>So we can get the component of a tensor by the above way.</p>"},{"location":"courses/mpm/preliminary/#cauchy-stress-tensor","title":"Cauchy Stress Tensor","text":"<p>The traction vector, the limiting value of the ratio of force over area, that is,</p> \\[ \\pmb{t}^{\\pmb{n}}=\\lim_{\\Delta_s\\rightarrow 0}\\frac{\\Delta F}{\\Delta S} \\] <p>where \\(\\pmb{n}\\) denotes normal vector to the surface.</p> <p>The stress \\(\\pmb{\\sigma}\\), a second order tensor which maps \\(\\pmb{n}\\) onto \\(\\pmb{t}\\)</p> \\[ \\pmb{t}=\\pmb{\\sigma}\\pmb{n} \\] <p>If we consider a coordinate system with base vectors \\(\\pmb{e}_i\\), then \\(\\pmb{\\sigma}=\\sum\\limits_{i,j=1}^d(\\sigma_{ij}\\pmb{e}_i \\otimes \\pmb{e}_j)\\) </p> \\[ \\pmb{\\sigma}=(\\sigma_{ij}) \\] <p>So </p> \\[ t_i \\pmb{e_i} = \\sum_{j=1}^3\\sigma_{ij}n_{j} \\pmb{e}_i \\] <p> </p> <p>For example, </p> \\[ \\pmb{\\sigma}\\pmb{e}_j=\\sum_{i=1}^3\\sigma_{ij}\\pmb{e}_i  \\] <p>which denotes the summation of the \\(j\\)th column of matrix \\(\\pmb{\\sigma}\\).</p> <p>So the components \\(\\sigma_{11}, \\sigma_{21}, \\sigma_{31}\\) of the stress tensor are the three components of the traction vector which acts on the plane with normal \\(\\pmb{e}_1\\).</p>"},{"location":"courses/mpm/preliminary/#hamilton-operator","title":"Hamilton Operator","text":"<p>First we want to introduce the operator </p> \\[ \\nabla=\\pmb{i}\\frac{\\partial }{\\partial x}+\\pmb{j}\\frac{\\partial }{\\partial y}+\\pmb{k}\\frac{\\partial }{\\partial z} \\] <p>then </p> \\[ \\nabla f= \\pmb{i}\\frac{\\partial f}{\\partial x}+\\pmb{j}\\frac{\\partial f}{\\partial y}+\\pmb{k}\\frac{\\partial f}{\\partial z}=\\text{grad} f \\] <p>we also have inner product </p> \\[ \\begin{align*} \\nabla\\cdot \\pmb{a}&amp;=\\left(\\pmb{i}\\frac{\\partial }{\\partial x}+\\pmb{j}\\frac{\\partial }{\\partial y}+\\pmb{k}\\frac{\\partial }{\\partial z}\\right)\\cdot (P\\pmb{i}+Q\\pmb{j}+R\\pmb{k})\\\\ &amp;=\\frac{\\partial P}{\\partial x}+\\frac{\\partial Q} {\\partial y}+\\frac{\\partial R}{\\partial z}\\\\ &amp;=\\text{div} \\pmb{a} \\end{align*} \\] <p>and cross product</p> \\[ \\begin{align*} \\nabla\\times \\pmb{a}&amp;=\\left(\\pmb{i}\\frac{\\partial }{\\partial x}+\\pmb{j}\\frac{\\partial }{\\partial y}+\\pmb{k}\\frac{\\partial }{\\partial z}\\right)\\times (P\\pmb{i}+Q\\pmb{j}+R\\pmb{k})\\\\ &amp;=\\left|\\begin{array}{ccc} \\pmb{i}&amp;\\pmb{j}&amp;\\pmb{k}\\\\ \\displaystyle \\frac{\\partial }{\\partial x}&amp;\\displaystyle \\frac{\\partial }{\\partial y} &amp; \\displaystyle \\frac{\\partial }{\\partial z}\\\\ P &amp; Q &amp; R\\\\ \\end{array} \\right|\\\\ &amp;=\\left( \\frac{\\partial R}{\\partial y}- \\frac{\\partial Q}{\\partial z}\\right)\\pmb{i}+\\left( \\frac{\\partial R}{\\partial x}- \\frac{\\partial P}{\\partial z}\\right)\\pmb{j}+\\left( \\frac{\\partial Q}{\\partial x}- \\frac{\\partial P}{\\partial y}\\right)\\pmb{k}\\\\ &amp;=\\text{rot} \\pmb{a} \\end{align*} \\] <p>Then Gauss Formula can be expressed by</p> \\[ \\iint_{\\partial\\Omega}\\pmb{a}d\\pmb{S}=\\iiint_{\\Omega}\\nabla\\cdot \\pmb{a}dV \\] <p>Stokes Formula can be expressed by</p> \\[ \\int_{\\partial \\Sigma}\\pmb{a}d\\pmb{s}=\\iint_{\\Sigma}(\\nabla\\times \\pmb{a})\\cdot d\\pmb{S} \\]"},{"location":"tag/","title":"Tag","text":"<p>Hey!</p>"}]}
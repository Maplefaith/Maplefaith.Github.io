{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Keep in low-key wind","text":"<p>Here you can catch a glimpse of notes of courses at ZJU from a student majoring in CSE.</p> <p>Also, I share some of my experience in Blog.</p> <p>Wish you a good reading time.</p> <ul> <li> <p> \u5fc3\u8a00\u5fc3\u8bed</p> <ul> <li>May Day Holiday</li> </ul> </li> </ul> <ul> <li> <p> \u7a7a\u8bf4\u65e0\u51ed</p> <ul> <li>Numerical Analysis</li> <li>Ordinary Differential Equation</li> <li>Modern Control Theory</li> </ul> </li> </ul>"},{"location":"friends/","title":"Welcome to Join my Friend Zone!","text":"Maythics Little Mouse SyncrnzdClk Medium Cat Little_Whale Great Whale LastingWind Giant in Stu'Union"},{"location":"blog/","title":"Blog","text":"<p>In this part, I hope you can stand my childish thoughts and points.</p>"},{"location":"blog/1124/","title":"\u665a\u5b89\u77ed\u4fe11124","text":"<p>\u5404\u4f4d\u670b\u53cb\u4eec\uff0c\u9996\u5148\u8fd8\u662f\u8981\u8bf4\uff1a</p> <p>\u6b22\u8fce\u5927\u5bb6\u52a0\u5165\u79d1\u534f\u5927\u5bb6\u5ead~</p>"},{"location":"blog/1124/#_1","title":"\u7b2c\u4e00\u6b21\u4f8b\u4f1a\u7684\u7167\u7247\u6b8b\u4f59","text":"<p>\u770b\u770b\u6211\u4eec\u7684\u90e8\u957f\u591a\u8ba4\u771f\uff01</p>"},{"location":"blog/1124/#_2","title":"\u81ea\u6211\u4ecb\u7ecd","text":"<p>\u6211\u662f\u8fd9\u4e00\u5e74\u5206\u7ba1\u54c1\u7ba1\u7684\u526f\u79d8\u4e66\u957f\uff0c\u6765\u81ea\u6d59\u6c5f\u6e29\u5dde\uff0c22\u7ea7\u81ea\u52a8\u5316\uff08\u63a7\u5236\uff09\u4e13\u4e1a\u3002\u867d\u7136\u6211\u4eba\u4f4f\u5728\u7389\u6cc9\uff0c\u4f46\u662f\u6211\u4e5f\u662f\u4f1a\u5b9a\u671f\u8d76\u56de\u7d2b\u91d1\u6e2f\u7684\uff01\u5927\u5bb6\u5982\u679c\u60f3\u8981\u7ea6\u6211\u7684\u8bdd\uff0c\u53ef\u4ee5\u6bcf\u5468\u4e00\u3001\u4e8c\u548c\u56db\uff08\u5927\u5bb6\u4f8b\u4f1a\u7684\u65f6\u95f4\u5728\u5468\u4e09\uff0c\u6240\u4ee5\u6211\u5927\u6982\u7387\u90fd\u4f1a\u7f3a\u5e2d\uff09\u3002</p> <p> </p>"},{"location":"blog/1124/#oppo","title":"\u5173\u4e8eOPPO","text":"<p>\u5927\u5bb6\u90fd\u77e5\u9053\u660e\u5929\u8981\u4e3e\u529e\u7684OPPO\u96c6\u5e02\u5417\uff1f\u6211\u4eec\u793e\u56e2\u662f\u6709\u53c2\u52a0\u7684\uff01</p> <p>\u5e0c\u671b\u5927\u5bb6\u660e\u5929\u6709\u7a7a\u7684\u8bdd\uff0c\u4e5f\u53ef\u4ee5\u53bb\u6587\u5e7f\u770b\u770b\uff0c\u53c2\u4e0e\u4e00\u4e0b\uff0c\u4e3a\u6211\u4eec\u7684\u793e\u56e2\u52a0\u52a0\u6cb9\uff01</p> <p>\u70b9\u51fb\u793e\u56e2\u4e2d\u5fc3\u7684\u63a8\u6587\u53ef\u8df3\u8f6c\u67e5\u770b\u66f4\u591a\u8be6\u7ec6\u5185\u5bb9~</p>"},{"location":"blog/1124/#_3","title":"\u793e\u56e2\u667a\u56ca\u56e2","text":"<p>\u6211\u8fd8\u5728\u79d8\u4e66\u957f\u7684\u6002\u607f\u4e4b\u4e0b\u62a5\u4e86\u8fd9\u4e2a\u793e\u56e2\u667a\u56ca\u56e2\uff0c\u7136\u540e\u4eca\u5929\u5c31\u88ab\u62c9\u53bb\u82e6\u529b\uff0c\u53bb\u5f53\u793e\u56e2\u661f\u9645\u8c03\u6574\u6750\u6599\u8bc4\u5ba1\u4f1a\u7684\u8bc4\u59d4\u3002\u5176\u5b9e\u5c31\u662f\u548c\u793e\u56e2\u4e2d\u5fc3\u7684\u51e0\u4e2a\u4eba\uff0c\u628a\u51e0\u5341\u5bb6\u6240\u6709\u4e09\u661f\u7ea7\u793e\u56e2\u7684\u793e\u56e2\u661f\u7ea7\u8c03\u6574\u6c47\u603b\u8868\u90fd\u8fc7\u4e86\u4e00\u904d\uff0c\u7ed9\u6bcf\u4e00\u4e2a\u793e\u56e2\u8bc4\u5206\uff08\u6240\u4ee5\u4eca\u5929\u7684\u8bc4\u5206\u4e0d\u6d89\u53ca\u6211\u4eec\u793e\u56e2\uff09\u3002\u4e0b\u5348\u7ed3\u675f\u7684\u65f6\u5019\u6211\u611f\u89c9\u6211\u7684\u8111\u5b50\u5df2\u7ecf\u70e7\u5e72\u4e86\u3002</p> <p>\u603b\u4f53\u6765\u8bf4\uff0c\u6709\u4e9b\u793e\u56e2\u975e\u5e38\u8ba4\u771f\uff0c\u4f46\u6709\u4e9b\u793e\u56e2\u975e\u5e38\u7684\u6c34\u3002\u4eca\u5929\u6bd4\u8f83\u597d\u73a9\u7684\u6709<code>\u7f51\u7403\u534f\u4f1a</code>\uff0c\u661f\u7ea7\u8c03\u6574\u610f\u5411\u662f\u5347\u661f\uff0c\u7136\u800c\u6587\u68486\u9875\u7684\u7eb8\uff0c\u603b\u5171\u5c31\u53ea\u586b\u4e86399\u4e2a\u5b57\uff08\u6ca1\u9519\uff0c\u4eca\u5929\u6709\u4f4d\u8bc4\u59d4\u6ca1\u4e8b\u5c31\u4e13\u95e8\u6570\u4e86\u4e00\u4e0b\uff09\uff0c\u4e8e\u662f\u5c31\u7ed9\u4ed6\u6253\u4e8639.9\u5206\u3002</p> <p>\u53ef\u662f\uff0c\u6bcf\u4e2a\u8bc4\u59d4\u90fd\u6709\u81ea\u5df1\u7684\u8bc4\u5206\u6807\u51c6\uff0c\u4f46\u662f\u4e0d\u80fd\u6700\u4f4e\u5206\u6309\u8981\u6c42\u4e0d\u4f4e\u4e8e40\u5206\uff08\u719f\u6089\u6d59\u6c5f\u9ad8\u8003\u7684\u7ae5\u978b\u5e94\u8be5\u660e\u767d\u9009\u8003\u8d4b\u5206\u7684\u6700\u4f4e\u5206\u4e5f\u662f40\u5206\uff09\u3002</p>"},{"location":"blog/1124/#_4","title":"\u65e0\u5956\u7ade\u731c","text":"<p>\u5927\u5bb6\u53ef\u4ee5\u731c\u4e00\u4e0b\uff0c\u8fd9\u4e2a\u662f\u7d2b\u91d1\u6e2f\u7684\u54ea\u91cc~</p> <p> </p>"},{"location":"blog/Travel/","title":"Preface","text":"<p>Write down the life before it passes by...</p>"},{"location":"blog/Travel/5th_05_24/","title":"\u5c0f\u957f\u5047\u51fa\u6e38","text":"<p>\u5047\u671f\u76ee\u6807\uff1a\u4ece\u5404\u7c7b\u4e8b\u9879\u4e2d\u89e3\u653e\u51fa\u6765\uff0c\u601d\u8003\u603b\u7ed3\u524d\u534a\u4e2a\u5b66\u671f\u7684\u751f\u5b58\u72b6\u6001\u3002</p> <p>\u884c\u52a8\u539f\u5219\uff1a\u6162\u4e0b\u6765\uff0c\u6162\u4e0b\u6765\u3002</p>"},{"location":"blog/Travel/5th_05_24/#430-52","title":"4.30-5.2","text":"<p>\u9996\u5148\u611f\u8c22\u591c\u5bb5\u7ec4\u4f19\u4f34\u7684\u5b89\u6392\uff0c\u7279\u522b\u611f\u8c22\u5c0f\ud83d\udc1f\u7684\u7edf\u7b79\uff0c\u5c0f\ud83c\udf43\u7684\u666f\u5fb7\u9547\u8ba1\u5212\uff0c\u8ba9\u6211\u4eec\u7684\u6574\u4e2a\u884c\u7a0b\u90fd\u975e\u5e38\u987a\u5229\u3002</p> <p>\u4e00\u5927\u65e9\u5c31\u5f00\u59cb\u7fd8\u8bfe\u51fa\u884c\uff0c\u6211\u4eec\u5148\u5750\u9ad8\u94c1\u5230\u8fbe\u5a7a\u6e90\uff0c\u542f\u52a8\u81ea\u9a7e\u6a21\u5f0f\uff0c\u4e09\u5929\u53bb\u4e86\u7bc1\u5cad\u3001\u4e09\u6e05\u5c71\u548c\u666f\u5fb7\u9547\u3002</p>"},{"location":"blog/Travel/5th_05_24/#430","title":"4.30","text":"<p>\u7b2c\u4e00\u5929\u7684\u7bc1\u5cad\u7740\u5b9e\u8ba9\u4eba\u773c\u524d\u4e00\u4eae\uff0c\u5f88\u591a\u7167\u7247\uff0c\u65e0\u8bba\u662f\u98ce\u666f\u8fd8\u662f\u4eba\u7269\uff0c\u90fd\u5f88\u4e0a\u955c\u3002\u4ece\u73bb\u7483\u6808\u9053\uff08\u5792\u5fc3\u6865\uff09\u5230\u508d\u5c71\u5c0f\u9547\uff08\u82b1\u6eaa\u6c34\u8857\uff09\uff0c\u4ece\u675c\u9e43\u56ed\u5230\u98d8\u96ea\u6c11\u56fd\u9986\uff0c\u7edd\u7f8e\u7684\u5c71\u95f4\u98ce\u5149\u3001\u7eaf\u51c0\u7684\u5c71\u95f4\u7a7a\u6c14\uff0c\u8ba9\u4e00\u4e2a\u957f\u671f\u751f\u6d3b\u5728\u95ed\u585e\u6821\u56ed\u91cc\u7684\u4eba\u611f\u5230\u65e2\u964c\u751f\u53c8\u4eb2\u5207\u3002</p>"},{"location":"blog/Travel/5th_05_24/#51","title":"5.1","text":"<p>\u65e9\u6668\u9192\u6765\uff0c\u521a\u53d1\u751f\u7684\u5e7f\u4e1c\u6885\u5927\u9ad8\u901f\u584c\u9677\u8ba9\u6211\u5403\u4e86\u4e00\u5927\u60ca\u3002\u540e\u9762\u4ece\u4e09\u6e05\u5c71\u56de\u6765\u5f00\u9ad8\u901f\u7684\u65f6\u5019\uff0c\u5176\u5b9e\u6709\u4e00\u70b9\u5bd2\u98a4\u3002</p> <p>\u4e09\u6e05\u5c71\u6211\u66fe\u7ecf\u53bb\u8fc7\u4e00\u6b21\uff0c\u8bb0\u5fc6\u4e2d\u5370\u8c61\u975e\u5e38\u4e0d\u9519\uff0c\u4e91\u96fe\u7f2d\u7ed5\uff0c\u4eba\u4eec\u508d\u5c71\u800c\u884c\uff0c\u5982\u4e34\u4ed9\u5883\u3002\u7136\u800c\uff0c\u8fd9\u6b21\u975e\u5e38\u7684\u4e0d\u884c\uff0c\u4e91\u96fe\u8fc7\u4e8e\u6d53\u91cd\uff0c\u5927\u90e8\u5206\u7684\u5c71\u95f4\u7f8e\u666f\u90fd\u65e0\u6cd5\u6e05\u65b0\u7528\u773c\u770b\u89c1\uff08\u6216\u8bb8\u53ef\u4ee5\u8bd5\u8bd5\u5176\u4ed6\u6ce2\u6bb5\u7684\u5149hh\uff09\u3002</p> <p>\u5728\u80fd\u89c1\u5ea6\u4e0d\u523010m\u7684\u5c71\u95f4\uff0c\u62cd\u7167\u51e0\u4e4e\u662f\u4e00\u4ef6\u5403\u529b\u4e0d\u8ba8\u597d\u7684\u4e8b\u60c5\u3002\u597d\u5728\u6211\u4eec\u6700\u540e\u4e5f\u627e\u5230\u4e86\u8bb8\u591a\u7684\u62cd\u7167\u70b9\uff0c\u8d4b\u4e88\u6d53\u96fe\u91cc\u7684\u5c71\u548c\u6211\u4eec\u4e00\u70b9\u7279\u522b\u7684\u610f\u4e49\u3002</p> <p>\u53e6\u5916\uff0c\u4e94\u4e00\u4eba\u6570\u8fc7\u591a\uff0c\u4e0b\u5c71\u6392\u961f\u7b49\u5019\u8d85\u8fc7\u4e00\u4e2a\u534a\u5c0f\u65f6\uff0c\u4e5f\u662f\u4e00\u4e2a\u51cf\u5206\u9879\u3002</p>"},{"location":"blog/Travel/5th_05_24/#52","title":"5.2","text":"<p>\u6211\u4eec\u53bb\u5230\u4e86\u666f\u5fb7\u9547\uff0c\u4f53\u4f1a\u4e86\u4e00\u4e0b\u4e2d\u56fd\u74f7\u90fd\u7684\u97f5\u5473\u3002\u6211\u672c\u4eba\u5bf9\u4e8e\u74f7\u5668\u5e76\u65e0\u592a\u5927\u611f\u53d7\uff0c\u74f7\u5668\u53ca\u5176\u4ecb\u7ecd\u4e5f\u53ea\u662f\u8d70\u9a6c\u89c2\u82b1\u5306\u5306\u800c\u8fc7\u3002\u4f46\u662f\uff0c\u7ec6\u7ec6\u8d70\u6765\uff0c\u53d1\u73b0\u5176\u5236\u4f5c\u6d41\u7a0b\u7684\u7cbe\u7ec6\u3001\u7e41\u6742\uff0c\u5f97\u5230\u7684\u74f7\u5668\u8d28\u91cf\u4e0a\u4e58\u3001\u4ef7\u503c\u9ad8\u6602\uff0c\u5185\u542b\u5927\u5bb6\u5bf9\u5320\u5fc3\u7684\u8ffd\u6c42\u3002</p> <p>\u665a\u4e0a\u56de\u6765\uff0c\u53c2\u4e0e\u4e86\u5927\u8868\u54e5\u7684\u5a5a\u793c\u3002\u4e0d\u77e5\u4e3a\u4f55\uff0c\u6211\u4e00\u8fb9\u4e3a\u4ed6\u4eec\u611f\u5230\u5f88\u5f00\u5fc3\uff0c\u4e00\u8fb9\u611f\u53d7\u5230\u65f6\u5149\u98de\u901d\uff0c\u6bcf\u4e2a\u4eba\u90fd\u5728\u5f80\u524d\u8d70\u3002\u6211\u5bf9\u672a\u6765\u6709\u671f\u5f85\uff0c\u4e5f\u6709\u7126\u8651\u3002</p>"},{"location":"blog/Travel/5th_05_24/#53-55","title":"5.3-5.5","text":"<p>\u56de\u5230\u6e29\u5dde\uff0c\u9664\u4e86\u4e45\u8fdd\u7684\u6d77\u9c9c\u4e0d\u65ad\u6ee1\u8db3\u6211\u81ea\u5df1\u4e4b\u5916\uff0c\u6211\u8fd8\u5c06\u81ea\u5df1\u7684\u751f\u6d3b\u6162\u4e86\u4e0b\u6765\u3002\u6211\u548c\u5bb6\u91cc\u4eba\u90fd\u76f8\u5904\u4e86\u4e00\u4e0b\uff0c\u6211\u660e\u767d\u4e4b\u540e\u53ef\u80fd\u6ca1\u6709\u5f88\u591a\u7684\u65f6\u95f4\u966a\u4f34\u5bb6\u4eba\u3002</p> <p>\u6211\u5237\u4e86\u4e00\u4e0b\u8001\u53cb\u8bb0\uff0c\u5176\u5b9e\u8fd9\u4e2a\u662f\u53ef\u4ee5\u4e0a\u763e\u7684\uff01\u770b\u5267\u4e0a\u763e\u7684\u673a\u5236\uff0c\u6e90\u81ea\u4e8e\u5267\u60c5\u5bf9\u4eba\u7684\u5174\u8da3\u7684\u4e0d\u65ad\u6311\u9017\uff0c\u4eba\u770b\u5267\u65f6\u80fd\u591f\u6709\u7684\u677e\u5f1b\u611f\u548c\u5fd8\u6211\u611f\u3002</p> <p>\u5b9e\u9645\u4e0a\uff0c\u8fd9\u4e2a\u4e1c\u897f\u662f\u53ef\u4ee5\u8c03\u63a7\u7684\uff0c\u56e0\u4e3a\u5f88\u591a\u79ef\u6781\u7684\u4e8b\u60c5\uff0c\u90fd\u5728\u52aa\u529b\u4e2d\uff0c\u53d8\u5f97\u4e0a\u763e\u3002</p> <p>\u9ebb\u5c06\u5f53\u7136\u4e5f\u662f\u4e00\u6b21\u805a\u4f1a\u7684\u597d\u65f6\u673a\u3002\u5927\u5bb6\u90fd\u80fd\u591f\u804a\u804a\u5404\u81ea\u7684\u751f\u6d3b\uff0c\u4e0d\u540c\u7684\u4eba\u4e0d\u540c\u7684\u5730\u65b9\uff0c\u6709\u7740\u76f8\u4f3c\u7684\u60c5\u611f\u3002</p> <p>\u6700\u91cd\u8981\u7684\u662f\uff0c\u6211\u80fd\u591f\u4e3a\u6211\u7684\u5bb6\u5ead\u628a\u628a\u8109\uff0c\u770b\u770b\u6709\u6ca1\u6709\u4ec0\u4e48\u6bdb\u75c5\uff0c\u63a5\u4e0b\u6765\u79bb\u5bb6\u540e\uff0c\u6211\u662f\u4e0d\u662f\u8981\u505a\u4e00\u4e9b\u8c03\u6574\u3002\u6700\u91cd\u8981\u7684\u662f\u6211\u548c\u7238\u5988\u7684\u8ddd\u79bb\u53d8\u5f97\u66f4\u52a0\u9065\u8fdc\uff0c\u6211\u53d1\u73b0\u4e4b\u524d\u65e5\u5e38\u7684\u4e2d\u9910\u5e76\u4e0d\u80fd\u5f88\u597d\u5730\u89e3\u51b3\u95ee\u9898\u3002\u6211\u9700\u8981\u591a\u53d1\u4e00\u4e9b\u7167\u7247\uff0c\u5173\u4e8e\u6211\u7684\u751f\u6d3b\uff0c\u6211\u7684\u5468\u672b\uff0c\u8ba9\u4ed6\u4eec\u591a\u770b\u770b\u6211\u5230\u5e95\u662f\u600e\u4e48\u505a\u7684\u3002</p> <p>\u5047\u671f\u7ed3\u5c3e\u6211\u7adf\u7136\u80fd\u591f\u8bfb\u5230\u4e00\u53e5\u597d\u6587\uff1a \u4e09\u8054\u751f\u6d3b\u5468\u520a<pre><code>\u4e0d\u8981\u8bd5\u56fe\u5bfb\u627e\u4e00\u79cd\u201c\u4e0d\u7126\u8651\u201d\u7684\u72b6\u6001\uff0c\u56e0\u4e3a\u7126\u8651\u4e0e\u4eba\u7684\u9009\u62e9\u548c\u751f\u5b58\u76f8\u4f34\u968f\uff0c\u4e0d\u5982\u5b66\u4f1a\u4e00\u4e9b\u5c0f\u7684\u6280\u5de7\uff0c\u8ba9\u81ea\u5df1\u80fd\u548c\u7126\u8651\u5171\u5904\u3002\n</code></pre></p> <p>\u5b89\u6170\u4e86\u66fe\u7ecf\u52aa\u529b\u7684\u81ea\u5df1\uff0c\u6e05\u6670\u4e86\u5bf9\u672a\u6765\u7684\u52aa\u529b\u3002</p> <p>\u4ee5\u524d\u63a2\u7d22\u7684\u5404\u79cd\u6709\u76ca\u7684\u65b9\u5f0f\uff0c\u90fd\u80fd\u591f\u548c\u7126\u8651\u4e32\u8054\u8d77\u6765\uff0c\u9a7e\u9a6d\u7126\u8651\uff0c\u4e3a\u6211\u6240\u7528\u3002</p> <p>\u611f\u89c9\u81ea\u5df1\u7684\u5f88\u591a\u53d1\u8a00\u90fd\u6709\u529f\u5229\u5b9e\u7528\u4e3b\u4e49\u7684\u8272\u5f69\uff0c\u65e5\u6e10\u957f\u5927\u7684\u81ea\u5df1\u5f80\u5f80\u90fd\u662f\u5bf9\u7406\u60f3\u5931\u53bb\u8010\u5fc3\u548c\u4fe1\u5fc3\uff0c\u8d70\u4e00\u6b65\u770b\u4e00\u6b65\uff0c\u800c\u603b\u662f\u8981\u56de\u5934\u770b\u770b\uff0c\u81ea\u5df1\u4e3a\u4ec0\u4e48\u51fa\u53d1\u3002</p> <p>\u6211\u89c9\u5f97\u5bf9\u6211\u6765\u8bf4\uff0c\u6211\u9700\u8981\u4e0d\u65ad\u6253\u5f00\u81ea\u5df1\u3002\u6211\u80fd\u591f\u653e\u4e0b\u5f88\u591a\u4ee5\u524d\u6240\u8ba4\u4e3a\u7684\u7981\u5fcc\uff0c\u53bb\u505a\u5f88\u591a\u5b9e\u9645\u4e0a\u90fd\u80fd\u591f\u505a\u7684\u4e8b\u60c5\u3002</p> <p>\u5176\u5b9e\u5f88\u591a\u4e1c\u897f\u90fd\u662f\u4f1a\u4e0a\u763e\u7684\u3002</p> <p>\u5b66\u4f1a\u628a\u4e0d\u65ad\u7684\u4e0a\u763e\u53d8\u6210\u81ea\u5df1\u6700\u559c\u6b22\u7684\u6a21\u6837\u3002</p>"},{"location":"courses/","title":"Content","text":"<p>The following contains some notes of courses at ZJU.</p>"},{"location":"courses/#introduction-to-robotics","title":"Introduction to Robotics","text":"<p>Completed.</p>"},{"location":"courses/#c-plus-plus","title":"C Plus Plus","text":"<p>To be continued...</p>"},{"location":"courses/#modern-control-theory","title":"Modern Control Theory","text":"<p>completed.</p>"},{"location":"courses/#numerical-analysis","title":"Numerical Analysis","text":"<p>To be continued...</p>"},{"location":"courses/#ordinary-differential-equation","title":"Ordinary Differential Equation","text":"<p>To be continued...</p>"},{"location":"courses/#sensing-detection","title":"Sensing &amp; Detection","text":"<p>Maybe will only present the midterm exam.</p>"},{"location":"courses/Modern_Control_Theory/MCT/","title":"Modern Control Theory","text":"<p>I show the outline of the course for the coming exam.</p> {\"url\": \"../MCT_review.pdf\"}"},{"location":"courses/Numerical_Analysis/","title":"Numerical Analysis","text":"<p>Reference</p> <p>Numerical analysis, Richard L. Burden, J. Douglas Faires</p>"},{"location":"courses/Numerical_Analysis/#matrix-calculation","title":"Matrix Calculation","text":""},{"location":"courses/Numerical_Analysis/#solving-equation-interpolation-approximation","title":"Solving Equation, Interpolation &amp; Approximation","text":""},{"location":"courses/Numerical_Analysis/#numerical-solution-of-differential-equations","title":"Numerical Solution of Differential Equations","text":""},{"location":"courses/Numerical_Analysis/MC/","title":"Matrix Calculation","text":""},{"location":"courses/Numerical_Analysis/MC/#direct-methods-for-solving-linear-systems","title":"Direct Methods for Solving Linear Systems","text":"<p>We focus on solving linear system </p> \\[ A\\vec{x} = \\vec{b} \\]"},{"location":"courses/Numerical_Analysis/MC/#gasussion-elimination","title":"Gasussion Elimination","text":"<p>Reduce A into an upper-triangular matrix, and then solve for the unknowns by a backward-substitution process</p>"},{"location":"courses/Numerical_Analysis/MC/#pivoting-stratages","title":"Pivoting Stratages","text":"<p>This part is to reduce the error caused by rounding/Truncation error.</p> <p>We can show that the pivoting element is of great significance.</p> Partial PivotingScaled Partial PivotingComplete Pivoting <p>(also known for not changing the columns)</p> <p>Determine the smallest \\(p\\geq k\\) (in the same column of \\(a^{(k)}_{kk}\\))such that </p> \\[ |a_{ok}^{(k)}| = \\max_{k\\leq i \\leq n}{|a_{ik}^{(k)}|} \\] <p>and perform \\((E_k) \\leftrightarrow (E_p)\\).</p> <p>For row \\(i\\), let</p> \\[ s_i = \\max_{1\\leq j\\leq n}{|a_{ij}|} \\] <p>(if \\(\\exists i, s.t. s_i=0\\), then the system has no unique root. So we assume \\(\\forall i, s_i&gt;0\\))</p> <p>For each procedure of executing \\(E_k \\leftarrow E_k - m_{k,i}E_i\\) for \\(k=i+1, \\cdots, n\\), where \\(m_{k, i} = a_{ki}/{a_{ii}}\\). let </p> \\[ p = \\arg \\max_{i\\leq k \\leq n}{\\frac{|a_{ki}|}{s_k}} \\] <p>perform \\((E_i)\\leftrightarrow(E_p)\\)</p> <p>Incorporate the interchange of both rows and columns.</p>"},{"location":"courses/Numerical_Analysis/MC/#time-cost","title":"Time Cost","text":"<p>As we all know the time expense for Gaussion elimination is</p> \\[ O(n^3) \\]"},{"location":"courses/Numerical_Analysis/MC/#lu-matrix-factorization","title":"LU Matrix Factorization","text":"<p>The idea is encouraged by Gaussion Elimination. See that a matrix \\(A\\) can be transformed into an upper-trianglar matrix \\(U\\) by primary row operations:</p> \\[ \\begin{equation}  U = M_{n-1}\\cdots M_2M_1A  \\label{eq: LU} \\end{equation}  \\] <p>where \\(M_k (k=1,2,\\cdots n-1)\\) denotes a series of row operations. There are two perspetives.</p> Version 1Version 2 <p>\\(M_k (k=1, \\cdots, n-1)\\) can be interpreted that the \\(k+1\\) row has to make its column \\(1\\) to \\(k\\) to be \\(0\\). That is,</p> \\[ E_{k+1} \\leftarrow E_{k+1} - \\sum_{j=1}^{k} m_{k+1, j}E_j \\] <p>\\(M_k (k=1,\\cdots, n-1)\\) can be defined in another way as </p> \\[ E_j \\leftarrow E_j - \\sum\\limits_{k=j}^{n}m_{j,k}E_k \\quad \\text{for } j=k+1, \\cdots n \\] <p>which is also a lower-triangular matrix. To be proved by readers. </p> <p>And we can see \\(M_k\\) formed through the above two interpretations are the same.</p> <p>If we denote \\(L_k = M_k^{-1}\\), then apply \\(L = L_1L_2\\cdots L_{n-1}\\) left to both sides of the equation \\(\\ref{eq: LU}\\), then</p> \\[ LU = L_1L_2\\cdots L_{n-1} \\cdot M_{n-1}\\cdots M_2M_1A = A \\] <p>We know that matrix \\(L_k\\) and \\(M_k\\) are lower-triangular matrix(explaned by definition, to be proved by readers), so the product of matrix L is alao a lower-triangular matrix.</p> <p>So with the triangular matrix, it can be much quicker to solve the solution. See that</p> \\[  \\begin{align*} A\\vec{x} &amp;= \\vec{b} \\\\ LU\\vec{x} &amp;= \\vec{b} \\end{align*}  \\] <p>First solve \\(L \\vec{y} = \\vec{b}\\), then solve \\(U \\vec{x} = \\vec{y}\\).</p>"},{"location":"courses/Numerical_Analysis/MC/#time-cost_1","title":"time cost","text":"<p>Eliminate \\(0.5n^2\\) elements, it needs time \\(O(0.5n^3)\\).</p> <p>Solving \\(y\\) and \\(x\\), it needs time \\(O(2n^2)\\).</p>"},{"location":"courses/Numerical_Analysis/MC/#iterative-techniques-in-matrix-algebra","title":"Iterative Techniques in Matrix Algebra","text":"<p>This section, we introduce the iterative thoughts from Fixed-Point Iteration\uff08\u4e0d\u52a8\u70b9\u6cd5\uff09 to solve a linear system.</p> <p>We aim to find a iterative equation like equation \\(\\pmb{x}^{k} = f(\\pmb{x}^{k-1})\\). To be more specific, a linear iterative equation like</p> \\[ \\pmb{x}^{k} = T \\pmb{x}^{k-1} + \\pmb{c} \\] <p>and its corresponding convergent relation is</p> \\[ \\pmb{x}= T \\pmb{x} + \\pmb{c} \\]"},{"location":"courses/Numerical_Analysis/MC/#preliminary-knowledge-norm-of-vectors-and-matrixes","title":"Preliminary knowledge: Norm of Vectors and Matrixes","text":"<p>Definition of Norm of vectors</p> <p>A vector norm on \\(\\mathbb{R}^n\\) is a function, denoted as \\(\\Vert \\cdot \\Vert\\), mapping from \\(\\mathbb{R}^n\\) into \\(\\mathbb{R}\\) with the following properties for all \\(x, y \\in\\mathbb{R}^n\\) and \\(\\alpha \\in \\mathbb{C}\\).</p> <ul> <li>\u6b63\u6027 | positive</li> </ul> \\[ \\Vert \\vec{x} \\Vert\\geq 0 \\] <ul> <li>\u5b9a\u6027 | definite</li> </ul> \\[ \\Vert \\vec{x}\\Vert = 0 \\Leftrightarrow \\vec{x}=\\vec{0} \\] <ul> <li>\u9f50\u6027 | homogeneous</li> </ul> \\[ \\Vert \\alpha\\vec{x} \\Vert = |\\alpha|\\Vert \\vec{x} \\Vert \\] <ul> <li>\u4e09\u89d2\u4e0d\u7b49\u5f0f | triangle inequality</li> </ul> \\[ \\Vert \\vec{x}+\\vec{y} \\Vert \\leq \\Vert \\vec{x} \\Vert+\\Vert \\vec{y} \\Vert \\] <p>We usually use \\(p\\) norm</p> \\[ \\Vert \\vec{x} \\Vert_p = \\left(\\sum_{i=1}^n|x_i|^p\\right)^{1/p} \\] <p>with its common forms:</p> \\[ \\Vert \\vec{x} \\Vert_1 = \\sum_{i=1}^{n}|x_i|, \\quad \\Vert \\vec{x} \\Vert_2 = \\sqrt{\\sum_{i=1}^{n}|x_i|^2}, \\quad \\Vert \\vec{x} \\Vert_\\infty = \\max_{1\\leq i\\leq n}|x_i| \\] <p>Definition of Norm of Matrixes</p> <p>A matrix norm on the set of all matrices \\(R \\in \\mathbb{R}^{n\\times n}\\) is a real-valued function, denoted as \\(\\Vert \\cdot \\Vert\\), defined on this set, satisfying for all \\(A, B \\in \\mathbb{R}^{n\\times n}\\) and all \\(\\alpha \\in \\mathbb{C}\\):</p> <ul> <li>\u6b63\u6027 | positive</li> </ul> \\[ \\Vert \\mathbfit{A} \\Vert\\geq 0 \\] <ul> <li>\u5b9a\u6027 | definite</li> </ul> \\[ \\Vert \\mathbfit{A} \\Vert = 0 \\Leftrightarrow \\mathbfit{A}=\\mathbfit{0} \\] <ul> <li>\u9f50\u6027 | homogeneous</li> </ul> \\[ \\Vert \\alpha\\mathbfit{A} \\Vert = |\\alpha|\\Vert \\mathbfit{A} \\Vert \\] <ul> <li>\u4e09\u89d2\u4e0d\u7b49\u5f0f | triangle inequality</li> </ul> \\[ \\Vert \\mathbfit{A}+\\mathbfit{B} \\Vert \\leq \\Vert \\mathbfit{A} \\Vert+\\Vert \\mathbfit{B} \\Vert \\] <ul> <li>\u4e00\u81f4\u6027 | consistent</li> </ul> \\[ \\Vert \\mathbfit{A}\\mathbfit{B} \\Vert\\leq \\Vert \\mathbfit{A} \\Vert \\cdot \\Vert \\mathbfit{B} \\Vert \\] <p>Usually we use Natural Norm:</p> \\[ \\Vert \\mathbfit{A} \\Vert = \\max_{\\vec{x}\\neq 0}\\frac{\\Vert \\mathbfit{A}\\vec{x} \\Vert_p}{\\Vert \\vec{x} \\Vert_p} = \\max_{\\Vert \\vec{x} \\Vert_p =1}\\Vert \\mathbfit{A}\\vec{x} \\Vert \\] <p>with its common forms:</p> \\[ \\begin{align*} \\Vert \\mathbfit{A} \\Vert_1 &amp;= \\max_{1\\leq i\\leq n}\\sum_{j=1}^{n}|a_{ij}| \\quad\\text{the maximum of row summation}\\\\ \\Vert \\mathbfit{A} \\Vert_2 &amp;= \\sqrt{\\max \\rho(A^T A)} \\quad\\text{the maximum of spectrum radius}\\\\ \\Vert \\mathbfit{A} \\Vert_\\infty &amp;= \\max_{1\\leq j\\leq n}\\sum_{i=1}^{n}|a_{ij}| \\quad\\text{the maximum of column summation}\\\\ \\end{align*} \\] <p>for \\(p=2\\) norm of matrix, we have special expression for special matrix:</p> <p>Sepecial Expreesion of Norm 2 of matrix</p> <p>If matrix \\(A\\) is symetrical, then</p> \\[ \\Vert \\mathbfit{A} \\Vert_2 = \\sqrt{\\max \\rho(A)}. \\] <p>If matrix \\(A\\) is orthogonal(only rotate), then</p> \\[ \\Vert \\mathbfit{A} \\Vert_2 = 1. \\]"},{"location":"courses/Numerical_Analysis/MC/#jacobis-method","title":"Jacobi's Method  <p>Here we denote \\(L\\) and \\(U\\) to be the lower-triangular and upper-triangular matrix of matrix \\(A\\) without its diagonal elements respectively. (different from \\(LU\\) factorization!) And then we denote \\(D\\) to be the diagonal elements of the matrix of \\(A\\). That is, </p> \\[ A = D - L -U \\] <p>Then</p> \\[  \\begin{align*} A\\pmb{x} &amp;= \\pmb{b} \\\\ (D-L-U)\\pmb{x} &amp;= \\pmb{b} \\\\ D\\pmb{x} &amp;= (L+U)\\pmb{x} + \\pmb{b} \\\\ \\pmb{x} &amp;= D^{-1}(L+U)\\pmb{x} + D^{-1}\\pmb{b}  \\end{align*}  \\] <p>which gives matrix form of the Jacobi iterative technique</p> \\[ \\pmb{x}^{k} = D^{-1}(L+U)\\pmb{x}^{k-1} + D^{-1}\\pmb{b} \\]","text":""},{"location":"courses/Numerical_Analysis/MC/#the-gauss-seidel-method","title":"The Gauss-Seidel Method <p>This method sees that a little slowness in Jacobi's Method. That is, for each itearion period(\\(\\pmb{x}^{k} \\leftarrow \\pmb{x}^{k-1}\\)), it makes use of the generated \\(\\pmb{x}^{k}_{i}\\) in the \\(i\\)th row of \\(\\pmb{x}^{k}\\) and use it to update the coressponding element in \\(\\pmb{x}^{k-1}\\).</p> <p>In matrix form, we have</p> \\[ D\\pmb{x}^{k} = U\\pmb{x}^{k-1} + L\\pmb{x}^{k}+ \\pmb{b} \\] <p>(to be proved by readers)</p> <p>then</p> \\[ \\pmb{x}^{k} = (D-L)^{-1}U\\pmb{x}^{k-1} + (D-L)^{-1}\\pmb{b} \\]","text":""},{"location":"courses/Numerical_Analysis/MC/#approximating-eigenvalues","title":"Approximating Eigenvalues","text":""},{"location":"courses/Numerical_Analysis/MC/#the-power-method","title":"The Power Method <p>Assume that \\(A\\) has eigenvalues \\(|\\lambda_1|&gt;|\\lambda_2|\\geq |\\lambda_3|\\geq \\cdots \\geq |\\lambda_n|\\geq 0\\), we can use the following method to make the largest \\(lambda_1\\) stand out.</p>  <p>\u5e42\u6cd5 | The Power Method</p> <p>Initialize randomly \\(\\vec{x}\\), which can be represented by \\(n\\) linearly irrelevant eigenvectors \\(\\vec{v}_1, \\vec{v}_2, \\cdots, \\vec{v}_n\\), with parameters \\(\\beta_1, \\beta_2, \\cdots, \\beta_n\\), such that</p> \\[ \\vec{x} = \\sum_{i=1}^{n}\\beta_i\\vec{v_i} \\] <p>multiply both sides by \\(A\\), according to \\(A\\vec{v_i}=\\lambda_i \\vec{v_i}\\), we get</p> \\[ A\\vec{x} = \\sum_{i=1}^{n}\\beta_i \\lambda_i \\vec{v_i} \\] <p>repeat this process for \\(n\\) times we get </p> \\[ A^n\\vec{x} = \\sum_{i=1}^{n}\\beta_i \\lambda_i^n \\vec{v_i} = \\lambda_1^n\\sum_{i=1}^{n}\\beta_i (\\frac{\\lambda_i}{\\lambda_1})^n \\vec{v_i}\\rightarrow \\lambda_1^n \\beta_1 \\vec{v_1} \\quad (n\\rightarrow \\infty) \\] <p>That is, we can neglect eigenvalues that are smaller than \\(\\lambda_1\\) through multiplying \\(A\\) and \"extract\" the biggest one.</p>  <p>To avoid divengence caused by \\(\\lambda_1&gt;0\\), we need to normalize \\(\\vec{x}^{k} = A\\vec{x}^{k-1}\\) each step after multiplying. Usually we choose \\(\\Vert\\  \\Vert_\\infty\\).</p> <p>To get the \\(\\lambda_1\\) out, we can use </p> \\[ \\frac{\\Vert\\vec{x}^{k}\\Vert}{\\Vert\\vec{x}^{k-1}\\Vert} \\approx \\lambda_1 \\quad (n\\rightarrow \\infty) \\] <p>to get \\(\\lambda_1\\).</p> <p>The next question is, naively, how about the speed of converging? Luckily, the question is easy to answer:</p> <p>rely on ratio \\(\\left|\\frac{\\lambda_2}{\\lambda_1}\\right|\\).</p>","text":""},{"location":"courses/Numerical_Analysis/MC/#inverse-power-method","title":"Inverse Power Method <p>This is a trick from the Power method. It comes from a question: what if we want to calculate the smallest eigenvalue of a matrix \\(A\\)?</p> <p>The answer is, by taking use of metrix inverse.</p>  <p>\u53cd\u5e42\u6cd5 | Inverse Power Method</p> <p>Matrix \\(A\\) has eigenvalues \\(|\\lambda_1| &lt; |\\lambda_2| \\leq \\cdots \\leq |\\lambda_n|\\), then matrix \\(A^{-1}\\) has eigenvalues </p> \\[ \\left|\\frac{1}{\\lambda_1}\\right| &gt; \\left|\\frac{1}{\\lambda_2}\\right| \\geq \\cdots \\geq \\left|\\frac{1}{\\lambda_n}\\right| \\] <p>Then use the power method to get \\(\\left|\\frac{1}{\\lambda_1}\\right|\\) out.</p>  <p>Actually, the above method is more often being used in situation where we have known an eigenvalue \\(\\lambda_1\\) (not neecssarily the largest or smallest) of \\(A\\) is close to a constant \\(q\\). That is, we can formulate matrix </p> \\[ (A-qI) \\] <p>which has eigenvalues </p> \\[ |\\lambda_1 - q| &lt; |\\lambda_2 - q| \\leq \\cdots \\leq \\left|\\lambda_n - q\\right| \\] <p>Then we can use the above method to get \\(\\lambda_1\\) out.</p>","text":""},{"location":"courses/Numerical_Analysis/NA/","title":"Numerical Analysis","text":""},{"location":"courses/Numerical_Analysis/NA/#numerical-analysis","title":"Numerical Analysis","text":""},{"location":"courses/Numerical_Analysis/NA/#basic-ideas-for-solving-equation","title":"Basic ideas for Solving Equation","text":"<p>To find the solution of an equation, we hope to have an iteration method which takes good advantage of Computer resources like</p> \\[ \\begin{equation}  \\pmb{x}^{k} = f(\\pmb{x}^{k-1}) \\label{eq: iterative eq}  \\end{equation} \\] <p>for \\(k = 1, 2, \\cdots n\\). We use \\(\\pmb{x}\\) instead of \\(x\\) because the above iteration method also applies to solving linear system.</p> <p>Hopefully, if the above equation converges, that is, for \\(k \\rightarrow \\infty\\), \\(\\pmb{x}^{k-1}\\rightarrow \\pmb{x}^*\\), \\(\\pmb{x}^{k}\\rightarrow \\pmb{x}^*\\), and the equation becomes</p> \\[ \\pmb{x}^* = f(\\pmb{x}^*) \\] <p>where \\(\\pmb{x^*}\\) is the sulution of the equation to be solved.</p> <p>If the above thought gets right, then we can consider the converging speed of the iterative process, which makes great sense in practical applications. That is, in a given definition of distence,</p> \\[ \\frac{\\|\\pmb{x}^{k+1} - \\pmb{x}^*\\|}{\\|\\pmb{x}^{k}-\\pmb{x}^*\\|^\\alpha}  \\] <p>to be small as much as possible for each \\(k\\).</p>"},{"location":"courses/Numerical_Analysis/NA/#preliminary-errors","title":"Preliminary: Errors","text":"<p>If a real number \\(x\\) is denoted as \\(0.d_1d_2d_3\\cdots \\times 10^{n}\\), then</p> <ul> <li>Truncation\uff08\u622a\u65ad\uff09 Error</li> </ul> <p>is induced when </p> \\[ \\hat{x}=0.d_1d_2d_3\\cdots d_k \\times 10^{n}  \\] <p>for some definite \\(k&lt;\\infty\\)</p> <ul> <li>Roundoff\uff08\u820d\u5165\uff09 Error</li> </ul> <p>is induced when </p> \\[ \\hat{x}=0. \\delta_1 \\delta_2 \\delta_3 \\cdots \\delta_k \\times 10^{n}  \\] <p>for some definite \\(k&lt;\\infty\\) </p> <p>where \\(\\delta_k &gt;d_k\\) if \\(d_{k+1}&gt;=5\\).</p>"},{"location":"courses/Numerical_Analysis/NA/#t-significant-digits","title":"t significant digits","text":"<p>The number \\(p^*\\) is said to approximate p to \\(t\\) significant digits(or figures) if \\(t\\) is the largest nonnegative integer for which the relative error </p> \\[ e = \\frac{\\Delta p}{p}=\\frac{\\|p-p^*\\|}{\\|p\\|}&lt;5\\times 10^{-t}  \\] <p>where \\(p^*\\) is the approximate number of the exact number \\(p\\).</p> <ul> <li>for Chopping:</li> </ul> \\[  \\begin{align*} e &amp;= \\left|\\frac{0.d_{k+1}d_{k+2}\\cdots}{0.d_1d_2\\cdots}\\right| \\times 10^{-k} \\\\ &amp;\\leq \\frac{1}{0.1} \\times 10^{-k} \\quad \\text{\"=\" for } d_{k+1}d_{k+2}\\cdots\\rightarrow\\overline{9}\\text{ and }d_{1}d_{2}\\cdots\\rightarrow 0 \\\\ &amp;=10^{-k+1}  \\end{align*}  \\] <ul> <li>for rounding:</li> </ul> \\[ \\begin{align*} e &amp;\\leq \\frac{0.5}{0.1} \\times 10^{-k} \\quad \\text{\"=\" for } d_{k+1}d_{k+2}\\cdots\\rightarrow 5\\overline{0}\\text{ and }d_{1}d_{2}\\cdots\\rightarrow 0 \\\\  &amp;=0.5\\times 10^{-k+1} \\end{align*}  \\]"},{"location":"courses/Numerical_Analysis/NA/#solutions-of-equations-in-one-variables","title":"Solutions of Equations in One Variables","text":""},{"location":"courses/Numerical_Analysis/NA/#the-bisection-method","title":"the Bisection Method\uff08\u4e8c\u5206\u6cd5\uff09","text":"<p>This method is quite intuitive. By choosing two end points \\(a, b\\), we get another point (Mid-point here)</p> \\[ p = a+\\frac{b-a}{2} \\] <p>Then update \\(a, b\\) by evaluating whether \\(f(p)&gt;0\\) or not to narrow down the interval.</p> <p>What is interesting is the stopping procedure. Readers can see the following question if interested.</p> <p>When we calculate the new point \\(p\\) based on \\(a, b\\), we need to judge whether \\(p\\) is an appropriate answer. Apart from \\(f(p)=0\\), which condition do you think is the best?</p> <ol> <li>\\((b-a)/{|\\min{(a, b)}|}&lt;\\epsilon\\)</li> <li>\\(|p-p_{prev}|=(b-a)/2 &lt; \\epsilon\\)</li> <li>\\(f(p)&lt;\\epsilon\\)</li> </ol> Choose an answerAnwser <ul> <li>1</li> <li>2</li> <li>3</li> </ul> <p>Choose 1, which is close to relative error, currently the best.</p> <p>2: consider \\(\\{p_n\\}=\\sum\\limits_{i=1}^{n}\\frac{1}{k}\\).</p> <p>3: easy to see.</p> <p>The converging speed can discribed as the following:</p> \\[ |x_n- x^*| &lt; \\frac{(b-a)}{2^{n}} \\]"},{"location":"courses/Numerical_Analysis/NA/#Fixed-Point-Iteration","title":"Fixed-Point Iteration\uff08\u4e0d\u52a8\u70b9\u6cd5\uff09","text":"<p>As we said previously in Basic ideas for solving equation, we hope to find an iterative relation such that the converging point \\(x^*\\) is exactly what we want, which in this case, means that </p> \\[ f(x^*) = 0 \\] <p>So intuitively, we ask: Whether can we derive a relation from </p> \\[ \\begin{equation} f(x) = 0 \\label{zero-equation} \\end{equation} \\] <p>to </p> \\[ x = g(x) \\] <p>for iterative method?</p> <p>The answer is, of course, YES!</p> <p>One of the easist way to transform is adding \\(x\\) to both sides of equation \\(\\ref{zero-equation}\\), but in most cases this does not work. Because we rely on \\(f(x)\\) ifself for the convergence! </p> <p>Thus, it is necessary to find the condition for \\(x = g(x)\\) to converge. The following theorem <p></p> gives a Sufficient condition.</p> <p>\u4e0d\u52a8\u70b9\u5b58\u5728\u5b9a\u7406 | Fixed-Point Theorom</p> <p>Let \\(g \\in C[a, b]\\) be such that \\(g(x) \\in [a, b]\\), for all \\(x\\) in \\([a, b]\\). Suppose, in addition, that \\(g'\\) exists on \\((a, b)\\) and that a constant \\(0 &lt; k &lt; 1\\) exists with</p> \\[ |g'(x)|\\leq k \\quad \\forall x \\in (a, b) \\] <p>Then for any initial number \\(p_0 \\in [a, b]\\), the sequence \\(\\{p_n\\}_{n=0}^{\\infty}\\) defined by </p> \\[ p_n = g( p_{n\u22121}) \\quad n \\geq 1 \\] <p>converges to the unique fixed point \\(p \\in [a, b]\\).</p> <p>Proving it is easily.</p> HintsProof <ul> <li>using the differential mean value theorem.</li> </ul> <p>\\(\\forall n \\geq 1, \\exists \\zeta_n \\in (p_{n-1}, p) \\subset (a, b)\\), we have</p> \\[ |p_n-p| = |g(p_{n-1}) - g(p)| = g'(\\zeta_n)|p_{n-1}-p|\\leq k|p_{n-1}-p| \\] <p>by induction, we have</p> \\[ |p_n-p|\\leq k^{n}|p_0-p| \\] <p>Let \\(n \\rightarrow \\infty\\), \\(|p_n-p| \\rightarrow 0\\), that is, \\(p_n\\) converges to \\(p\\). </p> <p>What we use in the proof will benefit us in identifying the speed of converging process.</p>"},{"location":"courses/Numerical_Analysis/NA/#newtons-method","title":"Newton's Method\uff08\u725b\u987f\u6cd5\uff09","text":"<p>This method is also a fixed-point method. There are two perspectives to get the inspirations.</p> HintsVersion 1Version 2 <ul> <li>version1: shrink the derivative of the iterative function \\(g(x)\\).</li> <li>version2: using Taylor's expansion.</li> </ul> <p>We can know that given a random function \\(f(x)\\), it may not be convergent to some point \\(x^*\\) for \\(x^{k} = f(x^{k-1}) + x^{k-1}\\) in a given interval. So the queation is, can we formulate a function \\(g(x)\\) such that \\(x^{k} = g(x^{k-1})\\) is convergent?</p> <p>The answer is, again, YES!</p> <p>The following content tells us we can formulate \\(g(x)= x - f(x)/(f'(x))\\) such that \\(g'(x) &lt; 1\\) in a given interval.</p> <p>Readers can easily see that </p> \\[ \\begin{align*} g'(x) &amp;= 1 - \\frac{f'^2(x)-f''(x)f(x)}{f'^2(x)}\\\\ &amp;=\\frac{f''(x)f(x)}{f'^2(x)} \\end{align*} \\] <p>if we add some constrictions, it can be easy to make \\(g'(x)&lt;1\\).</p> <p>Here we make use of the Taylor's expansion(or the derivatives) of the goal function. </p> <p>Suppose that \\(f \\in C^2[a, b]\\), Let \\(x_0 \\in [a, b]\\) be an approximation to \\(x^*\\) such that \\(f(x^*) \\neq 0\\) and \\(|x_0-x^*|\\) is \"small\". Consider the first Taylor polynomial for \\(f(x)\\) expanded at \\(x_0\\):</p> \\[  f(x) = f(x_0) + (x-x_0)f'(x_0) + \\frac{(x-x_0)^2}{2}f''(\\zeta). \\] <p>If we let \\(x = x^*\\), and according to \\(f(x^*)=0\\), we get </p> \\[ 0 = f(x_0) + (x^*-x_0)f'(x_0) + \\frac{(x^*-x_0)^2}{2}f''(\\zeta) \\] <p>neglecting the square item, we get </p> \\[ 0 \\approx f(x_0) + (x^*-x_0)f'(x_0) \\] <p>to represent \\(x^*\\), we get</p> \\[ x^* = x_0 - \\frac{f(x_0)}{f'(x_0)} \\] <p>Then we can define the iterative relation as</p> \\[  x_n = x_{n-1} - \\frac{f(x_{n-1})}{f'(x_{n-1})}  \\] <p>The following statement guarrantees the convergence of the above iterative method.</p> <p>\u725b\u987f\u6cd5\u6536\u655b\u6761\u4ef6 | conditions for convergence of Newton's method</p> <p>Let \\(f \\in C^2[a, b]\\). If \\(p \\in (a, b)\\) is such that \\(f (p) = 0\\) and \\(f'( p) \\neq 0\\), then there exists a \\(\\delta &gt; 0\\) such that Newton\u2019s method generates a sequence \\(\\{p_n\\}_{n=1}^{\\infty}\\) converging to \\(p\\) for any initial approximation \\(p_0 \\in [p \u2212 \\delta, p + \\delta]\\).</p> <p>Prove it.</p> HintsProof <ul> <li>make use of the condition \\(f(p) = 0\\) and \\(f'(p)\\neq 0\\)</li> </ul> <p>for \\(x \\in (a, b)\\), we aim to find a narrower interval \\((x^*-\\delta, x^*+\\delta)\\) to have \\(g(x)\\) map into itself. That is, </p> \\[ g(x)\\leq k, \\forall k\\in (0,1) \\] <p>Firstly, \\(f'(p)\\neq 0\\) implies that \\(\\exists \\delta_1 &gt;0\\) such that \\(f'(x)\\neq 0, \\forall x \\in [x^* - \\delta, x^*+\\delta]\\subset [a, b]\\).</p> <p>THus, we have</p> \\[ g'(x) = \\frac{f(x)f''(x)}{(f'^2(x))} \\] <p>capable of dividing non-zero numbers.</p> <p>Since \\(f\\in C^2[a,b]\\), we have \\(g' \\in C^1[x^*-\\delta_1, x^*+\\delta_1]\\) for the exact solution \\(x^*\\), we have \\(f(x^*)=0\\), so </p> \\[ g'(x^*) = 0 \\] <p>which implies that \\(\\exists 0&lt;\\delta &lt; \\delta_1\\), such that </p> \\[ g'(x)\\leq k, \\forall k \\in [x^*-delta, x^*+\\delta] \\] <p>By differential Mean Value Theorem, for \\(x \\in [x^*-delta, x^*+\\delta], \\exists \\zeta \\in [x, x^*]\\) such that </p> \\[ |g(x)-g(x^*)|=g'(\\zeta)|x - x^*|\\leq k|x-x^*|&lt;|x-x^*| \\] <p>which means that \\(g\\) maps into itself. By Fixed-Point Theorom, the sequence defined by Newton's method converges. </p>"},{"location":"courses/Numerical_Analysis/NA/#secant-method","title":"Secant Method","text":"<p>It may not be easy to find derivarive of function \\(f\\), so we can use difference instead. That is, we have to store two adjacent points for calculating differnce</p> \\[ f'(x^{k}) \\approx \\frac{f(x^{k}) - f(x^{k-1})}{x^{k}-x^{k-1}} \\] <p>generate \\(p_{k+1}\\) using the above approximation and iterate.</p>"},{"location":"courses/Numerical_Analysis/NA/#order-of-convergence","title":"Order of Convergence","text":"<p>So how to identify the speed of convergence? The following definition gives a glimpse.</p> <p>Suppose \\(\\{p_n\\}_{n=1}^{\\infty}\\) is a sequence that converges to \\(p\\), with \\(p_n \\neq p (\\forall n)\\). If positive constants \\(\\lambda\\) and \\(\\alpha\\) exist with</p> \\[ \\lim_{n\\rightarrow \\infty}{\\frac{|p_{n+1}-p|}{|p_n-p|^{\\alpha}}}=\\lambda \\] <p>then \\(\\{p_n\\}_{n=0}^{\\infty}\\) converges to \\(p\\) of order \\(\\alpha\\), with asymptotic error constant \\(\\lambda\\).</p> <ul> <li>(i) If \\(\\alpha=1 (\\lambda&lt;1)\\), the sequence is linearly convergent.</li> <li>(ii) If \\(\\alpha=2\\), the sequence is quadratically convergent.</li> </ul> <p>The following theorem gives a sufficient condition for linear convergence.</p> <p>\u7ebf\u6027\u6536\u655b\u7684\u5145\u5206\u6761\u4ef6 | sufficient condition of linear convergence</p> <p>Let \\(g \\in C[a, b]\\) be such that \\(g(x) \\in [a, b], \\forall x \\in [a, b]\\). Suppose, in addition, that \\(g\\) is continuous on \\((a, b)\\) and a positive constant \\(k &lt; 1\\) exists with</p> \\[ |g'(x)|\\leq k \\quad \\forall x \\in (a, b) \\] <p>If \\(g'(p) \\neq 0\\), then for any number \\(p_0=p\\) in \\([a, b]\\), the sequence </p> \\[ p_n=g(p_{n-1}) \\quad \\forall n \\geq 1 \\] <p>converges only linearly to the unique fixed point \\(p\\) in \\([a, b]\\).</p> <p>Prove it.</p> Hints <p>Prove that linear convergence represents \\(\\exists alpha=1, lambda\\), such that \\(\\lim\\limits_{n\\leftarrow \\infty}\\frac{|p_{n+1}-p^*|}{|p_{n}-p^*|} = \\lambda\\).</p>"},{"location":"courses/Numerical_Analysis/NA/#multiple-roots","title":"multiple roots","text":"<p>We see that the speed of convergence is limited by multiple roots.</p> <p>Here we have modified Newton's Method:</p> \\[ g(x)= x - \\frac{f(x)f'(x)}{f'^2(x)-f(x)f''(x)} \\]"},{"location":"courses/Numerical_Analysis/NA/#accelerating-convergence","title":"Accelerating convergence","text":""},{"location":"courses/Numerical_Analysis/NA/#aitkens-delta2-method","title":"Aitken's \\(\\Delta^2\\) Method","text":"<p>Suppose \\(\\{p_n\\}_{n=0}^{\\infty}\\) is a linearly convergent sequence with limit \\(p\\). To motivate the construction of a sequence \\(\\{\\hat{p}_n\\}_{n=1}^{\\infty}\\) that converges more rapidly to \\(p\\) than does \\(\\{p_n\\}_{n=0}^{\\infty}\\), let us first assume that the signs of \\(p_n-p\\), \\(p_{n+1}-p\\) and \\(p_{n+2}-p\\) agree and that \\(n\\) is sufficiently large that </p> \\[ \\frac{p_{n+1}-p}{p_n-p} \\approx \\frac{p_{n+2}-p}{p_{n+1}-p} \\] <p>Then solving for \\(p\\) gives</p> \\[ p \\approx \\frac{p_{n+2}p_n-p_{n+1}^2}{p_{n+2}-2p_{n+1}+p_n} \\] <p>And to get \\(p_n\\) out gives</p> \\[ \\begin{align*} p &amp;\\approx p_n - \\frac{(p_{n+1}-p_n)^2}{p_{n+2} - 2p_{n+1} + p_n}\\\\ \\Rightarrow \\hat{p}_n &amp;= p_n - \\frac{(\\Delta p_n)^2}{\\Delta p_{n+1}-\\Delta p_{n}} \\quad \\text{(denote $\\Delta p_n = p_{n+1} - p_n$)}\\\\ &amp;= p_n - \\frac{(\\Delta p_n)^2}{\\Delta^2 P_{n}}  \\end{align*}  \\]"},{"location":"courses/Numerical_Analysis/NA/#steffensens-method","title":"Steffensen's Method","text":"<p>The following thought is based on that the generated sequence \\(\\hat{p}\\) is a better approximation to true \\(p^*\\). We make use of the constructed sequence \\(\\{\\hat{p}_n\\}\\) to update the original sequence \\(\\{p_n\\}\\). That is, after generating a new \\(\\hat{p}\\), we can update \\(p_0 \\leftarrow p\\).</p>"},{"location":"courses/Numerical_Analysis/NA/#interpolation-and-polynomial-approximation","title":"Interpolation and Polynomial Approximation","text":""},{"location":"courses/Numerical_Analysis/NA/#lagrange-interpolating-polynomial","title":"Lagrange Interpolating Polynomial","text":"<p>Inspired by \u52a0\u6743\u5e73\u5747.</p> <p>There exists and only exists a \\(n\\)th Lagrange interpolating polynomial (\u62c9\u683c\u6717\u65e5\u57fa\u51fd\u6570) </p> \\[  L_n(x) = \\sum_{i=0}^{n}l_i(x)y_i = \\begin{bmatrix} l_0(x) &amp; l_1(x) &amp;l_2(x) &amp; \\cdots &amp; l_n(x)  \\end{bmatrix} \\begin{bmatrix} y_0 \\\\ y_1 \\\\ \\vdots \\\\ y_n  \\end{bmatrix} = \\Phi_n(x)\\vec{y}  \\] <p>such that for each pair of given points \\((x_i, y_i)\\), \\(i = 0, 1,2,\\cdots n\\), we have \\(y_i = L_n(x_i)\\).</p> <p>Here we can consider \\(l_i(x)\\) as a base of a linear space \\(\\mathcal{P}_n(x)\\), and it can be displayed by natural base \\(1, x, x^2, \\cdots x^n\\). To be more specific,</p> \\[ l_i(x) = \\prod_{j=0 \\atop j \\neq i }^{n}\\frac{(x - x_i)}{(x_i-x_j)} \\quad i=0,1,\\cdots n  \\] <p>readers can prove the above \\(n+1\\) polynomials are linearly irrelevant.</p> <p>In fact, if we assume \\(P_n(x) = \\sum\\limits_{i=0}^{n}a_ix^i\\)(natural base), and to get the parameters \\(\\{a_i\\}\\) such that \\(P_n(x_i) = y_i\\), we have to solve the following linear system</p> \\[ \\begin{bmatrix} 1 &amp; x_0 &amp; x_0^2 &amp;\\cdots &amp; x_0^n \\\\  1 &amp; x_1 &amp; x_1^2 &amp;\\cdots &amp; x_1^n \\\\  \\vdots &amp; \\vdots &amp; \\vdots &amp; &amp;\\vdots \\\\ 1 &amp; x_n &amp; x_n^2 &amp;\\cdots &amp; x_n^n  \\end{bmatrix} \\begin{bmatrix}  a_0 \\\\ a_1 \\\\ \\vdots \\\\ a_n  \\end{bmatrix}=  \\begin{bmatrix} y_0 \\\\ y_1 \\\\ \\vdots \\\\ y_n  \\end{bmatrix}  \\] <p>which is a little tedious.</p>"},{"location":"courses/Numerical_Analysis/NA/#error","title":"Error","text":"<p>The following theorem gives the error bound.</p> <p>\u62c9\u683c\u6717\u65e5\u57fa\u51fd\u6570\u7684\u4f59\u9879 | The remainder of Lagrange interpolating polynomial</p> <p>Suppose \\(x_0, x_1, \\cdots , x_n\\) are distinct numbers in the interval \\([a, b]\\) and \\(f \\in C^{n+1{[a, b]\\). Then, for each \\(x \\in [a, b]\\), a number \\(\\xi(x)\\) (generally unknown) between \\(x_0, x_1, \\cdots , x_n\\), and hence in \\((a, b)\\), exists with</p> \\[ f(x) = P(x) + \\frac{f^{n+1}(\\xi(x))}{(n+1)!}\\prod_{i=0}^{n}(x - x_i) \\] <p>where \\(P(x)\\) is the Lagrange interpolating polynomial.</p> <p>Prove it.</p> Hints <ul> <li>Using a function </li> </ul> \\[ g(t) = f(t) - P(t) - [f(t)-g(t)]\\prod_{i=0}^{n}\\frac{(t-x_i)}{x-x_i} \\] <p>which is \\(C^{n+1}[a, b]\\). Note that \\(f(x_k)=0\\) and \\(f(x)=0\\) for \\(x \\in [a, b]\\). We can see that \\(n+1+1=+\\) zero points here. </p> <ul> <li>Using generalized Rolle's Theorem: There exists \\(\\xi(x) \\in [a, b]\\) such that \\(g^{n+1}(\\xi(x))=0\\)</li> </ul> <p>And make \\(n+1\\) derivatives to \\(g(t)\\)(corresponding to \\(t\\)), and we can get the result. </p> <p>Compared to the remainder of Taylor's extension</p> \\[ R_n(x) = \\frac{f^{n+1}(\\xi(x))}{(n+1)!}(x-x_0)^{n+1} \\] <p>Because \\(\\xi(x)\\) is usually unknown, so we often use a number \\(x' \\in  [a, b]\\) such that </p> \\[ |f^{n+1}(\\xi(x))|\\leq |f^{n+1}(x')| \\]"},{"location":"courses/Numerical_Analysis/NA/#nevilles-method","title":"Neville's Method","text":"<p>An interpolation polynomial for a set of points \\(A =\\{x_0, x_1, \\cdots x_n \\}\\) can be expressed by two polynomials that interpolate \\(A\\) \\ \\(\\{ x_i\\}\\) and \\(A\\) \\ \\(\\{ x_j\\}\\). That is,</p> \\[ P_n(x) = \\frac{(x-x_i)P_{0,1,\\cdots,i-1,i+1,\\cdots,n}(x)-(x-x_j)P_{0,1,\\cdots,j-1,j+1,\\cdots,n}(x)}{(x_j-x_i)} \\] <p>where \\(P_{0,1,\\cdots,i-1,i+1,\\cdots,n}(x)\\) and \\((x-x_j)P_{0,1,\\cdots,j-1,j+1,\\cdots,n}(x)\\) denotes the polynomial that interpolates \\(A\\) \\ \\(\\{ x_i\\}\\) and \\(A\\) \\ \\(\\{ x_j\\}\\) respectively.</p>"},{"location":"courses/Numerical_Analysis/NA/#divided-differences","title":"Divided Differences","text":"<p>If we rewrite the \\(n\\)th Lagrange polynomial \\(P_n(x)\\) into another form:</p> \\[ P_n(x) = a_0+a_1(x-x_0)+a_2(x-x_0)(x-x_1)+\\cdots+a_n(x-x_0)\\cdots(x-x_n) \\] <p>By letting \\(x =x_0, x_1,\\cdots, x_n\\), we get</p> \\[ \\begin{align*} P_n(x_0) &amp;= a_0\\\\ P_n(x_1) &amp;= a_0 + a_1(x_1-x_0)\\\\ P_n(x_2) &amp;= a_0+a_1(x_2-x_0)+a_2(x_2-x_0)(x_2-x_1)\\\\ &amp;\\vdots\\\\ P_n(x_2) &amp;= a_0+a_1(x_2-x_0)+a_2(x_2-x_0)(x_2-x_1)+\\cdots+a_n(x_2-x_0)\\cdots(x_2-x_n) \\end{align*} \\] <p>Then we can define:</p> \\[ \\begin{align*} f_n[x_0] &amp;\\overset{\\Delta}{=} f(x_0) = a_0\\\\ f_n[x_0, x_1] &amp;\\overset{\\Delta}{=} \\frac{f(x_1) - f(x_0)}{x_1-x_0} = a_1\\\\ f_n[x_0, x_1, x_2] &amp;\\overset{\\Delta}{=} \\frac{f[x_1,x_2]-f[x_0,x_1]}{x_2-x_0} \\\\&amp;= \\frac{\\frac{f(x_2)-f(x_1)}{x_2-x_1}-\\frac{f(x_1)-f(x_0)}{x_1-x_0} }{x_2-x_0} \\\\&amp;= \\frac{\\frac{f(x_2)-f(x_0)-(f(x_1)-f(x_0))}{x_2-x_1}-a_1}{x_2-x_0} \\\\ &amp;= \\frac{\\frac{f(x_2)-f(x_0)}{x_2-x_1} - \\frac{a_1(x_1-x_0)}{x_2-x_1}-\\frac{a_1(x_2-x_1)}{x_2-x_1}}{x_2-x_0} \\\\ &amp;= \\frac{\\frac{f(x_2)-f(x_0)}{x_2-x_1} - \\frac{a_1(x_2-x_0)}{x_2-x_1}}{x_2-x_0}\\\\ &amp;= \\frac{f(x_2)-a_0 - a_1(x_2-x_0)}{(x_2-x_0)(x_2-x_1)}=a_2\\\\ &amp;\\vdots\\\\ f[x_1,x_2,\\cdots, x_n] &amp;\\overset{\\Delta}{=} \\frac{f[x_1,x_2,\\cdots,x_n] - f[x_0,x_1,\\cdots,x_{n-1}]}{x_n-x_0} = a_n \\end{align*} \\] <p>We can prove the above definition \\(f[x_0,x_1\\cdots,x_n]\\), which is called divided difference, to be equal to \\(a_n\\) by induction.</p> <p>This iterative method is quite useful in determining the parameters of \\(n\\)th Lagrange polynomial.</p> \\[ P_n(x) = f[x_0] + \\sum_{i=1}^{n}\\left(f[x_0,x_1,\\cdots,x_i]\\prod_{j=0}^{i-1}(x-x_j)\\right) \\] <p>Some properties are the followings.</p> <p>\u5dee\u5206\u4e0e\u5fae\u5206\u5747\u503c\u7684\u5173\u7cfb | Relationship of DD &amp; Maen Value Differentials</p> <p>Suppose that \\(f \\in C^n[a, b]\\) and \\(x_0, x_1, \\cdots, x_n\\) are distinct numbers in \\([a, b]\\). Then a number \\(\\xi\\) exists in \\((a, b)\\) with</p> \\[ f[x_0,x_1\\cdots,x_n] = \\frac{f^{(n)}(\\xi)}{n!} \\] <p>It is like </p> \\[ f[x_0,x_1] = \\frac{f(x_1)-f(x_0)}{x_1-x_0} = f'(\\xi) \\] <p>but add \\(n!\\) to the denominator.</p> <p>The following relation gives a slightly quicker way to compute \\(f[x_0,x_1,\\cdots,x_n]\\):</p> <p>Another expression of calculating divided difference</p> \\[ f[x_0,x_1,\\cdots,x_n] = \\sum_{k=0}^{n}\\frac{f(x_k)}{\\omega'_{n+1}(x_k)} \\] <p>where </p> \\[ \\omega_{n+1}(x) = \\prod_{i=0}^{n}(x-x_i),\\quad \\omega'_{n+1}(x_j) = \\prod_{i=0 \\atop i \\neq j}^{n}(x_j-x_i) \\]"},{"location":"courses/Numerical_Analysis/NA/#hermite-polynomials","title":"Hermite Polynomials","text":"<p>We want to consider the smoothness of interpolating polynomials, so we need to consider its derivatives, especially the first order derivative. </p> <p>Definition of Osculating Polynomial</p> <p>The osculating polynomial approximating \\(f\\) is the polynomial \\(P(x)\\) of least degree such that</p> \\[ \\frac{d^kP(x_i)}{dx^k} = \\frac{d^kf(x_i)}{dx^k},\\quad, \\forall i = 0,1,\\cdots, n, \\forall k = 0,1,\\cdots, m_i \\] <p>Where \\(n\\) is the total number of sampling points and \\(m_i\\) is the degree of smoothness at point \\(x_i\\).</p> <p>If \\(m_i=1\\) for all \\(i=0,1,\\cdots, n\\), then the above polynomial is called Hermite Polynomial.</p> <p>Composition of Hermite Polynomial</p> <p>If \\(f \\in C^1[a, b]\\) and \\(x_0, \\cdots , x_n \\in [a, b]\\) are distinct, the unique polynomial of least degree agreeing with \\(f\\) and \\(f'\\) at \\(x_0,\\cdots , x_n\\) is the Hermite polynomial of degree at most \\(2n + 1\\) given by</p> \\[ H_{2n+1}(x) = \\sum_{j=0}^{n}f(x_j)H_{n,j}(x) + \\sum_{j=0}^{n}f'(x_j)\\hat{H}_{n,j}(x) \\] <p>where, for \\(L_{n, j}(x)\\) denoting the \\(j\\)th Lagrange coefficient polynomial of degree \\(n\\), we have</p> \\[ H_{n,j}(x) = [1-2(x-x_j)L'_{n,j}(x_j)]L^2_{n,j}(x),\\quad \\hat{H}_{n,j}(x) = (x-x_j)L^2_{n,j}(x) \\] <p>The composition is usually a little tidious.</p>"},{"location":"courses/Numerical_Analysis/NA/#cubic-spline-interpolation","title":"Cubic Spline Interpolation","text":"<p>We change \\(\\frac{d^kP(x_i)}{dx^k} = \\frac{d^kf(x_i)}{dx^k}\\) into equations between adjacent curves. Because we ofte do not know the derivatives of the point.</p> <ul> <li>Natural Spline</li> </ul> <p>Let \\(S''(a) = 0\\) and \\(S''(b) = 0\\).</p> <ul> <li>Clamped Cubic Spline</li> </ul> <p>Let \\(S'(x_0) = f'(x_0)\\) and \\(S'(x_n) = f'(x_n)\\).</p>"},{"location":"courses/Numerical_Analysis/NA/#approximation-theory","title":"Approximation Theory","text":""},{"location":"courses/Numerical_Analysis/NA/#discrete-least-squares-approximation","title":"Discrete Least Squares Approximation","text":""},{"location":"courses/Numerical_Analysis/NA/#orthogonal-polynomials-and-least-squares-approximation","title":"Orthogonal Polynomials and Least Squares Approximation","text":"<p>If we choose </p> \\[ E = (P-y,P-y)=\\|P-y\\|^2 \\] <p>we can easily solve the approximation polynomial \\(\\sum\\limits_{i=0}^\\inftya_ix^i\\) by letting \\(\\frac{\\partial E}{\\partial a_k}=0\\), and get</p> \\[ \\sum_{j=0}^n(\\varphi_k, \\varphi_j)a_j=(\\varphi_k,f),\\quad k=0,1,\\cdots,n \\]"},{"location":"courses/Numerical_Analysis/NA/#chebyshev-polynomials-and-economization-of-power-series","title":"Chebyshev Polynomials and Economization of Power series","text":"<p>Coonsider \\(n+1\\) extreme points of \\(cosn\\theta\\) on \\([0,\\pi]\\).</p> <p>We still have \\(cos(n \\theta)=\\sum\\limits_{k=1}^n a_kcos(\\theta)^i\\).</p> <p>Define \\(T_0(x)=1\\), \\(T_1(x)=x\\), \\(T_{n+1}(x)=2xT_n(x)-T_{n-1}(x)\\), \\(n=1,2,\\cdots,n-1\\)</p>"},{"location":"courses/Ordinary_Differential_Equation/","title":"Ordinary Differential Equation","text":"<p>Reference</p> <ul> <li>\u300a\u5e38\u5fae\u5206\u65b9\u7a0b\u300b \u67f3\u5f6c</li> <li>\u300a\u5e38\u5fae\u5206\u65b9\u7a0b\u300b \u65b9\u9053\u5143</li> </ul>"},{"location":"courses/Ordinary_Differential_Equation/#elementary-integration-method","title":"\u521d\u7b49\u79ef\u5206\u6cd5 | Elementary Integration Method","text":""},{"location":"courses/Ordinary_Differential_Equation/#system-of-linear-differential-equations-lodes","title":"\u7ebf\u6027\u5fae\u5206\u65b9\u7a0b\u7ec4 | System of Linear Differential Equations (LODEs)","text":""},{"location":"courses/Ordinary_Differential_Equation/#general-theory-of-ode","title":"\u5fae\u5206\u65b9\u7a0b\u7684\u4e00\u822c\u7406\u8bba | General Theory of ODE","text":""},{"location":"courses/Ordinary_Differential_Equation/#existence-and-uniqueness-theorem","title":"\u5b58\u5728\u552f\u4e00\u6027\u5b9a\u7406 | Existence and Uniqueness Theorem","text":""},{"location":"courses/Ordinary_Differential_Equation/#contraction-mapping-method","title":"\u538b\u7f29\u6620\u5c04\u6cd5 | Contraction Mapping Method","text":""},{"location":"courses/Ordinary_Differential_Equation/#method-of-power-series","title":"\u5e42\u7ea7\u6570\u89e3\u6cd5 | Method of Power Series","text":""},{"location":"courses/Ordinary_Differential_Equation/EIM/","title":"Elementary Integration Method","text":""},{"location":"courses/Ordinary_Differential_Equation/EIM/#elementary-integration-method","title":"\u521d\u7b49\u79ef\u5206\u6cd5 | Elementary Integration Method","text":"<p>This chapter gives primary method of solving special differential functions, which plays a great role in future study.</p>"},{"location":"courses/Ordinary_Differential_Equation/EIM/#exact-equation","title":"\u6070\u5f53\u65b9\u7a0b | Exact Equation","text":"<p>We focus on the symmetrical form</p> \\[ \\begin{equation} M(x,y)dx + N(x,y)dy = 0 \\label{eq-exact} \\end{equation} \\] <p>This can bring us great convenience for digging into one-order ODE because it can gives us both the relation \\(y=f(x)\\) or \\(x=g(y)\\).</p> <p>\u5168\u5fae\u5206\u65b9\u7a0b\u3001\u6070\u5f53\u65b9\u7a0b\u7684\u5b9a\u4e49 | Definition of Exact Equation</p> <p>If there exists a \\(\\mathit{\\varphi}(x, y) \\in C^{1}(D)\\) such that </p> \\[ d\\mathit{\\varphi}(x, y) = M(x,y)dx + N(x,y)dy \\] <p>then equation \\(\\ref{eq-exact}\\) is called Exact Equation.</p> <p>There are some questions to answer:</p> <ul> <li>How to judge an equation to be exact Equation?</li> <li>If so, how to find original function \\(\\varphi(x, y)\\)?</li> <li>If not, how to transform it into one exact Equation?</li> </ul> <p>In this pattern, we answer the first two equation and leave the third one after learning LFODE.</p> <p>\u65b9\u7a0b\u662f\u6070\u5f53\u7684\u5145\u8981\u6761\u4ef6 | Necessary and Sufficient Condition for exact Equation</p> <p>Assume \\(D\\) is a simply connected region, and \\(M(x, y)\\), \\(N(x, y) \\in C(D)\\) with \\(\\frac{\\partial M}{\\partial y}\\) and \\(\\frac{\\partial N}{\\partial x} \\in C^{1}(D)\\). Then equation \\(\\ref{eq-exact}\\) is exact Equation if and only if</p> \\[ \\frac{\\partial M}{\\partial y} = \\frac{\\partial N}{\\partial x} \\] <p>Prove it.</p> Hints <p>\\(\\Rightarrow\\) is easy, by using second-order mixed partial derivatives of \\(\\mathit{\\varphi}\\).</p> <p>\\(\\Leftarrow\\). Using Green Formula/Theorem. </p> \\[ \\begin{align*} &amp;\\frac{\\partial P}{\\partial y} = \\frac{\\partial Q}{\\partial x} \\\\ \\Leftrightarrow \\ &amp;\\int_{\\gamma}P(x, y)dx+Q(x,y)dy = 0 \\quad \\forall\\text{closed loop } \\gamma\\\\ \\Leftrightarrow \\ &amp;\\int_{\\gamma}P(x, y)dx+Q(x,y)dy = C \\quad \\forall\\text{curve } \\gamma \\text{ connecting } (x_0, y_0), (x, y) \\\\ \\Leftrightarrow \\ &amp;\\exists \\mathit{\\varphi}(x,y) \\in C^{1}(D) \\text{ s.t } d\\mathit{\\varphi}(x, y) = P(x, y)dx+Q(x,y)dy \\end{align*} \\]"},{"location":"courses/Ordinary_Differential_Equation/EIM/#integral-factor","title":"\u79ef\u5206\u56e0\u5b50 | Integral Factor","text":"<p>This part we hope to find \\(\\mu(x, y)\\) so when we multiply it to both sides of equation \\(\\ref{eq-exact}\\)</p> \\[ \\mu(x,y)M(x,y)dx + \\mu(x,y)N(x,y)dy = 0  \\] <p>there exists \\(\\mathit{\\varphi}(x, y)\\) such that</p> \\[ d\\mathit{\\varphi}(x,y) = \\mu(x,y)M(x,y)dx + \\mu(x,y)N(x,y)dy  \\] <p>Naively, if \\(\\mathit{\\varphi}(x,y)\\in C^2\\), then </p> \\[ \\frac{\\partial (\\mu M)}{\\partial y} = \\frac{\\partial^2 \\mathit{\\varphi}}{\\partial x\\partial y}=\\frac{\\partial (\\mu N)}{\\partial x}  \\] <p>Theoretically speaking, we have to solve a PDE</p> \\[ \\begin{equation} M(x,y)\\frac{\\partial \\mu}{\\partial y} - N(x,y)\\frac{\\partial \\mu}{\\partial x} = \\left(\\frac{\\partial N}{\\partial x}-\\frac{\\partial M}{\\partial y}\\right)\\mu(x,y) \\label{eq-pde} \\end{equation} \\] <p>However, actually, it is very hard to solve the above PDE. So we focus on some special case like \\(\\mu(x,y)=\\mu(x)\\), \\(\\mu(y)\\), \\(\\mu(x+y)\\), \\(\\mu(xy)\\).</p> <p>Now we have the following theorem to judge whether we can get the above form of integral factors.</p> <p>\u65b9\u7a0b\u6709\u7279\u6b8a\u7c7b\u578b\u7684\u79ef\u5206\u56e0\u5b50\u7684\u5145\u8981\u6761\u4ef6 | Necessary and Sufficient Condition of special integral factor of ODE</p> <p>Equation \\(\\ref{eq-pde}\\) has solution \\(\\mu(x)\\) depending only on \\(x\\), if and only if</p> \\[ \\frac{\\frac{\\partial N}{\\partial x}-\\frac{\\partial M}{\\partial y}}{M} \\overset{\\Delta}{=} G(x) \\] <p>only depends only on \\(x\\). Then </p> \\[ \\mu(x) = e^{\\int_{x_0}^{x}G(t)dt} \\] <p>More generally, equation \\(\\ref{eq-pde}\\) has solution \\(\\mu(\\varphi(x,y))\\), if and only if</p> \\[ \\frac{\\frac{\\partial N}{\\partial x}-\\frac{\\partial M}{\\partial y}}{N\\frac{\\partial \\varphi}{\\partial x}-M\\frac{\\partial \\varphi}{\\partial y}} \\overset{\\Delta}{=} f(\\varphi(x,y)) \\]"},{"location":"courses/Ordinary_Differential_Equation/EIM/#variable-separation-equation","title":"\u53d8\u91cf\u5206\u79bb\u65b9\u7a0b | Variable Separation Equation","text":"<p>This chapter we discuss how to solve equation when it is not Exact Equation. The basic idea is, through transformation, we can convert an equation into an exact Equation.</p> <p>\u53d8\u91cf\u5206\u79bb\u65b9\u7a0b\u7684\u5b9a\u4e49 | Definition of Variable Separation Equation</p> <p>If there exists \\(M_1(x), M_2(y), N_1(x), N_2(y) \\in C^1(D)\\) such that </p> \\[ M(x, y) =M_1(x)M_2(y), N(x, y) = N_1(x), N_2(y) \\] <p>then we call equation \\(\\ref{eq-exact}\\) Variable Separation Equation.</p> <p>For this type of equation, we can multiply both sides </p> \\[ \\begin{equation} \\frac{1}{M_2(y)N_1(x)} \\label{eq-sep-factor} \\end{equation} \\] <p>equation \\(\\ref{eq-exact}\\) becomes</p> \\[ \\frac{M_1(x)}{N_1(x)}dx + \\frac{M_2(y)}{N_2(y)}dy = 0 \\] <p>This is an exact equation, and \\(\\ref{eq-sep-factor}\\) is called an Integral Factor of the equation.</p> <p>we can get its integral</p> \\[ \\int_{x_0}^{x}\\frac{M_1(t)}{N_1(t)}dt + \\int_{y_0}^{y}\\frac{M_2(s)}{N_2(s)}ds = c \\] <p>which is easily seen a solution of the original equation.</p> <p>And don't forget that if there exists \\(a_i (i= 1,2,\\cdots, m)\\) such that \\(N_1(a_i) = 0\\), or exists \\(b_j (j=1,2,\\cdots, n)\\) such that \\(M_2(b_j) = 0\\), then of course \\(x = a_i, y = b_j\\) are also solutions of the original solution.</p> <ul> <li>\u9f50\u6b21\u65b9\u7a0b | Homogeneous Equation</li> </ul> <p>The following equation can also be transferred into Variable Separation Equation.</p> <p>\u9f50\u6b21\u65b9\u7a0b\u7684\u5b9a\u4e49 | Definition of Homogeneous Equation</p> <p>We call \\(f(x, y)\\) Homogeneous Function of degree \\(n\\) if</p> \\[ f(tx, ty) = t^n f(x, y) \\] <p>and call equation \\(\\ref{eq-exact}\\) Homogeneous equation if \\(M(x, y), N(x, y)\\) are Homogeneous Function.</p> <p>When we let \\(y = u x\\), then \\(dy = xdu + udx\\), substitute in the equation and get</p> \\[ \\begin{align*} M(x, ux)dx+N(x,ux)(xdu+udx)&amp;=0 \\\\ \\Leftrightarrow [M(x, ux)+N(x,ux)u]dx+N(x,ux)xdu&amp;=0  \\end{align*} \\] <p>extract \\(x\\) out by definition of Homogeneous Equation:</p> \\[ x^n[M(1, u)+N(1,u)u]dx+x^{n+1}N(1,u)du=0  \\] <p>If \\(x^{n+1}[M(1, u)+N(1,u)u]\\neq 0\\), then we divide both sides by this and get</p> \\[ \\frac{1}{x}dx + \\frac{N(1,u)}{M(1,u)+uN(1,u)}du=0 \\] <p>which is also Variable Separation Equation.</p>"},{"location":"courses/Ordinary_Differential_Equation/EIM/#linear-first-order-differential-equation","title":"\u4e00\u9636\u7ebf\u6027\u5fae\u5206\u65b9\u7a0b | Linear First-Order Differential Equation","text":"<p>Now we focus on a really important expression of ODE: Linear First-Order Differential Equation(LFODE):</p> \\[ \\begin{equation} \\frac{dy}{dx} + p(x)y = q(x) \\label{eq-LFODE} \\end{equation} \\] <ul> <li>Homogeneous LFODE(H-LFODE)</li> </ul> <p>We let \\(q(x)\\equiv 0\\) in \\(\\ref{eq-LFODE}\\), we get</p> \\[ \\begin{equation} \\frac{dy}{dx} + p(x)y = 0 \\label{eq-H-LFODE} \\end{equation} \\] <p>rewrite it into symmetrical form:</p> \\[ p(x)ydx + dy = 0  \\] <p>which is Variable Separation Equation.</p> <p>So when \\(y\\neq 0\\), multiply both sides \\(1/y\\) and integrate</p> \\[ \\ln{|y|} + \\int_{x_0}^{x}p(t)dt = C \\] <p>get \\(y\\) out of form \\(x\\):</p> \\[ \\begin{align} y &amp;= \\pm e^{C}\\cdot e^{-\\int_{x_0}^{x}p(t)dt} \\nonumber \\\\ &amp;= C_1\\cdot e^{-\\int_{x_0}^{x}p(t)dt}  \\end{align} \\] <p>where \\(C_1=\\pm e^{C} \\neq 0\\), but we can include trivial solution \\(y \\equiv 0\\) by letting \\(C_1 = 0\\).</p> <ul> <li>Non-Homogeneous linear First-Order Differential Equation</li> </ul> <p>we have two ways to get the answer.</p> Version 1Version 2 <p>Making use of Integral Factors.</p> <p>To begin with, we convert equation \\(\\ref{eq-LFODE}\\) into symmetrical form:</p> \\[ \\begin{equation} (p(x)y-q(x))dx+dy=0 \\label{eq-SYM-LFODE} \\end{equation}  \\] <p>Multiply \\(e^{\\int_{x_0}^{x}p(t)dt}\\) to both sides of equation \\(\\ref{eq-SYM-LFODE}\\):</p> \\[ d\\left(\\ e^{\\int_{x_0}^{x}p(t)dt} y \\right) - e^{\\int_{x_0}^{x}p(t)dt} q(x)dx = 0 \\] <p>That is,</p> \\[ d\\left(\\ e^{\\int_{x_0}^{x}p(t)dt} y- \\int_{x_0}^{x}e^{\\int_{x_0}^{s}p(t)dt} q(x)ds \\right)  = 0 \\] <p>integrate and get</p> \\[ e^{\\int_{x_0}^{x}p(t)dt} y - \\int_{x_0}^{s}e^{\\int_{x_0}^{x}p(t)dt} q(s)ds + C = 0 \\] <p>extract \\(y\\) out and get:</p> \\[ y = e^{-\\int_{x_0}^{x}p(t)dt} \\left( C + \\int_{x_0}^{s}e^{\\int_{x_0}^{x}p(t)dt} q(s)ds \\right) \\] <p>Through Variation of Constants.</p> <p>We make a brave treatment: assume one special solution to equation \\(\\ref{eq-LFODE}\\) is </p> \\[ y = u\\cdot e^{-\\int_{x_0}^{x}p(t)dt} \\] <p>where \\(u\\) is a new variable.</p> <p>Subsititute in equation \\(\\ref{eq-LFODE}\\) and get</p> \\[ \\left(p(x) u e^{-\\int_{x_0}^{x}p(t)dt} -q(x) \\right) dx - u p(x) e^{-\\int_{x_0}^{x}p(t)dt} dx + e^{-\\int_{x_0}^{x}p(t)dt} du = 0 \\] <p>That is</p> \\[ -q(x) dx + e^{-\\int_{x_0}^{x}p(t)dt} du = 0 \\] <p>multiply \\(e^{\\int_{x_0}^{x}p(t)dt}\\) to both sides and  integrate </p> \\[ u =  \\int_{x_0}^{x}e^{\\int_{x_0}^{s}p(t)dt}q(s)ds \\] <p>So the special solution is</p> \\[ y =  e^{-\\int_{x_0}^{x}p(t)dt}\\cdot \\int_{x_0}^{x}e^{\\int_{x_0}^{s}p(t)dt}q(s)ds \\]"},{"location":"courses/Ordinary_Differential_Equation/EIM/#first-order-implicit-differential-equation","title":"\u4e00\u9636\u9690\u5f0f\u5fae\u5206\u65b9\u7a0b | First-order Implicit Differential Equation","text":"<p>Now we focus on equation</p> \\[ \\begin{equation} F(x,y, y') = 0 \\label{eq-para} \\end{equation} \\] <p>where \\(y'\\) cannot be explicitly solved out.</p> <ul> <li>\u53c2\u6570\u6cd5 | parametric method</li> </ul> <p>If we let \\(p = y'\\), then equation</p> \\[ \\begin{equation} F(x,y,p) = 0 \\label{eq-para-p} \\end{equation} \\] <p>represents a curved surface in 3-D space.</p> <p>And we can juggle equation \\(\\ref{eq-para-p}\\) and \\(dy=pdx\\) to get a curve in the space.</p>"},{"location":"courses/Ordinary_Differential_Equation/LODEs/","title":"System of Linear Differential Equations","text":""},{"location":"courses/Ordinary_Differential_Equation/LODEs/#system-of-linear-differential-equations-lodes","title":"\u7ebf\u6027\u5fae\u5206\u65b9\u7a0b\u7ec4 | System of Linear Differential Equations (LODEs)","text":"<p>This chapter we focus on </p> \\[ \\begin{equation} \\frac{d \\symbfit{X}(t)}{dt} = \\symbfit{A}(t)\\symbfit{X}(t)+\\symbfit{B}(t) \\label{eq-LODEs} \\end{equation} \\] <p>with initial condition</p> \\[ \\begin{equation} \\symbfit{X}(t_0)=\\symbfit{X}_0 \\label{eq-initial-LODEs} \\end{equation} \\] <p>where \\(t_0\\in I=(a,b)\\), \\(\\symbfit{X}_0 = (x_1^0,x_n^0,\\cdots, x_n^0)^T\\) is a given constant vector.</p>"},{"location":"courses/Ordinary_Differential_Equation/LODEs/#existence-and-uniqueness-of-lodes","title":"\u7ebf\u6027\u5fae\u5206\u65b9\u7a0b\u7ec4\u89e3\u7684\u5b58\u5728\u552f\u4e00\u6027 | Existence and Uniqueness of LODEs","text":"<p>This is quite similar to Picard Theorem in chapter Existence and Uniqueness Theorem, but it is still useful to give a special form of Picard Sequence for LODEs, which is also an approximation to solving it.</p> <p>\u7ebf\u6027\u5fae\u5206\u65b9\u7a0b\u7ec4\u89e3\u7684\u5b58\u5728\u552f\u4e00\u6027\u5b9a\u7406 | Theorem of Existence and Uniqueness of LODEs</p> <p>LODEs \\(\\ref{eq-LODEs}\\) with initial condition \\(\\ref{eq-initial-LODEs}\\) has only one solution on interval \\(I\\).</p> <p>Prove it.</p> Hints <p>We have to measure the distance in matrix. Now we need to give a definition of norm of vectors and matrixes(to see more details in Norm of vectors and matrixes) in Numerical Analysis.</p> \\[ \\Vert\\mathbfit{X} \\Vert = \\sum_{i=1}^{n}|x_i|, \\quad \\Vert\\mathbfit{A} \\Vert = \\sum_{i=1}^{n}\\sum_{j=1}^{n}|{a_{ij}}| \\] <p>It is easy to see that ...</p> <ul> <li>convert LODEs into its equivalent integral equations.</li> </ul> \\[ \\mathbfit{X}(t) = \\mathbfit{X}_0 + \\int_{t_0}^{t}\\left[ \\mathbfit{A}(s)\\mathbfit{X}(s)+\\mathbfit{B}(s)\\right] ds \\] <ul> <li>formulate Picard Sequence.</li> </ul> <p>Define:</p> \\[ \\begin{align} \\mathbfit{X}_0(t) &amp;= \\mathbfit{X}_0 \\nonumber\\\\ \\mathbfit{X}_1(t) &amp;= \\mathbfit{X}_0 + \\int_{t_0}^{t}\\left[ \\mathbfit{A}(s)\\mathbfit{X}_0(s)+\\mathbfit{B}(s)\\right] ds \\nonumber\\\\ \\mathbfit{X}_2(t) &amp;= \\mathbfit{X}_0 + \\int_{t_0}^{t}\\left[ \\mathbfit{A}(s)\\mathbfit{X}_1(s)+\\mathbfit{B}(s)\\right] ds \\nonumber\\\\ &amp;\\vdots \\nonumber\\\\ \\mathbfit{X}_n(t) &amp;= \\mathbfit{X}_0 + \\int_{t_0}^{t}\\left[ \\mathbfit{A}(s)\\mathbfit{X}_{n-1}(s)+\\mathbfit{B}(s)\\right] ds \\label{eq-LODEs-integral}\\\\ \\end{align} \\] <p>Consider similarly and we can say the above sequence is well-defined.</p> <ul> <li>Prove Picard Sequence convergent.</li> </ul> <p>denote</p> \\[ C = \\sup_{s\\in J}\\Vert\\mathbfit{A}(s)\\Vert, \\quad D = C\\Vert\\mathbfit{X}(s)\\Vert + \\sup_{s\\in J}\\Vert\\mathbfit{B}(s)\\Vert \\] <p>we can get </p> \\[ \\begin{align*} \\Vert \\mathbfit{X}_1(t) - \\mathbfit{X}_0(t) \\Vert &amp;\\leq D |t-t_0|\\\\ \\Vert\\mathbfit{X}_2(t) - \\mathbfit{X}_1(t) \\Vert &amp;\\leq \\int_{t_0}^{t} \\Vert \\mathbfit{A}(s) \\Vert \\Vert \\mathbfit{X}_{1}(s)- \\mathbfit{X}_{0}(s) \\Vert ds \\\\ &amp;\\leq \\int_{t_0}^{t} C D |s-t_0| ds = \\frac{D}{C} \\frac{(C|t-t_0|)^2}{2}\\\\ &amp;\\vdots\\\\ \\Vert\\mathbfit{X}_n(t) - \\mathbfit{X}_{n-1}(t) \\Vert &amp;\\leq \\frac{D}{C} \\frac{(C|t-t_0|)^{n}}{(n)!} \\end{align*} \\] <p>which shows the Picard Sequence converges.</p> <ul> <li>Prove the convergent function is solution of LODEs \\(\\ref{eq-LODEs}\\).</li> </ul> <p>If we denote \\(\\mathbfit{X}(t) = \\lim_{n\\rightarrow \\infty}\\mathbfit{X}_n(t)\\) and let \\(n\\rightarrow \\infty\\) on both sides of integral equation \\(\\ref{eq-LODEs-integral}\\), we get </p> \\[ \\mathbfit{X}(t) = \\mathbfit{X}_0 + \\int_{t_0}^{t}\\left[ \\mathbfit{A}(s)\\mathbfit{X}(s)+\\mathbfit{B}(s)\\right] ds \\] <p>which is a solution.</p> <ul> <li>prove uniqueness.</li> </ul> <p>Similar to proof in Picard Theorem.</p>"},{"location":"courses/Ordinary_Differential_Equation/LODEs/#boundary-problem-of-second-order-lode","title":"\u4e8c\u9636\u65b9\u7a0b\u8fb9\u503c\u95ee\u9898 | Boundary Problem of Second-Order LODE","text":"<p>This pattern we focus on LODE</p> \\[ \\begin{equation} y''+p(x)'+q(x)y = f(x) \\label{eq: BP-SecondOrder} \\end{equation} \\] <p>with Boundary Condition \\(y(a)=\\alpha, y(b)=\\beta\\), where \\(p(x), q(x) \\in C^1[a, b]\\)</p> <ul> <li>H-LODE</li> </ul> <p>\u5171\u8f6d\u70b9 | Conjugate Point</p> <p>If homogeneous LODE(H-LODE)</p> \\[ \\begin{equation} y''+p(x)'+q(x)y = 0 \\label{eq: BP-SO-H} \\end{equation} \\] <p>with boundary condition \\(y(a)=0, y(b)=0\\), has non-zero solution, then \\(\\{a, b\\}\\) is called the Conjugate Point of the H-LODE.</p> <p>We usually take use of the following method to check if the boundary point will induce indefinite solutions or no solutions.</p> <p>\u5171\u8f6d\u70b9\u7684\u5145\u8981\u6761\u4ef6 | Necessary and Sufficient Condition for Conjugate Point</p> <p>\\(\\{a, b\\}\\) is the Conjugate Point of H-LODE, if and only if \\(\\forall y_1, y_2\\) of the solution of H-LODE, which are linear irrelevant, satisfies</p> \\[ \\left| \\begin{array}{cc} y_1(a)&amp; y_2(a)\\\\ y_1(b)&amp; y_2(b) \\end{array} \\right| =0 \\] Hints <p>substitute the boundary condition and we get two linear equation system for parameters \\(c_1, c_2\\). And the above is the determinant of the system.</p> <p>\u9f50\u6b21\u65b9\u7a0b\u5b58\u5728\u552f\u4e00\u89e3\u7684\u5145\u8981\u6761\u4ef6 | Necessary and Sufficient Condition for existing only one solution for H-LODE</p> <p>H-LODE \\(\\ref{eq: BP-SO-H}\\) with boundary point \\(y(a)=\\alpha, y(b)=\\beta\\) has only one solution, if and only if \\(\\{a, b\\}\\) is not the conjugate point of the H-LODE.</p> Hints <p>Focus on the determinant of the linear irelevantly solutions of \\(y_1, y_2\\).</p> <ul> <li>Non-H-LODE</li> </ul> <p>We partition the solution of LODE \\(\\ref{eq: BP-SecondOrder}\\) into three parts.</p> <p>\u7ebf\u6027\u975e\u9f50\u6b21\u65b9\u7a0b\u5b58\u5728\u552f\u4e00\u89e3\u7684\u5145\u5206\u6761\u4ef6 | Sufficient Condition for existing only one solution for Non-H-LODE</p> <p>If \\(\\{a, b\\}\\) is not the conjugate point of H-LODE \\(\\ref{eq: BP-SO-H}\\), then Non-H-LODE \\(\\ref{eq: BP-SecondOrder}\\) has only one solution.</p> <p>There are two ways to prove it.</p> Version 1Version 2 <p>Assume \\(y_1(x)\\) is a solution of H-LODE </p> \\[ \\begin{equation} y'' + p(x)y' +q(x)y = 0, \\quad y(a) = 0, \\quad y(b) = 1 \\label{eq: BP-1} \\end{equation} \\] <p>and \\(y_2(x)\\) is a solution of H-LODE</p> \\[ \\begin{equation} y'' + p(x)y' +q(x)y = 0, \\quad y(a) = 1, \\quad y(b) = 0 \\label{eq: BP-2} \\end{equation} \\] <p>and \\(y_3(x)\\) is a solution of Non-H-LODE</p> \\[ \\begin{equation} y'' + p(x)y' +q(x)y = f(x), \\quad y(a) = 0, \\quad y(b) = 0 \\label{eq: BP-3} \\end{equation} \\] <p>and the solution of LODE \\(\\ref{eq: BP-SecondOrder}\\) can be represented as</p> \\[ y = \\alpha y_1 + \\beta y_2 + y_3 \\] <p>where \\(y_1, y_2\\) are linearly irrelevant because of  existence and uniqueness theorem.</p> <p>We can get \\(y_3\\) through Variation of Constant. That is, let \\(y_3 = u_1(x) y_1(x)+u_2(x)y_2(x)\\), then </p> \\[ u_1 = \\int \\frac{-y_2 f}{W}dt, \\quad u_2 = \\int \\frac{y_1 f}{W}dt \\] <p>Substitute the boundary condition \\(\\ref{eq: BP-1}, \\ref{eq: BP-2}, \\ref{eq: BP-3}\\) we get </p> \\[ u_2(a)=0, \\quad u_1(b)=0 \\] <p>Based on this, we can transform the \\(u_1(x), u_2(x)\\) to definite integral whose upper limit of integral is variable</p> \\[ u_1 = \\int_{x}^{b} \\frac{y_2 f}{W}dt, \\quad u_2 = \\int_{x}^{a} \\frac{y_1 f}{W}dt \\] <p>So we can write particular solution </p> \\[ y_3 = y_1(x) \\int_{x}^{b} \\frac{y_2(t) f(t)}{W(t)}dt  + y_2(x)\\int_{x}^{a} \\frac{y_1(x) f(x)}{W(x)}dt \\] <p>If we define Green Function as</p> \\[ G(x,t)= \\begin{cases} \\displaystyle \\frac{y_2(x)y_1(t)}{W(t)}, \\quad a\\leq t\\leq x  \\\\ \\displaystyle \\frac{y_1(x)y_2(t)}{W(t)}, \\quad x\\leq t\\leq b  \\end{cases} \\] <p>then </p> \\[ y_3(x) = \\int_{a}^{b}G(x, t)f(t)dt \\] <p>According to the textbook.</p>"},{"location":"courses/Ordinary_Differential_Equation/LODEs/#s-l-sturm-liouville-boundary-problem","title":"S-L \u8fb9\u503c\u95ee\u9898 | Sturm-Liouville Boundary Problem","text":"<p>Solve for PDE</p> \\[ \\begin{cases} \\displaystyle u_{tt} = a^2 u_{xx} , \\quad 0\\leq x\\leq L, t\\geq 0\\\\ \\displaystyle u|_{x=0} = u|_{t=0} = 0 \\end{cases} \\] <p>Assume we can seperate the variables \\(x, t\\). That is, let \\(u = X(x)T(t)\\) and substitute, we get </p> \\[ \\begin{align*} X(x)T''(t)&amp;=a^2X''(x)T(t)\\\\ \\Rightarrow \\frac{T''(t)}{a^2T(t)} = \\frac{X''(x)}{X(x)} &amp;\\overset{\\Delta}{=}constant =-\\lambda \\end{align*} \\] <p>Thus, we have to find \\(T(t), X(x)\\) such that </p> \\[ T''(t)+a^2\\lambda T(t)= 0 \\] \\[ \\begin{cases} X''(x) + \\lambda X(x)= 0 \\\\ X(0)=X(L)=0 \\end{cases} \\]"},{"location":"courses/Ordinary_Differential_Equation/General_Theory/","title":"General Theory of ODE","text":""},{"location":"courses/Ordinary_Differential_Equation/General_Theory/#existence-and-uniqueness-theorem","title":"\u5b58\u5728\u552f\u4e00\u6027\u5b9a\u7406 | Existence and Uniqueness Theorem","text":""},{"location":"courses/Ordinary_Differential_Equation/General_Theory/#proof-using-contraction-mapping","title":"\u538b\u7f29\u6620\u5c04\u6cd5 | Proof using Contraction Mapping","text":""},{"location":"courses/Ordinary_Differential_Equation/General_Theory/#extension-of-solution","title":"\u89e3\u7684\u5ef6\u62d3 | Extension of Solution","text":""},{"location":"courses/Ordinary_Differential_Equation/General_Theory/#method-of-power-series","title":"\u5e42\u7ea7\u6570\u89e3\u6cd5 | Method of Power Series","text":""},{"location":"courses/Ordinary_Differential_Equation/General_Theory/Contraction_Mapping_Method/","title":"Contraction Mapping Method","text":"<p>Reference</p> <p>Ordinary Differential Equation, Vladimir Igorevich Arnold</p> <p>\u300a\u5e38\u5fae\u5206\u65b9\u7a0b\u300b \u041b.\u0421.\u5e9e\u7279\u91cc\u4e9a\u91d1</p>"},{"location":"courses/Ordinary_Differential_Equation/General_Theory/Contraction_Mapping_Method/#proof-using-contraction-mapping","title":"\u538b\u7f29\u6620\u5c04\u6cd5 | Proof using Contraction Mapping","text":"<p>This part we want to prove Picard Theorem from another perspective. And this method is also universal on multi-dimensional Cauchy Problem. So to simplify the notation, we change the problem equivalently to </p> \\[ \\begin{equation} \\dot{\\mathbfit{x}} = \\mathbfit{f}(t, \\mathbfit{x}),\\quad  \\mathbfit{x}(t_0) = \\mathbfit{x}_0 \\label{eq-vector-cauchy} \\end{equation} \\] <p>where \\(\\mathbfit{x}\\in \\mathbb{R}^n\\) and \\(\\mathbfit{f}: \\mathbb{R}^{n+1} \\mapsto \\mathbb{R}^n \\in C(D)\\) is a vector function(or vector field). To simplify the problem, we assume \\(\\pmb{f} \\in C^r(D)\\), where \\(r\\geq 1\\).</p> <p>Assume we give a Euvlid Structure in region \\(D\\in \\mathbb{R}^{n+1}\\). For all \\((t_0,\\pmb{x}_0)\\in D\\), we consider a cylinder(\u67f1\u4f53) with sufficiently small parameters \\(a\\) and \\(b\\)</p> \\[ \\Gamma= \\{(t, \\pmb{x}): |t-t_0|\\leq a, \\|\\pmb{x}-\\pmb{x}_0\\|\\leq b\\} \\] <p>which is still a subset of \\(D\\).</p> <p>Then the Picard Theorem becomes:</p> <p>Picard Theorem</p> <p>Assume \\(\\pmb{f}\\) of problem \\(\\ref{eq-vector-cauchy}\\) is continous and differentiable(or Lipschitz condition) on region \\(\\Gamma\\), then for all given \\(\\pmb{x}\\) which is sufficiently close to \\(\\pmb{x}_0\\), there exsits a neighberhood of \\(t_0\\), such that there exists a unique solution \\(\\pmb{\\varphi}(t)\\).</p> Hints <ul> <li>Using the fixed point theorem (existence and uniqueness).</li> </ul>"},{"location":"courses/Ordinary_Differential_Equation/General_Theory/Contraction_Mapping_Method/#group-basis","title":"\u7fa4\u8bba\u57fa\u7840 | Group Basis","text":"<p>Firstly, let us introduce some basic ideas about groups.</p> <p>\u7fa4\u7684\u5b9a\u4e49 | Definition of Group</p> <p>Firstly, we have to define Law of Composition.</p> <p>Assume \\(S\\) is a set. A Law of Composition is a map </p> \\[ S\\times S \\mapsto S. \\] <p>\\(S\\times S\\) deontes the product set, whose elements are pairs \\(a\\), \\(b\\) of elements of \\(S\\).</p> <p>A group is a set \\(G\\) together with a law of composition(Here we use sign \\(\\cdot\\) (multiplicative notation), and sign \\(+\\) (additive notation) also can be applied) that has the following properties:</p> <ul> <li>The law of composition is associative.</li> </ul> \\[ (ab)c=a(bc), \\quad \\forall a,b,c\\in G \\] <ul> <li>\\(G\\) contains an identity element \\(1\\) (\\(0\\) in sign \\(+\\)) such that </li> </ul> \\[ 1a=a \\text{ and } a1=a,\\quad \\forall a\\in G \\] <ul> <li>Every element \\(a\\) of \\(G\\) has an inverse, i.e. an element \\(b\\) such that </li> </ul> \\[ ab=1 \\text{ and } ba=1 \\] <p>which is denoted by \\(a^{-1}\\) (\\(-a\\) in additive notation).</p> <p>\u7fa4\u7684\u6027\u8d28 | Properties of Group</p> <ul> <li>Cancellation Law</li> </ul> <p>Let \\(a\\), \\(b\\), \\(c\\) be elements of a group \\(G\\) whose law of composition is written multiplicatively. If </p> \\[ ab=ac \\text{ or } ba = ca \\Rightarrow b=c. \\] <p>If </p> \\[ ab=a \\text{ or } ba =a \\Rightarrow b=1. \\] Proof <p>Multiply both sides of the above equation on the left by \\(a^{-1}\\).</p> <p>Example.</p> <p>\\(n\\times n\\) general liear group is the group of all invertible \\(n\\times n\\) matrices, denoted by</p> \\[ GL_{n} =\\{n\\times n \\text{ invertible matrices } A\\}. \\] <p>where the law of composition is matrix multiplication.</p> <p>\u540c\u6001 | Homomorphism</p> <p>Let \\(G\\) and \\(G'\\) be groups written with multiplicative notation. A Homomorphism \\(\\varphi:G\\mapsto G'\\) is a map from \\(G\\) to \\(G'\\) such that </p> \\[ \\varphi(ab) = \\varphi(a)\\varphi(b), \\quad \\forall a,b\\in G \\] <p>If we let \\(\\varphi\\) to be a bijection, then we call it Isomorphism(\u540c\u6784).</p> <p>Example.</p> <p>the determinent function det: \\(GL_n(\\mathbb{R})\\mapsto \\mathbb{R}^\\times\\)</p> <p>\u540c\u6001\u7684\u6027\u8d28 | Properties of Homomorphism</p> <p>Let \\(\\varphi: G\\mapsto G'\\) be a group homomorphism.</p> <p>(i) If \\(a_1,a_2,\\cdots, a_k\\) are elements of \\(G\\), then </p> \\[ \\varphi(a_1 a_2\\cdots a_k)=\\varphi(a_1)\\varphi(a_2)\\cdots\\varphi(a_n) \\] <p>(ii) \\(\\varphi\\) maps the identity to identity, i.e. </p> \\[ \\varphi(1_G)=1_{G'} \\] <p>(iii) \\(\\varphi\\) maps inverses to inverses, i.e.</p> \\[ \\varphi(a^{-1}) = \\varphi(a)^{-1} \\] Proof <p>(i) by induction(Strong induction).</p> <p>(ii) using \\(1\\cdot 1=1\\) and \\(\\varphi(1)\\varphi(1)=\\varphi(1\\cdot 1)=\\varphi(1)\\), cancel both sides to obtain and get \\(\\varphi(1)=1_{G'}\\).</p> <p>(iii) similarly, \\(a^{-1}a=1\\) and \\(\\varphi(a^{-1})\\varphi(a)=\\varphi(a^{-1}a)=\\varphi(1_G)=1_{G'}\\), and we are done.</p>"},{"location":"courses/Ordinary_Differential_Equation/General_Theory/Contraction_Mapping_Method/#phase-space","title":"\u76f8\u7a7a\u95f4 | Phase Space","text":"<p>Then we have to introduce some comception about phase.</p> <p>\u5355\u53c2\u6570\u53d8\u6362\u7fa4 | One-Parameter Group of Transformation</p> <p>\\(M\\) is a set. A family of mapping \\(\\{g^t\\}_{t\\in \\mathbb{R}}\\) which maps set \\(M\\) into itself is called One-Parameter Group of Transformation of set \\(M\\), if </p> \\[ g^{t+s} = g^tg^s, \\quad \\forall t,s\\in \\mathbb{R} \\] <p>and \\(g^0\\) is identity mapping(\u6052\u7b49\u6620\u5c04).</p> <p>\u76f8\u6d41\u3001\u76f8\u7a7a\u95f4\u3001\u8fd0\u52a8\u3001\u76f8\u66f2\u7ebf\u7684\u5b9a\u4e49 | Definition of Phase Flow</p> <p>A couple composed of set \\(M\\) and its one-parameter group of transformation \\(\\{g^t\\}\\), denoted as \\((M, \\{g^t\\})\\), is called Phase Flow. And here \\(M\\) is called the phase space of the phase flow. The element of \\(M\\) is called Phase Point.</p> <p>Consider a mapping </p> \\[ \\begin{equation} \\varphi: \\mathbb{R}\\mapsto M, \\quad \\varphi(t) = g^tx,\\quad x\\in M \\label{map-motion} \\end{equation} \\] <p>which maps a real straight line into phase space. Then it is called a motion of phase point \\(x\\) under the action of phase flow. </p> <p>The image of \\(\\mathbb{R}\\) under mapping \\(\\varphi\\) is called the phase curve of phase flow \\((M,\\{g^t\\})\\).</p> <ul> <li>Fixed Point</li> </ul> <p>If the phase curve of a phase point \\(x\\in M\\) is itself, i.e.</p> \\[ g^tx=x,\\quad \\forall t\\in \\mathbb{R} \\] <p>then \\(x\\) is called the fixed point of the phase flow \\((M, \\{g^t\\})\\).</p> <p>In fact one-parameter group of transformation is exchangeable(\\(g^tg^s=g^{t+s}=g^{s+t}=g^sg^t\\)). And it is also a bijection. This is easy to prove. </p> <p>Firstly we prove it is surjection. \\(\\forall x\\in M\\), \\(\\exists g^{-t}x \\in M\\), such that \\(g^t(g^{-t}x)=x\\). Then we prove it is a injective mapping. \\(g^tx=g^ty\\), then \\(x=g^0x=g^{-t}g^tx=g^{-t}g^ty=g^0y=y\\).</p> <p>With the above property we can easily see the following theorem.</p> <p>\u76f8\u7a7a\u95f4\u4e2d\u7684\u70b9\u4ec5\u6709\u4e00\u6761\u76f8\u66f2\u7ebf</p> <p>For all \\(x\\in M\\), there only exists one phase curve.</p> Proof <p>Because \\(g^t\\) is a bijection.</p> <p>Now we introduce two important conceptions.</p> <p>\u6620\u5c04\u7684\u56fe\u5f62 | Graph of a mapping</p> <p>The graph of a mapping \\(f : A\\mapsto B\\) is a subset of the direct product(\u76f4\u79ef) \\(A\\times B\\):</p> \\[ \\{(a,f(a))| a\\in A\\} \\] <p>\u6269\u5f20\u76f8\u7a7a\u95f4\u3001\u79ef\u5206\u66f2\u7ebf | Expanded Phase Space, Integral Curve</p> <p>The Expanded Phase Space of phase flow \\((M, \\{g^t\\})\\) is the direct product \\(\\mathbb{R} \\times M\\).</p> <p>The Integral Curve of phase flow \\((M, \\{g^t\\})\\) is the graph of the motion \\(\\ref{map-motion}\\).</p> <p>Now we have to make use of DIfferential in Euclid Space.</p> <p>\u53ef\u5fae\u51fd\u6570\u3001\u53ef\u5fae\u6620\u5c04\u3001\u5fae\u5206\u540c\u80da | Differentiable Function, Differentiable Mapping, Diffeomorphism</p> <p>Assume \\(U \\subset\\mathbb{R}^n, V \\subset\\mathbb{R}^m\\). Then</p> <p>A Differentiable Function is a function \\(f: U\\mapsto \\mathbb{R}\\) which is \\(r\\) times differentiable.</p> <p>A Differentiable Mapping is a mapping \\(f:U\\mapsto V\\) defined by </p> \\[ y_i = f_i(x_1,x_2,\\cdots,x_n), \\quad i=1,2\\cdots, m \\] <p>where \\(f_i: U\\mapsto \\mathbb{R}\\) is a Differentiable function. If \\(y_i: V\\mapsto \\mathbb{R}\\) is a coordinate of \\(\\pmb{y}\\in \\mathbb{R}^m\\), then \\(y_i\\circ f: U\\mapsto \\mathbb{R}\\) is a Differentiable function in \\(U\\).</p> <p>A Diffeomorphism is a bijection \\(f:U\\mapsto V\\), such that \\(f\\) and \\(f^{-1}\\) are both Differentiable mappings.</p> <p>\u4e0e\u5750\u6807\u8f74\u6709\u5173\u7684\u76f8\u901f\u5ea6\u3001\u5411\u91cf\u573a | Phase Speed, Vector Field</p> <p>The Phase Speed \\(\\pmb{v}(x)\\) of phase flow \\(g^t\\) at point \\(x\\in M\\) is </p> \\[ \\pmb{v}(x) = \\frac{d}{dt}\\Bigg|_{t=0}g^tx \\] <p>And at time \\(\\tau\\), we have phase speed </p> \\[ \\pmb{v}(g^\\tau x) = \\frac{d}{dt}\\Bigg|_{t=\\tau}g^tx. \\] <p>Now we let \\(M\\) to be a region in Euclid Space \\(\\mathbb{R}^n\\) with coordinates \\(x_1,x_2,\\cdots,x_n\\). And if \\(x_i:M\\mapsto \\mathbb{R}\\) is the coordinate of \\(\\pmb{x}\\in M\\), then the vector \\(\\pmb{v}(x)\\) is defined by \\(v_i: M\\mapsto \\mathbb{R}, i=1,2\\cdots, n\\):</p> \\[ v_i(\\pmb{x}) = \\frac{d}{dt}\\Bigg|_{t=0}x_i(g^t\\pmb{x}) \\] <p>So we define a Vector Field \\(\\pmb{v}\\) on \\(M\\) when neglecting \\(t\\).</p> <p>\u4e0e\u5750\u6807\u8f74\u65e0\u5173\u7684\u76f8\u901f\u5ea6\u3001\u5411\u91cf\u573a | Phase Speed, Vector Field</p> <p>Firstly, we have to define that a coordinate \\(\\{y_i\\}_{i=1}^{n}:U\\mapsto \\mathbb{R}\\) is admissive, if mapping</p> \\[ y:U\\mapsto \\mathbb{R}^n,\\quad y(\\pmb{x}) = y_1(\\pmb{x})\\pmb{e}_1 + y_2(\\pmb{x})\\pmb{e}_2 + \\cdots +y_n(\\pmb{x})\\pmb{e}_n \\] <p>is a diffeomorphism.</p> <p>So we have a proposition:</p> <p>Two curves \\(\\varphi_1\\), \\(\\varphi_2\\) that pass \\(\\pmb{x}\\in U\\) are tangent with each other if and only if two curves \\(y\\circ \\varphi_1\\), \\(y\\circ\\varphi_2\\) that pass \\(y(\\pmb{x})\\in V\\) are tangent with each other.</p> <p><p> </p></p> <p>So the velocity vector of curve \\(\\varphi:I\\mapsto U\\) that pass \\(\\pmb{x}\\in U\\) is </p> \\[ \\pmb{v}=\\dot{\\varphi}(0), \\quad \\pmb{v}=\\frac{d\\varphi}{dt}\\Bigg|_{t=0}. \\] <p>\u5207\u5411\u91cf\u3001\u5207\u7a7a\u95f4 | Tangent Vector, Tangent Space</p> <p>Assume \\(U\\in \\mathbb{R}^n\\) with coordinates \\(x_1,x_2,\\cdots, n\\) and map \\(\\varphi:I\\mapsto U\\) maps an interval on \\(\\mathbb{R}\\) to \\(U\\) such that \\(\\varphi(0)=\\pmb{x}\\in U\\). and also its velocity is determined by </p> \\[ v_i = \\frac{d}{dt}\\Bigg|_{t=0}(x_i\\circ \\varphi), i=1,2,\\cdots, n. \\] <p>Two curves \\(\\varphi_1, \\varphi_2:I\\mapsto U\\) pass the same point \\(\\pmb{x}\\) are tangent with each other if \\(t\\rightarrow 0\\), \\(\\rho(\\varphi_1, \\varphi_2)\\rightarrow 0\\).</p> <p><p> </p></p> <p>A set composed by all tangent vectors of the curves passing \\(\\pmb{x}\\) is a linear space with dimension \\(n\\), which is called Tangent Space, denoted by \\(TU_x\\).</p> <p>Now we want to give a space without coordinates.</p> <p>\u6620\u5c04\u7684\u5bfc\u6570 | Derivative of Mapping</p> <p>Assume that \\(f:U\\mapsto V\\) is a differentiable mapping from \\(\\pmb{x}=(x_1,x_2,\\cdots,x_n)\\) to \\(\\pmb{y}=(y_1,y_2,\\cdots,y_m)\\). Let \\(\\pmb{y}=f(\\pmb{x})\\in V\\).</p> <p>The Derivative of mapping \\(f\\) at \\(\\pmb{x}\\) is a mapping from tangent space at \\(\\pmb{x}\\in U\\) to tangent space at \\(\\pmb{y}\\in V\\)</p> \\[ f_*|_{\\pmb{x}}: TU_{\\pmb{x}} \\mapsto TV|_{f(\\pmb{x})} \\] <p>This mapping maps velocity vector of curve \\(\\varphi\\) at \\(\\pmb{x}\\) into velocity vector of \\(f\\circ \\varphi\\) at \\(f(\\pmb{x})\\), i.e.</p> \\[ f_*|_{\\pmb{x}}\\left(\\frac{d\\varphi}{dt}\\Bigg|_{t=0} \\right) = \\frac{d}{dt}\\Bigg|_{t=0}(f\\circ \\varphi). \\] <p>which defines a linear mapping from \\(TU_{\\pmb{x}}\\) to \\(TV_{f(\\pmb{x})}\\).</p>"},{"location":"courses/Ordinary_Differential_Equation/General_Theory/Contraction_Mapping_Method/#contraction-mapping-in-metric-space","title":"\u5ea6\u91cf\u7a7a\u95f4\u4e0b\u7684\u538b\u7f29\u6620\u5c04 | Contraction Mapping in Metric Space","text":"<p>Then, let us recall the definition of metric space(\u5ea6\u91cf\u7a7a\u95f4).</p> <p>\u5ea6\u91cf\u7a7a\u95f4\u7684\u5b9a\u4e49 | Definition of Metric Space</p> <p>Assume \\(M\\) is a set. If there is a function \\(\\rho: M\\times M\\mapsto \\mathbb{R}\\) defined on it, satisfies that, \\(\\forall x,y,z \\in M\\)</p> <ul> <li>Positive</li> </ul> \\[ \\rho(x,y)&gt;0,\\quad x\\neq y \\] <ul> <li>Definite</li> </ul> \\[ \\rho(x, y)=0 \\Leftrightarrow x=y \\] <ul> <li>Symmetry</li> </ul> \\[ \\rho(x,y) = \\rho(y,x) \\] <ul> <li>Triangle inequality</li> </ul> \\[ \\rho(x,z) \\leq \\rho(x,y) +\\rho(y,z) \\] <p>then \\(M\\) is called metric space with metrc \\(\\rho\\).</p> <p>Now we can rewrite the Lipschitz Condition with terms of metric space.</p> <p>Lipschitz \u6761\u4ef6 | Lipschitz Condition</p> <p>Assume \\(A: M_1 \\mapsto M_2\\) is a mapping from a metric space \\(M_1\\) (with metric \\(\\rho_1\\)) to another matric space \\(M_2\\) (with metric \\(\\rho_1\\)). If there exists a constant \\(L&gt;0\\), s.t.</p> \\[ \\rho_2(Ax,Ay)\\leq L\\rho_1(x,y),\\quad \\forall x,y \\in M_1 \\] <p>then we say mapping \\(A\\) satisfies Lipschitz Condition.</p> <p>In Proof 1 we will show how to use Lipschitz condition to shrink the region of \\((t, \\pmb{x})\\)(Expanded Phase Space) such that the contraction mapping maps the metric space into itself.</p> <p>Usually, a mapping that maps a space \\(M\\) into itself would be of great use for us, so here comes the following definition.</p> <p>\u538b\u7f29\u6620\u5c04\u7684\u5b9a\u4e49 | Definition of Contraction mapping</p> <p>Assume \\(A: M \\mapsto M\\) is a mapping from complete metric space with metric \\(\\rho\\) to itself. If there exists a constant \\(k\\), \\(0&lt;k&lt;1\\), s.t. </p> \\[ \\rho(Ax,Ay)\\leq k\\rho(x, y),\\quad \\forall x, y\\in M. \\] <p>then \\(A\\) is called a Contraction mapping.</p> <p>If \\(Ax =x \\in M\\), we call \\(x\\) is a fixed point of mapping \\(A\\). Then the following theorem is quite useful in our future proof. </p> <p>Banach\u4e0d\u52a8\u70b9\u5b9a\u7406 | Banach Fixed-Point Theorem</p> <p>(This also known as contraction mapping theorem.)</p> <p>Assume \\(A: M\\mapsto M\\) is a contraction mapping, then \\(A\\) has a unique fixed-point.</p> HintsProof <p>We prove by showing that \\(A^nx\\) is a Cauchy Sequence and its limit \\(X\\) falls in \\(M\\). Then use the property of limits to show it is fixed-point of \\(A\\). Then prove the uniqueness by using property of metric \\(\\rho\\).</p> <p>Let \\(d = \\rho(x,Ax)\\), then </p> \\[ \\rho(A^nx,A^{n+1}x)\\leq k^n\\rho(x,Ax) = k^nd \\] <p>Sequence \\(A^nx\\), \\(n=1,2,\\cdots\\) is convergent. So its limit exists and let \\(X \\overset{\\Delta}{=} \\lim\\limits_{n\\rightarrow \\infty}A^nx\\).</p> <p>So </p> \\[ AX = A \\lim\\limits_{n\\rightarrow \\infty}A^nx = \\lim\\limits_{n\\rightarrow \\infty}A^{n+1}x = X \\] <p>which means \\(X\\) is a fixed-point of \\(A\\). Then we prove the uniqueness of fixed point. Assume there are two fixed points of \\(A\\), denoted as \\(X\\), \\(Y\\). See that</p> \\[ \\rho(X, Y)  =\\rho(AX, AY) \\leq k\\rho(X,Y) \\] <p>Because \\(0&lt;k&lt;1\\), so \\(\\rho(X,Y)=0\\) \\(\\Rightarrow\\) \\(X=Y\\).</p> <p>Readers can compare this theorem to Fixed-Point Theorem in Numerical Analysis. </p> <p>Note that we have an abstract set \\(M\\) with abstract metric \\(\\rho\\). In the next two parts, we will let \\(M\\) to be a set of mappings and \\(\\rho\\) to be defined based on natural norm of vectors. </p>"},{"location":"courses/Ordinary_Differential_Equation/General_Theory/Contraction_Mapping_Method/#Proof-1","title":"\u8bc1\u660e1 | Proof 1","text":"<p>Now we consider Picard mapping \\(A\\) is a mapping from a mapping to another mapping</p> \\[ A: (\\pmb{\\varphi}: t\\mapsto \\pmb{x}) \\mapsto (A\\pmb{\\varphi}: t\\mapsto \\pmb{x}) \\] <p>defined by</p> \\[ (A\\pmb{\\varphi})(t) = \\pmb{x}_0 + \\int_{t_0}^{t}\\pmb{f}(\\tau, \\pmb{\\varphi}(\\tau))d\\tau \\] <p>Geometrically speaking, tangent line of each point on \\(A\\pmb{\\varphi}\\), i.e. \\((A\\pmb{\\varphi})'(t)\\), equals to the vector field determined by \\(\\pmb{\\varphi}(t)\\), i.e. \\(f(t, \\pmb{\\varphi}(t))\\). This can help us find a smaller region for \\(A\\pmb{\\varphi}\\) to be well-defined.</p> <p>And note that \\(\\pmb{\\varphi}\\) is a solution of Cauchy Problem \\(\\ref{eq-vector-cauchy}\\) if and only if \\(\\pmb{\\varphi} = A \\pmb{\\varphi}\\). So we have to prove \\(A\\) is a contraction mapping in a complete metric space.</p> <ul> <li>\u5b9a\u4e49\u57df\\((t, \\pmb{x})\\) | Some Constants \\(a\\), \\(b\\), \\(M\\)</li> </ul> <p>Let </p> \\[ M=\\max_{\\pmb{x}\\in \\Gamma}\\|\\pmb{f}\\|, \\quad L= \\max_{\\pmb{x}\\in \\Gamma}\\|\\pmb{f}_*\\| \\text{(or by Lipschitz condition)} \\] <p>which can be obtained because \\(\\Gamma\\) is a compact set. Firstly, we consider a Cylinder</p> \\[ \\Gamma = \\{(t,\\pmb{x}): |t-t_0|\\leq a, \\|\\pmb{x}-\\pmb{x}_0\\|\\leq b\\} \\] <p><p> </p></p> <p>Now consider a Cone(\u9525\u4f53) with opening \\(M\\) and sufficient small height \\(a\\) at point \\((t_0,\\pmb{x}_0)\\)</p> \\[ K_0 = \\{(t,\\pmb{x}): |t-t_0|\\leq a, \\|\\pmb{x}-\\pmb{x}_0\\|\\leq M|t-t_0|\\} \\] <p>Which means our mapping \\(\\pmb{\\varphi}\\) after contraction mapping \\(A\\) is still well-defined. See details in the following proof.</p> Proof \\[ \\begin{align*} \\|A\\pmb{\\varphi}(t) - \\pmb{x}_0\\| &amp;=\\left\\|\\int_{t_0}^t\\pmb{f}(\\tau,\\pmb{\\varphi}(\\tau))d\\tau\\right\\| \\\\ &amp;\\leq \\left|\\int_{t_0}^t\\|\\pmb{f}(\\tau,\\pmb{\\varphi}(\\tau))\\|d\\tau\\right|\\\\ &amp;\\leq M |t-t_0| \\end{align*} \\] <p>So as long as \\(a&lt;b/M\\), \\(\\|A\\pmb{\\varphi}(t)-\\pmb{x}_0\\|\\leq b\\).</p> <ul> <li>Prove \\(\\pmb{\\varphi}\\) compose a complete metric space.</li> </ul> <p>We define a Space \\(M\\) composed of mappings \\(\\pmb{\\varphi}: \\mathbb{R}\\mapsto \\mathbb{R}^n\\) defined by \\(\\pmb{\\varphi}(t)=\\pmb{x}\\).</p> <p>Define a metric \\(\\rho\\) to be</p> \\[ \\rho(\\pmb{\\varphi}_1, \\pmb{\\varphi}_2) = \\|\\pmb{\\varphi}_1-\\pmb{\\varphi}_2\\| = \\max_{|t-t_0|\\leq a}|\\pmb{\\varphi}_1(t)-\\pmb{\\varphi}_2(t)| \\] <p>Then \\(\\{M, \\rho\\}\\) is a complete metric space.</p> <ul> <li>Prove \\(A\\) is a contraction mapping.</li> </ul> Proof <p>That is, \\(\\forall \\pmb{\\varphi}_1, \\pmb{\\varphi}_2 \\in M\\)</p> \\[ \\begin{align*} \\|A\\pmb{\\varphi}_1- A\\pmb{\\varphi}_2\\| &amp;= \\left\\|\\int_{t_0}^t[\\pmb{f}(\\tau, \\pmb{\\varphi}_1(\\tau))-\\pmb{f}(\\tau, \\pmb{\\varphi}_2(\\tau))]d\\tau\\right\\|\\\\ &amp;\\leq \\left|\\int_{t_0}^t\\|\\pmb{f}(\\tau, \\pmb{\\varphi}_1(\\tau))-\\pmb{f}(\\tau, \\pmb{\\varphi}_2(\\tau))\\|d\\tau \\right|\\\\ &amp;\\leq L \\int_{t_0}^t \\|\\pmb{\\varphi}_1(\\tau) - \\pmb{\\varphi}_2(\\tau)\\|d\\tau \\\\ &amp;\\leq L \\|\\pmb{\\varphi}_1 - \\pmb{\\varphi}_2\\| \\int_{t_0}^t d\\tau = L |t-t_0| \\|\\pmb{\\varphi}_1 - \\pmb{\\varphi}_2\\|  \\end{align*} \\] <p>So as long as \\(a&lt;1/L\\), \\(A\\) is a contraction mapping.</p> <p>By Banach Fixed Point Theorem, there exists only one soluion of ODE.</p>"},{"location":"courses/Ordinary_Differential_Equation/General_Theory/Contraction_Mapping_Method/#2-proof-2","title":"\u8bc1\u660e2 | Proof 2","text":"<p>In this part, we consider a different mapping.</p> <ul> <li>\u5b9a\u4e49\u57df\\((t, \\pmb{x})\\)</li> </ul> <p>In order to let all the possible cone to be in the cylinder, we have to shrink the region to make the region is well defined. That is, there exsits sufficiently small parameters \\(b'&lt;b\\) such that \\(\\|\\pmb{x}-\\pmb{x}_0\\|&lt;b'\\). So now we get a smaller region (a smaller cylinder)</p> \\[ \\Gamma' = \\{(t, \\pmb{x}): |t-t_0|&lt;a', \\|\\pmb{x}-\\pmb{x}_0\\|&lt;b'\\} \\] <p><p> </p></p> <p>We also have to prove that \\(A\\) maps \\(M\\) into itself. That is, \\(\\|\\varphi(t)-\\pmb{x}_0\\|\\leq b'\\). Here we have to make \\(a'\\) to satisfy some condition.</p> <p>Consider all the possible continuous mapping \\(\\pmb{h}\\) which maps the above cylinder into \\(\\mathbb{R}^n\\), which is a solution of original ODE problem. Assume \\(M\\) is a set composed of these mappings with additional condition</p> \\[ \\|\\pmb{h}(t, \\pmb{x})\\| \\leq C|t-t_0| \\] <p>Specially, we let \\(\\pmb{h}(t_0,\\pmb{x})=0\\). We introduce a metric </p> \\[ \\rho(\\pmb{h}_1,\\pmb{h}_2)=\\|\\pmb{h}_1-\\pmb{h}_2\\|=\\max_{(t,\\pmb{x})\\in \\Gamma' }\\|\\pmb{h}_1(t,\\pmb{x})-\\pmb{h}_2(t,\\pmb{x})\\| \\] <p> </p> <p>Note: Space \\(M\\) is dependent on \\(a'\\), \\(b'\\) and \\(C\\).</p> <p>Now we really have to consider a mapping </p> \\[ A: M\\mapsto M \\] <p>defined by</p> \\[ (A\\pmb{h})(t, \\pmb{x}) = \\int_{t_0}^t\\pmb{f}(\\tau, \\pmb{x}+\\pmb{h}(\\tau, \\pmb{x}))d\\tau \\] <p>\\(A\\) \u662f\u538b\u7f29\u6620\u5c04 | \\(A\\) is a contraction mapping</p> <p>\\(A\\) is a contraction mapping from \\(M\\) to \\(M\\), if \\(a'\\) is sufficiently small.</p> <p>Prove it. The following proof are set to find how small \\(a'\\) shoulc be.</p> HintsProof <p>Firstly, prove \\(A\\) maps \\(M\\) to itself. Then Prove if is a constraction mapping.</p> <ul> <li>\\(A\\) maps \\(M\\) to itself.</li> </ul> \\[ \\begin{align*} \\|(A\\pmb{h}(t, \\pmb{x}))\\| &amp;\\leq \\|\\int_{t_0}^t\\pmb{f}(\\tau, \\pmb{x}+\\pmb{h}(\\tau,\\pmb{x}))d\\tau\\| \\\\ &amp;\\leq |\\int_{t_0}^tCd\\tau| \\leq C|t-t_0| \\end{align*} \\] <p>so \\(AM\\subset M\\).</p> <ul> <li>\\(A\\) is a constraction mapping.</li> </ul> <p>Estimate \\(A\\pmb{h}_1- A\\pmb{h}_2\\):</p> \\[ (A\\pmb{h}_1-A\\pmb{h}_2)(t,\\pmb{x}) = \\int_{t_0}^t[\\pmb{f}(\\tau, \\pmb{x}+\\pmb{h}_1(\\tau,\\pmb{x}))-\\pmb{f}(\\tau, \\pmb{x}+\\pmb{h}_2(\\tau,\\pmb{x}))]d\\tau \\] <p>Denote \\(\\pmb{f}_i(\\tau) = \\pmb{f}(\\tau, \\pmb{x}+\\pmb{h}_i(\\tau,\\pmb{x}))\\), \\(i=1,2\\), so</p> \\[ \\begin{align*} \\|\\pmb{f}_1(\\tau)- \\pmb{f}_2(\\tau)\\| &amp;\\leq L\\|\\pmb{h}_1(\\tau)-\\pmb{h}_2(\\tau)\\| \\quad\\text{(using Lipschitz condition)}\\\\  &amp;\\leq L\\|\\pmb{h}_1-\\pmb{h}_2\\| = L\\rho(\\pmb{h}_1, \\pmb{h}_2) \\quad \\text{(by definition of metric)} \\end{align*} \\] <p>And </p> \\[ \\|(A\\pmb{h}_1-A\\pmb{h}_2)(t,\\pmb{x})\\| \\leq L a'\\rho(\\pmb{h}_1, \\pmb{h}_2) \\] <p>So if \\(La'&lt;1\\), then \\(A\\) is a contraction mapping.</p>"},{"location":"courses/Ordinary_Differential_Equation/General_Theory/Contraction_Mapping_Method/#lipschitz-appendix-from-continuity-differentiability-to-lipschitz-condition","title":"\u9644\u5f55\uff1a\u8fde\u7eed\u53ef\u5fae\u5230Lipschitz\u8fde\u7eed | Appendix: from Continuity &amp; Differentiability to Lipschitz Condition","text":"<p>we will choose natural metric </p> \\[ \\rho(\\pmb{x}, \\pmb{y}) = \\|\\pmb{x}-\\pmb{y}\\| = \\sqrt{(\\pmb{x}-\\pmb{y},\\pmb{x}-\\pmb{y})} \\] <p>so space \\(\\mathbb{R}^n\\) with the above metric is a complete metric space.</p> <p>So in this case, we have a parallel theorem for smoothness and Lipschitz condition in \\(\\mathbb{R}\\).</p> <p>\u8fde\u7eed\u53ef\u5fae\u6620\u5c04\u6ee1\u8db3Lipschitz\u6761\u4ef6 | Continuously Differentiable mapping satisfies Lipschitz condition</p> <p>Assume \\(V\\subset U \\subset\\mathbb{R}^m\\) is a convex and contract set, and continuously differentiable mapping \\(\\pmb{f}\\) which maps \\(U\\) to \\(\\mathbb{R}^n\\) satisfies Lipschitz condition, and the constant </p> \\[ L = \\sup_{\\pmb{x}\\in V} \\|\\pmb{f}_{*\\pmb{x}}\\|  \\] <p>where \\(\\pmb{f}_*|_{\\pmb{x}}=\\pmb{f}_{*\\pmb{x}}:T\\mathbb{R}^m_{\\pmb{x}}\\mapsto T\\mathbb{R}^n_{\\pmb{x}}\\)</p> <p><p> </p></p> <p>Prove it.</p> Hints <p>Let \\(\\pmb{z}(t)=\\pmb{x}+t(\\pmb{y}-\\pmb{x})\\), \\(0\\leq t\\leq 1\\). Then</p> \\[ \\begin{align*} \\pmb{f}(\\pmb{y})-\\pmb{f}(\\pmb{x})&amp;=\\int_{0}^1\\frac{d}{dt}\\pmb{f}(\\pmb{z}(\\tau))d\\tau\\\\ &amp;=\\int_0^1\\pmb{f}_{*\\pmb{z}(\\tau)}[\\pmb{\\dot{z}}(\\tau)]d\\tau \\\\ &amp;=\\int_0^1 \\pmb{f}_{*\\pmb{z}(\\tau)} (\\pmb{y}-\\pmb{x})d\\tau \\end{align*}\\\\ \\] <p>where \\(\\pmb{y}-\\pmb{x}\\) is a constant, so</p> \\[ \\begin{align*} \\left\\| \\pmb{f}(\\pmb{y})-\\pmb{f}(\\pmb{x}) \\right\\| &amp;=\\left\\|\\int_0^1 \\pmb{f}_{*\\pmb{z}(\\tau)} (\\pmb{y}-\\pmb{x})(\\tau)d\\tau\\right\\|\\\\ &amp;\\leq \\int_0^1 \\|\\pmb{f}_{*\\pmb{x}}\\| \\|\\pmb{y}-\\pmb{x}\\|d\\tau\\\\ &amp;\\leq L\\|\\pmb{y}-\\pmb{x}\\| \\end{align*} \\] <p>and we are done.</p>"},{"location":"courses/Ordinary_Differential_Equation/General_Theory/Existence_Uniqueness_Theorem/","title":"Existence & Uniqueness Theorem","text":""},{"location":"courses/Ordinary_Differential_Equation/General_Theory/Existence_Uniqueness_Theorem/#Existence-and-Uniqueness-Theorem","title":"\u5b58\u5728\u552f\u4e00\u6027\u5b9a\u7406 | Existence and Uniqueness Theorem","text":"<p>This chapter we focus on ODE with initial value problem</p> \\[ \\begin{equation} \\frac{dy}{dx} = f(x, y), \\quad y(x_0) = y_0. \\label{eq-cauchy} \\end{equation} \\] <p>The above problem is also called Cauchy Problem, for Cauchy firstly prove that the solution to the problem \\(\\ref{eq-cauchy}\\) exists uniquely when \\(f(x, y)\\) has continuous partial derivative to \\(y\\), that is, \\(\\frac{\\partial f}{\\partial y} \\in C(G)\\).</p>"},{"location":"courses/Ordinary_Differential_Equation/General_Theory/Existence_Uniqueness_Theorem/#preliminary-knowledge","title":"\u9884\u5907\u77e5\u8bc6 | Preliminary knowledge","text":"<p>The following inequation is really useful in estimating solution of ODE by offering the upper bound.</p> <p>Gronwall \u4e0d\u7b49\u5f0f | Gronwall Inequation</p> <p>Assume \\(\\alpha(x), u(x) \\in C[a, b]\\) are non-negative functions and \\(C, K\\) are non-negative constants. If </p> \\[ u(x) \\leq C + \\int_{a}^{b} \\left[ \\alpha(s) u(s)+K\\right] ds \\] <p>then </p> \\[ \\begin{align*} u(x) &amp;\\leq \\left[ C + \\int_{a}^{x}Ke^{-\\int_{a}^{s}\\alpha(t)dt}ds\\right]e^{\\int_{a}^{x}\\alpha(s)ds}\\\\ &amp;\\leq \\left[ C + K(x-a)\\right]e^{\\int_{a}^{x}\\alpha(s)ds} \\end{align*} \\] <p>Prove it.</p> Hints <p>Convert it into Ordinary Differential Inequation. Then solve it like what you do in solving ODE.</p> <p>\u4e00\u81f4\u6709\u754c\u7684\u5b9a\u4e49 | Definition of Uniform Bound</p> <p>Assume \\(\\Lambda\\) is an infinite set, set \\(I\\)(or interval \\([x, b]\\)) \\(\\subset \\mathbb{R}\\). A family of function \\(\\{f_\\lambda\\}_{\\lambda\\in\\Lambda}\\) defined on \\(I\\) is Uniformly Bounded, if \\(\\exists M&gt;0\\), s.t.</p> \\[ |f_\\lambda(x)|\\leq M, \\quad \\forall \\lambda\\in \\Lambda, \\forall x \\in I \\] <p>The above definition means that functions in \\(\\{f_\\lambda\\}_{\\lambda\\in\\Lambda}\\) are all bounded by some constant \\(M\\).</p> <p>\u7b49\u5ea6\u8fde\u7eed\u7684\u5b9a\u4e49 | Definition of Equicontinuous</p> <p>Assume \\(\\Lambda\\) is an infinite set, set \\(I\\)(or interval \\([x, b]\\)) \\(\\subset \\mathbb{R}\\). A family of function \\(\\{f_\\lambda\\}_{\\lambda\\in\\Lambda}\\) defined on \\(I\\) is Equicontinuous, if \\(\\forall \\varepsilon&gt;0, \\exists \\delta&gt;0\\), s.t. \\(\\forall x_1, x_2\\in I\\) and \\(|x_1-x_2|&lt;\\delta\\)</p> \\[ |f_\\lambda(x_1)-f_\\lambda(x_2)|&lt; \\varepsilon \\] <p>The above definition means that functions in \\(\\{f_\\lambda\\}_{\\lambda\\in\\Lambda}\\) are continuous equally. Usually \\(\\delta&gt;0\\) depends on \\(f_\\lambda\\), but here we mean the degree of continuouity of these functions is similar.</p> <p>Example. Function sequence \\(\\{f_n(x)\\}\\) satisfying</p> \\[ f_n(x) = (-1)^{n} + x^n \\] <p>is uniformly bounded and equicontinuous on region \\(\\{x: |x|\\leq 1/2\\}\\), is uniformly bounded but not equicontinuous on region \\(\\{x: |x|\\leq 1\\}\\), and is either not uniformly bounded and equicontinuous on region \\(\\{x: |x|\\leq 2\\}\\).</p> <p>The condition of the following theorem can be narrowed down to denumerable sets \\(\\Lambda\\) and interval \\(R \\subset \\mathbb{R}\\).</p> <p></p> <p>\u5f15\u74061: \u4e00\u81f4\u6709\u754c\u7684\u51fd\u6570\u5217\u6709\u6536\u655b\u51fd\u6570\u5b50\u5217(\u70b9\u6001) | Lemma 1: Uniformly Bounded Function Sequence has convergent Function Subsequence</p> <p>Assume set \\(\\Lambda\\) is denumerable. If a family of function \\(\\{f_\\lambda\\}_{\\lambda\\in\\Lambda}\\) defined on \\(I \\subset \\mathbb{R}\\) is uniformly bounded, then \\(\\forall A = \\{x_m\\}_{m=1}^{\\infty} \\subset I\\), \\(\\exists \\{f_{\\lambda_k}\\}_{k=1}^\\infty\\), a function subsequence of \\(\\{f_\\lambda\\}_{\\lambda\\in\\Lambda}\\), such that </p> \\[ \\{f_{\\lambda_k}(x)\\}, \\forall x \\in A \\] <p>is convergent.</p> <p>Prove it.</p> HintsProof <p>Using diagonal methods. </p> <p>Note that \\(\\{f_\\lambda(x_1)\\}_{\\lambda \\in \\Lambda}\\) is obviously bounded. So by Weierstrass Balzano Theorem, we get a convergent subsequence </p> \\[ f_{11}(x_1), f_{12}(x_1), \\cdots, f_{1n}(x_1) \\cdots \\] <p>which converges to \\(y_1\\).</p> <p>Now consider substitute \\(x_1\\) by \\(x_2\\), which becomes</p> \\[ f_{11}(x_2), f_{12}(x_2), \\cdots, f_{1n}(x_2) \\cdots \\] <p>According to the condition \"Uniformly Bounded\", the above sequence is also bounded, so we can find another convergent subsequence from it</p> \\[ f_{21}(x_2), f_{22}(x_2), \\cdots, f_{2n}(x_2) \\cdots \\] <p>which converges to \\(y_2\\).</p> <p>So we can say function subsequence of \\(\\{f_\\lambda(x_1)\\}_{\\lambda \\in \\Lambda}\\)</p> \\[ f_{21}(x), f_{22}(x), \\cdots, f_{2n}(x) \\cdots \\] <p>is convergent on \\(x\\in \\{x_1,x_2\\}\\). That is, \\(\\lim\\limits_{n\\rightarrow \\infty}f_{2n}(x_1) = y_1\\), \\(\\lim\\limits_{n\\rightarrow \\infty}f_{2n}(x_2) = y_2\\).</p> <p>Continue the above procedure, we can get another function subsequnce</p> \\[ f_{31}(x), f_{32}(x), \\cdots, f_{3n}(x) \\cdots \\] <p>which is convergent on \\(x \\in \\{x_1,x_2,x_3\\}\\). That is, \\(\\lim\\limits_{n\\rightarrow \\infty}f_{3n}(x_1) = y_1\\), \\(\\lim\\limits_{n\\rightarrow \\infty}f_{3n}(x_2) = y_2\\), \\(\\lim\\limits_{n\\rightarrow \\infty}f_{3n}(x_3) = y_3\\).</p> <p>Continue, it is not hard to imagine we get a function subsequence \\(\\{f_{nn}(x)\\}\\) which converges to \\(y(x)\\) on \\(x \\in \\{x_1,x_2,\\cdots, x_n\\}\\), with \\(y(x)\\) defined as </p> \\[ y(x_m) = y_m,\\quad  m=1,2,\\cdots \\] <p>So to express the subsequence more specificly, we can define \\(\\tilde{f}_n(x) = f_{nn}(x)\\), and get a subsequence \\(\\{\\tilde{f}_n(x)\\}_{n=1}^{\\infty}\\) such that </p> \\[ \\lim_{n\\rightarrow \\infty}\\tilde{f}_n(x) = y(x), \\forall x\\in I. \\] <p>And we are done.</p> <p>The above theorem offers that we can find a function sequence convergent on denumerable set \\(A\\), which is pointwise convergence.</p> <p>Now if we let the above function subsequence \\(\\{f_{\\lambda_k}(x)\\}\\) to be Equicontinuous, then it can be uniformly convergent on the whole interval \\(I\\). This is exactly the following theorem(in which \\(I\\) is a general region, not just an interval). Note that the above denumerable set can be said as \"dense set\".</p> <p></p> <p>\u5f15\u74062: \u70b9\u6001\u6536\u655b\u3001\u7b49\u5ea6\u8fde\u7eed\u7684\u51fd\u6570\u5217\u5728\u7d27\u96c6\u4e0a\u4e00\u81f4\u6536\u655b | Lemma 2: Equicontinuous Function Sequence is Uniformly Convergent given Pointwise Convergence on Denumerable Sets</p> <p>Assume \\(\\{f_n\\}_{n=1}^\\infty\\) defined on compact set \\(I \\subset \\mathbb{R}\\) is Equicontinuous, and there exists a dense subset \\(R \\subset I\\), such that \\(\\{f_n\\}_{n=1}^\\infty\\) is convergent on \\(R\\), then \\(\\{f_n\\}_{n=1}^\\infty\\) is uniformly convergent on \\(I\\). And denote</p> \\[ f_n(x) \\rightrightarrows f(x), \\quad n\\rightarrow \\infty \\] <p>where the convergent function \\(f(x)\\) is continuous on \\(I\\). </p> <p>Prove it.</p> HintsProofIf \\(I=[a,b]\\) <p>Using Equicontinuity and compact characteristic of set \\(I\\).</p> <p>We hope to prove that \\(\\forall \\varepsilon, \\exists N&gt;0\\), s.t. \\(\\forall n&gt;m&gt;N\\), \\(\\forall x \\in I\\), we have </p> \\[ |f_n(x)-f_m(x)| &lt; \\varepsilon \\] <ul> <li>use the Equicontinuity. </li> </ul> <p>That is, \\(\\forall \\varepsilon\\), \\(\\exists \\delta&gt;0\\), \\(\\forall x_1,x_2\\in I\\) and \\(|x_1-x_2|&lt;\\delta\\), \\(\\forall n\\), we have</p> \\[ |f(x_1)-f(x_2)| &lt; \\frac{\\varepsilon}{9} \\] <ul> <li>use the characteristic of compact set.</li> </ul> <p>Note that there exists open coverings of finite number for compact set \\(I\\). That is, we have \\(x_j\\in I\\), \\(j=1,2,\\cdots, k_0\\), and \\(\\{O_{\\delta'}(x_j)\\}_{j=1}^{k_0}\\) s.t. </p> \\[ I \\subset \\bigcup_{j=1}^{k_0}O_{\\delta'}(x_j) \\] <p>To let it help us prove, we can let \\(\\delta'&lt;\\delta\\) and we can still have a valid corresponding \\(k_0\\) which is finite.</p> <p>Because \\(R\\) is a dense set on \\(I\\), so \\(\\forall O(x_j)\\), \\(\\exists y_j\\in R\\) such that \\(y_j \\in O(x_j)\\).</p> <ul> <li>use the hypothesis of pointwise convergence.</li> </ul> <p>Because \\(\\{f_n\\}_{n=1}^{\\infty}\\) is convergent on dense set \\(R\\), we can have \\(\\forall \\varepsilon&gt;0\\)(choose the same one as the above one), \\(\\exists N&gt;0\\), \\(\\forall n&gt;m&gt;N\\), \\(\\forall y_j \\in R\\), we have</p> \\[ |f_n(y_j)-f_m(y_j)|&lt;\\frac{\\varepsilon}{9} \\] <p>That is, as long as \\(n&gt;m&gt;N\\), we have</p> \\[ \\begin{align} |f_n(x_j)-f_m(x_j)| &amp;\\leq |f_n(x_j)-f_n(y_j)| + |f_n(y_j)-f_m(y_j)| + |f_m(y_j)-f_m(x_j)| \\nonumber\\\\ &amp;&lt; |f_n(x_j)-f_n(y_j)| + \\frac{\\varepsilon}{9} + |f_m(y_j)-f_m(x_j)|\\quad \\text{by pointwise-convergence}\\nonumber\\\\ &amp;&lt;\\frac{\\varepsilon}{3} \\quad \\text{the 1st and 3rd hold for equicontinuity} \\label{bound-open-covering} \\end{align} \\] <p>Note that \\(\\forall x \\in I\\), \\(\\exists j\\in {1,2,\\cdots, k_0}\\), such that \\(x\\in O_{\\delta'}(x_j)\\), so</p> \\[ \\begin{align*} |f_n(x)-f_m(x)| &amp;\\leq |f_n(x)-f_n(x_j)| + |f_n(x_j)-f_m(x_j)| + |f_m(x_j)-f_m(x)|\\\\ &amp;&lt; |f_n(x)-f_n(x_j)| + \\frac{\\varepsilon}{3} + |f_m(x_j)-f_m(x)|\\quad \\text{by the above condition }\\ref{bound-open-covering} \\\\ &amp;&lt;\\varepsilon \\quad \\text{the 1st and 3rd hold for equicontinuity} \\end{align*} \\] <p>In this case, we do not have to find a open covering of \\([a,b]\\) but just partition the closed interval \\([a,b]\\) into \\(k_0\\) distinct closed intervals whose length are less then \\(\\delta\\). Denote these intervals as \\(I_j\\), \\(j=1,2\\cdots, k_0\\).</p> <p>Then \\(\\forall x \\in [a, b]\\), \\(\\exists j\\in {1,2,\\cdots, k_0}\\) such that \\(x\\in I_j\\). Because \\(R\\) is dense on \\([a,b]\\), then we are able to find an element \\(x_j \\in I_j\\), which means </p> \\[ |x-x_j|&lt; \\text{length of }I_q &lt;\\delta \\] <p>So </p> \\[ \\begin{align*} |f_n(x)-f_m(x)| &amp;\\leq |f_n(x)-f_n(x_j)| + |f_n(x_j)-f_m(x_j)| + |f_m(x_j)-f_m(x)|\\\\ &amp;&lt; |f_n(x)-f_n(x_j)| + \\frac{\\varepsilon}{9} + |f_m(x_j)-f_m(x)|\\quad \\text{by pointwise-convergence} \\\\ &amp;&lt;\\frac{\\varepsilon}{3}&lt;\\varepsilon \\quad \\text{the 1st and 3rd hold for equicontinuity} \\end{align*} \\] <p>By utilizing the above two theorems, we can prove the following important theorem.</p> <p>Ascoli-Arzel\u00e0 \u5b9a\u7406 | Ascoli-Arzel\u00e0 Theorem</p> <p>Assume \\(\\Lambda\\) is denumerable. If a family of sequence \\(\\{f_\\lambda\\}_{\\lambda\\in\\Lambda}\\) is uniformly bounded and equicontinuous on closed interval \\([a, b]\\), then there exists a function subsequence \\(\\{f_{\\lambda_k}\\}_{k=1}^{\\infty}\\) of \\(\\{f_\\lambda\\}_{\\lambda\\in\\Lambda}\\) which is uniformly convergent on \\([a, b]\\).</p> <p>Prove it.</p> HintsProof <p>Making use of the above two lemmas.</p> <p>Firstly, we have to find a dense set \\(R\\). Naively, we can choose rational numbers on \\([a,b]\\). That is,</p> \\[ R \\overset{\\Delta}{=}\\mathbb{Q}\\cap [a,b]. \\] <p>Then by lemma 1, using its uniformly bounded,  we can find a function subsequence of \\(\\{f_\\lambda(x)\\}_{\\lambda\\in\\Lambda}\\), denoted as \\(\\{f_{n}\\}_{n=1}^{\\infty}\\), which is convergent on \\(R\\). Finally, by lemma 2, using its equicontinous, the subsequence is uniformly convergent on \\([a,b]\\).</p> <p>And by property of uniformly convergent function sequence, we can see that the convergent funtion is continuous on \\(I\\).</p>"},{"location":"courses/Ordinary_Differential_Equation/General_Theory/Existence_Uniqueness_Theorem/#Picard-Theorem","title":"Picard\u5b58\u5728\u552f\u4e00\u6027\u5b9a\u7406 | Picard Theorem of Existence and Uniqueness","text":"<p>Picard uses the following condition to prove his theorem.</p> <p>Lipschitz\u6761\u4ef6\u7684\u5b9a\u4e49 | Definition of Lipschitz Condition</p> <p>Function \\(f(x, y)\\) defined at region \\(G\\), satisfies Lipschitz condition with respect to \\(y\\), if \\(\\exists L\\) s.t. \\(\\forall (x, y_1), (x, y_2) \\in G\\)</p> \\[ |f(x, y_1)-f(x,y_2)|\\leq L|y_1-y_2| \\] <p>Also, Picard focuses on a typical rectangular region</p> \\[ \\begin{equation} R = \\{(x,y): |x-x_0| \\leq a, |y-y_0|\\leq b\\} \\label{region} \\end{equation} \\] <p>to give his iterative method.</p> <p>Picard\u5b9a\u7406 | Picard Theorem</p> <p>Assume \\(f(x, y) \\in C(G)\\) satisfies Lipschitz condition with respect to \\(y\\), then Cauchy Problem has unique solution on interval \\([x-\\alpha, x+\\alpha]\\), where</p> \\[ \\alpha = \\min \\left\\{a, \\frac{b}{M} \\right\\}, \\quad M = \\max_{(x, y) \\in R}\\left\\{\\left|f(x, y)\\right|\\right\\} \\] <p>Prove it.</p> HintsClassical Proof <p>There typically 4 steps. </p> <p>Firstly, Convert the differential problem into an equivalent integral problem. Then, formulate the so-called Picard sequence and prove it convergent. Furthermore, we have to prove that Picard sequence converges to the solution of integral equation. Finally, we prove the uniqueness by Gronwall Inequation.</p> <ul> <li>Convert the differential problem into an integral problem. That is, solving equation \\(\\ref{eq-cauchy}\\) equals to solving integral equation</li> </ul> \\[ \\begin{align} y(x) &amp;= y(0) + \\int_{x_0}^{x} f(s, y(s))ds \\nonumber\\\\ &amp;=y_0+ \\int_{x_0}^{x} f(s, y(s))ds \\label{eq-integral} \\end{align} \\] <p>readers can prove its equivalence(by proving solution of one side is also solution of the other).</p> <ul> <li>Formulate Picard sequence.</li> </ul> <p>Define:</p> \\[ \\begin{align*} y_0(x) &amp;= y_0\\\\ y_1(x) &amp;= y_0 + \\int_{x_0}^{x} f(s, y_0(s))ds\\\\ y_2(x) &amp;= y_0 + \\int_{x_0}^{x} f(s, y_1(s))ds\\\\ &amp;\\vdots\\\\ y_n(x) &amp;= y_0 + \\int_{x_0}^{x} f(s, y_{n-1}(s))ds\\\\ \\end{align*} \\] <p>We can say the above function sequence \\(\\{y_n(x)\\}_{n=1}^\\infty\\) is well-defined because of the following condition it satisfies:</p> \\[ |y_n(x)-y(x)| \\leq b \\quad \\&amp; \\quad y_n(x)\\in C(R) \\] <p>The above conition enables \\(f(x, y_{n-1}(x))\\) still falls on \\(R\\) and can be integrated(readers can prove that above two condition by induction).</p> <ul> <li>Prove Picard Sequence convergent.</li> </ul> <p>This is the most magic part. The following deduction may be the inspiration:</p> \\[ \\begin{align*} |y_1(x)-y_0(x)| &amp;= \\left| \\int_{x_0}^{x} f(s, y_0)ds \\right| \\leq M\\left| x - x_0\\right| \\\\ |y_2(x)-y_1(x)| &amp;= \\left| \\int_{x_0}^{x}\\left[ f(s, y_1(s)) - f(s, y_0(s))\\right]ds \\right| \\\\  &amp;\\leq \\int_{x_0}^{x} \\left| f(s, y_1(s)) - f(s, y_0(s)) \\right| ds  \\\\  &amp;\\leq L \\int_{x_0}^{x} \\left|y_1(s) - y_0(s)\\right| ds\\quad \\text{(Using Lipschitz Condition)}\\\\ &amp;\\leq LM \\int_{x_0}^{x}\\left|s - x_0\\right|ds =\\frac{LM}{2}|x-x_0|^2 \\quad \\text{(Using the first item)} \\end{align*} \\] <p>So we can allege that</p> \\[ |y_n(x)-y_{n-1}(x)| \\leq \\frac{ML^{n-1}}{n!}|x-x_0|^n \\] <p>and prove it by induction(to be proved by readers).</p> <p>With the above condition, we can use Weierstrass test to prove Picard sequence converges. To be specific, we can see that the Picard Sequence is controlled by a Series of constant terms, which satisfies Cauchy Convergence Theorem. That is, \\(\\forall \\varepsilon&gt;0, \\exists N&gt;0, \\forall n&gt;m&gt;N\\), s.t.</p> \\[ \\begin{align*} |y_n(x)-y_{m}(x)| &amp;\\leq \\sum_{k=m+1}^{n}\\frac{ML^{k-1}}{k!}|x-x_0|^k \\\\ &amp;= \\frac{M}{L}\\sum_{k=m+1}^{n}\\frac{L^{k}}{k!}|x-x_0|^k \\\\ &amp;\\leq \\frac{M}{L}\\sum_{k=m+1}^{n}\\frac{(L\\alpha)^{k}}{k!} \\end{align*} \\] <p>And we notice that Series of constant terms</p> \\[ \\sum_{n=1}^\\infty \\frac{(L\\alpha)^{n}}{n!} \\] <p>converges(to be specific, converges to \\(e^{L\\alpha}\\)), so Picard Sequence also converges.</p> <ul> <li>Prove Picard Sequence converges to solution of equation \\(\\ref{eq-cauchy}\\).</li> </ul> <p>This part is quite easy, to be done by readers.</p> <ul> <li>Prove uniqueness.</li> </ul> <p>Follow the traditional logic: contradiction.</p> <p>Assume there are \\(\\varphi_1(x), \\varphi_2(x)\\) two distinct solutions to equation \\(\\ref{eq-cauchy}\\), then subtract one from the other:</p> \\[ \\begin{align} |\\varphi_1(x) - \\varphi_2(x)| &amp;=\\left| \\int_{x_0}^{x} \\left[ f(s, \\varphi_1(s)) - f(s, \\varphi_2(s))\\right]ds \\right| \\nonumber\\\\ &amp;\\leq L \\int_{x_0}^{x} \\left|\\varphi_1(s) - \\varphi_2(s)\\right|ds  \\quad \\text{(Using Lipschitz Condition)} \\label{eq-same} \\end{align} \\] <p>And by using Gronwall inequation, we can get </p> \\[ |\\varphi_1(x) - \\varphi_2(x)| \\leq 0 \\] <p>So \\(\\varphi_1(x) = \\varphi_2(x)\\), that is, there exists only one solution for equation \\(\\ref{eq-cauchy}\\).</p> <p>In another way, if we don't want to use Gronwall Inequation, we can still say that the two solutions are the same. Assume that \\(\\varphi_1(x), \\varphi_2(x)\\) have a common region \\(J = [x_0-d,x_0+d]\\), where \\(0&lt;d\\leq \\alpha\\). Then \\(|\\varphi_1(x)-\\varphi_2(x)|\\) is continuous and bounded on region \\(J\\) and denote</p> \\[ K = \\max_{x\\in J}\\{|\\varphi_1(x)-\\varphi_2(x)|\\} \\] <p>So the right side of inequation \\(\\ref{eq-same}\\) can be bounded:</p> \\[ \\begin{align} |\\varphi_1(x) - \\varphi_2(x)| &amp;\\leq L \\int_{x_0}^{x} \\left|\\varphi_1(s) - \\varphi_2(s)\\right|ds  \\nonumber\\\\ &amp;\\leq LK |x-x_0| \\label{eq-same1} \\end{align} \\] <p>Substitute inequation \\(\\ref{eq-same1}\\) again into the right side of inequation \\(\\ref{eq-same}\\) and get</p> \\[ \\begin{align*} |\\varphi_1(x) - \\varphi_2(x)| \\leq KL^2 \\frac{|x-x_0|^2}{2} \\end{align*} \\] <p>repeat the above method iteratively and we can prove the following by induction:</p> \\[ |\\varphi_1(x) - \\varphi_2(x)| \\leq K \\frac{(L|x-x_0|)^n}{n!} \\] <p>Let \\(n \\rightarrow \\infty\\), we get \\(|\\varphi_1(x) - \\varphi_2(x)| \\rightarrow 0\\).</p>"},{"location":"courses/Ordinary_Differential_Equation/General_Theory/Existence_Uniqueness_Theorem/#osgood-osgood-condition","title":"Osgood \u6761\u4ef6 | Osgood Condition","text":"<p>We consider a condition which is slightly weaker than Lipschitz condition but still can guarrantee the convergence of Picard Sequence and its uniqueness.</p> <p>Osgood \u6761\u4ef6 | Osgood Condition</p> <p>Assume \\(D\\) is a region on \\(\\mathbb{R}^2\\), and function \\(f(x,y)\\in C(D)\\). If \\(\\forall (x,y_1), (x,y_2)\\in D\\), s.t.</p> \\[ |f(x,y_1)-f(x,y_2)|\\leq F(|y_1-y_2|) \\] <p>where \\(F(r)&gt;0 \\in C(\\mathbb{R})\\) satisfies </p> \\[ \\int_{0}^{\\varepsilon}\\frac{1}{F(r)}dr = +\\infty, \\quad \\forall \\varepsilon &gt;0 \\] <p>then we say that \\(f(x,y)\\) satisfies Osgood condition with respect to \\(y\\).</p> <p>Obviously, if \\(f(x,y)\\) satisfies Lipschitz condition, it satisfies Osgood condition. In fact, we can choose \\(F(r) = Lr\\).</p> <p>Osgood \u5b9a\u7406 | Osgood Theorem</p> <p>If  \\(f(x,y) \\in C(D)\\) satisfies Osgood condition with respect to \\(y\\), then \\(\\forall (x_0,y_0)\\in D\\), th solution to Cauchy problem \\(\\ref{eq-cauchy}\\) exists uniquely.</p> <p>Prove it.</p> HintsProof <p>The existence is guarranteed by Peano Theorem in the following part. You only need to prove the uniqueness, which is proved by contradiction, which is similar to prove the uniqueness of \\(f(x, y)\\) that decrease monotonically with respect to \\(y\\).</p> <p>Assume we have two distinct solution \\(y_1(x)\\), \\(y_2(x)\\), then there exists \\(x_2\\) such that \\(y_1(x_2)\\neq y_2(x_2)\\). Let \\(y_1(x) &gt; y_2(x)\\). </p> <p>Then by feature of guarantee code(\u4fdd\u53f7\u6027), there must exist a region such that \\(y_1(x) &gt; y_2(x)\\), so let \\(x_1 = \\max\\{x\\in [x_0,x_2] : y_1(x) = y_2(x)\\}\\).</p> <p><p> </p></p> <p>So we have </p> \\[ y_1(x) &gt; y_2(x), \\quad \\forall x\\in (x_1,x_2] \\] <p>define \\(r(x) = y_1(x) - y_2(x) \\in (0, m]\\), where m is determined by \\(m = \\max\\limits_{x\\in (x_1,x_2]}\\{y_1(x) - y_2(x)\\}\\)</p> <p>So by condition of the proposition, we have</p> \\[ |r'(x)| = |y_1'(x) - y_2'(x)| = |f(x,y_1(x))- f(x,y_2(x))| \\leq F(|y_1(x) - y_2(x)|) \\] <p>because \\(y_1(x) &gt; y_2(x)\\), so we take \"\\(\\vert \\cdot \\vert\\)\" out and get</p> \\[ r'(x) = y_1'(x) - y_2'(x) = f(x,y_1(x))- f(x,y_2(x)) \\leq F(y_1(x) - y_2(x)) \\] <p>divide both sides \\(F(y_1(x) - y_2(x))\\) and integrate on \\((x_1, x_2]\\), which is an improper integral on the left</p> \\[ \\int_{x_1}^{x_2}\\frac{r'(x)}{F(r(x))}dx\\leq \\int_{x_1}^{x_2}dx = x_2-x_1  \\] <p>so the left item can be \\(+\\infty\\) by condition while the right item is less than \\(+\\infty\\), i.e.</p> \\[ +\\infty=\\int_{0}^{r(x_2)}\\frac{1}{F(r)}dr=\\int_{r(x_1)}^{r(x_2)}\\frac{1}{F(r)}dr\\leq x_2-x_1 &lt;+\\infty \\] <p>which contradicts!</p>"},{"location":"courses/Ordinary_Differential_Equation/General_Theory/Existence_Uniqueness_Theorem/#Peano-Theorem","title":"Peano\u5b9a\u7406 | Peano Theorem","text":"<p>When \\(f(x,y)\\) does not satisfy Lipschitz Condition with respect to \\(y\\), we cannot guarantee the existence and uniqueness of the solution to Cauchy Problem \\(\\ref{eq-cauchy}\\). However, when \\(f(x,y)\\) is continuous, Peano proved that Cauchy Problem \\(\\ref{eq-cauchy}\\) has solution.</p> <p>Peano\u5b9a\u7406 | Peano Theorem</p> <p>Assume \\(f(x, y)\\) is continous in \\(R(\\ref{region})\\), then Cauchy Problem \\(\\ref{eq-cauchy}\\) has at least one solution in interval \\([x_0-\\alpha, x_0+\\alpha]\\), where</p> \\[ \\alpha = \\min \\left\\{a, \\frac{b}{M} \\right\\}, \\quad M = \\max_{(x, y) \\in R}\\left\\{\\left|f(x, y)\\right|\\right\\} \\] <p>Prove it.</p> HintsProof <p>There are several methosd to prove Peano Theorem, like Euler's Arc method, Tonelli(\u6258\u5185\u5229) Sequence method, fixed-point method in functional analysis, while Euler's Arc method is thought to be the dawm to calculating ODE numerically, so we will prove it this way. </p> <p>The idea is to construct approximation solution which converges to the real solution. In fact, Picard Sequence is also an approximation solution.</p> <ul> <li>Cauchy Problem \\(\\ref{eq-cauchy}\\) can be converted equivalently to integral equation \\(\\ref{eq-integral}\\).</li> </ul> <p>We only discuss the existence of solution of right side(\\([x_0, x_0+\\alpha]\\)). We can make similar treatment to the left side.</p> <ul> <li>Formulate Euler Polygons/Polygonal Arc(\u6b27\u62c9\u6298\u7ebf).</li> </ul> <p>The idea is simple: go ahead step by step as small as possible, employing the slope of this sampling points.</p> <p>Firstly, we partition the region \\([x_0,x_0+\\alpha]\\) into \\(n\\) parts(usually of equal length), that is</p> \\[ x_0 &lt; x_1 &lt; x_2 &lt; \\cdots &lt; x_n = x_0 + \\alpha \\] <p>and then define curves</p> \\[ \\begin{align*} l_1: y &amp;= y_0 + f(x_0, y_0)(x-x_0), \\quad x_0\\leq x\\leq x_1 \\\\ l_2: y &amp;= y_1 + f(x_1, y_1)(x- x_1), \\quad x_1 \\leq x\\leq x_2\\\\ &amp;\\vdots \\\\ l_n: y &amp;= y_{n-1} + f(x_{n-1}, y_{n-1})(x - x_{n-1}), \\quad x_{n-1} \\leq x \\leq x_n \\end{align*} \\] <p><p> </p> we can formulate a polygonal arc on \\([x_0, x_0+\\alpha]\\):</p> \\[ E_n = \\bigcup_{s=1}^{n}l_s \\] <p>and its function expresstion is </p> \\[ \\begin{equation} y_n(x) = y_0 + \\sum_{i=0}^{j-1}f(x_i, y_i)(x_{i+1} - x_{i}) + f(x_j, y_j)(x - x_j) \\label{eq-euler} \\end{equation} \\] <p>where \\(\\forall x \\in (x_0, x_0 +\\alpha], \\exists j\\) s.t. </p> \\[ x_j &lt; x\\leq x_{j+1} \\] <p>Here closed interval \\([x_0, x_0+\\alpha]\\) is of great importance, for it can guarantee the condition for Ascoli-Arzel\u00e0 Theorem to be true.</p> <ul> <li>\u4e00\u81f4\u6709\u754c | Uniform Bound</li> </ul> <p>By definition, it is easy to prove that </p> \\[ |y_n(x)-y_0| \\leq b, \\quad \\forall n, \\forall x \\in [x_0,x_0+\\alpha] \\] <ul> <li>\u7b49\u5ea6\u8fde\u7eed | Equicontinuous</li> </ul> <p>By definition, it is easy to see that \\(\\forall \\varepsilon&gt;0\\), \\(\\exists \\delta = \\varepsilon/M\\), s.t. \\(\\forall x_1, x_2 \\in [x_0,x_0+\\alpha]\\) and \\(|x_1-x_2|&lt;\\delta\\), we have</p> \\[ |y_n(x_1)-y_n(x_2)|\\leq \\max_{(x,y)\\in R}|f(x,y)| |x_1-x_2| &lt; M \\cdot \\frac{\\varepsilon}{M} = \\varepsilon \\] <ul> <li>Using Ascoli-Arzel\u00e0 Theorem</li> </ul> <p>So the function sequence \\(\\{y_n(x)\\}\\) has a uniformly convergent subsequence \\(\\{y_{n_j}(x)\\}\\).</p> <ul> <li>Prove Euler's Arc converges to the solution of ODE.</li> </ul> <p>This means that we have to consider the error of \\(y_{n_j}(x)\\) and \\(y(x)\\). Firstly, we rewrite the formula of Euler's Arc \\(\\ref{eq-euler}\\).</p> <p>Note that</p> \\[ \\begin{align*} f(x_i,y_i)(x_{i+1}-x_i)&amp;=\\int_{x_i}^{x_{i+1}}f(x_i,y_i)dx\\\\ &amp;=\\int_{x_i}^{x_{i+1}}f(x,y_n(x)) + f(x_i,y_i) - f(x,y_n(x))dx\\\\ \\end{align*} \\] <p>Define</p> \\[ d_n(i) \\overset{\\Delta}{=} \\int_{x_i}^{x_{i+1}}[f(x_i,y_i) - f(x,y_n(x))]dx \\] <p>then </p> \\[ \\begin{equation} f(x_i,y_i)(x_{i+1}-x_i) = \\int_{x_i}^{x_{i+1}}f(x,y_n(x))dx + d_n(i) \\label{eq-front} \\end{equation} \\] <p>Similarly, we have \\(x_j&lt;x\\leq x_{j+1}\\)</p> \\[ \\begin{align*} f(x_j,y_j)(x-x_j)&amp;=\\int_{x_j}^{x_{j+1}}f(x_j,y_j)dx\\\\ &amp;=\\int_{x_j}^{x_{j+1}}f(x,y_n(x)) + f(x_j,y_j) - f(x,y_n(x))dx\\\\ \\end{align*} \\] <p>Define</p> \\[ d^*_n(x) \\overset{\\Delta}{=} \\int_{x_j}^{x}[f(x_j,y_j) - f(x,y_n(x))]dx  \\] <p>then </p> \\[ \\begin{equation} f(x_j,y_j)(x-x_j) = \\int_{x_j}^{x_{j+1}}f(x,y_n(x))dx +d_n^*(x) \\label{eq-back} \\end{equation} \\] <p>So with the above two transformation \\(\\ref{eq-front}\\), \\(\\ref{eq-back}\\), the formula of Euler's Arc \\(\\ref{eq-euler}\\)(summation of linear expresstions) becomes an integral-like expression:</p> \\[ \\begin{align*} y_n(x) &amp;= y_0 + \\sum_{i=0}^{j-1}f(x_k, y_k)(x_{i+1} - x_{i}) + f(x_j, y_j)(x - x_j)\\\\ &amp; = y_0 + \\int_{x_0}^{x}f(x,y_n(x))dx + \\delta_n(x) \\end{align*} \\] <p>where </p> \\[ \\begin{equation} \\delta_n(x) = \\sum_{i=0}^{j-1}d_n(i) + d^*_n(x) \\label{eq-residue} \\end{equation} \\] <p>we know that for each \\(y_n\\), we can have variables \\(x, y\\) bounded by</p> \\[ |x-x_j|\\leq \\frac{\\alpha}{n}, \\quad |y-y_j|\\leq M \\frac{\\alpha}{n} \\] <p>So \\(\\forall \\varepsilon &gt;0\\), \\(\\exists N(\\varepsilon) = \\frac{\\alpha}{\\varepsilon}\\), s.t. \\(n&gt;N\\), </p> \\[ |x-x_j|\\leq \\frac{\\alpha}{N} = \\varepsilon, \\quad |y-y_j|\\leq M \\frac{\\alpha}{N} = M\\varepsilon \\] <p>which means the region of each integral interval can be as small as possible so long as \\(n\\rightarrow \\infty\\).</p> <p>So in this case, for each item in residue \\(\\ref{eq-residue}\\), we can bound it into small value corresponding to \\(\\varepsilon\\) by making use of the continuity of \\(f(x,y)\\), which means \\(\\forall\\varepsilon&gt;0\\), \\(\\exists \\delta&gt;0\\), s.t. \\(\\forall (x,y) \\in O[(x_j,y_j),\\delta]\\), </p> \\[ |f(x,y)-f(x_j,y_j)| &lt;\\varepsilon \\] <p>The above one is easy to accomplish because we can let \\(n\\) be large enough, so all \\(x \\in [x_j,x_{j+1}]\\) with its \\(y_n(x)\\) can fall in \\(O[(x_j,y_j),\\delta]\\).</p> <p>So the whole residue \\(\\ref{eq-residue}\\) can be bounded:</p> \\[ \\delta_n(x) \\leq \\varepsilon \\frac{\\alpha}{n} + j \\varepsilon \\frac{\\alpha}{n} &lt; \\alpha \\varepsilon \\] <p>So we can say \\(y_{n_j}(x)\\) defined as</p> \\[ y_{n_j}(x) = y_0 + \\int_{x_0}^{x}f(s, y_{n_j}(s))ds + \\delta_{n_j}(x), \\quad x \\in [x_0,x_0+\\alpha] \\] <p>where \\(\\delta_{n_j}(x)\\) satisfies</p> \\[ \\delta_{n_j}(x)\\rightrightarrows 0 \\] <p>Which also means, if we denote </p> \\[ \\varphi(x) \\overset{\\Delta}{=} \\lim_{n_j\\rightarrow \\infty}y_{n_j}(x) \\] <p>and let \\(n_j \\rightarrow \\infty\\), we get</p> \\[ \\varphi(x) = y_0 + \\int_{x_0}^{x}f(s, \\varphi(s))ds \\] <p>which means function subsequence \\(\\{y_{n_j}(x)\\}\\) converges to the solution of integral form \\(\\ref{eq-integral}\\) of ODE \\(\\ref{eq-cauchy}\\).</p>"},{"location":"courses/Ordinary_Differential_Equation/General_Theory/Existence_Uniqueness_Theorem/#comments-on-previous","title":"Comments on Previous","text":"<p>Here we list the difference between Picard Theorem and Peano Theorem.</p> Theorem Picard Theorem Peano Theorem Condition \\(f\\in C(R)\\), Lipschitz Condition with respect to \\(y\\) only \\(f\\in C(R)\\) Sequence the whole Picard Sequence \\(\\{y_n(x)\\}\\) converges subsequence of Euler's Arc \\(\\{\\varphi_n(x)\\}\\) converges Solution exists uniquely exists only, might have a lot of solutions <ul> <li>Note 1: Uniqueness can help bound Euler's Arc. See the following theorem.</li> </ul> <p>\u6709\u552f\u4e00\u89e3\u7684\u67ef\u897f\u95ee\u9898\uff0c\u5176\u6b27\u62c9\u6298\u7ebf\u5168\u5e8f\u5217\u6536\u655b | Euler's Arc converges given uniqueness</p> <p>If Cauchy problem \\(\\ref{eq-cauchy}\\) has unique solution, then the whole sequence of Euler's Arc \\(\\{\\varphi_n(x)\\}\\) converges.</p> <p>Prove it.</p> HintsProof <p>Use contradiction.</p> <p>Assume that the conclusion does not hold, then by definition of negative proposition, we have</p> \\[ \\exists \\varepsilon_0&gt;0, \\exists \\overline{x} \\in I, \\forall N_0&gt;0, \\exists n_0,m_0&gt;N_0, |\\varphi_{n_0}(\\overline{x})-\\varphi_{m_0}(\\overline{x})|\\geq\\varepsilon_0 \\] <p>which is the negative proposition of the Cauchy Convergence form</p> \\[ \\forall \\varepsilon&gt;o, \\forall x\\in I, \\exists N&gt;0, \\forall n&gt;m&gt;N, |\\varphi_{n}(x)-\\varphi_{m}(x)|&lt;\\varepsilon. \\] <p>In order to help us prove, we can take two sequence from the above negative proposition. That is, let \\(N_0=j\\), \\(j=1,2,\\cdots\\), we can take the corresponding \\(n_j\\), \\(m_j\\) out to compose a subsequence of Euler's arc \\(\\{\\varphi_n(x)\\}\\), i.e.</p> \\[ \\begin{equation} \\exists \\varepsilon_0&gt;0, \\exists \\overline{x} \\in I, \\forall j=1,2,\\cdots, \\exists n_j,m_j&gt;j, |\\varphi_{n_j}(\\overline{x})-\\varphi_{m_j}(\\overline{x})|\\geq\\varepsilon_0 \\label{con-non-convergent} \\end{equation} \\] <p>See that \\(\\{\\varphi_{n_j}(x)\\}\\) is still uniformly bounded and equicontinous on \\(I\\), so there exists its subsequence \\(\\{\\tilde{\\varphi}_{n_j}(x)\\}\\) such that</p> \\[ \\{\\tilde{\\varphi}_{n_j}(x)\\} \\rightrightarrows \\varphi_1(x), \\quad \\forall x\\in I \\] <p>and also \\(\\varphi_1(x)\\) is still the solution of Cauchy problem \\(\\ref{eq-cauchy}\\). Similarly, we can have  </p> \\[ \\{\\tilde{\\varphi}_{m_j}(x)\\} \\rightrightarrows \\varphi_2(x), \\quad \\forall x\\in I \\] <p>where \\(\\{\\tilde{\\varphi}_{m_j}(x)\\}\\) is a subsequence of \\(\\{\\varphi_{m_j}(x)\\}\\). According to the condition of the theorem, we have </p> \\[ \\varphi_1(x) = \\varphi_2(x), \\quad \\forall x \\in I \\] <p>while in \\(\\ref{con-non-convergent}\\) we also take the subsequence of \\(\\{\\varphi_{n_j}(x)\\}\\) and \\(\\{\\varphi_{m_j}(x)\\}\\)</p> \\[ |\\tilde{\\varphi}_{n_j}(\\overline{x})-\\tilde{\\varphi}_{m_j}(\\overline{x})|\\geq\\varepsilon_0 \\] <p>which means \\(\\varphi_1(x)\\) and \\(\\varphi_2(x)\\) must have distance on some \\(\\overline{x}\\), which contradicts with \\(\\varphi_1(x) = \\varphi_2(x)\\) on all \\(x\\in I\\)!</p>"},{"location":"courses/Ordinary_Differential_Equation/General_Theory/Extension_of_Solution/","title":"Extension of Solution","text":""},{"location":"courses/Ordinary_Differential_Equation/General_Theory/Extension_of_Solution/#extension-of-solution","title":"\u89e3\u7684\u5ef6\u62d3 | Extension of Solution","text":"<p>For Cauchy problem</p> \\[ \\begin{equation} \\frac{dy}{dx} = f(x, y), \\quad y(x_0) = y_0. \\label{eq-cauchy} \\end{equation} \\] <p>it is clear that we are satisfied with the result of intervals in the previous chapter, especially when we have to shrink the interval of solution using Contraction Mapping Method.</p> <p>So a naive idea is, can we use the Peano/Picard theorem repeatedly to extend the interval of parameter \\(t\\)? If so, in what cases can we extend it and to where can we extend?</p> <p>Here comes the following theorem.</p> <p>\u89e3\u7684\u5ef6\u62d3\u5b9a\u7406 | Theorem of extension of solution</p> <p>Assume \\(f(x,y)\\in C(G)\\), where \\(G\\) is an open set(region). Then the solution of Cauchy problem \\(\\ref{eq-cauchy}\\) can extend to its boundary. </p> HintsProof <p>The proof is equivalent to prove that, for each closed set \\(G_1\\subset G\\) and \\((x_0,y_0)\\in G_1\\), the solution \\(\\Gamma\\) can extend to \\(G\\) \\ \\(G_1\\).</p> <p>We only consider positive extension, i.e. \\(x\\geq x_0\\).</p> <p>Consider closed region \\(G_1\\subset G\\) that has point \\((x_0,y_0)\\) with the solution denoted by \\(\\varphi(x)\\) that satisfies initial condition \\(y(x_0)=y_0\\). </p> <ul> <li>Use the property of Open Set.</li> </ul> <p>Because \\(G\\) is an open set, the distance between \\(G\\) and \\(G_1\\) can not be zero, that is, \\(\\exists \\delta_0&gt;0\\)., s.t.</p> \\[ \\{(x,y): |x-a|&lt;\\delta_0, |y-b|&lt;\\delta_0, (a,b)\\in \\partial G_1\\} \\subset G \\] <p>Here \\(\\delta_0\\) is a constaint condition that guarantees the extended interval of solution will not exceed the boundary of \\(G_1\\).</p> <ul> <li>Using \\(\\delta_0\\) to generate extended intervals with length \\(\\delta_0'\\)</li> </ul> <p>For any closed region \\(G_1\\subset G\\), Denote </p> \\[ M=\\max_{(x,y)\\in G_1}|f(x_y)|+1 &lt; \\infty, \\quad R_{\\delta_0}(x',y')=\\{(x,y): |x-x'|\\leq \\delta_0, |y-y'|\\leq \\delta_0\\} \\] <p>where \\((x',y')\\in G_1\\).</p> <p>For Cauchy problem with initial condition \\(y(x_0)=y_0\\) in region \\(R_{\\delta_0} (x_0,y_0)\\), according to Peano Theorem, we can have a solution \\(\\varphi_0(x)\\) on interval \\([x_0-\\delta',x_0+\\delta']\\), where \\(\\delta'=\\min{\\delta_0, \\delta_0/M}\\). If there exists point on the curve \\((x,\\varphi_0(x)) \\in G\\)\\\\(G_1\\), then we prove it.</p> <p>If not, then the furthest point on the right \\((x_0+\\delta', \\varphi_0(x_0+\\delta'))\\) must be in \\(G_1\\). So consider cauchy problem with initial condition \\(y(x_0+\\delta') = \\varphi_0(x_0+\\delta')\\) in region \\(R_{\\delta_0} (x_0+\\delta',\\varphi_0(x_0+\\delta'))\\), according to Peano Theorem, we can get another solution \\(\\varphi_1(x)\\) on interval \\(x_0,x_0+2\\delta'\\).</p> <p>Repeat the above procedure, we can say the interval can be extended to \\([x_0, x_0+n\\delta']\\). If we denote the distance between \\((x_0,y_0)\\) and \\(\\partial G_1\\) as \\(D_1\\), distance between \\((x_0,y_0)\\) and \\(\\partial G\\) as \\(D\\), then choose \\(n\\) such that \\(n\\delta'&gt;D_1\\) and in the same time \\(n\\delta'&lt;D\\).</p> <p>And we are done.    </p>"},{"location":"courses/Ordinary_Differential_Equation/General_Theory/Power_Series/","title":"Method of Power Series","text":"<p>Lots of ODE cannot be sovled using elementary integration method, so we have to give up solution of finite form and try to find solution of infinite form, like series.</p> <p>To have a more global understanding, we focus on</p> \\[ \\begin{equation} \\frac{d \\pmb{y}}{dx}=\\pmb{f}(x,\\pmb{y}), \\quad \\pmb{y}(x_0)=\\pmb{y}_0 \\label{eq-cauchy} \\end{equation} \\] <p>where \\(\\pmb{f}(x,\\pmb{y})\\) is analytic on region \\(R\\subset \\mathbb{R}\\times \\mathbb{R}^n\\), i.e.</p> \\[ \\pmb{f}(x,\\pmb{y})=\\sum_{i,j_1,j_2,\\cdots,j_n=0}^\\infty \\pmb{a}_{i{j_1}{j_2}\\cdots{j_n}}(x-x_0)^i(y_1-y_{10})^{j_1}\\cdots(y_n-y_{n0})^{j_n} \\] <p>where \\(\\pmb{a}_{i{j_1}{j_2}\\cdots{j_n}}\\in \\mathbb{R}^n\\).</p> <p>To simplify the notation, we denote \\(\\pmb{j}=(j_1,j_2,\\cdots,j_n)\\), and </p> \\[ (\\pmb{y}-\\pmb{y}_0)^{\\pmb{j}}=(y_1-y_{10})^{j_1}\\cdots(y_n-y_{n0})^{j_n} \\] <p>denote </p> \\[ \\sum_{i=0,\\pmb{j}=0}^\\infty=\\sum_{i,j_1,j_2,\\cdots,j_n=0}^\\infty \\] <p>Since \\(\\pmb{f}(x,\\pmb{y})\\) is analytic with respect to \\(\\pmb{y}\\), so by Picard Theorem, there exists only one solution. Now the question is, to prove that the solution is analytic, i.e. \\(\\exists \\delta&gt;0\\), s.t. \\(x\\in O_\\delta(x_0)\\)</p> \\[ \\pmb{y}(x)=\\sum_{k=0}^{\\infty}\\pmb{c}_k(x-x_0)^k \\] <p>where \\(\\pmb{c}_k=(c_{n1},c_{n2},\\cdots,c_{nk}) \\in \\mathbb{R}^n\\).</p>"},{"location":"courses/Ordinary_Differential_Equation/General_Theory/Power_Series/#excellent-series","title":"Excellent Series","text":"<p>To prove that the solution is analytic, we have to use a method of proof, that is, using Excellent Series.</p> <p>Definition of Excellent Series</p> <p>Assume there are two power series</p> \\[ \\begin{equation} \\sum_{i=0.\\pmb{j}=0}^\\infty a_{i\\pmb{j}}(x-x_0)^i(\\pmb{y}-\\pmb{y}_0)^{\\pmb{j}} \\label{series1} \\end{equation} \\] <p>and</p> \\[ \\begin{equation} \\sum_{i=0.\\pmb{j}=0}^\\infty A_{i\\pmb{j}}(x-x_0)^i(\\pmb{y}-\\pmb{y}_0)^{\\pmb{j}}. \\label{series2} \\end{equation} \\] <p>If \\(A_{i\\pmb{j}}&gt;0\\) and they satisfies</p> \\[ |a_{i\\pmb{j}}|&lt;A_{i\\pmb{j}}, \\quad \\forall i,\\pmb{j} \\] <p>Then we call series \\(\\ref{series2}\\) is an excellent series of series \\(\\ref{series1}\\). If series \\(\\ref{series2}\\) converges on closed region</p> \\[ \\{(x,y): |x-x_0|\\leq \\alpha,|\\pmb{y}-\\pmb{y}_0|\\leq\\beta\\} \\] <p>then we call its summing function \\(\\pmb{f}(x,\\pmb{y})\\) is an Excellent function of series \\(\\ref{series1}\\).(note that in this case, series \\(\\ref{series1}\\) also converges)</p> <p></p> <p>\u5f15\u74061: \u89e3\u6790\u51fd\u6570\u6709\u5b9a\u4e49\u57df\u6536\u7f29\u7684\u4f18\u51fd\u6570 | Lemma 1: A analytic function has an excellent function within smaller region</p> <p>If \\(f(x,\\pmb{y})\\) is analytic on region</p> \\[ R: \\{(x,y): |x-x_0|&lt;\\alpha, |\\pmb{y}-\\pmb{y}_0|&lt;\\beta\\} \\] <p>then \\(\\exists M&gt;0\\), s.t.</p> \\[ F(x,\\pmb{y})=\\frac{M}{\\left(1-\\frac{x-x_0}{a}\\right)\\left(1-\\frac{y_1-y_{10}}{b}\\right)\\cdots\\left(1-\\frac{y_n-y_{n0}}{b}\\right)} \\] <p>is an Excellent function of \\(f(x,\\pmb{y})\\) on a smaller region </p> \\[ R_0:\\{(x,y): |x-x_0|&lt;a,|\\pmb{y}-\\pmb{y}_0|&lt;b\\}. \\] HintsProof <p>Use Abel Second Theorem. Note that we can not guarrantee the convergence on boundaries, so we choose open intervals.</p> <p>We can represent \\(f(x,\\pmb{y})\\) in terms of power series</p> \\[ f(x,\\pmb{y})=\\sum_{i=0.\\pmb{j}=0}^\\infty a_{i\\pmb{j}}(x-x_0)^i(\\pmb{y}-\\pmb{y}_0)^{\\pmb{j}} \\] <p>in region \\(R\\). Then by Abel Second Theorem, \\(\\exists a\\in(0,\\alpha), b\\in(0,\\beta)\\) s.t.</p> \\[ \\sum_{i=0.\\pmb{j}=0}^\\infty a_{i\\pmb{j}}a^i b^{j_1+j_2+\\cdots+j_n} \\] <p>is convergent, so each item of the above series can be bounded by a number \\(M&gt;0\\):</p> \\[ |a_{i\\pmb{j}}|a^i b^{j_1+j_2+\\cdots+j_n}\\leq M \\Rightarrow |a_{i\\pmb{j}}|\\leq \\frac{M}{a^i b^{j_1+j_2+\\cdots+j_n}} \\] <p>Now, the following thing is a little tricky. Define </p> \\[ A_{i\\pmb{j}}=\\frac{M}{a^i b^{j_1+j_2+\\cdots+j_n}} \\] <p>Consider a power of series </p> \\[ \\begin{equation} \\sum_{i=0.\\pmb{j}=0}^\\infty A_{i\\pmb{j}}(x-x_0)^i(\\pmb{y}-\\pmb{y}_0)^{\\pmb{j}} \\label{series-exce} \\end{equation} \\] <p>which is convergent because it can add up to:</p> \\[ F(x,\\pmb{y})=\\frac{M}{\\left(1-\\frac{x-x_0}{a}\\right)\\left(1-\\frac{y_1-y_{10}}{b}\\right)\\cdots\\left(1-\\frac{y_n-y_{n0}}{b}\\right)} \\] <p>with its range of definition \\(R_0\\). By definition, it is an excellent function of \\(f(x,\\pmb{y})\\).</p> <p>With the definition of Excellent function, we have to use it to formulate an excellent series of the original series. This is the following theorem.</p> <p></p> <p>\u5f15\u74062: \u7528\u4e0a\u8ff0\u4f18\u51fd\u6570\u5efa\u7acb\u7684\u5fae\u5206\u65b9\u7a0b\u6709\u89e3\u6790\u89e3 | Lemma2: ODE combined with The above Excellent Function has a solution that can be represented by Power Series</p> <p>Cauchy problem </p> \\[ \\frac{d \\pmb{y}}{dx}=\\pmb{F}(x,\\pmb{y}), \\quad \\pmb{y}(x_0)=\\pmb{y}_0 \\label{eq-cauchy-prime} \\] <p>has a analytic solution \\(\\pmb{y}=\\pmb{y}(x)\\) on region \\(O_\\rho(x_0)\\), where \\(F_i(x,\\pmb{y}) = F(x,\\pmb{y})\\) that is same all over \\(i\\) is given from the above lemma and </p> \\[ \\rho=a\\{1-e^{b/[(n+1)aM]}\\}. \\] HintsProof <p>use elementary integration method.</p> <p>We let \\(u=y_i\\), \\(i=1,2,\\cdots,n\\) and we only need to solve the equation </p> \\[ \\frac{d u}{dx}=F(x,u), \\quad u(x_0)=u_0 \\] <p>where </p> \\[ F(x,u) = \\frac{M}{\\left(1-\\frac{x-x_0}{a}\\right)\\left(1-\\frac{u-u_{0}}{b}\\right)^n} \\] <p>(Here readers can see that \\(u-u_0=y_i-y_{i0}\\).)</p> <p>The above ODE is a variable separation equation. So change the form and integrate on \\([x_0,x]\\)</p> \\[ \\frac{-b}{n+1}\\left(1-\\frac{u-u_0}{b}\\right)^{n+1} +\\frac{b}{n+1}= -aM\\ln\\left(1-\\frac{x-x_0}{a}\\right) \\] <p>get \\(u\\) out:</p> \\[ u = u_0 + b- b\\left[\\frac{aM(n+1)}{b} \\ln\\left(1-\\frac{x-x_0}{a}\\right) + 1\\right]^{\\frac{1}{n+1}} \\] <p>That is,</p> \\[ y_i(x) = y_{i0} + b- b\\left[\\frac{aM(n+1)}{b} \\ln\\left(1-\\frac{x-x_0}{a}\\right) + 1\\right]^{\\frac{1}{n+1}}, \\quad \\forall i=1,2\\cdots,n \\] <p>We want to use this form to get a power series. See that \\(\\ln{\\left(1-\\frac{x-x_0}{a}\\right)}\\) can be represented by power series of \\((x-x_0)\\) once \\(|x-x_0|&lt;a\\). And also \\((1+s)^{\\frac{1}{n+1}}\\) can be represented by power series of \\(s\\) when \\(|s|&lt;1\\). So by combine the above two, we know \\(y_i(x)\\) can be represented by \\((x-x_0)\\) once \\(|x-x_0|&lt;a\\). To be more specific, We have to let the radius of converence to satisfy</p> \\[ \\begin{cases} \\displaystyle 1-\\frac{\\rho}{a} \\geq 0 \\\\ \\displaystyle \\frac{aM(n+1)}{b}\\ln\\left(1-\\frac{\\rho}{a}\\right)\\leq 1  \\end{cases} \\Rightarrow \\begin{cases} \\rho\\leq a \\\\ \\rho\\leq a\\{1-e^{b/[(n+1)aM]}\\} \\end{cases} \\] <p>choose</p> \\[ \\rho=a\\{1-e^{b/[(n+1)aM]}\\} \\] <p>So solution of the above ODE</p> \\[ \\pmb{y}(x)=(y_1(x),y_2(x),\\cdots, y_n(x)) \\] <p>can be represented by power series of \\((x-x_0)\\) when \\(|x-x_0|&lt;\\rho\\).</p>"},{"location":"courses/Ordinary_Differential_Equation/General_Theory/Power_Series/#proof","title":"\u8bc1\u660e | Proof","text":"<p>Cauchy \u5b9a\u7406 | Cauchy Theorem</p> <p>Assume \\(\\pmb{f}(x,\\pmb{y})=[f_1(x,\\pmb{y}), f_2(x,\\pmb{y}),\\cdots, f_n(x,\\pmb{y})]\\) is an analytic function on region \\(R\\). So problem \\(\\ref{eq-cauchy}\\) has a unique analytic solution \\(\\pmb{y} = \\pmb{y}(x)\\) on \\(O_\\rho(x)\\), where \\(\\rho\\) is given in Lemma 2.</p> HintsProof <ul> <li> <p>Represent solution with power series. Show that it is unique.</p> </li> <li> <p>Use an excellent series to prove the above power series convergent.</p> </li> </ul> <ul> <li>Represent \\(f_k(x,\\pmb{y})\\) with power series</li> </ul> \\[ \\begin{equation} f_k(x,\\pmb{y})=\\sum_{i=0,\\pmb{j}=0}^\\infty a_{i\\pmb{j}}^k (x-x_0)^i(\\pmb{y}-\\pmb{y}_0)^{\\pmb{j}} \\label{eq-f} \\end{equation} \\] <p>And represent solution with power series</p> \\[ \\begin{equation} y_k(x) = y_{k0} + \\sum_{i=1}^\\infty c_i^k(x-x_0)^i,\\quad k=1,2,\\cdots,n \\label{eq-y} \\end{equation} \\] <p>substitute \\(\\ref{eq-f}\\) and \\(\\ref{eq-y}\\) into ODE</p> \\[ \\frac{d y_k}{dx} = f_k(x,\\pmb{y}),\\quad k=1,2\\cdots,n \\] <p>and get</p> \\[ \\begin{align*} \\sum_{i=0}^\\infty (i +1) c_{i+1}^k(x-x_0)^i = \\sum_{i,j_1,j_2,\\cdots,j_n=0}^\\infty \\Bigg\\{ &amp; a^k_{i{j_1}{j_2}\\cdots{j_n}}(x-x_0)^i \\times \\\\  &amp;\\left[\\sum_{i=1}^\\infty c_i^1(x-x_0)^i\\right]^{j_1} \\times \\\\ &amp;\\left[\\sum_{i=1}^\\infty c_i^2(x-x_0)^i \\right]^{j_2} \\times \\\\ &amp;\\cdots \\\\ &amp; \\left[\\sum_{i=1}^\\infty c_i^n(x-x_0)^i \\right]^{j_n}\\Bigg\\},\\quad k=1,2\\cdots,n. \\end{align*} \\] <p>Denote \\(X = x-x_0\\), and we have</p> \\[ \\sum_{i=0}^\\infty (i +1) c_{i+1}^kX^i = \\sum_{i,j_1,j_2,\\cdots,j_n=0}^\\infty a^k_{i{j_1}{j_2}\\cdots{j_n}}X^i \\left(\\sum_{i=1}^\\infty c_i^1X^i\\right)^{j_1} \\left(\\sum_{i=1}^\\infty c_i^2X^i\\right)^{j_2} \\cdots \\left(\\sum_{i=1}^\\infty c_i^nX^i\\right)^{j_n} \\] <p>and get \\(c_i^k\\) out in terms of \\(a^k_{i\\pmb{j}}\\)</p> \\[ \\begin{align*} c_1^k &amp;= a^k_{00\\cdots 0}\\\\ c_2^k &amp;= \\frac{1}{2!} (a^k_{10\\cdots 0}+a^k_{010\\cdots 0}c^1_1 + a^k_{0010\\cdots 0}c^2_1+\\cdots a^k_{00\\cdots01}c^n_1) \\\\ &amp;= \\frac{1}{2!} (a^k_{10\\cdots 0} + a^k_{010\\cdots 0}a^1_{00\\cdots 0} + a^k_{0010\\cdots 0}a^2_{00\\cdots 0}+\\cdots a^k_{00\\cdots01}a^n_{00\\cdots 0}) \\end{align*} \\] <p>Generally, we have </p> \\[ c^k_m=P_m^k(a^l_{00\\cdots 0}, a^l_{01\\cdots 0},\\cdots,a^l_{i{j_1}\\cdots{j_n}}) \\] <p>where \\(i+j_1+j_2+\\cdots+j_n\\leq m-1\\), \\(1\\leq l\\leq n\\). Thus, \\(P_m^k\\) is a polynomial represented by \\(a^l_{00\\cdots 0}\\), \\(a^l_{01\\cdots 0}\\), \\(\\cdots\\), \\(a^l_{i{j_1}\\cdots{j_n}}\\) with positve operator \"+\". Theoretically, we can represent the solution by definite power series.</p> <p>We leave the proof of this part as an additional work in Appendix at the end of the doc.</p> <ul> <li>Prove the above series converges.</li> </ul> <p>Here we formulate another ODE and use Excellent function to bound the above power series.</p> <p>Since \\(f_k(x,\\pmb{y})\\) is analytic on region \\(R\\), by Lemma 1, there exists an excellent function of \\(f_k(x,\\pmb{y})\\) on a smaller region \\(R_0\\):</p> \\[ F_k(x,\\pmb{y}) = \\frac{M}{\\left(1-\\frac{x-x_0}{a}\\right)\\left(1-\\frac{y_1-y_{10}}{b}\\right)\\cdots\\left(1-\\frac{y_n-y_{n0}}{b}\\right)}, \\quad k=1,2\\cdots n \\] <p>If we represent both of them in terms of power series</p> \\[ \\begin{align*} f_k(x,\\pmb{y})&amp;=\\sum_{i=0.\\pmb{j}=0}^\\infty a_{i\\pmb{j}}(x-x_0)^i(\\pmb{y}-\\pmb{y}_0)^{\\pmb{j}} \\\\ F_k(x,\\pmb{y})&amp;=\\sum_{i=0.\\pmb{j}=0}^\\infty A_{i\\pmb{j}}(x-x_0)^i(\\pmb{y}-\\pmb{y}_0)^{\\pmb{j}} \\end{align*} \\] <p>then we have a relation \\(|a_{i\\pmb{j}}|&lt;A_{i\\pmb{j}}\\), which matters in the following proof.</p> <p>Now consider an ODE</p> \\[ \\frac{d y_k}{dx} = F_k(x,\\pmb{y}),\\quad y_k(x_0)=y_k,\\quad  k=1,2,\\cdots,n,\\quad   \\] <p>by Lemma 2, the above ODE has an analytic solution \\(\\pmb{y}=\\pmb{y}(x)\\), represented by power series</p> \\[ y_k(x) = y_{k0} + \\sum_{i=1}^\\infty C_i^k(x-x_0)^i,\\quad k=1,2,\\cdots,n \\] <p>Similarly, we have </p> \\[ \\begin{align*} C^k_m&amp;=P_m^k(A^l_{00\\cdots 0}, A^l_{01\\cdots 0},\\cdots,A^l_{i{j_1}\\cdots{j_n}})\\\\ &amp;=P_m^k(|A^l_{00\\cdots 0}|, |A^l_{01\\cdots 0}|,\\cdots,|A^l_{i{j_1}\\cdots{j_n}}|)\\\\ &amp;\\geq P_m^k(|a^l_{00\\cdots 0}|, |a^l_{01\\cdots 0}|,\\cdots,|a^l_{i{j_1}\\cdots{j_n}}|)\\\\ &amp;\\geq |c_m^k| \\end{align*} \\] <p>So power series \\(\\sum\\limits_{i=1}^\\infty C_i^k(x-x_0)^i\\) is en excellent series of \\(\\sum\\limits_{i=1}^\\infty c_i^k(x-x_0)^i\\). Since the former is convergent by Lemma 2, so the latter also converges.</p>"},{"location":"courses/Ordinary_Differential_Equation/General_Theory/Power_Series/#Appendix","title":"\u9644\u5f55 | Appendix: Relation between \\(c\\) and \\(a\\)","text":"<p> Theorem. Prove  \\[ c^k_m=P_m^k(a^l_{00\\cdots 0}, a^l_{01\\cdots 0},\\cdots,a^l_{i{j_1}\\cdots{j_n}}) \\] <p>where \\(i+j_1+j_2+\\cdots+j_n\\leq m-1\\), \\(1\\leq l\\leq n\\). Thus, \\(P_m^k\\) is a polynomial represented by \\(a^l_{00\\cdots 0}\\), \\(a^l_{01\\cdots 0}\\), \\(\\cdots\\), \\(a^l_{i{j_1}\\cdots{j_n}}\\) with positve operator \"+\".  </p> HintsProof <p>Use induction.</p>"},{"location":"courses/Sensing%26Detection/Midterm/","title":"Midterm Exam","text":""},{"location":"courses/Sensing%26Detection/Midterm/#_1","title":"\u7b80\u7b54\u9898","text":""},{"location":"courses/Sensing%26Detection/Midterm/#_2","title":"\u201c\u5dee\u52a8\u201d\u548c\u201c\u53c2\u6bd4\u201d\u8bbe\u8ba1\u65b9\u6cd5","text":"\u7b80\u8981\u6bd4\u8f83\u4e00\u4e0b\u201c\u5dee\u52a8\u5f0f\u201d\u548c\u201c\u53c2\u6bd4\u5f0f\u201d\u4e24\u79cd\u68c0\u6d4b\u4eea\u8868\u8bbe\u8ba1\u65b9\u6cd5\u7684\u5f02\u540c\u70b9\u3002  <p>\u5f02\uff1a</p> <ul> <li>\u5dee\u52a8\u5f0f\u662f\u91c7\u7528\u4e24\u4e2a\u8f6c\u6362\u5143\u4ef6\u540c\u65f6\u611f\u53d7\u654f\u611f\u5143\u4ef6\u7684\u8f93\u51fa\u91cf\uff0c\u5e76\u628a\u5b83\u8f6c\u6362\u6210\u4e24\u4e2a\u6027\u8d28\u76f8\u540c\uff0c\u4f46\u6cbf\u76f8\u53cd\u65b9\u5411\u53d8\u5316\u7684\u7269\u7406\u91cf\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u4f7f\u5f97\u6709\u6548\u8f93\u51fa\u4fe1\u53f7\u63d0\u9ad8\u4e00\u500d\uff0c\u4fe1\u566a\u6bd4\u5f97\u5230\u6539\u5584\uff0c\u975e\u7ebf\u6027\u8bef\u5dee\u51cf\u5c0f\uff1b\u6613\u4e8e\u5b9e\u73b0\u521d\u59cb\u72b6\u6001\uff08\u201c\u96f6\u201d\u8f93\u5165\uff09\u7684\u96f6\u8f93\u51fa\uff0c\u80fd\u6d88\u9664\u90e8\u5206\u73af\u5883\u56e0\u7d20\u7684\u5f71\u54cd\u3002</li> <li>\u53c2\u6bd4\u5f0f\u662f\u91c7\u7528\u4e24\u4e2a\u6027\u80fd\u5b8c\u5168\u76f8\u540c\u7684\u68c0\u6d4b\u5143\u4ef6\uff0c\u4ed6\u4eec\u540c\u65f6\u611f\u53d7\u73af\u5883\u6761\u4ef6\u91cf\uff0c\u4f46\u53ea\u6709\u4e00\u4e2a\u611f\u53d7\u88ab\u6d4b\u91cf\u3002\u5176\u4f5c\u7528\u662f\u5c06\u540c\u65f6\u4f5c\u7528\u5728\u4e24\u4e2a\u68c0\u6d4b\u5143\u4ef6\u4e0a\u7684\u73af\u5883\u6761\u4ef6\u91cf\u7684\u5e72\u6270\u4fe1\u606f\u9664\u53bb\uff0c\u5bf9\u88ab\u6d4b\u91cf\u4fe1\u606f\u8fdb\u884c\u653e\u5927\u3002\u53c2\u6bd4\u53ef\u4ee5\u8f83\u597d\u5730\u6d88\u9664\u5e72\u6270\u6765\u6e90\u660e\u786e\u5730\u73af\u5883\u6761\u4ef6\u91cf\u7684\u5f71\u54cd\u3002</li> </ul> <p>\u540c\uff1a</p> <ul> <li>\u90fd\u80fd\u4e00\u5b9a\u7a0b\u5ea6\u514b\u670d\u73af\u5883\u5e72\u6270\u3002</li> </ul>"},{"location":"courses/Sensing%26Detection/Midterm/#_3","title":"\u538b\u963b\u5f0f\u3001\u538b\u7535\u5f0f\u3001\u538b\u78c1\u5f0f","text":"\u4ece\u5e94\u7528\u89d2\u5ea6\u8ba8\u8bba\u5e76\u5206\u6790\u538b\u963b\u5f0f\u3001\u538b\u7535\u5f0f\u548c\u538b\u78c1\u5f0f\u68c0\u6d4b\u5143\u4ef6\u5404\u6709\u4ec0\u4e48\u7279\u70b9\uff1f  <ul> <li>\u538b\u963b\u5f0f</li> </ul> <p>\u6d4b\u91cf\u8303\u56f4\u5bbd\u3001\u51c6\u786e\u5ea6\u9ad8\uff0c\u54cd\u5e94\u901f\u5ea6\u5feb\uff0c\u9002\u5408\u9759\u6001\u548c\u52a8\u6001\u6d4b\u91cf\uff0c\u4f7f\u7528\u5bff\u547d\u957f\uff0c\u6027\u80fd\u7a33\u5b9a\uff0c\u4ef7\u683c\u4fbf\u5b9c\uff0c\u53ef\u4ee5\u5728\u9ad8\u5f3a\u5ea6\u6076\u52a3\u73af\u5883\u4e0b\u5de5\u4f5c\u3002</p> <p>\u4f46\u6709\u8f93\u51fa\u4fe1\u53f7\u5fae\u5f31\uff0c\u6297\u5e72\u6270\u80fd\u529b\u5dee\uff0c\u6613\u53d7\u6e29\u5ea6\u7b49\u73af\u5883\u56e0\u7d20\u7684\u5f71\u54cd\uff0c\u5927\u5e94\u53d8\u72b6\u6001\u4e0b\u6709\u8f83\u5927\u7684\u975e\u7ebf\u6027\u3002</p> <p>\u5e94\u7528\uff1a\u88ab\u7c98\u8d34\u5728\u5404\u79cd\u5f39\u6027\u5143\u4ef6\u4e0a\uff0c\u4ee5\u611f\u53d7\u538b\u529b\u53d8\u5316\u3002</p> <ul> <li>\u538b\u7535\u5f0f</li> </ul> <p>\u5177\u6709\u9891\u5e26\u5bbd\u3001\u7075\u654f\u5ea6\u9ad8\u3001\u7ed3\u6784\u7b80\u5355\u3001\u5de5\u4f5c\u53ef\u9760\u3001\u91cd\u91cf\u8f7b\u7b49\u4f18\u70b9\u3002</p> <p>\u4f46\u53ea\u9002\u5408\u52a8\u6001\u6d4b\u91cf\u3002</p> <p>\u5e94\u7528\uff1a\u53ef\u4ee5\u5b9e\u73b0\u529b\u3001\u538b\u529b\u3001\u52a0\u901f\u5ea6\u548c\u626d\u77e9\u7b49\u7269\u7406\u91cf\u7684\u6d4b\u91cf\u3002</p> <ul> <li>\u538b\u78c1\u5f0f</li> </ul> <p>\u8f93\u51fa\u529f\u7387\u5927\uff0c\u6297\u5e72\u6270\u80fd\u529b\u53ca\u8fc7\u8f7d\u80fd\u529b\u5f3a\uff0c\u4fbf\u4e8e\u5236\u9020\uff0c\u7ecf\u6d4e\u5b9e\u7528\uff0c\u5e76\u80fd\u5728\u6076\u52a3\u7684\u6761\u4ef6\u4e0b\u957f\u671f\u4f7f\u7528\u3002</p> <p>\u4f46\u6d4b\u91cf\u7cbe\u5ea6\u4e0d\u9ad8\uff0c\u53cd\u5e94\u901f\u5ea6\u8f83\u6162\u3002</p> <p>\u5e94\u7528\uff1a\u4e3b\u8981\u5e94\u7528\u4e8e\u6d4b\u529b\u3001\u79f0\u91cd\u3001\u6e29\u5ea6\u6d4b\u91cf\u53ca\u76c8\u5229\u65e0\u635f\u68c0\u6d4b\u7b49\u65b9\u9762\u3002</p>"},{"location":"courses/Sensing%26Detection/Midterm/#_4","title":"\u51cf\u5c11\u968f\u673a\u8bef\u5dee\u3001\u975e\u7ebf\u6027\u8865\u507f","text":"\u68c0\u6d4b\u4eea\u8868\u5e38\u7528\u7684\u51cf\u5c11\u968f\u673a\u8bef\u5dee\u548c\u8fdb\u884c\u975e\u7ebf\u6027\u8865\u507f\u7684\u65b9\u6cd5\u4e3b\u8981\u6709\u54ea\u4e9b\uff1f  <ul> <li>\u51cf\u5c11\u968f\u673a\u8bef\u5dee\u7684\u65b9\u6cd5\u3002</li> </ul> <p>\u63d0\u9ad8\u68c0\u6d4b\u7cfb\u7edf\u51c6\u786e\u5ea6\uff0c\u5bf9\u6d4b\u91cf\u7ed3\u679c\u8fdb\u884c\u7edf\u8ba1\u5904\u7406\uff0c\u6291\u5236\u566a\u58f0\u5e72\u6270\u3002</p> <ul> <li>\u8fdb\u884c\u975e\u7ebf\u6027\u8865\u507f\u3002</li> </ul> <p>\u76f4\u63a5\u4e32\u8054\u6cd5\uff0c\u975e\u7ebf\u6027\u8d1f\u53cd\u9988\u6cd5\uff0c\u8f6f\u4ef6\u7ebf\u6027\u5316\u6cd5</p>"},{"location":"courses/Sensing%26Detection/Midterm/#_5","title":"\u5f00\u73af\u3001\u95ed\u73af\u7ed3\u6784\u4eea\u8868","text":"\u6bd4\u8f83\u5206\u6790\u4e00\u4e0b\u5f00\u73af\u7ed3\u6784\u548c\u95ed\u73af\u7ed3\u6784\u4eea\u8868\u5404\u81ea\u7684\u7279\u70b9\u3002(\u53cb\u60c5\u63d0\u9192:\u7b2c\u4e00\u7ae0\u548c\u7b2c\u4e09\u7ae0\u7684\u76f8\u5173\u5185\u5bb9\u8981\u4e00\u8d77\u8003\u8651\uff0c\u5e76\u4ece\u81ea\u52a8\u63a7\u5236\u539f\u7406\u7684\u89d2\u5ea6\u6765\u8fdb\u884c\u5206\u6790\u3002)  <ul> <li>\u5f00\u73af\u7ed3\u6784\u4eea\u8868</li> </ul> <p>\u7531\u82e5\u5e72\u4e2a\u73af\u8282\u4e32\u8054\u7ec4\u6210\uff0c\u4eea\u8868\u7684\u4fe1\u606f\u548c\u53d8\u6362\u53ea\u6cbf\u4e00\u4e2a\u65b9\u5411\u4f20\u9012\u3002\u5176\u603b\u7684\u4f20\u9012\u51fd\u6570\u4e3a\u5404\u73af\u8282\u4f20\u9012\u51fd\u6570\u4e4b\u79ef\uff0c\u6574\u53f0\u4eea\u8868\u7684\u76f8\u5bf9\u8bef\u5dee\u4e3a\u5404\u4e2a\u73af\u8282\u7684\u76f8\u5bf9\u8bef\u5dee\u4e4b\u548c\u3002</p> <p>\u603b\u4f53\u6765\u8bf4\u7ed3\u6784\u7b80\u5355\u3002\u5f53\u7ec4\u6210\u4eea\u8868\u7684\u73af\u8282\u8f83\u591a\u65f6\uff0c\u51c6\u786e\u5ea6\u8f83\u4f4e\u3002</p> <ul> <li>\u95ed\u73af\u7ed3\u6784\u4eea\u8868</li> </ul> <p>\u7531\u6b63\u5411\u901a\u9053\u548c\u53cd\u9988\u901a\u9053\u7ec4\u6210\u3002\u5bf9\u4e8e\u4e00\u9636\u73af\u8282\\(G = \\frac{k}{1+Ts}\\)\uff0c\u82e5\u6709\u53cd\u9988\u589e\u76ca\\(\\beta\\)\uff0c\u5176\u653e\u5927\u500d\u6570\u548c\u65f6\u95f4\u5e38\u6570\u90fd\u662f\u5f00\u73af\u7ed3\u6784\u4eea\u8868\u7684\\(1/(1+k\\beta)\\)\u3002\u82e5\\(k\\beta \\rightarrow \\infty\\)\uff0c\u5219\\(G' = K_0/\\beta\\)\uff0c\u5373\u4eea\u8868\u7279\u6027\u4e3b\u8981\u53d6\u51b3\u4e8e\u53cd\u9988\u901a\u9053\u7279\u6027\u3002</p> <p>\u603b\u4f53\u7ed3\u6784\u4f1a\u590d\u6742\u4e00\u4e9b\uff0c\u7a33\u5b9a\u6027\u4f1a\u8f83\u5dee\u3002\u4f46\u662f\u53cd\u5e94\u901f\u5ea6\u5feb\uff0c\u7ebf\u6027\u597d\uff0c\u51c6\u786e\u5ea6\u9ad8\u3002</p>"},{"location":"courses/Sensing%26Detection/Midterm/#_6","title":"\u6807\u51c6\u5dee\u4e0e\u7cfb\u7edf\u3001\u7c97\u5927\u8bef\u5dee","text":"\u8bf7\u5217\u51fa\u6d4b\u91cf\u4fe1\u53f7\u5747\u503c\u548c\u6807\u51c6\u5dee\u7684\u5b9a\u4e49\u516c\u5f0f\uff0c\u5e76\u7b80\u8981\u8bba\u8ff0\u4e00\u4e0b\u8be5\u4e24\u4e2a\u91cd\u8981\u7edf\u8ba1\u91cf\u5728\u7cfb\u7edf\u8bef\u5dee\u548c\u7c97\u5927\u8bef\u5dee\u5224\u522b\u4e2d\u7684\u4f5c\u7528\u3002  <ul> <li>\u5747\u503c</li> </ul> \\[ \\overline{x}=\\sum\\limits_{i=1}^{n}x_i \\] <ul> <li>\u6807\u51c6\u5dee</li> </ul> <p>\u5355\u6b21\u6d4b\u91cf\u503c\u7684\u6807\u51c6\u5dee(\u8d1d\u585e\u5c14Bessel\u516c\u5f0f)</p> \\[ \\sigma_B=s(x) = \\sqrt{\\frac{\\sum\\limits_{i=1}^{n}(x_i - \\overline{x})^2}{n-1}} \\] <p>\u7b97\u672f\u5e73\u5747\u503c\\(\\overline{x}\\)\u7684\u6807\u51c6\u5dee</p> \\[ s(\\overline{x}) = \\frac{s(x)}{\\sqrt{n}} \\] <p>\u53ef\u4ee5\u7528\u6807\u51c6\u5dee\u5224\u636e\uff0c\u770b\u4e24\u4e2a\u4f30\u8ba1\u6807\u51c6\u5dee\u662f\u5426\u6e10\u8fdb\u76f8\u7b49\u3002\u82e5\u5b58\u5728\u7cfb\u7edf\u8bef\u5dee\uff0c\u5219\u4e24\u8005\u76f8\u5dee\u5f88\u5927\u3002</p> <p>\\(\\sigma\\)\u6cd5\u53ef\u4ee5\u5254\u9664\u7c97\u5927\u8bef\u5dee\u3002\u5f53\u6d4b\u91cf\u503c\\(x_i\\)\u6ee1\u8db3\u62c9\u4f0a\u8fbe\u6cd5\u6216\u683c\u62c9\u5e03\u65af\u6cd5\u7684\u6761\u4ef6\u65f6\uff0c\u53ef\u4ee5\u8ba4\u5b9a\u4e3a\u7c97\u5927\u8bef\u5dee\uff08\u5982\\(3\\sigma\\)\uff09\u3002</p>"},{"location":"courses/Sensing%26Detection/Midterm/#_7","title":"\u6700\u5927\u7edd\u5bf9\u8bef\u5dee\u3001\u6807\u51c6\u5dee\u7684\u4f30\u8ba1","text":"\u73b0\u6709\u4e00\u53f0\u6e29\u5ea6\u4f20\u611f\u5668\uff0c\u91cf\u7a0b\u4e3a0~100\u00b0C\uff0c\u5176\u51c6\u786e\u5ea6(\u7cbe\u5ea6)\u7b49\u7ea7\u4e3a1.0\u3002\u8bf7\u95ee\u8be5\u4f20\u611f\u5668\u5728\u91cf\u7a0b\u8303\u56f4\u5185\u53ef\u80fd\u51fa\u73b0\u7684\u6700\u5927\u7edd\u5bf9\u8bef\u5dee\u7684\u5408\u7406\u4f30\u8ba1\u503c\u548c\u53ef\u80fd\u51fa\u73b0\u7684\u6700\u5927\u6807\u51c6\u5dee\u7684\u5408\u7406\u4f30\u8ba1\u503c\u5206\u522b\u4e3a\u591a\u5c11?\u4e3a\u4ec0\u4e48?(\u53cb\u60c5\u63d0\u9192:\u4ed4\u7ec6\u770b\u4e00\u4e0b\u6559\u6750P13\u9875)  <ul> <li>\u6700\u5927\u7edd\u5bf9\u8bef\u5dee</li> </ul> \\[ e = 100 \\times 1\\% = 1 \u00b0C \\] <ul> <li>\u6700\u5927\u6807\u51c6\u5dee</li> </ul> \\[ 3\\sigma = 1 \u00b0C\\Rightarrow \\sigma = 1/3\u00b0C \\]"},{"location":"courses/Sensing%26Detection/Midterm/#_8","title":"\u8ba1\u7b97\u9898","text":"<p>\u4e3a\u4e86\u4fdd\u8bc1\u6d4b\u91cf\u51c6\u786e\u5ea6\uff0c\u5728\u538b\u529b\u68c0\u6d4b\u8868\u9009\u578b\u65f6\uff0c\u4e00\u822c\u8981\u6c42\u6700\u5927\u5de5\u4f5c\u538b\u529b\u4e0d\u5e94\u8d85\u8fc7\u4eea\u8868\u6ee1\u91cf\u7a0b\u7684\\(3/4\\)\uff0c\u6700\u5c0f\u5de5\u4f5c\u538b\u529b\u4e0d\u5e94\u4f4e\u4e8e\u6ee1\u91cf\u7a0b\u7684\\(1/3\\)\u3002\u76ee\u524d\u6211\u56fd\u51fa\u5382\u7684\u538b\u529b(\u5305\u62ec\u5dee\u538b)\u68c0\u6d4b\u4eea\u8868\u6709\u7edf\u4e00\u7684\u91cf\u7a0b\u7cfb\u5217\uff0c\u5b83\u4eec\u662f\\(1\\)\u3001\\(1.6\\)\u3001\\(2.5\\)\u3001\\(4.0\\)\u3001\\(6.0kPa\\) \u4ee5\u53ca\u5b83\u4eec\u7684\\(10^n\\)\u500d\u6570(\\(n\\)\u4e3a\u6574\u6570)\u3002</p> <p>\u67d0\u538b\u529b\u5bb9\u5668\u6b63\u5e38\u5de5\u4f5c\u65f6\u538b\u529b\u8303\u56f4\u4e3a\\(1.0~1.5MPa\\),\u8981\u6c42\u6d4b\u91cf\u8bef\u5dee\u4e0d\u5927\u4e8e\u88ab\u6d4b\u538b\u529b\u76845%,\u8bd5\u786e\u5b9a\u8be5\u8868\u7684\u91cf\u7a0b\u548c\u51c6\u786e\u5ea6\u7b49\u7ea7\u3002</p> <ul> <li>\u89e3\uff1a</li> </ul> <p>\u8bbe\u91cf\u7a0b\u4e3a\\(A (MPa)\\)\uff0c\u5219\u6ee1\u8db3</p> \\[ \\begin{cases} \\frac{1}{3}A \\leq 1.0 \\\\ 1.5 \\leq \\frac{3}{4}A \\end{cases} \\] <p>\u89e3\u5f97\\(2.0\\leq A \\leq 3.0\\)\uff0c\u6545\u53d6\\(A=2.5 MPa\\)</p> <p>\u7531\u9898\u610f\uff0c\u4eea\u8868\u57fa\u672c\u8bef\u5dee\\(e \\leq 5\\% P\\)\uff0c\u6545\u5fc5\u987b\u8981</p> \\[ e \\leq 5\\% \\min{P} = 0.05\\times 1.0 = 0.05MPa \\] <p>\u6240\u4ee5\u4eea\u8868\u6ee1\u91cf\u7a0b\u57fa\u672c\u8bef\u5dee</p> \\[ e' \\leq \\frac{e}{A} = 2\\% \\] <p>\u6545\u9009\u62e9\u51c6\u786e\u5ea6\\(1.5\\)\u3002</p>"},{"location":"courses/cpp/cpp/","title":"C Plus Plus","text":""},{"location":"courses/cpp/cpp/#c","title":"C++","text":""},{"location":"courses/cpp/cpp/#basic-ideas","title":"Basic Ideas","text":"<ul> <li>object-oriented | \u7269\u4ef6\u5bfc\u5411-\u9762\u5411\u5bf9\u8c61</li> </ul> <p>An object, or entity(either visible or invisible), is a variable in programming languages, made up of two primary components:</p> <ul> <li>Attibutes, or Data, representing the object's properties and status</li> <li>Services, or Operations, refered to as functions in programming.</li> </ul> <p>C++ Focuses on things instead of operations.</p>"},{"location":"courses/cpp/cpp/#key-words","title":"Key words","text":"<ul> <li>interface</li> <li>communications</li> <li>protection</li> <li>the hidden implementation</li> <li>encapsulation</li> </ul> First Program of C++<pre><code># include &lt;iostream&gt;\nusing namespace std;\nint main(){\n  cout&lt;&lt;\u201cHello,World!I am\u201d\n}\n</code></pre>"},{"location":"courses/cpp/cpp/#oop-characteristics","title":"OOP characteristics","text":"<ul> <li>Everything is an object.</li> <li>A program is a bunch of objects telling each other what to do/(not how to do) by sending messages</li> <li>Each object has its own memory made up of other objects.</li> <li>Every object has a type.</li> <li>All objects of a particular type can receive the same messages. (Using the method to distinguish between different types or classes)</li> </ul>"},{"location":"courses/cpp/cpp/#header-files","title":"Header Files","text":"<p>To prevent defining repeatedly:</p> <p>x.h<pre><code># pragma once //\u53ea\u5305\u542b\u8fd9\u4e2a\u5934\u6587\u4ef6\u4e00\u6b21\n</code></pre> or the same as C language:</p> log.h<pre><code># ifndef _LOG_H\n# define _LOG_H\n//...//\n# endif\n</code></pre>"},{"location":"courses/cpp/cpp/#declaration","title":"declaration","text":"<ul> <li>external variables</li> <li>function prototypes</li> <li>class/struct declarations</li> </ul>"},{"location":"courses/cpp/cpp/#resolver","title":":: resolver","text":"::<pre><code>&lt;Class Name&gt;::&lt;function name&gt;//not free\n::&lt;function name&gt;\n\nvoid S::f(){\n  ::f();//would be recursive otherwise\n  ::a++;//select the global a\n  a\u2014;//select the partial a\n}\n</code></pre>"},{"location":"courses/cpp/cpp/#string","title":"String","text":"C++<pre><code>#include&lt;string&gt;\nString age, name; \ncin &gt;&gt; age &gt;&gt; name;\ncout &lt;&lt; name; \n</code></pre> <ul> <li>A class type, not a primitive type.</li> <li>Initially <code>name</code> is all zero. No matter it is static or global.</li> <li>No <code>\\0</code> at the end of the string.</li> </ul>"},{"location":"courses/cpp/cpp/#reference","title":"Reference","text":"<p>Reference make use of  the idead of a pointer, but is used as a normal variable rather than pointer. It is widely used in passing variables into a function.</p> C++<pre><code>char c;\nchar &amp; r = c;// a reference to c;\n</code></pre> <p>Some notes on reference:</p> <ul> <li>cannot be <code>NULL</code>.</li> <li>cannot calculate.</li> <li>No reference to reference</li> </ul> C++<pre><code>int &amp;* r; // No pointer to reference \nint *&amp; r; // We have reference to pointer\n</code></pre>"},{"location":"courses/cpp/cpp/#classstruct","title":"Class(Struct)","text":"<p>Intuitively, we put a function into a <code>struct</code> and it bacome <code>class</code>.(we can use functional point in C) act like a type.</p> <ul> <li>Definition We all know the principle of designing:</li> </ul> <p>separated .h &amp;.cpp are used to define one class</p> <ul> <li> <p>Header file(.h): class declaration&amp;prototype</p> </li> <li> <p>Source file(.cpp): all the bodies of functions</p> </li> </ul> <p>Hidden parameter: <code>this</code>, which is a pointer to the variable.</p> Publicprivate C++<pre><code>struct point{\n  float x;\n  float y;\n  void init(int x, int y){\n    this-&gt;x=x;\n    this-&gt;y=y;\n  }\n  void print(){\n    cout &lt;&lt; x &lt;&lt; \", \" &lt;&lt; y &lt;&lt;endl;\n  }\n}\n</code></pre> C++<pre><code>class point{\nprivate:\n  float x;\n  float y; //the above data is protected.\n\npublic://the followings can be accessed from outside\n  void init(int x, int y){\n    this-&gt;x=x;\n    this-&gt;y=y;\n  }\n  void print(){\n    cout &lt;&lt; x &lt;&lt; \", \" &lt;&lt; y &lt;&lt;endl;\n  }\n}\n</code></pre>"},{"location":"courses/cpp/cpp/#object-an-instance-of-class","title":"Object | an instance of class","text":""},{"location":"courses/cpp/cpp/#ctor-constructor","title":"C\u2019tor (constructor)","text":"<ul> <li>Initialization List</li> </ul> <p>C++<pre><code>class A{\n  private:\n    int i;\n    int j= i;\n\n  public:\n    A():i(11){}\n}\n</code></pre> Initialization versus Assignment</p> <p>C++<pre><code>Stu::Stu(string s):name(s){} // better to use\nStu::Stu(string s){name=s;}\n</code></pre> Equivalent:</p> C++<pre><code>string place(\u201cHangzhou\u201d);\nstring place = \u201cHangzhou\u201d;\n\nint i = 6;\nint i(6); \n</code></pre> <p>A constructor function:</p> <p>C++<pre><code>point::point(int x, int y){\n  this-&gt;x=x;\n  this-&gt;y=y;\n}\n</code></pre> then use it:</p> <p>C++<pre><code>point a(1,2);\n</code></pre> And if the constructor function only takes in one parameter, like </p> <p>C++<pre><code>point::point(int dep){\n  this-&gt;x=this-&gt;y=dep;\n}\n</code></pre> then we can use it to initialize:</p> <p>C++<pre><code>point a(1);\npoint a=10;\n</code></pre> we can not initialize a point like we do in struct:</p> C++<pre><code>point a ={1,2}; // this can succeed only when the class does not have a constructor function and the parameter is public.\n</code></pre> <ul> <li>Default Constructor</li> </ul> <p>It is a function that can be called with no arguments input.</p> <p>If we don\u2019t give any constructor function, then the system can give one that does nothing.</p> <p>If we offer a constructor function that takes input of more than one parameter, we have no default constructor function.</p> <p>C++<pre><code>struct Y{\n  float y;\n  int i;\n  Y(int a);\n}\n</code></pre> then:</p> C++<pre><code>Y y1[] = {Y(1), Y(2)}; // right\nY y2[2] {Y(1)}; // wrong\nY y3[7]; //wrong\nY y4;  //wrong\n</code></pre>"},{"location":"courses/cpp/cpp/#dtordestructor","title":"D\u2019tor(destructor)","text":"<p>This function would be implemented before the memory is recycled.</p> <p>To design it, please add tilde <code>~</code> before the name of the class. The function have no input and output.</p> <p>It will delete local objects in a reverse manner.(caused by stack action)</p> C++<pre><code>class point{\nprivate:\n  float x;\n  float y;\n//the above data is protected.\n\npublic://can be accessed from outside\n  point(int dep); // reload\n  point(int x, int y);\n  ~point();\n  void print();\n}\n</code></pre>"},{"location":"courses/cpp/cpp/#stlstandard-template-library","title":"STL(standard template library)","text":"<p>All the following identifiers in library are in <code>std</code> namespace.</p> <p>This is also called Container(lowercase)</p>"},{"location":"courses/cpp/cpp/#sequential","title":"Sequential","text":"<ul> <li>vector(variable array)</li> </ul> <p>It is easy to use index to search and save memory</p> C++<pre><code>vector&lt;typeName&gt; vt(n_ele);\nvector&lt;int&gt; x;\nx.push_back(1);\n</code></pre> <p>reload p++ </p> <p>C++<pre><code>vector&lt;int&gt;::iterator p;\nfor(p=x.begin(); p&lt;x.end(); p++)\n  cout &lt;&lt; *p &lt;&lt; \u201c \u201c;\n</code></pre> or  C++<pre><code>for(auto k: x)\ncout &lt;&lt; k &lt;&lt; \u201c \u201c;\n</code></pre></p> <p><code>auto</code> means the compiler can identify the type of the variable itself.</p> <p>for \u4ec5\u7528\u4e8e\u904d\u5386\u5168\u90e8(range-based for loops)</p> <p>\u653e\u8fdb\u5bb9\u5668\u91cc\u7684\u5185\u5bb9\u662fclone</p> <p>\u76f4\u63a5\u4f7f\u7528\u4e0b\u6807\uff0c\u4e0d\u4f7f\u7528<code>push_back</code>/<code>pop</code> \u662f\u4e0d\u4f1a\u6539\u53d8size\u7684\u3002</p> C++<pre><code>x[999] = 9; // no error but invalid\n</code></pre> <ul> <li> <p>array</p> </li> <li> <p>fixed length, use stack.</p> </li> </ul> <p>C++<pre><code>array&lt;typeName, n_ele&gt; arr;\n</code></pre> <code>n_ele</code> should be constant</p> <ul> <li>list(double-linked-list) It can insert/delete items very quickly.</li> </ul> <p>C++<pre><code>list&lt;int&gt; L;\nlist&lt;int&gt;::iterator li;\nli = L.begin();\nL.erase();\n++li; //wrong! li has been removed.\n</code></pre> right:</p> C++<pre><code>li = L.erase(li)\nli now points to nest node\n</code></pre> <ul> <li>others</li> </ul> <p>Deque(double ended queue), forward_list, maps(HashMap)</p> C++<pre><code>map&lt;string, float&gt; price;\nprice[\u201csnapple\u201d] =0.75\n</code></pre>"},{"location":"courses/cpp/cpp/#using-class","title":"Using Class","text":""},{"location":"courses/cpp/cpp/#function-overloading","title":"Function overloading","text":"<p>For function with the same name, compiler will choose from different function according to different input/labels.</p> <p>Pay attention to primitive type input</p> C++<pre><code>void f(int i){}\nvoid f(double d){}\n\nint main() {\n  f(\u2018a\u2019); //despite smaller than int, it can be transformed\n  f(2); // ambiguous\n  f(2L); // ambiguous\n  f(3.2); // ok\n}\n</code></pre>"},{"location":"courses/cpp/cpp/#default-argument","title":"Default argument","text":"<p>we should give default argument from right to left:</p> C++<pre><code>int harpo(int n, int m, int j=5);\nint chico(int n, int m=6, int j); //illegal\n</code></pre> <p>default argument must be written in declaration, i.e. in \".h\" file. We can not write it in definition, but always in calling back.</p>"},{"location":"courses/cpp/cpp/#friend","title":"Friend","text":"<p>A declaration, which cannot append.</p> <p>Two classes can be a friend relationship when they cannot be \"is-a\"(public inheritance) or \"has-a\"(embedding), like Tv and remote.</p> C++<pre><code>class Tv\n{\n  friend class Remote; // a class Remote can access all the private member functions/variables of Tv.\n}\n</code></pre> <p>In fact, a class can use member functions of another class to achieve some usage, not by accessing directly its member variables. Only some functions must access these member variables.</p> C++<pre><code>class Tv\n{\n  friend void Remote::set_chan(Tv &amp;t, int c); // here only this function can affect private member variables of Tv.\n  // compiler need to first know Remote class.\n};\n</code></pre> <p>So </p> C++<pre><code>// this is right:\nclass Tv; //forward declaration\nclass Remote{...}; // Remote::set_chan declaration\nclass Tv{...};\n\n// this is wrong\nclass Remote; // forward declaration\nclass Tv{...}; // Tv declaration in which we have set_chan which has not been declared.\nclass Remote{...};\n</code></pre> <p>But if <code>Remote</code> has inline function which calls a function of <code>Tv</code>, the above right forward declaration does not work. So the solution becomes </p> C++<pre><code>class Tv;\nclass Remote{...}; // Tv0using methods declaration without definition.\nclass Tv{...};\n</code></pre>"},{"location":"courses/cpp/cpp/#inline-function","title":"inline function | \u5185\u8054","text":"<p>It can check the type, which is better than Macro(\u5b8f)!</p> <p>C++<pre><code>inline double square(double x);\ninline double square(double x){ return x*x;}\n</code></pre> is only a declaration instead of a definition, so the compiler must see the body of function!</p> <ul> <li>compiler must see body so it can compile!(not just write down as a declaration)</li> </ul> <p>Body of inline function must be put in \".h\" files so it can be used in another file!</p> <ul> <li>if you write a inline function in a \".cpp\" file, you mean the function should only be used locally.</li> </ul> <p>Function that can be used <code>inline</code>:</p> <ul> <li>small function</li> <li>frequently called function</li> </ul> <p>others that cannot be used <code>inline</code>:</p> <ul> <li>long function</li> <li>recursive function</li> </ul>"},{"location":"courses/cpp/cpp/#const","title":"Const","text":"<p>constants are variables -    (instant \u7acb\u5373\u6570)</p> C++<pre><code>const int a = 6; \n</code></pre> <ul> <li>literal -&gt; 6</li> <li>constant -&gt; a</li> </ul> <p>Distinguish:</p> C++<pre><code>String p1(\u201cFred\u201d);\nconst string * p = &amp;p1; //(1)\nstring const * p = &amp;p1; //(2)\nstring * const p = &amp;p1; //(3)\n</code></pre> HintsAnswer <p>Const only restrict one variable.</p> <p>(1)(2) are the same: <code>(*p)</code> can not change; That is, we cannot change <code>p1</code> through <code>(*p)</code>.</p> <p>(3) means the pointer <code>p</code> itself cannot change but p1 itself can still change.</p> C++<pre><code>int i; \nconst int ci = 3;\n\nint *ip = &amp;i; \nint *ip = &amp;ci; // illegal, that is, a changeable pointer now points a non-changeable variable, which is illegal in compiler.\n\nconst int * cip = &amp;i;\nconst int * cip = &amp;ci;\n</code></pre> <ul> <li>Passing &amp; returning by const value</li> </ul> <p>We do this in case that we change some value in a function. So we let compiler check.</p> <ul> <li>Const object </li> </ul> <p>in a function, we pass a pointer instead of a copy!</p> <p>(1) public</p> <p>(2) change value by inner function</p> C++<pre><code>int get_day(void) const;\nint get_day(void) const {return day;}\n\nconst A a; // must provide a an initial value!(or constructor) because later we cannot change it! Like below:\n\nconst int i=1; // we cannot do this before C11\n\npublic:\n  A(int k): i(k){}\n</code></pre> <ul> <li>constant i cannot change during execution, but need a value in initialization.</li> </ul> <p>C++<pre><code>class A {\nprivate:\n  int i=0;\npublic:\n  void f() {i=10;\n    cout &lt;&lt; \u201cA::f()\u201d&lt;&lt; end;\n  }\n  void f() const {\n    cout &lt;&lt; \u201cA::f() const\u201d&lt;&lt; end;\n  }\n}\n</code></pre> The above means:</p> C++<pre><code>public:\n  void f(A *this) {i=10;\n    cout &lt;&lt; \u201cA::f()\u201d&lt;&lt; end;\n  }\n  void f(const A *this) const {\n    cout &lt;&lt; \u201cA::f() const\u201d&lt;&lt; end;\n  }\n</code></pre> <p>So:</p> <p>C++<pre><code>const A a;\nA b;\na.f(); // &lt;&lt;\u201cA::f() const\u201d\nb.f(); // &lt;&lt;\u201cA::f()\u201d\n</code></pre> The above code is using overload, and there are two different f() that have been distinguished by \u201cconst\u201d.</p> <p>So a const object can only use function attached to \"const\" and cannot use function with no \"const\".</p>"},{"location":"courses/cpp/cpp/#static","title":"Static","text":"<p>on members which are </p> <ul> <li>Hidden</li> <li>Persistent</li> </ul> <p>static variable is actually global variable.</p> <p>static function can only access static variable!</p> <p>C++<pre><code>static int i; // can be accessed by all the objects of same class\n</code></pre> we must define the static variable (global variable) before main!</p> <p>C++<pre><code>int A::i;\n</code></pre> Note: without static!</p> <p>C++<pre><code>static void sf(){\n  i++;\n}\n</code></pre> we can call static function without creating an object! Just use class!</p> C++<pre><code>int main(){\n  A::sf();\n}\n</code></pre> <p>We can eliminate global variable because we can limit it in class, which can prevent arbitrary changes!</p>"},{"location":"courses/cpp/cpp/#overloaded-operator","title":"Overloaded Operator","text":"<p>operators of primitive class cannot be changed. C++<pre><code>Integer x(1), y(2);\nz = x + y // x.operator+(y)\nz = x + 3 // x.operator+(Interger(3))\nz = 3 + x // not allowed\n</code></pre></p> <p>Global operator. C++<pre><code>z = 3 + 7; // pass 10 to initialize z\n</code></pre></p> <p><code>= () [] -&gt; -&gt;*</code> must be members and all other binary operators(\u53cc\u76ee) should be non-members.</p> <p>Will the operator change the operation number\uff1f</p> <ul> <li>If not, use <code>const</code>.</li> </ul> Common Operators<pre><code>// + - * / % ^ &amp; | ~\nconst T operatorX(const T&amp;I, const T&amp;L)\n\n// ! &amp;&amp; || &lt; &lt;= == &gt;= &gt;\nbool operatorX(const T&amp;I, const T&amp;L)\n\n// []\nE&amp; T::operator[](int index);\n\n// prefix ++ -- e.g. ++a\nconst Integer&amp; Integer::operator++(){\n  *this +=1;\n  return *this; // reference for old one, without copying new\n}\n\n// postfix ++ -- e.g. a++\nconst Integer&amp; Integer::operator++(int){\n  Integer old(*this);\n  ++(*this);\n  return old;\n}\n\n// Relational operators\nbool Integer::operator==(const Integer &amp; rhs) const{\n  return i == rhs.i;\n}\n\nbool Integer::operator!=(const Integer &amp; rhs) const{\n  return !(*this == rhs);\n}\n\nbool Integer::operator&lt;(const Integer &amp; rhs) const{\n  return !(i &lt; rhs.i);\n}\n\nbool Integer::operator&gt;(const Integer &amp; rhs) const{\n  return rhs &lt; *this;\n}\n\nbool Integer::operator&lt;=(const Integer &amp; rhs) const{\n  return !(rhs &lt; *this);\n}\n\nbool Integer::operator&gt;=(const Integer &amp; rhs) const{\n  return !(*this &lt; rhs);\n}\n\n// operator [] must be member function, single argument\n</code></pre> <ul> <li>Extractor &amp; Inserter</li> </ul> Extractor &amp; Inserter<pre><code>// stream extractor cin &gt;&gt;: global function\noperator&gt;&gt;(istream &amp;is, T&amp; obj){\n  // ...\n  return is; // always this\n}\n\ncin &gt;&gt; a &gt;&gt; b // ((cin &gt;&gt; a ) &gt;&gt; b)\n\n// stream inserter cout &lt;&lt;: global function\noperator&lt;&lt;(ostream&amp; os, const T&amp; obj){\n  // ...\n  return os;\n}\n</code></pre> <ul> <li>Assignment <code>=</code></li> </ul> =<pre><code>// member function\n// usually before calling \"=\" the object being assigned already has had sth. \nT &amp; T::operator=(con T &amp; rhs){\n  // check for self assignment\n  if(*this != ths) // otherwise will cause error (you will delete allocated memory and new one according to the deleted memory)\n  {\n    // ...\n  }\n  return *this;\n}\n</code></pre>"},{"location":"courses/cpp/cpp/#inheritance","title":"Inheritance","text":"<p>Allow sharing of design for</p> <ul> <li>Member data</li> <li>Member functions</li> <li>Interfaces</li> </ul> <p>Advantages:</p> <ul> <li>extendable</li> <li>avoid code duplication</li> <li>code reuse </li> </ul> <p>B is a A:</p> <ul> <li>A: Base/super/parent class</li> <li>B: derived/sub/child class</li> </ul> employee.h<pre><code>class Employee\n{\npublic: \n  Employee(const string&amp; _name, const string &amp; _ssn): name(_name), ssn(_ssn){}\n\n  void print() const\n{\n  cout &lt;&lt; name &lt;&lt; endl;\n  cout &lt;&lt; ssn &lt;&lt; endl;\n}\n\n  void print(const string &amp; msg) const {\n  cout &lt;&lt; msg &lt;&lt; endl;\n  print(); // we rewrite the print function, then the child cannot access print from parent!\n// Name Hide!\n}\n\nprotected:\n// private: // can not access from child class\n  const string name;\n  const string ssn;\n}\n</code></pre> manager.h<pre><code>class Manager: public Employee\n{\npublic:\n  Manager(const string &amp; _name, const string &amp; _ssn, const string &amp; _title): Employee(_name, _ssn), title(_title) {}\n\n  const string &amp; getTitle() const\n{\n  return title;\n}\n\n  void print() const\n{\n  Employee::print();\n  cout &lt;&lt; title &lt;&lt; end;\n}\n\nprotected:\n  const string title;\n}\n</code></pre>"},{"location":"courses/cpp/cpp/#polymorphism","title":"Polymorphism","text":""},{"location":"courses/cpp/cpp/#up-casting","title":"up-casting | \u9020\u578b","text":"<ul> <li>cast: \u7c7b\u578b\u8f6c\u6362</li> </ul> C++<pre><code>int i = (int)3.62;\n</code></pre> <p>But up-casting is</p> <ul> <li>is the act of converting from a derived reference or pointer to a base class reference or pointer.</li> <li>take an object of a derived class as an object of the base one.</li> </ul> <p>(\u6539\u53d8\u4e86\u773c\u5149\uff0c\u4e0d\u6539\u53d8\u5185\u5bb9)</p> <p>encapsulation \u5c01\u88c5\uff1b\u5305\u88c5\uff1b capsulation \u5c01\u88c5\uff1b[\u9ad8\u5206\u5b50] \u5305\u56ca\u5316\u4f5c\u7528\uff1b bonding \u90a6\u5b9a</p>"},{"location":"courses/cpp/cpp/#binding","title":"Binding | \u7ed1\u5b9a","text":"<p>binding: which function to be called</p> <ul> <li>Static binding: call the function as the code(quick) - Non-virtual func</li> <li>Dynamic binding: call the function of the object(slow) - Virtual func</li> </ul> C++<pre><code>Manager Pete(\u201cPete\u201d,\u201d4\u201d, \u201cBakery\u201d);\nEmployee* ep = &amp;pete;\nEmployee &amp; er = pete;\n</code></pre>"},{"location":"courses/cpp/cpp/#override","title":"override | \u8986\u76d6","text":"C++<pre><code>class A\n{\nprotected:\n  int i;\npublic:\n  A() {i=10;}\n  virtual void f() {cout&lt;&lt; \u201cA::f()\u201d&lt;&lt; endl;}\n}\n\nclass B: public A\n{\npublic:\n  int i;\npublic: \n  B(){ i=20; cout&lt;&lt; \u201cB::i\u201d &lt;&lt; i &lt;&lt;endl;}\n  void f() {cout&lt;&lt; \u201cB::f()\u201d&lt;&lt; endl;}\n}\n\ncout &lt;&lt; sizeof(A) &lt;&lt;\u201c, \u201c &lt;&lt; sizeof(B) &lt;&lt; endl;\n// not just 4, 8 but a complex one!\n</code></pre> <ul> <li>polymorphic variable(* &amp;)</li> </ul> <p>static type  dynamic type</p>"},{"location":"courses/cpp/cpp/#virtual-function","title":"virtual function","text":"<p>a class which has a virtual function: vtable -&gt; table of the address of virtual functions(8 \u4f4dfor 64) (this is formed while compiling)</p> C++<pre><code>B b;\nA * p = &amp;b;\nlong long **vp (long long **)p;\n// vp is a pointer to *long long, that is, to a pointer of type long long.\n\nvoid (*pf)() = (void (*)())(**vp);\n// pf is a function pointer, which matches the definition of f()\n\npf(); // \u201cB::f()\u201d\n</code></pre> <p>Initialize: A will create A\u2019s vtable but B will than create B\u2019s vtable and change the point to the vtable.</p>"},{"location":"courses/cpp/cpp/#slice-off","title":"Slice off","text":"<ul> <li>will copy the content of child object to the parent object while ignoring the extra thins of the child.</li> </ul> <p>C++<pre><code>a = b;\np = &amp;a;\np-&gt;f(); // still execute A\u2019s func\n</code></pre> -   Never redefine an inherited non-virtual function. -   Never redefine an inherited default parameter value.</p> <p>Abstract base classes:</p> <ul> <li>has pure virtual functions</li> <li>Cannot be instantiated</li> </ul> <p>C++<pre><code>virtual void f() =0;\n</code></pre> - Multiple inheritance Day No</p>"},{"location":"courses/cpp/cpp/#dynamic-memory-allocation","title":"Dynamic memory allocation","text":""},{"location":"courses/cpp/cpp/#new-delete","title":"<code>new</code> &amp; <code>delete</code>","text":"<p><code>new</code> (operator) has 2 steps:</p> <ul> <li>allocate space</li> <li>call the constructor function</li> </ul> C++<pre><code>int *p1 = new int // malloc(sizeof(int)); + constructor\nint *p2 = new int [10] // continuous space allocated\n</code></pre> <p><code>delete</code> + pointer</p> <p>If delete a constructed type, it will implement <code>D\u2019tor</code> function</p> <p>C++<pre><code>p2 = new student [10];\ndelete p2; // remove the first one\ndelete[] p2; // remove whole 10 objects\n</code></pre> it is safe to delete a <code>NULL</code>.</p>"},{"location":"courses/cpp/cpp/#copy-constructor","title":"Copy constructor","text":"<p>has a unique signature</p> <p>C++<pre><code>T::T(const T&amp;)\n</code></pre> call by reference</p> <p>compiler (in-line) would do it automatically.</p> <p>But how?</p> <ul> <li>member-wise (versus bit-wise)</li> </ul> <p>if it has a class defined, it will iteratively call its copy function.</p> <p>It is neccesary to define a copy constructor when you have pointer member or what you don't want to be copied in your class.</p> C++<pre><code>A b(a);\nA c = a;\n\n// define function passing in value\nvoid f(A aa)\n{\n\n}\n\n// return value\nA f()\n{\n  A aa(19);\n  return aa;\n}\n</code></pre> <p>Assignment: can be done alot of time. Ctor can only be called once.</p>"},{"location":"courses/cpp/cpp/#template","title":"Template(\u5143\u4ee3\u7801)","text":"<p>Reuse source code. It generates code for compiling.</p> <ul> <li>Generic programming(\u6cdb\u578b, universal type)vs \u8303\u5f62 model shape. Use type as parameters in class or function definitions.</li> </ul> <p>Function Template | \u51fd\u6570\u6a21\u677f</p> C++<pre><code>void myswap(int &amp;x, int &amp;y) // only calls when passing in \"int\"\n{\n  int temp = x;\n  x = y;\n  y = temp;\n}\n\n// T is tyep parameter class\ntemplate &lt;class T&gt;\nvoid myswap(T &amp;x, T&amp;y)\n{\n  T temp = x;\n  x = y;\n  y = temp;\n}\n\nint main()\n{\n  int a=6,a=5;\n  double c=1,d=2;\n  myswap(a.b); // right\n  myswap(a,c); // error, parameters a,c must be the same type\n}\n\n// but if we do this\ntemplate &lt;class T1,class T2&gt;\nvoid myswap(T1 &amp;x, T2&amp;y)\n{\n  T1 temp = x;\n  x = y;\n  y = temp;\n}\n</code></pre> <p>compiler need to know the type <code>T</code> if we do not explicitly give varible of type.</p> C++<pre><code>template&lt;class T&gt;\nvoid f()\n{\n  T a;\n}\n\nint main()\n{\n  f&lt;double&gt;();\n}\n</code></pre> <p>Class Template | \u7c7b\u6a21\u677f</p> <p>A class declaration.</p> <p>All the functions in the template are function template.</p> C++<pre><code>template &lt;class T&gt;\nclass vector{\n  public:\n  vector(int s)size(s){}\n  T&amp; operator[](int s);\n};\n\n// definition\ntemplate &lt;class T&gt;\nT&amp; vector&lt;T&gt;::operator[](int s){\n  return 0;\n}\n\nTemplate nest:\n\n```c++\nvector&lt; vector&lt;double*&gt; &gt;\n</code></pre> <p>Template arguments can be constant expresstions, Non-type parameters with a default argument.</p> C++<pre><code>template&lt;class T, int bounds=100&gt;\nclass FixedVector{\npublic:\n  FixedVector();\n  T &amp; operator[](int);\nprivate:\n  T elements[bounds]; \n};\n\ntemplate&lt;class T, int bounds=100&gt;\nT &amp; FixedVector&lt;T, bounds&gt;::operator[](int i){\n  return elements[i];\n}\n\nint main()\n{\n  FixedVector&lt;int, 10&gt; v1;\n  FixedVector&lt;int&gt; v2; // use default parameter 100\n\n}\n</code></pre> <p>All the content of template should be put in <code>.h</code> file for they are only declarations. Remember we also have to put inline function and static member variables in <code>.h</code> file.</p>"},{"location":"courses/cpp/cpp/#exception","title":"Exception","text":"<ul> <li>Exception type</li> </ul> C++<pre><code>class VectorIndexError\n{\npublic:\n  VectorIndexError(int v):m_badValue(v){}\n  VectorIndexError({}\n  void diagnostic(){\n    cout&lt;&lt;\"index\"&lt;&lt; m_badValue &lt;&lt;\"out of range!\";\n  }\nprivate:\n  int m_badValue;\n}\n</code></pre> <p>C++<pre><code>throw &lt;&lt;something&gt;&gt;\n</code></pre> <code>throw</code> raises exception.</p> <ul> <li>Try blocks can select type of exceptions</li> </ul> C++<pre><code>try {...}\n  catch {...}\n</code></pre> C++<pre><code>try{\n  func();\n} catch(VIE v){ // take a single argument\n  cout&lt;&lt; \"8\\n\";\n} catch (...){ // others\n  cout&lt;&lt; \"7\\n\";\n}\n</code></pre> <p>it can re-raise exceptions</p> <ul> <li>Hierarchy of exception types</li> </ul> C++<pre><code>class MathErr{\n  ...\n  virtual void diagnostic();\n};\n\nclass OverflowErr : public MathErr {...}\nclass UnderflowErr : public MathErr {...}\n</code></pre> <p>a catch of parent Exception Type can catch its child type.</p> C++<pre><code>try{\n  throw VIEE(idx);\n}catch (VIE v){\n  cout&lt;&lt; \"7\\n\";\n}catch (...){\n  cout&lt;&lt; \"6\\n\";\n}\n// output 7\n\ntry{\n  throw VIEE(idx);\n}catch (VIEE v){\n  cout&lt;&lt; \"8\\n\";\n}catch (VIE v){\n  cout&lt;&lt; \"7\\n\";\n}catch (...){\n  cout&lt;&lt; \"6\\n\";\n}\n// output 8\n\ntry{\n  throw VIEE(idx);\n}catch (VIE v){\n  cout&lt;&lt; \"8\\n\";\n}catch (VIEE v){ // this expression is useless\n  cout&lt;&lt; \"7\\n\";\n}catch (...){\n  cout&lt;&lt; \"6\\n\";\n}\n// output 8\n</code></pre> <ul> <li> <p>Standard Library Exception</p> </li> <li> <p>Failure in C'tor</p> </li> </ul>"},{"location":"tag/","title":"Tag","text":"<p>Hey!</p>"}]}